<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Code Cloning Analysis: scipy.stats._entropy Namespace Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../clipboard.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Code Cloning Analysis
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('db/d43/namespacescipy_1_1stats_1_1__entropy.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">scipy.stats._entropy Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a380fa277399a04d09cc26299819e3aa3" id="r_a380fa277399a04d09cc26299819e3aa3"><td class="memItemLeft" align="right" valign="top">np.number|np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a380fa277399a04d09cc26299819e3aa3">entropy</a> (np.typing.ArrayLike pk, np.typing.ArrayLike|None qk=None, float|None base=None, int axis=0)</td></tr>
<tr class="separator:a380fa277399a04d09cc26299819e3aa3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aed93073865f48caca12491e0d550b6e6" id="r_aed93073865f48caca12491e0d550b6e6"><td class="memItemLeft" align="right" valign="top">np.number|np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aed93073865f48caca12491e0d550b6e6">differential_entropy</a> (np.typing.ArrayLike values, *, int|None window_length=None, float|None base=None, int axis=0, str method=&quot;auto&quot;)</td></tr>
<tr class="separator:aed93073865f48caca12491e0d550b6e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8da8253d95c4150183c20f4d9f9d95ea" id="r_a8da8253d95c4150183c20f4d9f9d95ea"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8da8253d95c4150183c20f4d9f9d95ea">_pad_along_last_axis</a> (X, m)</td></tr>
<tr class="separator:a8da8253d95c4150183c20f4d9f9d95ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a01b018776843abfb9261305cd30c68" id="r_a2a01b018776843abfb9261305cd30c68"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2a01b018776843abfb9261305cd30c68">_vasicek_entropy</a> (X, m)</td></tr>
<tr class="separator:a2a01b018776843abfb9261305cd30c68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad777cd154d4b45a745d1ccd812dd3af2" id="r_ad777cd154d4b45a745d1ccd812dd3af2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ad777cd154d4b45a745d1ccd812dd3af2">_van_es_entropy</a> (X, m)</td></tr>
<tr class="separator:ad777cd154d4b45a745d1ccd812dd3af2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac9b0d2377dc302465e2da02678b76846" id="r_ac9b0d2377dc302465e2da02678b76846"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac9b0d2377dc302465e2da02678b76846">_ebrahimi_entropy</a> (X, m)</td></tr>
<tr class="separator:ac9b0d2377dc302465e2da02678b76846"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2d17de2a6387d3fa0b41294c50f91be" id="r_ae2d17de2a6387d3fa0b41294c50f91be"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ae2d17de2a6387d3fa0b41294c50f91be">_correa_entropy</a> (X, m)</td></tr>
<tr class="separator:ae2d17de2a6387d3fa0b41294c50f91be"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a5e66c878132652ec57633b93a8fc0903" id="r_a5e66c878132652ec57633b93a8fc0903"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a5e66c878132652ec57633b93a8fc0903">__all__</a> = ['<a class="el" href="#a380fa277399a04d09cc26299819e3aa3">entropy</a>', '<a class="el" href="#aed93073865f48caca12491e0d550b6e6">differential_entropy</a>']</td></tr>
<tr class="separator:a5e66c878132652ec57633b93a8fc0903"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Created on Fri Apr  2 09:06:05 2021

@author: matth
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="ae2d17de2a6387d3fa0b41294c50f91be" name="ae2d17de2a6387d3fa0b41294c50f91be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2d17de2a6387d3fa0b41294c50f91be">&#9670;&#160;</a></span>_correa_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._correa_entropy </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>m</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel protected">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Correa estimator as described in [6].</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00382">382</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="ac9b0d2377dc302465e2da02678b76846" name="ac9b0d2377dc302465e2da02678b76846"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac9b0d2377dc302465e2da02678b76846">&#9670;&#160;</a></span>_ebrahimi_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._ebrahimi_entropy </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>m</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel protected">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Ebrahimi estimator as described in [6].</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00365">365</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="a8da8253d95c4150183c20f4d9f9d95ea" name="a8da8253d95c4150183c20f4d9f9d95ea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8da8253d95c4150183c20f4d9f9d95ea">&#9670;&#160;</a></span>_pad_along_last_axis()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._pad_along_last_axis </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>m</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel protected">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Pad the data for computing the rolling window difference.</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00335">335</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="ad777cd154d4b45a745d1ccd812dd3af2" name="ad777cd154d4b45a745d1ccd812dd3af2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad777cd154d4b45a745d1ccd812dd3af2">&#9670;&#160;</a></span>_van_es_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._van_es_entropy </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>m</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel protected">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the van Es estimator as described in [6].</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00354">354</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="a2a01b018776843abfb9261305cd30c68" name="a2a01b018776843abfb9261305cd30c68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a01b018776843abfb9261305cd30c68">&#9670;&#160;</a></span>_vasicek_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._vasicek_entropy </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>m</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel protected">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Vasicek estimator as described in [6] Eq. 1.3.</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00345">345</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="aed93073865f48caca12491e0d550b6e6" name="aed93073865f48caca12491e0d550b6e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed93073865f48caca12491e0d550b6e6">&#9670;&#160;</a></span>differential_entropy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.number | np.ndarray scipy.stats._entropy.differential_entropy </td>
          <td>(</td>
          <td class="paramtype">np.typing.ArrayLike</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*</td>          <td class="paramname"><span class="paramname"><em></em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int | None </td>          <td class="paramname"><span class="paramname"><em>window_length</em></span><span class="paramdefsep"> = </span><span class="paramdefval">None</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | None </td>          <td class="paramname"><span class="paramname"><em>base</em></span><span class="paramdefsep"> = </span><span class="paramdefval">None</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int </td>          <td class="paramname"><span class="paramname"><em>axis</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str </td>          <td class="paramname"><span class="paramname"><em>method</em></span><span class="paramdefsep"> = </span><span class="paramdefval">&quot;auto&quot;</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Given a sample of a distribution, estimate the differential entropy.

Several estimation methods are available using the `method` parameter. By
default, a method is selected based the size of the sample.

Parameters
----------
values : sequence
    Sample from a continuous distribution.
window_length : int, optional
    Window length for computing Vasicek estimate. Must be an integer
    between 1 and half of the sample size. If ``None`` (the default), it
    uses the heuristic value

    .. math::
        \left \lfloor \sqrt{n} + 0.5 \right \rfloor

    where :math:`n` is the sample size. This heuristic was originally
    proposed in [2]_ and has become common in the literature.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the differential entropy is calculated.
    Default is 0.
method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional
    The method used to estimate the differential entropy from the sample.
    Default is ``'auto'``.  See Notes for more information.

Returns
-------
entropy : float
    The calculated differential entropy.

Notes
-----
This function will converge to the true differential entropy in the limit

.. math::
    n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0

The optimal choice of ``window_length`` for a given sample size depends on
the (unknown) distribution. Typically, the smoother the density of the
distribution, the larger the optimal value of ``window_length`` [1]_.

The following options are available for the `method` parameter.

* ``'vasicek'`` uses the estimator presented in [1]_. This is
  one of the first and most influential estimators of differential entropy.
* ``'van es'`` uses the bias-corrected estimator presented in [3]_, which
  is not only consistent but, under some conditions, asymptotically normal.
* ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown
  in simulation to have smaller bias and mean squared error than
  the Vasicek estimator.
* ``'correa'`` uses the estimator presented in [5]_ based on local linear
  regression. In a simulation study, it had consistently smaller mean
  square error than the Vasiceck estimator, but it is more expensive to
  compute.
* ``'auto'`` selects the method automatically (default). Currently,
  this selects ``'van es'`` for very small samples (&lt;10), ``'ebrahimi'``
  for moderate sample sizes (11-1000), and ``'vasicek'`` for larger
  samples, but this behavior is subject to change in future versions.

All estimators are implemented as described in [6]_.

References
----------
.. [1] Vasicek, O. (1976). A test for normality based on sample entropy.
       Journal of the Royal Statistical Society:
       Series B (Methodological), 38(1), 54-59.
.. [2] Crzcgorzewski, P., &amp; Wirczorkowski, R. (1999). Entropy-based
       goodness-of-fit test for exponentiality. Communications in
       Statistics-Theory and Methods, 28(5), 1183-1202.
.. [3] Van Es, B. (1992). Estimating functionals related to a density by a
       class of statistics based on spacings. Scandinavian Journal of
       Statistics, 61-72.
.. [4] Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures
       of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.
.. [5] Correa, J. C. (1995). A new estimator of entropy. Communications
       in Statistics-Theory and Methods, 24(10), 2439-2449.
.. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.
       Annals of Data Science, 2(2), 231-241.
       https://link.springer.com/article/10.1007/s40745-015-0045-9

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from scipy.stats import differential_entropy, norm

Entropy of a standard normal distribution:

&gt;&gt;&gt; rng = np.random.default_rng()
&gt;&gt;&gt; values = rng.standard_normal(100)
&gt;&gt;&gt; differential_entropy(values)
1.3407817436640392

Compare with the true entropy:

&gt;&gt;&gt; float(norm.entropy())
1.4189385332046727

For several sample sizes between 5 and 1000, compare the accuracy of
the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,
compare the root mean squared error (over 1000 trials) between the estimate
and the true differential entropy of the distribution.

&gt;&gt;&gt; from scipy import stats
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; def rmse(res, expected):
...     '''Root mean squared error'''
...     return np.sqrt(np.mean((res - expected)**2))
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; a, b = np.log10(5), np.log10(1000)
&gt;&gt;&gt; ns = np.round(np.logspace(a, b, 10)).astype(int)
&gt;&gt;&gt; reps = 1000  # number of repetitions for each sample size
&gt;&gt;&gt; expected = stats.expon.entropy()
&gt;&gt;&gt;
&gt;&gt;&gt; method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}
&gt;&gt;&gt; for method in method_errors:
...     for n in ns:
...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)
...        res = stats.differential_entropy(rvs, method=method, axis=-1)
...        error = rmse(res, expected)
...        method_errors[method].append(error)
&gt;&gt;&gt;
&gt;&gt;&gt; for method, errors in method_errors.items():
...     plt.loglog(ns, errors, label=method)
&gt;&gt;&gt;
&gt;&gt;&gt; plt.legend()
&gt;&gt;&gt; plt.xlabel('sample size')
&gt;&gt;&gt; plt.ylabel('RMSE (1000 trials)')
&gt;&gt;&gt; plt.title('Entropy Estimator Error (Exponential Distribution)')
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00147">147</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<a id="a380fa277399a04d09cc26299819e3aa3" name="a380fa277399a04d09cc26299819e3aa3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a380fa277399a04d09cc26299819e3aa3">&#9670;&#160;</a></span>entropy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.number | np.ndarray scipy.stats._entropy.entropy </td>
          <td>(</td>
          <td class="paramtype">np.typing.ArrayLike</td>          <td class="paramname"><span class="paramname"><em>pk</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">np.typing.ArrayLike | None </td>          <td class="paramname"><span class="paramname"><em>qk</em></span><span class="paramdefsep"> = </span><span class="paramdefval">None</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | None </td>          <td class="paramname"><span class="paramname"><em>base</em></span><span class="paramdefsep"> = </span><span class="paramdefval">None</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int </td>          <td class="paramname"><span class="paramname"><em>axis</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Calculate the Shannon entropy/relative entropy of given distribution(s).

If only probabilities `pk` are given, the Shannon entropy is calculated as
``H = -sum(pk * log(pk))``.

If `qk` is not None, then compute the relative entropy
``D = sum(pk * log(pk / qk))``. This quantity is also known
as the Kullback-Leibler divergence.

This routine will normalize `pk` and `qk` if they don't sum to 1.

Parameters
----------
pk : array_like
    Defines the (discrete) distribution. Along each axis-slice of ``pk``,
    element ``i`` is the  (possibly unnormalized) probability of event
    ``i``.
qk : array_like, optional
    Sequence against which the relative entropy is computed. Should be in
    the same format as `pk`.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the entropy is calculated. Default is 0.

Returns
-------
S : {float, array_like}
    The calculated entropy.

Notes
-----
Informally, the Shannon entropy quantifies the expected uncertainty
inherent in the possible outcomes of a discrete random variable.
For example,
if messages consisting of sequences of symbols from a set are to be
encoded and transmitted over a noiseless channel, then the Shannon entropy
``H(pk)`` gives a tight lower bound for the average number of units of
information needed per symbol if the symbols occur with frequencies
governed by the discrete distribution `pk` [1]_. The choice of base
determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.

The relative entropy, ``D(pk|qk)``, quantifies the increase in the average
number of units of information needed per symbol if the encoding is
optimized for the probability distribution `qk` instead of the true
distribution `pk`. Informally, the relative entropy quantifies the expected
excess in surprise experienced if one believes the true distribution is
`qk` when it is actually `pk`.

A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the
equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with
the formula ``CE = -sum(pk * log(qk))``. It gives the average
number of units of information needed per symbol if an encoding is
optimized for the probability distribution `qk` when the true distribution
is `pk`. It is not computed directly by `entropy`, but it can be computed
using two calls to the function (see Examples).

See [2]_ for more information.

References
----------
.. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.
       Bell System Technical Journal, 27: 379-423.
       https://doi.org/10.1002/j.1538-7305.1948.tb01338.x
.. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information
       Theory (Wiley Series in Telecommunications and Signal Processing).
       Wiley-Interscience, USA.


Examples
--------
The outcome of a fair coin is the most uncertain:

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from scipy.stats import entropy
&gt;&gt;&gt; base = 2  # work in units of bits
&gt;&gt;&gt; pk = np.array([1/2, 1/2])  # fair coin
&gt;&gt;&gt; H = entropy(pk, base=base)
&gt;&gt;&gt; H
1.0
&gt;&gt;&gt; H == -np.sum(pk * np.log(pk)) / np.log(base)
True

The outcome of a biased coin is less uncertain:

&gt;&gt;&gt; qk = np.array([9/10, 1/10])  # biased coin
&gt;&gt;&gt; entropy(qk, base=base)
0.46899559358928117

The relative entropy between the fair coin and biased coin is calculated
as:

&gt;&gt;&gt; D = entropy(pk, qk, base=base)
&gt;&gt;&gt; D
0.7369655941662062
&gt;&gt;&gt; D == np.sum(pk * np.log(pk/qk)) / np.log(base)
True

The cross entropy can be calculated as the sum of the entropy and
relative entropy`:

&gt;&gt;&gt; CE = entropy(pk, base=base) + entropy(pk, qk, base=base)
&gt;&gt;&gt; CE
1.736965594166206
&gt;&gt;&gt; CE == -np.sum(pk * np.log(qk)) / np.log(base)
True
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00015">15</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a5e66c878132652ec57633b93a8fc0903" name="a5e66c878132652ec57633b93a8fc0903"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5e66c878132652ec57633b93a8fc0903">&#9670;&#160;</a></span>__all__</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">list scipy.stats._entropy.__all__ = ['<a class="el" href="#a380fa277399a04d09cc26299819e3aa3">entropy</a>', '<a class="el" href="#aed93073865f48caca12491e0d550b6e6">differential_entropy</a>']</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel private">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d2/dc6/__entropy_8py_source.html#l00012">12</a> of file <a class="el" href="../../d2/dc6/__entropy_8py_source.html">_entropy.py</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="../../db/d0f/namespacescipy.html">scipy</a></li><li class="navelem"><a class="el" href="../../d0/df6/namespacescipy_1_1stats.html">stats</a></li><li class="navelem"><a class="el" href="../../db/d43/namespacescipy_1_1stats_1_1__entropy.html">_entropy</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
