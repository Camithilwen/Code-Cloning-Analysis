\doxysection{sklearn.\+cluster.\+\_\+kmeans Namespace Reference}
\hypertarget{namespacesklearn_1_1cluster_1_1__kmeans}{}\label{namespacesklearn_1_1cluster_1_1__kmeans}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1cluster_1_1__kmeans_1_1__BaseKMeans}{\+\_\+\+Base\+KMeans}}
\item 
class \mbox{\hyperlink{classsklearn_1_1cluster_1_1__kmeans_1_1KMeans}{KMeans}}
\item 
class \mbox{\hyperlink{classsklearn_1_1cluster_1_1__kmeans_1_1MiniBatchKMeans}{Mini\+Batch\+KMeans}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_aac7e701fb5c1c3aa4bab6b62b9ce2c9a}{kmeans\+\_\+plusplus}} (X, n\+\_\+clusters, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, x\+\_\+squared\+\_\+norms=None, random\+\_\+state=None, n\+\_\+local\+\_\+trials=None)
\begin{DoxyCompactList}\small\item\em Initialization heuristic. \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_a4e172c2f6ed5ad2c482eff9adbd5338c}{\+\_\+kmeans\+\_\+plusplus}} (X, n\+\_\+clusters, x\+\_\+squared\+\_\+norms, sample\+\_\+weight, random\+\_\+state, n\+\_\+local\+\_\+trials=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_a0356aeee2ae6949ea63f88544e6c001c}{\+\_\+tolerance}} (X, tol)
\begin{DoxyCompactList}\small\item\em K-\/means batch estimation by EM (expectation maximization) \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_a5ac8891e5f594dfbd220bc2f29ab3fc5}{k\+\_\+means}} (X, n\+\_\+clusters, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, init="{}k-\/means++"{}, n\+\_\+init="{}auto"{}, max\+\_\+iter=300, verbose=False, tol=1e-\/4, random\+\_\+state=None, copy\+\_\+x=\mbox{\hyperlink{classTrue}{True}}, algorithm="{}lloyd"{}, return\+\_\+n\+\_\+iter=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_afbb8b78ac11a704cb89b6e39861434a1}{\+\_\+kmeans\+\_\+single\+\_\+elkan}} (X, sample\+\_\+weight, centers\+\_\+init, max\+\_\+iter=300, verbose=False, tol=1e-\/4, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_a4a0ac50b409c45c08e0df1b7395caf84}{\+\_\+kmeans\+\_\+single\+\_\+lloyd}} (X, sample\+\_\+weight, centers\+\_\+init, max\+\_\+iter=300, verbose=False, tol=1e-\/4, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_a84655647c43732303456d530f53626a7}{\+\_\+labels\+\_\+inertia}} (X, sample\+\_\+weight, centers, n\+\_\+threads=1, return\+\_\+inertia=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_ace6f192df3fc71b46fb71270278cbdb8}{\+\_\+mini\+\_\+batch\+\_\+step}} (X, sample\+\_\+weight, centers, centers\+\_\+new, weight\+\_\+sums, random\+\_\+state, random\+\_\+reassign=False, reassignment\+\_\+ratio=0.\+01, verbose=False, n\+\_\+threads=1)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__kmeans_ac46467f373b1b8bca43ba702be130f13}{\+\_\+labels\+\_\+inertia\+\_\+threadpool\+\_\+limit}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}K-means clustering.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_a4e172c2f6ed5ad2c482eff9adbd5338c}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_kmeans\_plusplus@{\_kmeans\_plusplus}}
\index{\_kmeans\_plusplus@{\_kmeans\_plusplus}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_kmeans\_plusplus()}{\_kmeans\_plusplus()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_a4e172c2f6ed5ad2c482eff9adbd5338c} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+kmeans\+\_\+plusplus (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+clusters}{, }\item[{}]{x\+\_\+squared\+\_\+norms}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{random\+\_\+state}{, }\item[{}]{n\+\_\+local\+\_\+trials}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computational component for initialization of n_clusters by
k-means++. Prior validation of data is assumed.

Parameters
----------
X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The data to pick seeds for.

n_clusters : int
    The number of seeds to choose.

sample_weight : ndarray of shape (n_samples,)
    The weights for each observation in `X`.

x_squared_norms : ndarray of shape (n_samples,)
    Squared Euclidean norm of each data point.

random_state : RandomState instance
    The generator used to initialize the centers.
    See :term:`Glossary <random_state>`.

n_local_trials : int, default=None
    The number of seeding trials for each center (except the first),
    of which the one reducing inertia the most is greedily chosen.
    Set to None to make the number of trials depend logarithmically
    on the number of seeds (2+log(k)); this is the default.

Returns
-------
centers : ndarray of shape (n_clusters, n_features)
    The initial centers for k-means.

indices : ndarray of shape (n_clusters,)
    The index location of the chosen centers in the data array X. For a
    given index and center, X[index] = center.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00174}{174}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.



Referenced by \mbox{\hyperlink{__kmeans_8py_source_l00964}{sklearn.\+cluster.\+\_\+kmeans.\+\_\+\+Base\+KMeans.\+\_\+init\+\_\+centroids()}}, and \mbox{\hyperlink{__kmeans_8py_source_l00076}{kmeans\+\_\+plusplus()}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_afbb8b78ac11a704cb89b6e39861434a1}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_kmeans\_single\_elkan@{\_kmeans\_single\_elkan}}
\index{\_kmeans\_single\_elkan@{\_kmeans\_single\_elkan}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_kmeans\_single\_elkan()}{\_kmeans\_single\_elkan()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_afbb8b78ac11a704cb89b6e39861434a1} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+kmeans\+\_\+single\+\_\+elkan (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{centers\+\_\+init}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 300}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}A single run of k-means elkan, assumes preparation completed prior.

Parameters
----------
X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The observations to cluster. If sparse matrix, must be in CSR format.

sample_weight : array-like of shape (n_samples,)
    The weights for each observation in X.

centers_init : ndarray of shape (n_clusters, n_features)
    The initial centers.

max_iter : int, default=300
    Maximum number of iterations of the k-means algorithm to run.

verbose : bool, default=False
    Verbosity mode.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations to declare
    convergence.
    It's not advised to set `tol=0` since convergence might never be
    declared due to rounding errors. Use a very small number instead.

n_threads : int, default=1
    The number of OpenMP threads to use for the computation. Parallelism is
    sample-wise on the main cython loop which assigns each sample to its
    closest center.

Returns
-------
centroid : ndarray of shape (n_clusters, n_features)
    Centroids found at the last iteration of k-means.

label : ndarray of shape (n_samples,)
    label[i] is the code or index of the centroid the
    i'th observation is closest to.

inertia : float
    The final value of the inertia criterion (sum of squared distances to
    the closest centroid for all observations in the training set).

n_iter : int
    Number of iterations run.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00456}{456}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_a4a0ac50b409c45c08e0df1b7395caf84}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_kmeans\_single\_lloyd@{\_kmeans\_single\_lloyd}}
\index{\_kmeans\_single\_lloyd@{\_kmeans\_single\_lloyd}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_kmeans\_single\_lloyd()}{\_kmeans\_single\_lloyd()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_a4a0ac50b409c45c08e0df1b7395caf84} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+kmeans\+\_\+single\+\_\+lloyd (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{centers\+\_\+init}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 300}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}A single run of k-means lloyd, assumes preparation completed prior.

Parameters
----------
X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The observations to cluster. If sparse matrix, must be in CSR format.

sample_weight : ndarray of shape (n_samples,)
    The weights for each observation in X.

centers_init : ndarray of shape (n_clusters, n_features)
    The initial centers.

max_iter : int, default=300
    Maximum number of iterations of the k-means algorithm to run.

verbose : bool, default=False
    Verbosity mode

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations to declare
    convergence.
    It's not advised to set `tol=0` since convergence might never be
    declared due to rounding errors. Use a very small number instead.

n_threads : int, default=1
    The number of OpenMP threads to use for the computation. Parallelism is
    sample-wise on the main cython loop which assigns each sample to its
    closest center.

Returns
-------
centroid : ndarray of shape (n_clusters, n_features)
    Centroids found at the last iteration of k-means.

label : ndarray of shape (n_samples,)
    label[i] is the code or index of the centroid the
    i'th observation is closest to.

inertia : float
    The final value of the inertia criterion (sum of squared distances to
    the closest centroid for all observations in the training set).

n_iter : int
    Number of iterations run.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00624}{624}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_a84655647c43732303456d530f53626a7}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_labels\_inertia@{\_labels\_inertia}}
\index{\_labels\_inertia@{\_labels\_inertia}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_labels\_inertia()}{\_labels\_inertia()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_a84655647c43732303456d530f53626a7} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+labels\+\_\+inertia (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{centers}{, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}, }\item[{}]{return\+\_\+inertia}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}E step of the K-means EM algorithm.

Compute the labels and the inertia of the given samples and centers.

Parameters
----------
X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The input samples to assign to the labels. If sparse matrix, must
    be in CSR format.

sample_weight : ndarray of shape (n_samples,)
    The weights for each observation in X.

x_squared_norms : ndarray of shape (n_samples,)
    Precomputed squared euclidean norm of each data point, to speed up
    computations.

centers : ndarray of shape (n_clusters, n_features)
    The cluster centers.

n_threads : int, default=1
    The number of OpenMP threads to use for the computation. Parallelism is
    sample-wise on the main cython loop which assigns each sample to its
    closest center.

return_inertia : bool, default=True
    Whether to compute and return the inertia.

Returns
-------
labels : ndarray of shape (n_samples,)
    The resulting assignment.

inertia : float
    Sum of squared distances of samples to their closest cluster center.
    Inertia is only returned if return_inertia is True.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00755}{755}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.



Referenced by \mbox{\hyperlink{__kmeans_8py_source_l01568}{\+\_\+mini\+\_\+batch\+\_\+step()}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_ace6f192df3fc71b46fb71270278cbdb8}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_mini\_batch\_step@{\_mini\_batch\_step}}
\index{\_mini\_batch\_step@{\_mini\_batch\_step}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_mini\_batch\_step()}{\_mini\_batch\_step()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_ace6f192df3fc71b46fb71270278cbdb8} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+mini\+\_\+batch\+\_\+step (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{centers}{, }\item[{}]{centers\+\_\+new}{, }\item[{}]{weight\+\_\+sums}{, }\item[{}]{random\+\_\+state}{, }\item[{}]{random\+\_\+reassign}{ = {\ttfamily False}, }\item[{}]{reassignment\+\_\+ratio}{ = {\ttfamily 0.01}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Incremental update of the centers for the Minibatch K-Means algorithm.

Parameters
----------

X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The original data array. If sparse, must be in CSR format.

x_squared_norms : ndarray of shape (n_samples,)
    Squared euclidean norm of each data point.

sample_weight : ndarray of shape (n_samples,)
    The weights for each observation in `X`.

centers : ndarray of shape (n_clusters, n_features)
    The cluster centers before the current iteration

centers_new : ndarray of shape (n_clusters, n_features)
    The cluster centers after the current iteration. Modified in-place.

weight_sums : ndarray of shape (n_clusters,)
    The vector in which we keep track of the numbers of points in a
    cluster. This array is modified in place.

random_state : RandomState instance
    Determines random number generation for low count centers reassignment.
    See :term:`Glossary <random_state>`.

random_reassign : boolean, default=False
    If True, centers with very low counts are randomly reassigned
    to observations.

reassignment_ratio : float, default=0.01
    Control the fraction of the maximum number of counts for a
    center to be reassigned. A higher value means that low count
    centers are more likely to be reassigned, which means that the
    model will take longer to converge, but should converge in a
    better clustering.

verbose : bool, default=False
    Controls the verbosity.

n_threads : int, default=1
    The number of OpenMP threads to use for the computation.

Returns
-------
inertia : float
    Sum of squared distances of samples to their closest cluster center.
    The inertia is computed after finding the labels and before updating
    the centers.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l01557}{1557}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.



References \mbox{\hyperlink{__kmeans_8py_source_l00755}{\+\_\+labels\+\_\+inertia()}}.



Referenced by \mbox{\hyperlink{__kmeans_8py_source_l02046}{sklearn.\+cluster.\+\_\+kmeans.\+Mini\+Batch\+KMeans.\+fit()}}, and \mbox{\hyperlink{__kmeans_8py_source_l02203}{sklearn.\+cluster.\+\_\+kmeans.\+Mini\+Batch\+KMeans.\+partial\+\_\+fit()}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_a0356aeee2ae6949ea63f88544e6c001c}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_tolerance@{\_tolerance}}
\index{\_tolerance@{\_tolerance}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_tolerance()}{\_tolerance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_a0356aeee2ae6949ea63f88544e6c001c} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+tolerance (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{tol}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



K-\/means batch estimation by EM (expectation maximization) 

\begin{DoxyVerb}Return a tolerance which is dependent on the dataset.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00279}{279}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_a5ac8891e5f594dfbd220bc2f29ab3fc5}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!k\_means@{k\_means}}
\index{k\_means@{k\_means}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{k\_means()}{k\_means()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_a5ac8891e5f594dfbd220bc2f29ab3fc5} 
sklearn.\+cluster.\+\_\+kmeans.\+k\+\_\+means (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+clusters}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{init}{ = {\ttfamily "{}k-\/means++"{}}, }\item[{}]{n\+\_\+init}{ = {\ttfamily "{}auto"{}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 300}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{copy\+\_\+x}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{algorithm}{ = {\ttfamily "{}lloyd"{}}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Perform K-means clustering algorithm.

Read more in the :ref:`User Guide <k_means>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The observations to cluster. It must be noted that the data
    will be converted to C ordering, which will cause a memory copy
    if the given data is not C-contiguous.

n_clusters : int
    The number of clusters to form as well as the number of
    centroids to generate.

sample_weight : array-like of shape (n_samples,), default=None
    The weights for each observation in `X`. If `None`, all observations
    are assigned equal weight. `sample_weight` is not used during
    initialization if `init` is a callable or a user provided array.

init : {'k-means++', 'random'}, callable or array-like of shape \
        (n_clusters, n_features), default='k-means++'
    Method for initialization:

    - `'k-means++'` : selects initial cluster centers for k-mean
      clustering in a smart way to speed up convergence. See section
      Notes in k_init for more details.
    - `'random'`: choose `n_clusters` observations (rows) at random from data
      for the initial centroids.
    - If an array is passed, it should be of shape `(n_clusters, n_features)`
      and gives the initial centers.
    - If a callable is passed, it should take arguments `X`, `n_clusters` and a
      random state and return an initialization.

n_init : 'auto' or int, default="auto"
    Number of time the k-means algorithm will be run with different
    centroid seeds. The final results will be the best output of
    n_init consecutive runs in terms of inertia.

    When `n_init='auto'`, the number of runs depends on the value of init:
    10 if using `init='random'` or `init` is a callable;
    1 if using `init='k-means++'` or `init` is an array-like.

    .. versionadded:: 1.2
       Added 'auto' option for `n_init`.

    .. versionchanged:: 1.4
       Default value for `n_init` changed to `'auto'`.

max_iter : int, default=300
    Maximum number of iterations of the k-means algorithm to run.

verbose : bool, default=False
    Verbosity mode.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations to declare
    convergence.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization. Use
    an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

copy_x : bool, default=True
    When pre-computing distances it is more numerically accurate to center
    the data first. If `copy_x` is True (default), then the original data is
    not modified. If False, the original data is modified, and put back
    before the function returns, but small numerical differences may be
    introduced by subtracting and then adding the data mean. Note that if
    the original data is not C-contiguous, a copy will be made even if
    `copy_x` is False. If the original data is sparse, but not in CSR format,
    a copy will be made even if `copy_x` is False.

algorithm : {"lloyd", "elkan"}, default="lloyd"
    K-means algorithm to use. The classical EM-style algorithm is `"lloyd"`.
    The `"elkan"` variation can be more efficient on some datasets with
    well-defined clusters, by using the triangle inequality. However it's
    more memory intensive due to the allocation of an extra array of shape
    `(n_samples, n_clusters)`.

    .. versionchanged:: 0.18
        Added Elkan algorithm

    .. versionchanged:: 1.1
        Renamed "full" to "lloyd", and deprecated "auto" and "full".
        Changed "auto" to use "lloyd" instead of "elkan".

return_n_iter : bool, default=False
    Whether or not to return the number of iterations.

Returns
-------
centroid : ndarray of shape (n_clusters, n_features)
    Centroids found at the last iteration of k-means.

label : ndarray of shape (n_samples,)
    The `label[i]` is the code or index of the centroid the
    i'th observation is closest to.

inertia : float
    The final value of the inertia criterion (sum of squared distances to
    the closest centroid for all observations in the training set).

best_n_iter : int
    Number of iterations corresponding to the best results.
    Returned only if `return_n_iter` is set to True.

Examples
--------
>>> import numpy as np
>>> from sklearn.cluster import k_means
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
>>> centroid, label, inertia = k_means(
...     X, n_clusters=2, n_init="auto", random_state=0
... )
>>> centroid
array([[10.,  2.],
       [ 1.,  2.]])
>>> label
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> inertia
16.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00298}{298}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_aac7e701fb5c1c3aa4bab6b62b9ce2c9a}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!kmeans\_plusplus@{kmeans\_plusplus}}
\index{kmeans\_plusplus@{kmeans\_plusplus}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{kmeans\_plusplus()}{kmeans\_plusplus()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_aac7e701fb5c1c3aa4bab6b62b9ce2c9a} 
sklearn.\+cluster.\+\_\+kmeans.\+kmeans\+\_\+plusplus (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+clusters}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{x\+\_\+squared\+\_\+norms}{ = {\ttfamily None}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{n\+\_\+local\+\_\+trials}{ = {\ttfamily None}}\end{DoxyParamCaption})}



Initialization heuristic. 

\begin{DoxyVerb}Init n_clusters seeds according to k-means++.

.. versionadded:: 0.24

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data to pick seeds from.

n_clusters : int
    The number of centroids to initialize.

sample_weight : array-like of shape (n_samples,), default=None
    The weights for each observation in `X`. If `None`, all observations
    are assigned equal weight. `sample_weight` is ignored if `init`
    is a callable or a user provided array.

    .. versionadded:: 1.3

x_squared_norms : array-like of shape (n_samples,), default=None
    Squared Euclidean norm of each data point.

random_state : int or RandomState instance, default=None
    Determines random number generation for centroid initialization. Pass
    an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

n_local_trials : int, default=None
    The number of seeding trials for each center (except the first),
    of which the one reducing inertia the most is greedily chosen.
    Set to None to make the number of trials depend logarithmically
    on the number of seeds (2+log(k)) which is the recommended setting.
    Setting to 1 disables the greedy cluster selection and recovers the
    vanilla k-means++ algorithm which was empirically shown to work less
    well than its greedy variant.

Returns
-------
centers : ndarray of shape (n_clusters, n_features)
    The initial centers for k-means.

indices : ndarray of shape (n_clusters,)
    The index location of the chosen centers in the data array X. For a
    given index and center, X[index] = center.

Notes
-----
Selects initial cluster centers for k-mean clustering in a smart way
to speed up convergence. see: Arthur, D. and Vassilvitskii, S.
"k-means++: the advantages of careful seeding". ACM-SIAM symposium
on Discrete algorithms. 2007

Examples
--------

>>> from sklearn.cluster import kmeans_plusplus
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
>>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)
>>> centers
array([[10,  2],
       [ 1,  0]])
>>> indices
array([3, 2])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00068}{68}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.



References \mbox{\hyperlink{__kmeans_8py_source_l00176}{\+\_\+kmeans\+\_\+plusplus()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1cluster_1_1__kmeans_ac46467f373b1b8bca43ba702be130f13}\index{sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}!\_labels\_inertia\_threadpool\_limit@{\_labels\_inertia\_threadpool\_limit}}
\index{\_labels\_inertia\_threadpool\_limit@{\_labels\_inertia\_threadpool\_limit}!sklearn.cluster.\_kmeans@{sklearn.cluster.\_kmeans}}
\doxysubsubsection{\texorpdfstring{\_labels\_inertia\_threadpool\_limit}{\_labels\_inertia\_threadpool\_limit}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__kmeans_ac46467f373b1b8bca43ba702be130f13} 
sklearn.\+cluster.\+\_\+kmeans.\+\_\+labels\+\_\+inertia\+\_\+threadpool\+\_\+limit\hspace{0.3cm}{\ttfamily [protected]}}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ \_threadpool\_controller\_decorator(}
\DoxyCodeLine{00002\ \ \ \ \ limits=1,\ user\_api=\textcolor{stringliteral}{"{}blas"{}}}
\DoxyCodeLine{00003\ )(\_labels\_inertia)}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{__kmeans_8py_source_l00826}{826}} of file \mbox{\hyperlink{__kmeans_8py_source}{\+\_\+kmeans.\+py}}.

