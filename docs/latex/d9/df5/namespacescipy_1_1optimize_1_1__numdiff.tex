\doxysection{scipy.\+optimize.\+\_\+numdiff Namespace Reference}
\hypertarget{namespacescipy_1_1optimize_1_1__numdiff}{}\label{namespacescipy_1_1optimize_1_1__numdiff}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_ae3092dc376024776f6b2f8f74d046892}{\+\_\+adjust\+\_\+scheme\+\_\+to\+\_\+bounds}} (x0, h, num\+\_\+steps, scheme, lb, ub)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a7225c7f3c07b9435d5cfd72c00e2a843}{\+\_\+eps\+\_\+for\+\_\+method}} (x0\+\_\+dtype, f0\+\_\+dtype, method)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a3f1b761f2778bc54ce59add17282f8ad}{\+\_\+compute\+\_\+absolute\+\_\+step}} (rel\+\_\+step, x0, f0, method)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a899f48c0590f3095356a8cff49019c44}{\+\_\+prepare\+\_\+bounds}} (bounds, x0)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a81c9baed80ae81b795e95751bbac5f22}{group\+\_\+columns}} (A, order=0)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a6d1a55482bf2058bbc720e9be8d9bf94}{approx\+\_\+derivative}} (fun, x0, method=\textquotesingle{}3-\/point\textquotesingle{}, rel\+\_\+step=None, abs\+\_\+step=None, f0=None, bounds=(-\/np.\+inf, np.\+inf), sparsity=None, as\+\_\+linear\+\_\+operator=False, args=(), kwargs=\{\})
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a0f051ca4b32f22e7027791a76528bfc8}{\+\_\+linear\+\_\+operator\+\_\+difference}} (fun, x0, f0, h, method)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a3859a545f34729436a75a7cc72b0af42}{\+\_\+dense\+\_\+difference}} (fun, x0, f0, h, use\+\_\+one\+\_\+sided, method)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a36b03aa7984e234b40277bb9faa90309}{\+\_\+sparse\+\_\+difference}} (fun, x0, f0, h, use\+\_\+one\+\_\+sided, structure, groups, method)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__numdiff_a48593fae5c9c1b6a1365ff53c6e3d576}{check\+\_\+derivative}} (fun, jac, x0, bounds=(-\/np.\+inf, np.\+inf), args=(), kwargs=\{\})
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Routines for numerical differentiation.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_ae3092dc376024776f6b2f8f74d046892}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_adjust\_scheme\_to\_bounds@{\_adjust\_scheme\_to\_bounds}}
\index{\_adjust\_scheme\_to\_bounds@{\_adjust\_scheme\_to\_bounds}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_adjust\_scheme\_to\_bounds()}{\_adjust\_scheme\_to\_bounds()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_ae3092dc376024776f6b2f8f74d046892} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+adjust\+\_\+scheme\+\_\+to\+\_\+bounds (\begin{DoxyParamCaption}\item[{}]{x0}{, }\item[{}]{h}{, }\item[{}]{num\+\_\+steps}{, }\item[{}]{scheme}{, }\item[{}]{lb}{, }\item[{}]{ub}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Adjust final difference scheme to the presence of bounds.

Parameters
----------
x0 : ndarray, shape (n,)
    Point at which we wish to estimate derivative.
h : ndarray, shape (n,)
    Desired absolute finite difference steps.
num_steps : int
    Number of `h` steps in one direction required to implement finite
    difference scheme. For example, 2 means that we need to evaluate
    f(x0 + 2 * h) or f(x0 - 2 * h)
scheme : {'1-sided', '2-sided'}
    Whether steps in one or both directions are required. In other
    words '1-sided' applies to forward and backward schemes, '2-sided'
    applies to center schemes.
lb : ndarray, shape (n,)
    Lower bounds on independent variables.
ub : ndarray, shape (n,)
    Upper bounds on independent variables.

Returns
-------
h_adjusted : ndarray, shape (n,)
    Adjusted absolute step sizes. Step size decreases only if a sign flip
    or switching to one-sided scheme doesn't allow to take a full step.
use_one_sided : ndarray of bool, shape (n,)
    Whether to switch to one-sided scheme. Informative only for
    ``scheme='2-sided'``.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00011}{11}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a3f1b761f2778bc54ce59add17282f8ad}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_compute\_absolute\_step@{\_compute\_absolute\_step}}
\index{\_compute\_absolute\_step@{\_compute\_absolute\_step}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_compute\_absolute\_step()}{\_compute\_absolute\_step()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a3f1b761f2778bc54ce59add17282f8ad} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+compute\+\_\+absolute\+\_\+step (\begin{DoxyParamCaption}\item[{}]{rel\+\_\+step}{, }\item[{}]{x0}{, }\item[{}]{f0}{, }\item[{}]{method}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes an absolute step from a relative step for finite difference
calculation.

Parameters
----------
rel_step: None or array-like
    Relative step for the finite difference calculation
x0 : np.ndarray
    Parameter vector
f0 : np.ndarray or scalar
method : {'2-point', '3-point', 'cs'}

Returns
-------
h : float
    The absolute step size

Notes
-----
`h` will always be np.float64. However, if `x0` or `f0` are
smaller floating point dtypes (e.g. np.float32), then the absolute
step size will be calculated from the smallest floating point size.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00144}{144}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



References \mbox{\hyperlink{__numdiff_8py_source_l00091}{\+\_\+eps\+\_\+for\+\_\+method()}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a3859a545f34729436a75a7cc72b0af42}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_dense\_difference@{\_dense\_difference}}
\index{\_dense\_difference@{\_dense\_difference}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_dense\_difference()}{\_dense\_difference()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a3859a545f34729436a75a7cc72b0af42} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+dense\+\_\+difference (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{f0}{, }\item[{}]{h}{, }\item[{}]{use\+\_\+one\+\_\+sided}{, }\item[{}]{method}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00566}{566}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a7225c7f3c07b9435d5cfd72c00e2a843}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_eps\_for\_method@{\_eps\_for\_method}}
\index{\_eps\_for\_method@{\_eps\_for\_method}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_eps\_for\_method()}{\_eps\_for\_method()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a7225c7f3c07b9435d5cfd72c00e2a843} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+eps\+\_\+for\+\_\+method (\begin{DoxyParamCaption}\item[{}]{x0\+\_\+dtype}{, }\item[{}]{f0\+\_\+dtype}{, }\item[{}]{method}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Calculates relative EPS step to use for a given data type
and numdiff step method.

Progressively smaller steps are used for larger floating point types.

Parameters
----------
f0_dtype: np.dtype
    dtype of function evaluation

x0_dtype: np.dtype
    dtype of parameter vector

method: {'2-point', '3-point', 'cs'}

Returns
-------
EPS: float
    relative step size. May be np.float16, np.float32, np.float64

Notes
-----
The default relative step will be np.float64. However, if x0 or f0 are
smaller floating point types (np.float16, np.float32), then the smallest
floating point type is chosen.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00091}{91}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00144}{\+\_\+compute\+\_\+absolute\+\_\+step()}}, and \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a0f051ca4b32f22e7027791a76528bfc8}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_linear\_operator\_difference@{\_linear\_operator\_difference}}
\index{\_linear\_operator\_difference@{\_linear\_operator\_difference}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_linear\_operator\_difference()}{\_linear\_operator\_difference()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a0f051ca4b32f22e7027791a76528bfc8} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+linear\+\_\+operator\+\_\+difference (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{f0}{, }\item[{}]{h}{, }\item[{}]{method}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00525}{525}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a899f48c0590f3095356a8cff49019c44}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_prepare\_bounds@{\_prepare\_bounds}}
\index{\_prepare\_bounds@{\_prepare\_bounds}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_prepare\_bounds()}{\_prepare\_bounds()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a899f48c0590f3095356a8cff49019c44} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+prepare\+\_\+bounds (\begin{DoxyParamCaption}\item[{}]{bounds}{, }\item[{}]{x0}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Prepares new-style bounds from a two-tuple specifying the lower and upper
limits for values in x0. If a value is not bound then the lower/upper bound
will be expected to be -np.inf/np.inf.

Examples
--------
>>> _prepare_bounds([(0, 1, 2), (1, 2, np.inf)], [0.5, 1.5, 2.5])
(array([0., 1., 2.]), array([ 1.,  2., inf]))
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00193}{193}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a36b03aa7984e234b40277bb9faa90309}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!\_sparse\_difference@{\_sparse\_difference}}
\index{\_sparse\_difference@{\_sparse\_difference}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{\_sparse\_difference()}{\_sparse\_difference()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a36b03aa7984e234b40277bb9faa90309} 
scipy.\+optimize.\+\_\+numdiff.\+\_\+sparse\+\_\+difference (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{f0}{, }\item[{}]{h}{, }\item[{}]{use\+\_\+one\+\_\+sided}{, }\item[{}]{structure}{, }\item[{}]{groups}{, }\item[{}]{method}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00606}{606}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a6d1a55482bf2058bbc720e9be8d9bf94}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!approx\_derivative@{approx\_derivative}}
\index{approx\_derivative@{approx\_derivative}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{approx\_derivative()}{approx\_derivative()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a6d1a55482bf2058bbc720e9be8d9bf94} 
scipy.\+optimize.\+\_\+numdiff.\+approx\+\_\+derivative (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{method}{ = {\ttfamily \textquotesingle{}3-\/point\textquotesingle{}}, }\item[{}]{rel\+\_\+step}{ = {\ttfamily None}, }\item[{}]{abs\+\_\+step}{ = {\ttfamily None}, }\item[{}]{f0}{ = {\ttfamily None}, }\item[{}]{bounds}{ = {\ttfamily (-\/np.inf,~np.inf)}, }\item[{}]{sparsity}{ = {\ttfamily None}, }\item[{}]{as\+\_\+linear\+\_\+operator}{ = {\ttfamily False}, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{kwargs}{ = {\ttfamily \{\}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute finite difference approximation of the derivatives of a
vector-valued function.

If a function maps from R^n to R^m, its derivatives form m-by-n matrix
called the Jacobian, where an element (i, j) is a partial derivative of
f[i] with respect to x[j].

Parameters
----------
fun : callable
    Function of which to estimate the derivatives. The argument x
    passed to this function is ndarray of shape (n,) (never a scalar
    even if n=1). It must return 1-D array_like of shape (m,) or a scalar.
x0 : array_like of shape (n,) or float
    Point at which to estimate the derivatives. Float will be converted
    to a 1-D array.
method : {'3-point', '2-point', 'cs'}, optional
    Finite difference method to use:
        - '2-point' - use the first order accuracy forward or backward
                      difference.
        - '3-point' - use central difference in interior points and the
                      second order accuracy forward or backward difference
                      near the boundary.
        - 'cs' - use a complex-step finite difference scheme. This assumes
                 that the user function is real-valued and can be
                 analytically continued to the complex plane. Otherwise,
                 produces bogus results.
rel_step : None or array_like, optional
    Relative step size to use. If None (default) the absolute step size is
    computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``, with
    `rel_step` being selected automatically, see Notes. Otherwise
    ``h = rel_step * sign(x0) * abs(x0)``. For ``method='3-point'`` the
    sign of `h` is ignored. The calculated step size is possibly adjusted
    to fit into the bounds.
abs_step : array_like, optional
    Absolute step size to use, possibly adjusted to fit into the bounds.
    For ``method='3-point'`` the sign of `abs_step` is ignored. By default
    relative steps are used, only if ``abs_step is not None`` are absolute
    steps used.
f0 : None or array_like, optional
    If not None it is assumed to be equal to ``fun(x0)``, in this case
    the ``fun(x0)`` is not called. Default is None.
bounds : tuple of array_like, optional
    Lower and upper bounds on independent variables. Defaults to no bounds.
    Each bound must match the size of `x0` or be a scalar, in the latter
    case the bound will be the same for all variables. Use it to limit the
    range of function evaluation. Bounds checking is not implemented
    when `as_linear_operator` is True.
sparsity : {None, array_like, sparse matrix, 2-tuple}, optional
    Defines a sparsity structure of the Jacobian matrix. If the Jacobian
    matrix is known to have only few non-zero elements in each row, then
    it's possible to estimate its several columns by a single function
    evaluation [3]_. To perform such economic computations two ingredients
    are required:

    * structure : array_like or sparse matrix of shape (m, n). A zero
      element means that a corresponding element of the Jacobian
      identically equals to zero.
    * groups : array_like of shape (n,). A column grouping for a given
      sparsity structure, use `group_columns` to obtain it.

    A single array or a sparse matrix is interpreted as a sparsity
    structure, and groups are computed inside the function. A tuple is
    interpreted as (structure, groups). If None (default), a standard
    dense differencing will be used.

    Note, that sparse differencing makes sense only for large Jacobian
    matrices where each row contains few non-zero elements.
as_linear_operator : bool, optional
    When True the function returns an `scipy.sparse.linalg.LinearOperator`.
    Otherwise it returns a dense array or a sparse matrix depending on
    `sparsity`. The linear operator provides an efficient way of computing
    ``J.dot(p)`` for any vector ``p`` of shape (n,), but does not allow
    direct access to individual elements of the matrix. By default
    `as_linear_operator` is False.
args, kwargs : tuple and dict, optional
    Additional arguments passed to `fun`. Both empty by default.
    The calling signature is ``fun(x, *args, **kwargs)``.

Returns
-------
J : {ndarray, sparse matrix, LinearOperator}
    Finite difference approximation of the Jacobian matrix.
    If `as_linear_operator` is True returns a LinearOperator
    with shape (m, n). Otherwise it returns a dense array or sparse
    matrix depending on how `sparsity` is defined. If `sparsity`
    is None then a ndarray with shape (m, n) is returned. If
    `sparsity` is not None returns a csr_matrix with shape (m, n).
    For sparse matrices and linear operators it is always returned as
    a 2-D structure, for ndarrays, if m=1 it is returned
    as a 1-D gradient array with shape (n,).

See Also
--------
check_derivative : Check correctness of a function computing derivatives.

Notes
-----
If `rel_step` is not provided, it assigned as ``EPS**(1/s)``, where EPS is
determined from the smallest floating point dtype of `x0` or `fun(x0)`,
``np.finfo(x0.dtype).eps``, s=2 for '2-point' method and
s=3 for '3-point' method. Such relative step approximately minimizes a sum
of truncation and round-off errors, see [1]_. Relative steps are used by
default. However, absolute steps are used when ``abs_step is not None``.
If any of the absolute or relative steps produces an indistinguishable
difference from the original `x0`, ``(x0 + dx) - x0 == 0``, then a
automatic step size is substituted for that particular entry.

A finite difference scheme for '3-point' method is selected automatically.
The well-known central difference scheme is used for points sufficiently
far from the boundary, and 3-point forward or backward scheme is used for
points near the boundary. Both schemes have the second-order accuracy in
terms of Taylor expansion. Refer to [2]_ for the formulas of 3-point
forward and backward difference schemes.

For dense differencing when m=1 Jacobian is returned with a shape (n,),
on the other hand when n=1 Jacobian is returned with a shape (m, 1).
Our motivation is the following: a) It handles a case of gradient
computation (m=1) in a conventional way. b) It clearly separates these two
different cases. b) In all cases np.atleast_2d can be called to get 2-D
Jacobian with correct dimensions.

References
----------
.. [1] W. H. Press et. al. "Numerical Recipes. The Art of Scientific
       Computing. 3rd edition", sec. 5.7.

.. [2] A. Curtis, M. J. D. Powell, and J. Reid, "On the estimation of
       sparse Jacobian matrices", Journal of the Institute of Mathematics
       and its Applications, 13 (1974), pp. 117-120.

.. [3] B. Fornberg, "Generation of Finite Difference Formulas on
       Arbitrarily Spaced Grids", Mathematics of Computation 51, 1988.

Examples
--------
>>> import numpy as np
>>> from scipy.optimize._numdiff import approx_derivative
>>>
>>> def f(x, c1, c2):
...     return np.array([x[0] * np.sin(c1 * x[1]),
...                      x[0] * np.cos(c2 * x[1])])
...
>>> x0 = np.array([1.0, 0.5 * np.pi])
>>> approx_derivative(f, x0, args=(1, 2))
array([[ 1.,  0.],
       [-1.,  0.]])

Bounds can be used to limit the region of function evaluation.
In the example below we compute left and right derivative at point 1.0.

>>> def g(x):
...     return x**2 if x >= 1 else x
...
>>> x0 = 1.0
>>> approx_derivative(g, x0, bounds=(-np.inf, 1.0))
array([ 1.])
>>> approx_derivative(g, x0, bounds=(1.0, np.inf))
array([ 2.])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00275}{275}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



References \mbox{\hyperlink{__numdiff_8py_source_l00011}{\+\_\+adjust\+\_\+scheme\+\_\+to\+\_\+bounds()}}, \mbox{\hyperlink{__numdiff_8py_source_l00144}{\+\_\+compute\+\_\+absolute\+\_\+step()}}, \mbox{\hyperlink{__numdiff_8py_source_l00091}{\+\_\+eps\+\_\+for\+\_\+method()}}, \mbox{\hyperlink{__numdiff_8py_source_l00193}{\+\_\+prepare\+\_\+bounds()}}, and \mbox{\hyperlink{__numdiff_8py_source_l00214}{group\+\_\+columns()}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00687}{check\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a48593fae5c9c1b6a1365ff53c6e3d576}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!check\_derivative@{check\_derivative}}
\index{check\_derivative@{check\_derivative}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{check\_derivative()}{check\_derivative()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a48593fae5c9c1b6a1365ff53c6e3d576} 
scipy.\+optimize.\+\_\+numdiff.\+check\+\_\+derivative (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{jac}{, }\item[{}]{x0}{, }\item[{}]{bounds}{ = {\ttfamily (-\/np.inf,~np.inf)}, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{kwargs}{ = {\ttfamily \{\}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Check correctness of a function computing derivatives (Jacobian or
gradient) by comparison with a finite difference approximation.

Parameters
----------
fun : callable
    Function of which to estimate the derivatives. The argument x
    passed to this function is ndarray of shape (n,) (never a scalar
    even if n=1). It must return 1-D array_like of shape (m,) or a scalar.
jac : callable
    Function which computes Jacobian matrix of `fun`. It must work with
    argument x the same way as `fun`. The return value must be array_like
    or sparse matrix with an appropriate shape.
x0 : array_like of shape (n,) or float
    Point at which to estimate the derivatives. Float will be converted
    to 1-D array.
bounds : 2-tuple of array_like, optional
    Lower and upper bounds on independent variables. Defaults to no bounds.
    Each bound must match the size of `x0` or be a scalar, in the latter
    case the bound will be the same for all variables. Use it to limit the
    range of function evaluation.
args, kwargs : tuple and dict, optional
    Additional arguments passed to `fun` and `jac`. Both empty by default.
    The calling signature is ``fun(x, *args, **kwargs)`` and the same
    for `jac`.

Returns
-------
accuracy : float
    The maximum among all relative errors for elements with absolute values
    higher than 1 and absolute errors for elements with absolute values
    less or equal than 1. If `accuracy` is on the order of 1e-6 or lower,
    then it is likely that your `jac` implementation is correct.

See Also
--------
approx_derivative : Compute finite difference approximation of derivative.

Examples
--------
>>> import numpy as np
>>> from scipy.optimize._numdiff import check_derivative
>>>
>>>
>>> def f(x, c1, c2):
...     return np.array([x[0] * np.sin(c1 * x[1]),
...                      x[0] * np.cos(c2 * x[1])])
...
>>> def jac(x, c1, c2):
...     return np.array([
...         [np.sin(c1 * x[1]),  c1 * x[0] * np.cos(c1 * x[1])],
...         [np.cos(c2 * x[1]), -c2 * x[0] * np.sin(c2 * x[1])]
...     ])
...
>>>
>>> x0 = np.array([1.0, 0.5 * np.pi])
>>> check_derivative(f, jac, x0, args=(1, 2))
2.4492935982947064e-16
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00686}{686}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



References \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__numdiff_a81c9baed80ae81b795e95751bbac5f22}\index{scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}!group\_columns@{group\_columns}}
\index{group\_columns@{group\_columns}!scipy.optimize.\_numdiff@{scipy.optimize.\_numdiff}}
\doxysubsubsection{\texorpdfstring{group\_columns()}{group\_columns()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__numdiff_a81c9baed80ae81b795e95751bbac5f22} 
scipy.\+optimize.\+\_\+numdiff.\+group\+\_\+columns (\begin{DoxyParamCaption}\item[{}]{A}{, }\item[{}]{order}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Group columns of a 2-D matrix for sparse finite differencing [1]_.

Two columns are in the same group if in each row at least one of them
has zero. A greedy sequential algorithm is used to construct groups.

Parameters
----------
A : array_like or sparse matrix, shape (m, n)
    Matrix of which to group columns.
order : int, iterable of int with shape (n,) or None
    Permutation array which defines the order of columns enumeration.
    If int or None, a random permutation is used with `order` used as
    a random seed. Default is 0, that is use a random permutation but
    guarantee repeatability.

Returns
-------
groups : ndarray of int, shape (n,)
    Contains values from 0 to n_groups-1, where n_groups is the number
    of found groups. Each value ``groups[i]`` is an index of a group to
    which ith column assigned. The procedure was helpful only if
    n_groups is significantly less than n.

References
----------
.. [1] A. Curtis, M. J. D. Powell, and J. Reid, "On the estimation of
       sparse Jacobian matrices", Journal of the Institute of Mathematics
       and its Applications, 13 (1974), pp. 117-120.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__numdiff_8py_source_l00214}{214}} of file \mbox{\hyperlink{__numdiff_8py_source}{\+\_\+numdiff.\+py}}.



Referenced by \mbox{\hyperlink{__numdiff_8py_source_l00277}{approx\+\_\+derivative()}}.

