\doxysection{scipy.\+optimize.\+\_\+lbfgsb\+\_\+py Namespace Reference}
\hypertarget{namespacescipy_1_1optimize_1_1__lbfgsb__py}{}\label{namespacescipy_1_1optimize_1_1__lbfgsb__py}\index{scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__lbfgsb__py_1_1LbfgsInvHessProduct}{Lbfgs\+Inv\+Hess\+Product}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__lbfgsb__py_a577fc761e80b4a6d16727f96afc4fa05}{fmin\+\_\+l\+\_\+bfgs\+\_\+b}} (func, x0, fprime=None, args=(), approx\+\_\+grad=0, bounds=None, m=10, factr=1e7, pgtol=1e-\/5, epsilon=1e-\/8, iprint=-\/1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__lbfgsb__py_af8b51e06e060fcdd7ce8a447d35e836e}{\+\_\+minimize\+\_\+lbfgsb}} (fun, x0, args=(), jac=None, bounds=None, disp=None, maxcor=10, ftol=2.\+2204460492503131e-\/09, gtol=1e-\/5, eps=1e-\/8, maxfun=15000, maxiter=15000, iprint=-\/1, callback=None, maxls=20, finite\+\_\+diff\+\_\+rel\+\_\+step=None, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}unknown\+\_\+options)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__lbfgsb__py_ab30d4f5b2ea3bffa099a86f28365f1e2}{\+\_\+\+\_\+all\+\_\+\+\_\+}} = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__lbfgsb__py_a577fc761e80b4a6d16727f96afc4fa05}{fmin\+\_\+l\+\_\+bfgs\+\_\+b}}\textquotesingle{}, \textquotesingle{}\mbox{\hyperlink{classscipy_1_1optimize_1_1__lbfgsb__py_1_1LbfgsInvHessProduct}{Lbfgs\+Inv\+Hess\+Product}}\textquotesingle{}\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Functions
---------
.. autosummary::
   :toctree: generated/

    fmin_l_bfgs_b
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__lbfgsb__py_af8b51e06e060fcdd7ce8a447d35e836e}\index{scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}!\_minimize\_lbfgsb@{\_minimize\_lbfgsb}}
\index{\_minimize\_lbfgsb@{\_minimize\_lbfgsb}!scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}}
\doxysubsubsection{\texorpdfstring{\_minimize\_lbfgsb()}{\_minimize\_lbfgsb()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__lbfgsb__py_af8b51e06e060fcdd7ce8a447d35e836e} 
scipy.\+optimize.\+\_\+lbfgsb\+\_\+py.\+\_\+minimize\+\_\+lbfgsb (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{jac}{ = {\ttfamily None}, }\item[{}]{bounds}{ = {\ttfamily None}, }\item[{}]{disp}{ = {\ttfamily None}, }\item[{}]{maxcor}{ = {\ttfamily 10}, }\item[{}]{ftol}{ = {\ttfamily 2.2204460492503131e-\/09}, }\item[{}]{gtol}{ = {\ttfamily 1e-\/5}, }\item[{}]{eps}{ = {\ttfamily 1e-\/8}, }\item[{}]{maxfun}{ = {\ttfamily 15000}, }\item[{}]{maxiter}{ = {\ttfamily 15000}, }\item[{}]{iprint}{ = {\ttfamily -\/1}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{maxls}{ = {\ttfamily 20}, }\item[{}]{finite\+\_\+diff\+\_\+rel\+\_\+step}{ = {\ttfamily None}, }\item[{\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{unknown\+\_\+options}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Minimize a scalar function of one or more variables using the L-BFGS-B
algorithm.

Options
-------
disp : None or int
    If `disp is None` (the default), then the supplied version of `iprint`
    is used. If `disp is not None`, then it overrides the supplied version
    of `iprint` with the behaviour you outlined.
maxcor : int
    The maximum number of variable metric corrections used to
    define the limited memory matrix. (The limited memory BFGS
    method does not store the full hessian but uses this many terms
    in an approximation to it.)
ftol : float
    The iteration stops when ``(f^k -
    f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.
gtol : float
    The iteration will stop when ``max{|proj g_i | i = 1, ..., n}
    <= gtol`` where ``pg_i`` is the i-th component of the
    projected gradient.
eps : float or ndarray
    If `jac is None` the absolute step size used for numerical
    approximation of the jacobian via forward differences.
maxfun : int
    Maximum number of function evaluations. Note that this function
    may violate the limit because of evaluating gradients by numerical
    differentiation.
maxiter : int
    Maximum number of iterations.
iprint : int, optional
    Controls the frequency of output. ``iprint < 0`` means no output;
    ``iprint = 0``    print only one line at the last iteration;
    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;
    ``iprint = 99``   print details of every iteration except n-vectors;
    ``iprint = 100``  print also the changes of active set and final x;
    ``iprint > 100``  print details of every iteration including x and g.
maxls : int, optional
    Maximum number of line search steps (per iteration). Default is 20.
finite_diff_rel_step : None or array_like, optional
    If `jac in ['2-point', '3-point', 'cs']` the relative step size to
    use for numerical approximation of the jacobian. The absolute step
    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,
    possibly adjusted to fit into the bounds. For ``method='3-point'``
    the sign of `h` is ignored. If None (default) then step is selected
    automatically.

Notes
-----
The option `ftol` is exposed via the `scipy.optimize.minimize` interface,
but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The
relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.
I.e., `factr` multiplies the default machine floating-point precision to
arrive at `ftol`.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__lbfgsb__py_8py_source_l00212}{212}} of file \mbox{\hyperlink{__lbfgsb__py_8py_source}{\+\_\+lbfgsb\+\_\+py.\+py}}.



References \mbox{\hyperlink{__optimize_8py_source_l00156}{scipy.\+optimize.\+\_\+optimize.\+\_\+call\+\_\+callback\+\_\+maybe\+\_\+halt()}}, and \mbox{\hyperlink{__optimize_8py_source_l00297}{scipy.\+optimize.\+\_\+optimize.\+\_\+prepare\+\_\+scalar\+\_\+function()}}.



Referenced by \mbox{\hyperlink{__lbfgsb__py_8py_source_l00054}{fmin\+\_\+l\+\_\+bfgs\+\_\+b()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__lbfgsb__py_a577fc761e80b4a6d16727f96afc4fa05}\index{scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}!fmin\_l\_bfgs\_b@{fmin\_l\_bfgs\_b}}
\index{fmin\_l\_bfgs\_b@{fmin\_l\_bfgs\_b}!scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}}
\doxysubsubsection{\texorpdfstring{fmin\_l\_bfgs\_b()}{fmin\_l\_bfgs\_b()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__lbfgsb__py_a577fc761e80b4a6d16727f96afc4fa05} 
scipy.\+optimize.\+\_\+lbfgsb\+\_\+py.\+fmin\+\_\+l\+\_\+bfgs\+\_\+b (\begin{DoxyParamCaption}\item[{}]{func}{, }\item[{}]{x0}{, }\item[{}]{fprime}{ = {\ttfamily None}, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{approx\+\_\+grad}{ = {\ttfamily 0}, }\item[{}]{bounds}{ = {\ttfamily None}, }\item[{}]{m}{ = {\ttfamily 10}, }\item[{}]{factr}{ = {\ttfamily 1e7}, }\item[{}]{pgtol}{ = {\ttfamily 1e-\/5}, }\item[{}]{epsilon}{ = {\ttfamily 1e-\/8}, }\item[{}]{iprint}{ = {\ttfamily -\/1}, }\item[{}]{maxfun}{ = {\ttfamily 15000}, }\item[{}]{maxiter}{ = {\ttfamily 15000}, }\item[{}]{disp}{ = {\ttfamily None}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{maxls}{ = {\ttfamily 20}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Minimize a function func using the L-BFGS-B algorithm.

Parameters
----------
func : callable f(x,*args)
    Function to minimize.
x0 : ndarray
    Initial guess.
fprime : callable fprime(x,*args), optional
    The gradient of `func`. If None, then `func` returns the function
    value and the gradient (``f, g = func(x, *args)``), unless
    `approx_grad` is True in which case `func` returns only ``f``.
args : sequence, optional
    Arguments to pass to `func` and `fprime`.
approx_grad : bool, optional
    Whether to approximate the gradient numerically (in which case
    `func` returns only the function value).
bounds : list, optional
    ``(min, max)`` pairs for each element in ``x``, defining
    the bounds on that parameter. Use None or +-inf for one of ``min`` or
    ``max`` when there is no bound in that direction.
m : int, optional
    The maximum number of variable metric corrections
    used to define the limited memory matrix. (The limited memory BFGS
    method does not store the full hessian but uses this many terms in an
    approximation to it.)
factr : float, optional
    The iteration stops when
    ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,
    where ``eps`` is the machine precision, which is automatically
    generated by the code. Typical values for `factr` are: 1e12 for
    low accuracy; 1e7 for moderate accuracy; 10.0 for extremely
    high accuracy. See Notes for relationship to `ftol`, which is exposed
    (instead of `factr`) by the `scipy.optimize.minimize` interface to
    L-BFGS-B.
pgtol : float, optional
    The iteration will stop when
    ``max{|proj g_i | i = 1, ..., n} <= pgtol``
    where ``pg_i`` is the i-th component of the projected gradient.
epsilon : float, optional
    Step size used when `approx_grad` is True, for numerically
    calculating the gradient
iprint : int, optional
    Controls the frequency of output. ``iprint < 0`` means no output;
    ``iprint = 0``    print only one line at the last iteration;
    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;
    ``iprint = 99``   print details of every iteration except n-vectors;
    ``iprint = 100``  print also the changes of active set and final x;
    ``iprint > 100``  print details of every iteration including x and g.
disp : int, optional
    If zero, then no output. If a positive number, then this over-rides
    `iprint` (i.e., `iprint` gets the value of `disp`).
maxfun : int, optional
    Maximum number of function evaluations. Note that this function
    may violate the limit because of evaluating gradients by numerical
    differentiation.
maxiter : int, optional
    Maximum number of iterations.
callback : callable, optional
    Called after each iteration, as ``callback(xk)``, where ``xk`` is the
    current parameter vector.
maxls : int, optional
    Maximum number of line search steps (per iteration). Default is 20.

Returns
-------
x : array_like
    Estimated position of the minimum.
f : float
    Value of `func` at the minimum.
d : dict
    Information dictionary.

    * d['warnflag'] is

      - 0 if converged,
      - 1 if too many function evaluations or too many iterations,
      - 2 if stopped for another reason, given in d['task']

    * d['grad'] is the gradient at the minimum (should be 0 ish)
    * d['funcalls'] is the number of function calls made.
    * d['nit'] is the number of iterations.

See also
--------
minimize: Interface to minimization algorithms for multivariate
    functions. See the 'L-BFGS-B' `method` in particular. Note that the
    `ftol` option is made available via that interface, while `factr` is
    provided via this interface, where `factr` is the factor multiplying
    the default machine floating-point precision to arrive at `ftol`:
    ``ftol = factr * numpy.finfo(float).eps``.

Notes
-----
License of L-BFGS-B (FORTRAN code):

The version included here (in fortran code) is 3.0
(released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,
and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following
condition for use:

This software is freely available, but we expect that all publications
describing work using this software, or all commercial products using it,
quote at least one of the references given below. This software is released
under the BSD License.

References
----------
* R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound
  Constrained Optimization, (1995), SIAM Journal on Scientific and
  Statistical Computing, 16, 5, pp. 1190-1208.
* C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,
  FORTRAN routines for large scale bound constrained optimization (1997),
  ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.
* J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,
  FORTRAN routines for large scale bound constrained optimization (2011),
  ACM Transactions on Mathematical Software, 38, 1.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__lbfgsb__py_8py_source_l00049}{49}} of file \mbox{\hyperlink{__lbfgsb__py_8py_source}{\+\_\+lbfgsb\+\_\+py.\+py}}.



References \mbox{\hyperlink{__lbfgsb__py_8py_source_l00216}{\+\_\+minimize\+\_\+lbfgsb()}}, and \mbox{\hyperlink{__optimize_8py_source_l00135}{scipy.\+optimize.\+\_\+optimize.\+\_\+wrap\+\_\+callback()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__lbfgsb__py_ab30d4f5b2ea3bffa099a86f28365f1e2}\index{scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}!\_\_all\_\_@{\_\_all\_\_}}
\index{\_\_all\_\_@{\_\_all\_\_}!scipy.optimize.\_lbfgsb\_py@{scipy.optimize.\_lbfgsb\_py}}
\doxysubsubsection{\texorpdfstring{\_\_all\_\_}{\_\_all\_\_}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__lbfgsb__py_ab30d4f5b2ea3bffa099a86f28365f1e2} 
list scipy.\+optimize.\+\_\+lbfgsb\+\_\+py.\+\_\+\+\_\+all\+\_\+\+\_\+ = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__lbfgsb__py_a577fc761e80b4a6d16727f96afc4fa05}{fmin\+\_\+l\+\_\+bfgs\+\_\+b}}\textquotesingle{}, \textquotesingle{}\mbox{\hyperlink{classscipy_1_1optimize_1_1__lbfgsb__py_1_1LbfgsInvHessProduct}{Lbfgs\+Inv\+Hess\+Product}}\textquotesingle{}\mbox{]}\hspace{0.3cm}{\ttfamily [private]}}



Definition at line \mbox{\hyperlink{__lbfgsb__py_8py_source_l00046}{46}} of file \mbox{\hyperlink{__lbfgsb__py_8py_source}{\+\_\+lbfgsb\+\_\+py.\+py}}.

