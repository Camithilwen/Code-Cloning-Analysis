\doxysection{\+\_\+sag.\+py}
\hypertarget{__sag_8py_source}{}\label{__sag_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_sag.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_sag.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}Solvers\ for\ Ridge\ and\ LogisticRegression\ using\ SAG\ algorithm"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00002}00002\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00003}00003\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00004}00004\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00005}00005\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00006}00006\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00008}00008\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00009}00009\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00010}00010\ \textcolor{keyword}{from}\ ..exceptions\ \textcolor{keyword}{import}\ ConvergenceWarning}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00011}00011\ \textcolor{keyword}{from}\ ..utils\ \textcolor{keyword}{import}\ check\_array}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00012}00012\ \textcolor{keyword}{from}\ ..utils.extmath\ \textcolor{keyword}{import}\ row\_norms}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00013}00013\ \textcolor{keyword}{from}\ ..utils.validation\ \textcolor{keyword}{import}\ \_check\_sample\_weight}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00014}00014\ \textcolor{keyword}{from}\ .\_base\ \textcolor{keyword}{import}\ make\_dataset}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00015}00015\ \textcolor{keyword}{from}\ .\_sag\_fast\ \textcolor{keyword}{import}\ sag32,\ sag64}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00016}00016\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00017}00017\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00018}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag_af82a53319ebeaf50db3edb2c31aa1976}{00018}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag_af82a53319ebeaf50db3edb2c31aa1976}{get\_auto\_step\_size}}(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00019}00019\ \ \ \ \ max\_squared\_sum,\ alpha\_scaled,\ loss,\ fit\_intercept,\ n\_samples=None,\ is\_saga=False}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00020}00020\ ):}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00021}00021\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ automatic\ step\ size\ for\ SAG\ solver.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00022}00022\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00023}00023\ \textcolor{stringliteral}{\ \ \ \ The\ step\ size\ is\ set\ to\ 1\ /\ (alpha\_scaled\ +\ L\ +\ fit\_intercept)\ where\ L\ is}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00024}00024\ \textcolor{stringliteral}{\ \ \ \ the\ max\ sum\ of\ squares\ for\ over\ all\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00025}00025\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00026}00026\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00027}00027\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00028}00028\ \textcolor{stringliteral}{\ \ \ \ max\_squared\_sum\ :\ float}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00029}00029\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Maximum\ squared\ sum\ of\ X\ over\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00030}00030\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00031}00031\ \textcolor{stringliteral}{\ \ \ \ alpha\_scaled\ :\ float}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00032}00032\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ regularization\ term,\ scaled\ by}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00033}00033\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 1.\ /\ n\_samples,\ the\ number\ of\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00034}00034\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00035}00035\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ \{'log',\ 'squared',\ 'multinomial'\}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00036}00036\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ loss\ function\ used\ in\ SAG\ solver.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00037}00037\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00038}00038\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00039}00039\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Specifies\ if\ a\ constant\ (a.k.a.\ bias\ or\ intercept)\ will\ be}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00040}00040\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ added\ to\ the\ decision\ function.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00041}00041\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00042}00042\ \textcolor{stringliteral}{\ \ \ \ n\_samples\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00043}00043\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ rows\ in\ X.\ Useful\ if\ is\_saga=True.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00044}00044\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00045}00045\ \textcolor{stringliteral}{\ \ \ \ is\_saga\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00046}00046\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ to\ return\ step\ size\ for\ the\ SAGA\ algorithm\ or\ the\ SAG}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00047}00047\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ algorithm.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00048}00048\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00049}00049\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00050}00050\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00051}00051\ \textcolor{stringliteral}{\ \ \ \ step\_size\ :\ float}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00052}00052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Step\ size\ used\ in\ SAG\ solver.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00053}00053\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00054}00054\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00055}00055\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00056}00056\ \textcolor{stringliteral}{\ \ \ \ Schmidt,\ M.,\ Roux,\ N.\ L.,\ \&\ Bach,\ F.\ (2013).}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00057}00057\ \textcolor{stringliteral}{\ \ \ \ Minimizing\ finite\ sums\ with\ the\ stochastic\ average\ gradient}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00058}00058\ \textcolor{stringliteral}{\ \ \ \ https://hal.inria.fr/hal-\/00860051/document}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00059}00059\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00060}00060\ \textcolor{stringliteral}{\ \ \ \ :arxiv:\`{}Defazio,\ A.,\ Bach\ F.\ \&\ Lacoste-\/Julien\ S.\ (2014).}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00061}00061\ \textcolor{stringliteral}{\ \ \ \ "{}SAGA:\ A\ Fast\ Incremental\ Gradient\ Method\ With\ Support}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00062}00062\ \textcolor{stringliteral}{\ \ \ \ for\ Non-\/Strongly\ Convex\ Composite\ Objectives"{}\ <1407.0202>\`{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00063}00063\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00064}00064\ \ \ \ \ \textcolor{keywordflow}{if}\ loss\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}log"{}},\ \textcolor{stringliteral}{"{}multinomial"{}}):}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00065}00065\ \ \ \ \ \ \ \ \ L\ =\ 0.25\ *\ (max\_squared\_sum\ +\ int(fit\_intercept))\ +\ alpha\_scaled}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00066}00066\ \ \ \ \ \textcolor{keywordflow}{elif}\ loss\ ==\ \textcolor{stringliteral}{"{}squared"{}}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ inverse\ Lipschitz\ constant\ for\ squared\ loss}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ L\ =\ max\_squared\_sum\ +\ int(fit\_intercept)\ +\ alpha\_scaled}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00069}00069\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00070}00070\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00071}00071\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Unknown\ loss\ function\ for\ SAG\ solver,\ got\ \%s\ instead\ of\ 'log'\ or\ 'squared'"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00072}00072\ \ \ \ \ \ \ \ \ \ \ \ \ \%\ loss}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00074}00074\ \ \ \ \ \textcolor{keywordflow}{if}\ is\_saga:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ SAGA\ theoretical\ step\ size\ is\ 1/3L\ or\ 1\ /\ (2\ *\ (L\ +\ mu\ n))}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ Defazio\ et\ al.\ 2014}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ mun\ =\ min(2\ *\ n\_samples\ *\ alpha\_scaled,\ L)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ step\ =\ 1.0\ /\ (2\ *\ L\ +\ mun)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00079}00079\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00080}00080\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ SAG\ theoretical\ step\ size\ is\ 1/16L\ but\ it\ is\ recommended\ to\ use\ 1\ /\ L}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ see\ http://www.birs.ca//workshops//2014/14w5003/files/schmidt.pdf,}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ slide\ 65}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ step\ =\ 1.0\ /\ L}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00084}00084\ \ \ \ \ \textcolor{keywordflow}{return}\ step}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00085}00085\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00086}00086\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00087}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag_a0b6301682ade2a002a5d95303be0fa94}{00087}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag_a0b6301682ade2a002a5d95303be0fa94}{sag\_solver}}(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00088}00088\ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00089}00089\ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00090}00090\ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00091}00091\ \ \ \ \ loss="{}log"{},}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00092}00092\ \ \ \ \ alpha=1.0,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00093}00093\ \ \ \ \ beta=0.0,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00094}00094\ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00095}00095\ \ \ \ \ tol=0.001,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00096}00096\ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00097}00097\ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00098}00098\ \ \ \ \ check\_input=True,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00099}00099\ \ \ \ \ max\_squared\_sum=None,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00100}00100\ \ \ \ \ warm\_start\_mem=None,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00101}00101\ \ \ \ \ is\_saga=False,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00102}00102\ ):}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00103}00103\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}SAG\ solver\ for\ Ridge\ and\ LogisticRegression.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00104}00104\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ SAG\ stands\ for\ Stochastic\ Average\ Gradient:\ the\ gradient\ of\ the\ loss\ is}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ estimated\ each\ sample\ at\ a\ time\ and\ the\ model\ is\ updated\ along\ the\ way\ with}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ a\ constant\ learning\ rate.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00108}00108\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00109}00109\ \textcolor{stringliteral}{\ \ \ \ IMPORTANT\ NOTE:\ 'sag'\ solver\ converges\ faster\ on\ columns\ that\ are\ on\ the}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00110}00110\ \textcolor{stringliteral}{\ \ \ \ same\ scale.\ You\ can\ normalize\ the\ data\ by\ using}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ sklearn.preprocessing.StandardScaler\ on\ your\ data\ before\ passing\ it\ to\ the}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ fit\ method.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00113}00113\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ This\ implementation\ works\ with\ data\ represented\ as\ dense\ numpy\ arrays\ or}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ sparse\ scipy\ arrays\ of\ floating\ point\ values\ for\ the\ features.\ It\ will}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ fit\ the\ data\ according\ to\ squared\ loss\ or\ log\ loss.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00117}00117\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00118}00118\ \textcolor{stringliteral}{\ \ \ \ The\ regularizer\ is\ a\ penalty\ added\ to\ the\ loss\ function\ that\ shrinks\ model}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00119}00119\ \textcolor{stringliteral}{\ \ \ \ parameters\ towards\ the\ zero\ vector\ using\ the\ squared\ euclidean\ norm\ L2.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00120}00120\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00121}00121\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 0.17}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00122}00122\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00123}00123\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00124}00124\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00125}00125\ \textcolor{stringliteral}{\ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00126}00126\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00127}00127\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00128}00128\ \textcolor{stringliteral}{\ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00129}00129\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Target\ values.\ With\ loss='multinomial',\ y\ must\ be\ label\ encoded}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00130}00130\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (see\ preprocessing.LabelEncoder).\ For\ loss='log'\ it\ must\ be\ in\ [0,\ 1].}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00131}00131\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00132}00132\ \textcolor{stringliteral}{\ \ \ \ sample\_weight\ :\ array-\/like\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00133}00133\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples\ (1.\ for\ unweighted).}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00134}00134\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00135}00135\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ \{'log',\ 'squared',\ 'multinomial'\},\ default='log'}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00136}00136\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Loss\ function\ that\ will\ be\ optimized:}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00137}00137\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'log'\ is\ the\ binary\ logistic\ loss,\ as\ used\ in\ LogisticRegression.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00138}00138\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'squared'\ is\ the\ squared\ loss,\ as\ used\ in\ Ridge.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00139}00139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'multinomial'\ is\ the\ multinomial\ logistic\ loss,\ as\ used\ in}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00140}00140\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ LogisticRegression.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00141}00141\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00142}00142\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00143}00143\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ *loss='multinomial'*}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00144}00144\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00145}00145\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=1.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00146}00146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ L2\ regularization\ term\ in\ the\ objective\ function}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00147}00147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}(0.5\ *\ alpha\ *\ ||\ W\ ||\_F\string^2)\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00148}00148\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00149}00149\ \textcolor{stringliteral}{\ \ \ \ beta\ :\ float,\ default=0.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00150}00150\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ L1\ regularization\ term\ in\ the\ objective\ function}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00151}00151\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}(beta\ *\ ||\ W\ ||\_1)\`{}\`{}.\ Only\ applied\ if\ \`{}\`{}is\_saga\`{}\`{}\ is\ set\ to\ True.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00152}00152\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00153}00153\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=1000}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00154}00154\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ max\ number\ of\ passes\ over\ the\ training\ data\ if\ the\ stopping}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00155}00155\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ criteria\ is\ not\ reached.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00156}00156\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00157}00157\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=0.001}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00158}00158\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ stopping\ criteria\ for\ the\ weights.\ The\ iterations\ will\ stop\ when}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00159}00159\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ max(change\ in\ weights)\ /\ max(weights)\ <\ tol.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00160}00160\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00161}00161\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00162}00162\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ verbosity\ level.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00163}00163\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00164}00164\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance\ or\ None,\ default=None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00165}00165\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Used\ when\ shuffling\ the\ data.\ Pass\ an\ int\ for\ reproducible\ output}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00166}00166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ across\ multiple\ function\ calls.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00167}00167\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}Glossary\ <random\_state>\`{}.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00168}00168\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00169}00169\ \textcolor{stringliteral}{\ \ \ \ check\_input\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00170}00170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ False,\ the\ input\ arrays\ X\ and\ y\ will\ not\ be\ checked.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00171}00171\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00172}00172\ \textcolor{stringliteral}{\ \ \ \ max\_squared\_sum\ :\ float,\ default=None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00173}00173\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Maximum\ squared\ sum\ of\ X\ over\ samples.\ If\ None,\ it\ will\ be\ computed,}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00174}00174\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ going\ through\ all\ the\ samples.\ The\ value\ should\ be\ precomputed}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ to\ speed\ up\ cross\ validation.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00176}00176\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ warm\_start\_mem\ :\ dict,\ default=None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initialization\ parameters\ used\ for\ warm\ starting.\ Warm\ starting\ is}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ currently\ used\ in\ LogisticRegression\ but\ not\ in\ Ridge.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ contains:}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'coef':\ the\ weight\ vector,\ with\ the\ intercept\ in\ last\ line}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00182}00182\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if\ the\ intercept\ is\ fitted.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00183}00183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'gradient\_memory':\ the\ scalar\ gradient\ for\ all\ seen\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00184}00184\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'sum\_gradient':\ the\ sum\ of\ gradient\ over\ all\ seen\ samples,}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00185}00185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ for\ each\ feature.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00186}00186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'intercept\_sum\_gradient':\ the\ sum\ of\ gradient\ over\ all\ seen}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00187}00187\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ samples,\ for\ the\ intercept.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00188}00188\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'seen':\ array\ of\ boolean\ describing\ the\ seen\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00189}00189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ -\/\ 'num\_seen':\ the\ number\ of\ seen\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00190}00190\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00191}00191\ \textcolor{stringliteral}{\ \ \ \ is\_saga\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00192}00192\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ to\ use\ the\ SAGA\ algorithm\ or\ the\ SAG\ algorithm.\ SAGA\ behaves}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00193}00193\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ better\ in\ the\ first\ epochs,\ and\ allow\ for\ l1\ regularisation.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00194}00194\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00195}00195\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00196}00196\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00197}00197\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ ndarray\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00198}00198\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weight\ vector.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00199}00199\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00200}00200\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00201}00201\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ full\ pass\ on\ all\ samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00202}00202\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00203}00203\ \textcolor{stringliteral}{\ \ \ \ warm\_start\_mem\ :\ dict}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00204}00204\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Contains\ a\ 'coef'\ key\ with\ the\ fitted\ result,\ and\ possibly\ the}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00205}00205\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ fitted\ intercept\ at\ the\ end\ of\ the\ array.\ Contains\ also\ other\ keys}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00206}00206\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ used\ for\ warm\ starting.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00207}00207\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00208}00208\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00209}00209\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00210}00210\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00211}00211\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn\ import\ linear\_model}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00212}00212\ \textcolor{stringliteral}{\ \ \ \ >>>\ n\_samples,\ n\_features\ =\ 10,\ 5}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00213}00213\ \textcolor{stringliteral}{\ \ \ \ >>>\ rng\ =\ np.random.RandomState(0)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00214}00214\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ rng.randn(n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00215}00215\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ rng.randn(n\_samples)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00216}00216\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.Ridge(solver='sag')}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00217}00217\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00218}00218\ \textcolor{stringliteral}{\ \ \ \ Ridge(solver='sag')}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00219}00219\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00220}00220\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ np.array([[-\/1,\ -\/1],\ [-\/2,\ -\/1],\ [1,\ 1],\ [2,\ 1]])}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00221}00221\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ np.array([1,\ 1,\ 2,\ 2])}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00222}00222\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.LogisticRegression(solver='sag')}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00223}00223\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00224}00224\ \textcolor{stringliteral}{\ \ \ \ LogisticRegression(solver='sag')}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00225}00225\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00226}00226\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00227}00227\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00228}00228\ \textcolor{stringliteral}{\ \ \ \ Schmidt,\ M.,\ Roux,\ N.\ L.,\ \&\ Bach,\ F.\ (2013).}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00229}00229\ \textcolor{stringliteral}{\ \ \ \ Minimizing\ finite\ sums\ with\ the\ stochastic\ average\ gradient}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00230}00230\ \textcolor{stringliteral}{\ \ \ \ https://hal.inria.fr/hal-\/00860051/document}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00231}00231\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00232}00232\ \textcolor{stringliteral}{\ \ \ \ :arxiv:\`{}Defazio,\ A.,\ Bach\ F.\ \&\ Lacoste-\/Julien\ S.\ (2014).}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00233}00233\ \textcolor{stringliteral}{\ \ \ \ "{}SAGA:\ A\ Fast\ Incremental\ Gradient\ Method\ With\ Support}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00234}00234\ \textcolor{stringliteral}{\ \ \ \ for\ Non-\/Strongly\ Convex\ Composite\ Objectives"{}\ <1407.0202>\`{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00235}00235\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00236}00236\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00237}00237\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00238}00238\ \textcolor{stringliteral}{\ \ \ \ Ridge,\ SGDRegressor,\ ElasticNet,\ Lasso,\ SVR,}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00239}00239\ \textcolor{stringliteral}{\ \ \ \ LogisticRegression,\ SGDClassifier,\ LinearSVC,\ Perceptron}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00240}00240\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00241}00241\ \ \ \ \ \textcolor{keywordflow}{if}\ warm\_start\_mem\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ warm\_start\_mem\ =\ \{\}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00243}00243\ \ \ \ \ \textcolor{comment}{\#\ Ridge\ default\ max\_iter\ is\ None}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00244}00244\ \ \ \ \ \textcolor{keywordflow}{if}\ max\_iter\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ max\_iter\ =\ 1000}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00246}00246\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00247}00247\ \ \ \ \ \textcolor{keywordflow}{if}\ check\_input:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \_dtype\ =\ [np.float64,\ np.float32]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ X\ =\ check\_array(X,\ dtype=\_dtype,\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ y\ =\ check\_array(y,\ dtype=\_dtype,\ ensure\_2d=\textcolor{keyword}{False},\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00251}00251\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00252}00252\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape[0],\ X.shape[1]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00253}00253\ \ \ \ \ \textcolor{comment}{\#\ As\ in\ SGD,\ the\ alpha\ is\ scaled\ by\ n\_samples.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00254}00254\ \ \ \ \ alpha\_scaled\ =\ float(alpha)\ /\ n\_samples}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00255}00255\ \ \ \ \ beta\_scaled\ =\ float(beta)\ /\ n\_samples}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00256}00256\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00257}00257\ \ \ \ \ \textcolor{comment}{\#\ if\ loss\ ==\ 'multinomial',\ y\ should\ be\ label\ encoded.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00258}00258\ \ \ \ \ n\_classes\ =\ int(y.max())\ +\ 1\ \textcolor{keywordflow}{if}\ loss\ ==\ \textcolor{stringliteral}{"{}multinomial"{}}\ \textcolor{keywordflow}{else}\ 1}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00259}00259\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00260}00260\ \ \ \ \ \textcolor{comment}{\#\ initialization}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00261}00261\ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00262}00262\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00263}00263\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}coef"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ coef\_init\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}coef"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00265}00265\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ assume\ fit\_intercept\ is\ False}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00267}00267\ \ \ \ \ \ \ \ \ coef\_init\ =\ np.zeros((n\_features,\ n\_classes),\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00268}00268\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00269}00269\ \ \ \ \ \textcolor{comment}{\#\ coef\_init\ contains\ possibly\ the\ intercept\_init\ at\ the\ end.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00270}00270\ \ \ \ \ \textcolor{comment}{\#\ Note\ that\ Ridge\ centers\ the\ data\ before\ fitting,\ so\ fit\_intercept=False.}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00271}00271\ \ \ \ \ fit\_intercept\ =\ coef\_init.shape[0]\ ==\ (n\_features\ +\ 1)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00272}00272\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ intercept\_init\ =\ coef\_init[-\/1,\ :]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ coef\_init\ =\ coef\_init[:-\/1,\ :]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00275}00275\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ intercept\_init\ =\ np.zeros(n\_classes,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00277}00277\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00278}00278\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}intercept\_sum\_gradient"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ intercept\_sum\_gradient\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}intercept\_sum\_gradient"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00280}00280\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00281}00281\ \ \ \ \ \ \ \ \ intercept\_sum\_gradient\ =\ np.zeros(n\_classes,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00282}00282\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00283}00283\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}gradient\_memory"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ gradient\_memory\_init\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}gradient\_memory"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00285}00285\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ gradient\_memory\_init\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ \ \ \ \ (n\_samples,\ n\_classes),\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00289}00289\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}sum\_gradient"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ sum\_gradient\_init\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}sum\_gradient"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00291}00291\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00292}00292\ \ \ \ \ \ \ \ \ sum\_gradient\_init\ =\ np.zeros((n\_features,\ n\_classes),\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00293}00293\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00294}00294\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}seen"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00295}00295\ \ \ \ \ \ \ \ \ seen\_init\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}seen"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00296}00296\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ seen\_init\ =\ np.zeros(n\_samples,\ dtype=np.int32,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00298}00298\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00299}00299\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}num\_seen"{}}\ \textcolor{keywordflow}{in}\ warm\_start\_mem.keys():}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ num\_seen\_init\ =\ warm\_start\_mem[\textcolor{stringliteral}{"{}num\_seen"{}}]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00301}00301\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ num\_seen\_init\ =\ 0}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00303}00303\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00304}00304\ \ \ \ \ dataset,\ intercept\_decay\ =\ make\_dataset(X,\ y,\ sample\_weight,\ random\_state)}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00305}00305\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00306}00306\ \ \ \ \ \textcolor{keywordflow}{if}\ max\_squared\_sum\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00307}00307\ \ \ \ \ \ \ \ \ max\_squared\_sum\ =\ row\_norms(X,\ squared=\textcolor{keyword}{True}).max()}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00308}00308\ \ \ \ \ step\_size\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__sag_af82a53319ebeaf50db3edb2c31aa1976}{get\_auto\_step\_size}}(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ max\_squared\_sum,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ alpha\_scaled,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ is\_saga=is\_saga,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00315}00315\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00316}00316\ \ \ \ \ \textcolor{keywordflow}{if}\ step\_size\ *\ alpha\_scaled\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ ZeroDivisionError(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00318}00318\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Current\ sag\ implementation\ does\ not\ handle\ "{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}the\ case\ step\_size\ *\ alpha\_scaled\ ==\ 1"{}}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00320}00320\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00321}00321\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00322}00322\ \ \ \ \ sag\ =\ sag64\ \textcolor{keywordflow}{if}\ X.dtype\ ==\ np.float64\ \textcolor{keywordflow}{else}\ sag32}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00323}00323\ \ \ \ \ num\_seen,\ n\_iter\_\ =\ sag(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ dataset,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00325}00325\ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ intercept\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ n\_samples,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00328}00328\ \ \ \ \ \ \ \ \ n\_features,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00329}00329\ \ \ \ \ \ \ \ \ n\_classes,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00330}00330\ \ \ \ \ \ \ \ \ tol,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00331}00331\ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00332}00332\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00333}00333\ \ \ \ \ \ \ \ \ step\_size,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00334}00334\ \ \ \ \ \ \ \ \ alpha\_scaled,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ beta\_scaled,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ sum\_gradient\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ gradient\_memory\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ seen\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ num\_seen\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ intercept\_sum\_gradient,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ intercept\_decay,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ is\_saga,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ verbose,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00345}00345\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00346}00346\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00347}00347\ \ \ \ \ \textcolor{keywordflow}{if}\ n\_iter\_\ ==\ max\_iter:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00348}00348\ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ max\_iter\ was\ reached\ which\ means\ the\ coef\_\ did\ not\ converge"{}},}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ \ \ \ \ ConvergenceWarning,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00351}00351\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00352}00352\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00353}00353\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00354}00354\ \ \ \ \ \ \ \ \ coef\_init\ =\ np.vstack((coef\_init,\ intercept\_init))}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00355}00355\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00356}00356\ \ \ \ \ warm\_start\_mem\ =\ \{}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00357}00357\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}coef"{}}:\ coef\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}sum\_gradient"{}}:\ sum\_gradient\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}intercept\_sum\_gradient"{}}:\ intercept\_sum\_gradient,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00360}00360\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}gradient\_memory"{}}:\ gradient\_memory\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00361}00361\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}seen"{}}:\ seen\_init,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00362}00362\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}num\_seen"{}}:\ num\_seen,}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00363}00363\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00364}00364\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00365}00365\ \ \ \ \ \textcolor{keywordflow}{if}\ loss\ ==\ \textcolor{stringliteral}{"{}multinomial"{}}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ coef\_\ =\ coef\_init.T}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00367}00367\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ coef\_\ =\ coef\_init[:,\ 0]}
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00369}00369\ }
\DoxyCodeLine{\Hypertarget{__sag_8py_source_l00370}00370\ \ \ \ \ \textcolor{keywordflow}{return}\ coef\_,\ n\_iter\_,\ warm\_start\_mem}

\end{DoxyCode}
