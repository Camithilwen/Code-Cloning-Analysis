\doxysection{\+\_\+stochastic\+\_\+gradient.\+py}
\hypertarget{__stochastic__gradient_8py_source}{}\label{__stochastic__gradient_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_stochastic\_gradient.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_stochastic\_gradient.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient}{00001}}\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00002}00002\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00004}00004\ \textcolor{stringliteral}{"{}"{}"{}Classification,\ regression\ and\ One-\/Class\ SVM\ using\ Stochastic\ Gradient}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00005}00005\ \textcolor{stringliteral}{Descent\ (SGD).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00006}00006\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00008}00008\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00009}00009\ \textcolor{keyword}{from}\ abc\ \textcolor{keyword}{import}\ ABCMeta,\ abstractmethod}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00010}00010\ \textcolor{keyword}{from}\ numbers\ \textcolor{keyword}{import}\ Integral,\ Real}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00011}00011\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00012}00012\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00014}00014\ \textcolor{keyword}{from}\ ..\_loss.\_loss\ \textcolor{keyword}{import}\ CyHalfBinomialLoss,\ CyHalfSquaredError,\ CyHuberLoss}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00015}00015\ \textcolor{keyword}{from}\ ..base\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00016}00016\ \ \ \ \ BaseEstimator,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00017}00017\ \ \ \ \ OutlierMixin,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00018}00018\ \ \ \ \ RegressorMixin,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00019}00019\ \ \ \ \ \_fit\_context,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00020}00020\ \ \ \ \ clone,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00021}00021\ \ \ \ \ is\_classifier,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00022}00022\ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00023}00023\ \textcolor{keyword}{from}\ ..exceptions\ \textcolor{keyword}{import}\ ConvergenceWarning}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00024}00024\ \textcolor{keyword}{from}\ ..model\_selection\ \textcolor{keyword}{import}\ ShuffleSplit,\ StratifiedShuffleSplit}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00025}00025\ \textcolor{keyword}{from}\ ..utils\ \textcolor{keyword}{import}\ check\_random\_state,\ compute\_class\_weight}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00026}00026\ \textcolor{keyword}{from}\ ..utils.\_param\_validation\ \textcolor{keyword}{import}\ Hidden,\ Interval,\ StrOptions}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00027}00027\ \textcolor{keyword}{from}\ ..utils.extmath\ \textcolor{keyword}{import}\ safe\_sparse\_dot}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00028}00028\ \textcolor{keyword}{from}\ ..utils.metaestimators\ \textcolor{keyword}{import}\ available\_if}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00029}00029\ \textcolor{keyword}{from}\ ..utils.multiclass\ \textcolor{keyword}{import}\ \_check\_partial\_fit\_first\_call}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00030}00030\ \textcolor{keyword}{from}\ ..utils.parallel\ \textcolor{keyword}{import}\ Parallel,\ delayed}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00031}00031\ \textcolor{keyword}{from}\ ..utils.validation\ \textcolor{keyword}{import}\ \_check\_sample\_weight,\ check\_is\_fitted,\ validate\_data}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00032}00032\ \textcolor{keyword}{from}\ .\_base\ \textcolor{keyword}{import}\ LinearClassifierMixin,\ SparseCoefMixin,\ make\_dataset}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00033}00033\ \textcolor{keyword}{from}\ .\_sgd\_fast\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00034}00034\ \ \ \ \ EpsilonInsensitive,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00035}00035\ \ \ \ \ Hinge,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00036}00036\ \ \ \ \ ModifiedHuber,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00037}00037\ \ \ \ \ SquaredEpsilonInsensitive,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00038}00038\ \ \ \ \ SquaredHinge,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00039}00039\ \ \ \ \ \_plain\_sgd32,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00040}00040\ \ \ \ \ \_plain\_sgd64,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00041}00041\ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00042}00042\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00043}00043\ LEARNING\_RATE\_TYPES\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00044}00044\ \ \ \ \ \textcolor{stringliteral}{"{}constant"{}}:\ 1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00045}00045\ \ \ \ \ \textcolor{stringliteral}{"{}optimal"{}}:\ 2,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00046}00046\ \ \ \ \ \textcolor{stringliteral}{"{}invscaling"{}}:\ 3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00047}00047\ \ \ \ \ \textcolor{stringliteral}{"{}adaptive"{}}:\ 4,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00048}00048\ \ \ \ \ \textcolor{stringliteral}{"{}pa1"{}}:\ 5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00049}00049\ \ \ \ \ \textcolor{stringliteral}{"{}pa2"{}}:\ 6,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00050}00050\ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00051}00051\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00052}00052\ PENALTY\_TYPES\ =\ \{\textcolor{stringliteral}{"{}none"{}}:\ 0,\ \textcolor{stringliteral}{"{}l2"{}}:\ 2,\ \textcolor{stringliteral}{"{}l1"{}}:\ 1,\ \textcolor{stringliteral}{"{}elasticnet"{}}:\ 3\}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00053}00053\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00054}00054\ DEFAULT\_EPSILON\ =\ 0.1}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00055}00055\ \textcolor{comment}{\#\ Default\ value\ of\ \`{}\`{}epsilon``\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00056}00056\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00057}00057\ MAX\_INT\ =\ np.iinfo(np.int32).max}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00058}00058\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00059}00059\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00060}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback}{00060}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback}{\_ValidationScoreCallback}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00061}00061\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Callback\ for\ early\ stopping\ based\ on\ validation\ score"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00062}00062\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00063}00063\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ estimator,\ X\_val,\ y\_val,\ sample\_weight\_val,\ classes=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00064}00064\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa2bc59002831b5ddafb3f2b4b402a05e}{estimator}}\ =\ clone(estimator)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00065}00065\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa2bc59002831b5ddafb3f2b4b402a05e}{estimator}}.t\_\ =\ 1\ \ \textcolor{comment}{\#\ to\ pass\ check\_is\_fitted}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00066}00066\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ classes\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa2bc59002831b5ddafb3f2b4b402a05e}{estimator}}.classes\_\ =\ classes}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa6b70b0b8970944a54a188dffbb9f654}{X\_val}}\ =\ X\_val}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00069}00069\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_a8a472e42f22cb6032ec8db9e2dd15b64}{y\_val}}\ =\ y\_val}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00070}00070\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_a48c969e326178922dc1c51df74c6286f}{sample\_weight\_val}}\ =\ sample\_weight\_val}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00071}00071\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00072}00072\ \ \ \ \ \textcolor{keyword}{def\ }\_\_call\_\_(self,\ coef,\ intercept):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ est\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa2bc59002831b5ddafb3f2b4b402a05e}{estimator}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ est.coef\_\ =\ coef.reshape(1,\ -\/1)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ est.intercept\_\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ est.score(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_aa6b70b0b8970944a54a188dffbb9f654}{X\_val}},\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_a8a472e42f22cb6032ec8db9e2dd15b64}{y\_val}},\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback_a48c969e326178922dc1c51df74c6286f}{sample\_weight\_val}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00077}00077\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00078}00078\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00079}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD}{00079}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD}{BaseSGD}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin}{SparseCoefMixin}},\ \mbox{\hyperlink{classsklearn_1_1base_1_1BaseEstimator}{BaseEstimator}},\ \mbox{\hyperlink{classmetaclass}{metaclass}}=ABCMeta):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00080}00080\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Base\ class\ for\ SGD\ classification\ and\ regression."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00081}00081\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00082}00082\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}fit\_intercept"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00084}00084\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}max\_iter"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00085}00085\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}tol"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00086}00086\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}shuffle"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00087}00087\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}verbose"{}}:\ [\textcolor{stringliteral}{"{}verbose"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00088}00088\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}random\_state"{}}:\ [\textcolor{stringliteral}{"{}random\_state"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00089}00089\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}warm\_start"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00090}00090\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}average"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}}),\ \textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00091}00091\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00092}00092\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00093}00093\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00094}00094\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00095}00095\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00096}00096\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00097}00097\ \ \ \ \ \ \ \ \ penalty="{}l2"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00098}00098\ \ \ \ \ \ \ \ \ alpha=0.0001,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00099}00099\ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00100}00100\ \ \ \ \ \ \ \ \ l1\_ratio=0.15,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00101}00101\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00102}00102\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00103}00103\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00104}00104\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00105}00105\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00106}00106\ \ \ \ \ \ \ \ \ epsilon=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00107}00107\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00108}00108\ \ \ \ \ \ \ \ \ learning\_rate="{}optimal"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00109}00109\ \ \ \ \ \ \ \ \ eta0=0.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00110}00110\ \ \ \ \ \ \ \ \ power\_t=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00111}00111\ \ \ \ \ \ \ \ \ early\_stopping=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00112}00112\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00113}00113\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00114}00114\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00115}00115\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00116}00116\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00117}00117\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}\ =\ loss}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00118}00118\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a54e5935459afe8ab0025a1a240b1306e}{penalty}}\ =\ penalty}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00119}00119\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}}\ =\ learning\_rate}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00120}00120\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a7a743eba8a379b68494f537423aee531}{epsilon}}\ =\ epsilon}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00121}00121\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b842303176fb0c02ee9039e7bf77ff5}{alpha}}\ =\ alpha}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00122}00122\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3fe523b2fd9df3a701fd5f369f8016fc}{C}}\ =\ C}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_acdd6e3b5c1a5fd7a79dd170071f264a8}{l1\_ratio}}\ =\ l1\_ratio}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a6fb4d8bcd04de6ef03e14478f3626b2e}{fit\_intercept}}\ =\ fit\_intercept}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a65505cf8c52af32b031af3b524212573}{shuffle}}\ =\ shuffle}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}}\ =\ random\_state}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a765908f4c904561675197a917fe5ca7a}{verbose}}\ =\ verbose}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00128}00128\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b55861c795a8d839caaa8bb556cba1c}{eta0}}\ =\ eta0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a634cccd56c7f3d053e23890a5b60a564}{power\_t}}\ =\ power\_t}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a35b3a2d80cb33a4e256b88c1854ee310}{early\_stopping}}\ =\ early\_stopping}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a23a01565a7aacdd692749059ab5ef2a3}{validation\_fraction}}\ =\ validation\_fraction}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00132}00132\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a13be23dfb5b6535863d8e3e1725bd0bb}{n\_iter\_no\_change}}\ =\ n\_iter\_no\_change}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_ab915396b2d26eac3638d1d0c051e0d53}{warm\_start}}\ =\ warm\_start}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ =\ average}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00135}00135\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}}\ =\ max\_iter}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00136}00136\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ =\ tol}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00137}00137\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00138}00138\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00139}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaae800d238fab5afb4371861136ee550}{00139}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaae800d238fab5afb4371861136ee550}{fit}}(self,\ X,\ y):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ model."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00141}00141\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00142}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{00142}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}(self,\ for\_partial\_fit=False):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Validate\ input\ params."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00144}00144\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.early\_stopping\ \textcolor{keywordflow}{and}\ for\_partial\_fit:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00145}00145\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}early\_stopping\ should\ be\ False\ with\ partial\_fit"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00146}00146\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00147}00147\ \ \ \ \ \ \ \ \ \ \ \ \ self.learning\_rate\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}constant"{}},\ \textcolor{stringliteral}{"{}invscaling"{}},\ \textcolor{stringliteral}{"{}adaptive"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.eta0\ <=\ 0.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00149}00149\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00150}00150\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}eta0\ must\ be\ >\ 0"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00151}00151\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.learning\_rate\ ==\ \textcolor{stringliteral}{"{}optimal"{}}\ \textcolor{keywordflow}{and}\ self.alpha\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00152}00152\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00153}00153\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}alpha\ must\ be\ >\ 0\ since\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00154}00154\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}learning\_rate\ is\ 'optimal'.\ alpha\ is\ used\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}to\ compute\ the\ optimal\ learning\ rate."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00157}00157\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.penalty\ ==\ \textcolor{stringliteral}{"{}elasticnet"{}}\ \textcolor{keywordflow}{and}\ self.l1\_ratio\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}l1\_ratio\ must\ be\ set\ when\ penalty\ is\ 'elasticnet'"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00159}00159\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ raises\ ValueError\ if\ not\ registered}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ self.\_get\_penalty\_type(self.penalty)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00162}00162\ \ \ \ \ \ \ \ \ self.\_get\_learning\_rate\_type(self.learning\_rate)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00163}00163\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00164}00164\ \ \ \ \ \textcolor{keyword}{def\ }\_get\_l1\_ratio(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.l1\_ratio\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ plain\_sgd\ expects\ a\ float.\ Any\ value\ is\ fine\ since\ at\ this\ point}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00167}00167\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ penalty\ can't\ be\ "{}elsaticnet"{}\ so\ l1\_ratio\ is\ not\ used.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ 0.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.l1\_ratio}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00170}00170\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00171}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a8273e23a4d6abfb3450dd63875cc92bf}{00171}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a8273e23a4d6abfb3450dd63875cc92bf}{\_get\_loss\_function}}(self,\ loss):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00172}00172\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Get\ concrete\ \`{}\`{}LossFunction\`{}\`{}\ object\ for\ str\ \`{}\`{}loss\`{}\`{}."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ loss\_\ =\ self.loss\_functions[loss]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00174}00174\ \ \ \ \ \ \ \ \ loss\_class,\ args\ =\ loss\_[0],\ loss\_[1:]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00175}00175\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}huber"{}},\ \textcolor{stringliteral}{"{}epsilon\_insensitive"{}},\ \textcolor{stringliteral}{"{}squared\_epsilon\_insensitive"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00176}00176\ \ \ \ \ \ \ \ \ \ \ \ \ args\ =\ (self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a7a743eba8a379b68494f537423aee531}{epsilon}},)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00177}00177\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss\_class(*args)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00178}00178\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00179}00179\ \ \ \ \ \textcolor{keyword}{def\ }\_get\_learning\_rate\_type(self,\ learning\_rate):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00180}00180\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ LEARNING\_RATE\_TYPES[learning\_rate]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00181}00181\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00182}00182\ \ \ \ \ \textcolor{keyword}{def\ }\_get\_penalty\_type(self,\ penalty):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00183}00183\ \ \ \ \ \ \ \ \ penalty\ =\ str(penalty).lower()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00184}00184\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ PENALTY\_TYPES[penalty]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00185}00185\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00186}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a24c5e64c46bf66749287ae2a8f06836f}{00186}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a24c5e64c46bf66749287ae2a8f06836f}{\_allocate\_parameter\_mem}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00187}00187\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00188}00188\ \ \ \ \ \ \ \ \ n\_classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00189}00189\ \ \ \ \ \ \ \ \ n\_features,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00190}00190\ \ \ \ \ \ \ \ \ input\_dtype,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00191}00191\ \ \ \ \ \ \ \ \ coef\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ intercept\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00193}00193\ \ \ \ \ \ \ \ \ one\_class=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00194}00194\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00195}00195\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Allocate\ mem\ for\ parameters;\ initialize\ if\ provided."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00196}00196\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\_classes\ >\ 2:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ allocate\ coef\_\ for\ multi-\/class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00198}00198\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ np.asarray(coef\_init,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init.shape\ !=\ (n\_classes,\ n\_features):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}Provided\ \`{}\`{}coef\_``\ does\ not\ match\ dataset.\ "{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ coef\_init}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00204}00204\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00205}00205\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (n\_classes,\ n\_features),\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00206}00206\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00207}00207\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00208}00208\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ allocate\ intercept\_\ for\ multi-\/class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00209}00209\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00210}00210\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init\ =\ np.asarray(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00211}00211\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init,\ order=\textcolor{stringliteral}{"{}C"{}},\ dtype=input\_dtype}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00212}00212\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00213}00213\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init.shape\ !=\ (n\_classes,):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}Provided\ intercept\_init\ does\ not\ match\ dataset."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00215}00215\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ intercept\_init}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00216}00216\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.zeros(n\_classes,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00218}00218\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00219}00219\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ allocate\ coef\_}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00221}00221\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ np.asarray(coef\_init,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ coef\_init.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00223}00223\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init.shape\ !=\ (n\_features,):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}Provided\ coef\_init\ does\ not\ match\ dataset."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00225}00225\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ coef\_init}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00226}00226\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00227}00227\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ np.zeros(n\_features,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00228}00228\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00229}00229\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ allocate\ intercept\_}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00230}00230\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00231}00231\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init\ =\ np.asarray(intercept\_init,\ dtype=input\_dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00232}00232\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init.shape\ !=\ (1,)\ \textcolor{keywordflow}{and}\ intercept\_init.shape\ !=\ ():}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00233}00233\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}Provided\ intercept\_init\ does\ not\ match\ dataset."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00234}00234\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ one\_class:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00235}00235\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ intercept\_init.reshape(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00237}00237\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00238}00238\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00239}00239\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ intercept\_init.reshape(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00240}00240\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00241}00241\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00243}00243\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ one\_class:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ np.zeros(1,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.zeros(1,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00247}00247\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ initialize\ average\ parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.shape,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ one\_class:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ 1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00258}00258\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00260}00260\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}.shape,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00262}00262\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00263}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a457a1068cea451fa2ce0e860900e6cbc}{00263}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a457a1068cea451fa2ce0e860900e6cbc}{\_make\_validation\_split}}(self,\ y,\ sample\_mask):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Split\ the\ dataset\ between\ training\ set\ and\ validation\ set.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00265}00265\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00266}00266\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00267}00267\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00268}00268\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,\ )}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00269}00269\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Target\ values.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00270}00270\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00271}00271\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_mask\ :\ ndarray\ of\ shape\ (n\_samples,\ )}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00272}00272\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ boolean\ array\ indicating\ whether\ each\ sample\ should\ be\ included}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00273}00273\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ for\ validation\ set.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00274}00274\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00275}00275\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00276}00276\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00277}00277\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\_mask\ :\ ndarray\ of\ shape\ (n\_samples,\ )}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00278}00278\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Equal\ to\ True\ on\ the\ validation\ set,\ False\ on\ the\ training\ set.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00279}00279\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ n\_samples\ =\ y.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00281}00281\ \ \ \ \ \ \ \ \ validation\_mask\ =\ np.zeros(n\_samples,\ dtype=np.bool\_)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00282}00282\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a35b3a2d80cb33a4e256b88c1854ee310}{early\_stopping}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00283}00283\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ use\ the\ full\ set\ for\ training,\ with\ an\ empty\ validation\ set}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ validation\_mask}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00285}00285\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ is\_classifier(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ \ \ \ \ splitter\_type\ =\ StratifiedShuffleSplit}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00289}00289\ \ \ \ \ \ \ \ \ \ \ \ \ splitter\_type\ =\ ShuffleSplit}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ cv\ =\ splitter\_type(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00291}00291\ \ \ \ \ \ \ \ \ \ \ \ \ test\_size=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a23a01565a7aacdd692749059ab5ef2a3}{validation\_fraction}},\ random\_state=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00292}00292\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00293}00293\ \ \ \ \ \ \ \ \ idx\_train,\ idx\_val\ =\ next(cv.split(np.zeros(shape=(y.shape[0],\ 1)),\ y))}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00294}00294\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00295}00295\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ np.any(sample\_mask[idx\_val]):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00296}00296\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ sample\ weights\ for\ validation\ set\ are\ all\ zero,\ consider\ using\ a"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00298}00298\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}\ different\ random\ state."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00300}00300\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ idx\_train.shape[0]\ ==\ 0\ \textcolor{keywordflow}{or}\ idx\_val.shape[0]\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Splitting\ \%d\ samples\ into\ a\ train\ set\ and\ a\ validation\ set\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}with\ validation\_fraction=\%r\ led\ to\ an\ empty\ set\ (\%d\ and\ \%d\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}samples).\ Please\ either\ change\ validation\_fraction,\ increase\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00306}00306\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}number\ of\ samples,\ or\ disable\ early\_stopping."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00307}00307\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00308}00308\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_samples,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a23a01565a7aacdd692749059ab5ef2a3}{validation\_fraction}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ idx\_train.shape[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ idx\_val.shape[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00314}00314\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00315}00315\ \ \ \ \ \ \ \ \ validation\_mask[idx\_val]\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ validation\_mask}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00317}00317\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00318}00318\ \ \ \ \ \textcolor{keyword}{def\ }\_make\_validation\_score\_cb(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ self,\ validation\_mask,\ X,\ y,\ sample\_weight,\ classes=None}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00320}00320\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00321}00321\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a35b3a2d80cb33a4e256b88c1854ee310}{early\_stopping}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00323}00323\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1__ValidationScoreCallback}{\_ValidationScoreCallback}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00325}00325\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ \ \ \ \ X[validation\_mask],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ \ \ \ \ y[validation\_mask],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00328}00328\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight[validation\_mask],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00329}00329\ \ \ \ \ \ \ \ \ \ \ \ \ classes=classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00330}00330\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00331}00331\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00332}00332\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00333}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a87988d7ddbe785bce180349dd55ebe49}{00333}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a87988d7ddbe785bce180349dd55ebe49}{\_prepare\_fit\_binary}}(est,\ y,\ i,\ input\_dtype,\ label\_encode=True):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00334}00334\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Initialization\ for\ fit\_binary.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00335}00335\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00336}00336\ \textcolor{stringliteral}{\ \ \ \ Returns\ y,\ coef,\ intercept,\ average\_coef,\ average\_intercept.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00337}00337\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00338}00338\ \ \ \ \ y\_i\ =\ np.ones(y.shape,\ dtype=input\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00339}00339\ \ \ \ \ \textcolor{keywordflow}{if}\ label\_encode:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ y\ in\ \{0,\ 1\}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ y\_i[y\ !=\ est.classes\_[i]]\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00342}00342\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ y\ in\ \{-\/1,\ +1\}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ y\_i[y\ !=\ est.classes\_[i]]\ =\ -\/1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00345}00345\ \ \ \ \ average\_intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00346}00346\ \ \ \ \ average\_coef\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00347}00347\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00348}00348\ \ \ \ \ \textcolor{keywordflow}{if}\ len(est.classes\_)\ ==\ 2:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ est.average:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ est.coef\_.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00351}00351\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ est.intercept\_[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00352}00352\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00353}00353\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ est.\_standard\_coef.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00354}00354\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ est.\_standard\_intercept[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00355}00355\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ est.\_average\_coef.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00356}00356\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ est.\_average\_intercept[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00357}00357\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ est.average:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ est.coef\_[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00360}00360\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ est.intercept\_[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00361}00361\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00362}00362\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ est.\_standard\_coef[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00363}00363\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ est.\_standard\_intercept[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00364}00364\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ est.\_average\_coef[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00365}00365\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ est.\_average\_intercept[i]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00366}00366\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00367}00367\ \ \ \ \ \textcolor{keywordflow}{return}\ y\_i,\ coef,\ intercept,\ average\_coef,\ average\_intercept}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00368}00368\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00369}00369\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00370}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a3fd14655251b0ded5218c7267fcd7e9f}{00370}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a3fd14655251b0ded5218c7267fcd7e9f}{fit\_binary}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00371}00371\ \ \ \ \ est,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00372}00372\ \ \ \ \ i,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00373}00373\ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00374}00374\ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00375}00375\ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00376}00376\ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00377}00377\ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00378}00378\ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00379}00379\ \ \ \ \ pos\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00380}00380\ \ \ \ \ neg\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00381}00381\ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00382}00382\ \ \ \ \ validation\_mask=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00383}00383\ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00384}00384\ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00385}00385\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ a\ single\ binary\ classifier.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00386}00386\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00387}00387\ \textcolor{stringliteral}{\ \ \ \ The\ i'th\ class\ is\ considered\ the\ "{}positive"{}\ class.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00388}00388\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00389}00389\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00390}00390\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00391}00391\ \textcolor{stringliteral}{\ \ \ \ est\ :\ Estimator\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00392}00392\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ estimator\ to\ fit}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00393}00393\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00394}00394\ \textcolor{stringliteral}{\ \ \ \ i\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00395}00395\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Index\ of\ the\ positive\ class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00396}00396\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00397}00397\ \textcolor{stringliteral}{\ \ \ \ X\ :\ numpy\ array\ or\ sparse\ matrix\ of\ shape\ [n\_samples,n\_features]}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00398}00398\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Training\ data}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00399}00399\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00400}00400\ \textcolor{stringliteral}{\ \ \ \ y\ :\ numpy\ array\ of\ shape\ [n\_samples,\ ]}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00401}00401\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Target\ values}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00402}00402\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00403}00403\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00404}00404\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ regularization\ parameter}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00405}00405\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00406}00406\ \textcolor{stringliteral}{\ \ \ \ C\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00407}00407\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Maximum\ step\ size\ for\ passive\ aggressive}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00408}00408\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00409}00409\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ str}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00410}00410\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ learning\ rate.\ Accepted\ values\ are\ 'constant',\ 'optimal',}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00411}00411\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'invscaling',\ 'pa1'\ and\ 'pa2'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00412}00412\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00413}00413\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00414}00414\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximum\ number\ of\ iterations\ (epochs)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00415}00415\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00416}00416\ \textcolor{stringliteral}{\ \ \ \ pos\_weight\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00417}00417\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weight\ of\ the\ positive\ class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00418}00418\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00419}00419\ \textcolor{stringliteral}{\ \ \ \ neg\_weight\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00420}00420\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weight\ of\ the\ negative\ class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00421}00421\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00422}00422\ \textcolor{stringliteral}{\ \ \ \ sample\_weight\ :\ numpy\ array\ of\ shape\ [n\_samples,\ ]}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00423}00423\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weight\ of\ each\ sample}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00424}00424\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00425}00425\ \textcolor{stringliteral}{\ \ \ \ validation\_mask\ :\ numpy\ array\ of\ shape\ [n\_samples,\ ],\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00426}00426\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Precomputed\ validation\ mask\ in\ case\ \_fit\_binary\ is\ called\ in\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00427}00427\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ context\ of\ a\ one-\/vs-\/rest\ reduction.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00428}00428\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00429}00429\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance,\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00430}00430\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ int,\ random\_state\ is\ the\ seed\ used\ by\ the\ random\ number\ generator;}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00431}00431\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ RandomState\ instance,\ random\_state\ is\ the\ random\ number\ generator;}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00432}00432\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ None,\ the\ random\ number\ generator\ is\ the\ RandomState\ instance\ used}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00433}00433\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ by\ \`{}np.random\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00434}00434\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00435}00435\ \ \ \ \ \textcolor{comment}{\#\ if\ average\ is\ not\ true,\ average\_coef,\ and\ average\_intercept\ will\ be}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00436}00436\ \ \ \ \ \textcolor{comment}{\#\ unused}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00437}00437\ \ \ \ \ label\_encode\ =\ isinstance(est.\_loss\_function\_,\ CyHalfBinomialLoss)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00438}00438\ \ \ \ \ y\_i,\ coef,\ intercept,\ average\_coef,\ average\_intercept\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a87988d7ddbe785bce180349dd55ebe49}{\_prepare\_fit\_binary}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ est,\ y,\ i,\ input\_dtype=X.dtype,\ label\_encode=label\_encode}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00440}00440\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00441}00441\ \ \ \ \ \textcolor{keyword}{assert}\ y\_i.shape[0]\ ==\ y.shape[0]\ ==\ sample\_weight.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00442}00442\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00443}00443\ \ \ \ \ random\_state\ =\ check\_random\_state(random\_state)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00444}00444\ \ \ \ \ dataset,\ intercept\_decay\ =\ make\_dataset(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ X,\ y\_i,\ sample\_weight,\ random\_state=random\_state}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00446}00446\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00447}00447\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00448}00448\ \ \ \ \ penalty\_type\ =\ est.\_get\_penalty\_type(est.penalty)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00449}00449\ \ \ \ \ learning\_rate\_type\ =\ est.\_get\_learning\_rate\_type(learning\_rate)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00450}00450\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00451}00451\ \ \ \ \ \textcolor{keywordflow}{if}\ validation\_mask\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ validation\_mask\ =\ est.\_make\_validation\_split(y\_i,\ sample\_mask=sample\_weight\ >\ 0)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00453}00453\ \ \ \ \ classes\ =\ np.array([-\/1,\ 1],\ dtype=y\_i.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00454}00454\ \ \ \ \ validation\_score\_cb\ =\ est.\_make\_validation\_score\_cb(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00455}00455\ \ \ \ \ \ \ \ \ validation\_mask,\ X,\ y\_i,\ sample\_weight,\ classes=classes}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00456}00456\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00457}00457\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00458}00458\ \ \ \ \ \textcolor{comment}{\#\ numpy\ mtrand\ expects\ a\ C\ long\ which\ is\ a\ signed\ 32\ bit\ integer\ under}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00459}00459\ \ \ \ \ \textcolor{comment}{\#\ Windows}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00460}00460\ \ \ \ \ seed\ =\ random\_state.randint(MAX\_INT)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00461}00461\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00462}00462\ \ \ \ \ tol\ =\ est.tol\ \textcolor{keywordflow}{if}\ est.tol\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00463}00463\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00464}00464\ \ \ \ \ \_plain\_sgd\ =\ \_get\_plain\_sgd\_function(input\_dtype=coef.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00465}00465\ \ \ \ \ coef,\ intercept,\ average\_coef,\ average\_intercept,\ n\_iter\_\ =\ \_plain\_sgd(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00466}00466\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00467}00467\ \ \ \ \ \ \ \ \ intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00468}00468\ \ \ \ \ \ \ \ \ average\_coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00469}00469\ \ \ \ \ \ \ \ \ average\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00470}00470\ \ \ \ \ \ \ \ \ est.\_loss\_function\_,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00471}00471\ \ \ \ \ \ \ \ \ penalty\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00472}00472\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00473}00473\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00474}00474\ \ \ \ \ \ \ \ \ est.\_get\_l1\_ratio(),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00475}00475\ \ \ \ \ \ \ \ \ dataset,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00476}00476\ \ \ \ \ \ \ \ \ validation\_mask,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00477}00477\ \ \ \ \ \ \ \ \ est.early\_stopping,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00478}00478\ \ \ \ \ \ \ \ \ validation\_score\_cb,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00479}00479\ \ \ \ \ \ \ \ \ int(est.n\_iter\_no\_change),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00480}00480\ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00481}00481\ \ \ \ \ \ \ \ \ tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00482}00482\ \ \ \ \ \ \ \ \ int(est.fit\_intercept),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00483}00483\ \ \ \ \ \ \ \ \ int(est.verbose),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ int(est.shuffle),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ seed,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00486}00486\ \ \ \ \ \ \ \ \ pos\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00487}00487\ \ \ \ \ \ \ \ \ neg\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ learning\_rate\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00489}00489\ \ \ \ \ \ \ \ \ est.eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00490}00490\ \ \ \ \ \ \ \ \ est.power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00491}00491\ \ \ \ \ \ \ \ \ 0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00492}00492\ \ \ \ \ \ \ \ \ est.t\_,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00493}00493\ \ \ \ \ \ \ \ \ intercept\_decay,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00494}00494\ \ \ \ \ \ \ \ \ est.average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00495}00495\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00496}00496\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00497}00497\ \ \ \ \ \textcolor{keywordflow}{if}\ est.average:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ len(est.classes\_)\ ==\ 2:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \ \ \ \ est.\_average\_intercept[0]\ =\ average\_intercept}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00501}00501\ \ \ \ \ \ \ \ \ \ \ \ \ est.\_average\_intercept[i]\ =\ average\_intercept}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00502}00502\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00503}00503\ \ \ \ \ \textcolor{keywordflow}{return}\ coef,\ intercept,\ n\_iter\_}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00504}00504\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00505}00505\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00506}00506\ \textcolor{keyword}{def\ }\_get\_plain\_sgd\_function(input\_dtype):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00507}00507\ \ \ \ \ \textcolor{keywordflow}{return}\ \_plain\_sgd32\ \textcolor{keywordflow}{if}\ input\_dtype\ ==\ np.float32\ \textcolor{keywordflow}{else}\ \_plain\_sgd64}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00508}00508\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00509}00509\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00510}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier}{00510}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier}{BaseSGDClassifier}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin}{LinearClassifierMixin}},\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD}{BaseSGD}},\ \mbox{\hyperlink{classmetaclass}{metaclass}}=ABCMeta):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00511}00511\ \ \ \ \ loss\_functions\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00512}00512\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}hinge"{}}:\ (Hinge,\ 1.0),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00513}00513\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}squared\_hinge"{}}:\ (SquaredHinge,\ 1.0),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00514}00514\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}perceptron"{}}:\ (Hinge,\ 0.0),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00515}00515\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}log\_loss"{}}:\ (CyHalfBinomialLoss,),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00516}00516\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}modified\_huber"{}}:\ (ModifiedHuber,),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00517}00517\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}squared\_error"{}}:\ (CyHalfSquaredError,),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00518}00518\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}huber"{}}:\ (CyHuberLoss,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00519}00519\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}epsilon\_insensitive"{}}:\ (EpsilonInsensitive,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00520}00520\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}squared\_epsilon\_insensitive"{}}:\ (SquaredEpsilonInsensitive,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00521}00521\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00522}00522\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00523}00523\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00524}00524\ \ \ \ \ \ \ \ \ **BaseSGD.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00525}00525\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}loss"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(set(loss\_functions))],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00526}00526\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}early\_stopping"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00527}00527\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}validation\_fraction"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ 1,\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00528}00528\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_iter\_no\_change"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00529}00529\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_jobs"{}}:\ [Integral,\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00530}00530\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}class\_weight"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}balanced"{}}\}),\ dict,\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00531}00531\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00532}00532\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00533}00533\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00534}00534\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00535}00535\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00536}00536\ \ \ \ \ \ \ \ \ loss="{}hinge"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00537}00537\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00538}00538\ \ \ \ \ \ \ \ \ penalty="{}l2"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ alpha=0.0001,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00540}00540\ \ \ \ \ \ \ \ \ l1\_ratio=0.15,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00541}00541\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00542}00542\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00543}00543\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00544}00544\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00545}00545\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00546}00546\ \ \ \ \ \ \ \ \ epsilon=DEFAULT\_EPSILON,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00547}00547\ \ \ \ \ \ \ \ \ n\_jobs=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00548}00548\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00549}00549\ \ \ \ \ \ \ \ \ learning\_rate="{}optimal"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00550}00550\ \ \ \ \ \ \ \ \ eta0=0.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00551}00551\ \ \ \ \ \ \ \ \ power\_t=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00552}00552\ \ \ \ \ \ \ \ \ early\_stopping=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00553}00553\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00554}00554\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00555}00555\ \ \ \ \ \ \ \ \ class\_weight=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00556}00556\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00557}00557\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00558}00558\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00559}00559\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00560}00560\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00561}00561\ \ \ \ \ \ \ \ \ \ \ \ \ penalty=penalty,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00562}00562\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00563}00563\ \ \ \ \ \ \ \ \ \ \ \ \ l1\_ratio=l1\_ratio,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00567}00567\ \ \ \ \ \ \ \ \ \ \ \ \ shuffle=shuffle,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00568}00568\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00569}00569\ \ \ \ \ \ \ \ \ \ \ \ \ epsilon=epsilon,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00570}00570\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00571}00571\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00572}00572\ \ \ \ \ \ \ \ \ \ \ \ \ eta0=eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00573}00573\ \ \ \ \ \ \ \ \ \ \ \ \ power\_t=power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00574}00574\ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping=early\_stopping,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00575}00575\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00576}00576\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00577}00577\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00578}00578\ \ \ \ \ \ \ \ \ \ \ \ \ average=average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00579}00579\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00580}00580\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afcaf42a7853b47b202c1de226a462ce7}{class\_weight}}\ =\ class\_weight}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00581}00581\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afce3a140475bee1914231068067a37b2}{n\_jobs}}\ =\ n\_jobs}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00582}00582\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00583}00583\ \ \ \ \ \textcolor{keyword}{def\ }\_partial\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00584}00584\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00585}00585\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00586}00586\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00587}00587\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00588}00588\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00589}00589\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00590}00590\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00591}00591\ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00592}00592\ \ \ \ \ \ \ \ \ classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00593}00593\ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00594}00594\ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00595}00595\ \ \ \ \ \ \ \ \ intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00596}00596\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00597}00597\ \ \ \ \ \ \ \ \ first\_call\ =\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}classes\_"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00598}00598\ \ \ \ \ \ \ \ \ X,\ y\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00599}00599\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00600}00600\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00601}00601\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00602}00602\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00603}00603\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=[np.float64,\ np.float32],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00604}00604\ \ \ \ \ \ \ \ \ \ \ \ \ order=\textcolor{stringliteral}{"{}C"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00605}00605\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_large\_sparse=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00606}00606\ \ \ \ \ \ \ \ \ \ \ \ \ reset=first\_call,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00607}00607\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00608}00608\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00609}00609\ \ \ \ \ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00610}00610\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00611}00611\ \ \ \ \ \ \ \ \ \_check\_partial\_fit\_first\_call(self,\ classes)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00612}00612\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00613}00613\ \ \ \ \ \ \ \ \ n\_classes\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a68471036bb7f66daf0cedfd513125f45}{classes\_}}.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00614}00614\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00615}00615\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Allocate\ datastructures\ from\ input\ arguments}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00616}00616\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a66ccf1640c975f50a0881c41e97fb25b}{\_expanded\_class\_weight}}\ =\ compute\_class\_weight(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00617}00617\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afcaf42a7853b47b202c1de226a462ce7}{class\_weight}},\ classes=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a68471036bb7f66daf0cedfd513125f45}{classes\_}},\ y=y}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00618}00618\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00619}00619\ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00620}00620\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00621}00621\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ getattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{or}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00622}00622\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a24c5e64c46bf66749287ae2a8f06836f}{\_allocate\_parameter\_mem}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00623}00623\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=n\_classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00624}00624\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00625}00625\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ input\_dtype=X.dtype,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00626}00626\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00627}00627\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00628}00628\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00629}00629\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ n\_features\ !=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.shape[-\/1]:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00630}00630\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00631}00631\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Number\ of\ features\ \%d\ does\ not\ match\ previous\ data\ \%d."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00632}00632\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ (n\_features,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.shape[-\/1])}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00633}00633\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00634}00634\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00635}00635\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a6b801f418702f3c8e9dc5df3c864cf22}{\_loss\_function\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a8273e23a4d6abfb3450dd63875cc92bf}{\_get\_loss\_function}}(loss)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00636}00636\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}t\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00637}00637\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00638}00638\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00639}00639\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ delegate\ to\ concrete\ training\ procedure}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00640}00640\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\_classes\ >\ 2:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00641}00641\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_af1d60f803d68cdee27d5b71914bfd429}{\_fit\_multiclass}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00642}00642\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00643}00643\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00644}00644\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00645}00645\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ C=C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00646}00646\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00647}00647\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00648}00648\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00649}00649\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00650}00650\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ n\_classes\ ==\ 2:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00651}00651\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_aa68d6e071fb186af611375b8a3292414}{\_fit\_binary}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00652}00652\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00653}00653\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00654}00654\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00655}00655\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ C=C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00656}00656\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00657}00657\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00658}00658\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00659}00659\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00660}00660\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00661}00661\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00662}00662\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ number\ of\ classes\ has\ to\ be\ greater\ than\ one;\ got\ \%d\ class"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00663}00663\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ n\_classes}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00664}00664\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00665}00665\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00666}00666\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00667}00667\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00668}00668\ \ \ \ \ \textcolor{keyword}{def\ }\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00669}00669\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00670}00670\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00671}00671\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00672}00672\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00673}00673\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00674}00674\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00675}00675\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00676}00676\ \ \ \ \ \ \ \ \ coef\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00677}00677\ \ \ \ \ \ \ \ \ intercept\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00678}00678\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00679}00679\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00680}00680\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}classes\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00681}00681\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ delete\ the\ attribute\ otherwise\ \_partial\_fit\ thinks\ it's\ not\ the\ first\ call}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00682}00682\ \ \ \ \ \ \ \ \ \ \ \ \ delattr(self,\ \textcolor{stringliteral}{"{}classes\_"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00683}00683\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00684}00684\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ labels\ can\ be\ encoded\ as\ float,\ int,\ or\ string\ literals}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00685}00685\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ np.unique\ sorts\ in\ asc\ order;\ largest\ class\ id\ is\ positive\ class}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00686}00686\ \ \ \ \ \ \ \ \ y\ =\ validate\_data(self,\ y=y)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00687}00687\ \ \ \ \ \ \ \ \ classes\ =\ np.unique(y)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00688}00688\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00689}00689\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_ab915396b2d26eac3638d1d0c051e0d53}{warm\_start}}\ \textcolor{keywordflow}{and}\ hasattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00690}00690\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00691}00691\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00692}00692\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00693}00693\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00694}00694\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00695}00695\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00696}00696\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00697}00697\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00698}00698\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00699}00699\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00700}00700\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00701}00701\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00702}00702\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00703}00703\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00704}00704\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Clear\ iteration\ count\ for\ multiple\ call\ to\ fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00705}00705\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00706}00706\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00707}00707\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a7845b268c0487dacb008c0cd198d29eb}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00708}00708\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00709}00709\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00710}00710\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00711}00711\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00712}00712\ \ \ \ \ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ \ \ \ \ classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00717}00717\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00718}00718\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00719}00719\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00720}00720\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00723}00723\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ >\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00724}00724\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a0474f6c395235d59a0aa0ab441476639}{n\_iter\_}}\ ==\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00726}00726\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00727}00727\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00728}00728\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Maximum\ number\ of\ iteration\ reached\ before\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00729}00729\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}convergence.\ Consider\ increasing\ max\_iter\ to\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00730}00730\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}improve\ the\ fit."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00731}00731\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00732}00732\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ConvergenceWarning,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00733}00733\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00734}00734\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00735}00735\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00736}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_aa68d6e071fb186af611375b8a3292414}{00736}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_aa68d6e071fb186af611375b8a3292414}{\_fit\_binary}}(self,\ X,\ y,\ alpha,\ C,\ sample\_weight,\ learning\_rate,\ max\_iter):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00737}00737\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ a\ binary\ classifier\ on\ X\ and\ y."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00738}00738\ \ \ \ \ \ \ \ \ coef,\ intercept,\ n\_iter\_\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__stochastic__gradient_a3fd14655251b0ded5218c7267fcd7e9f}{fit\_binary}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00739}00739\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00740}00740\ \ \ \ \ \ \ \ \ \ \ \ \ 1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00741}00741\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00742}00742\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00743}00743\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00744}00744\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00745}00745\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00746}00746\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00747}00747\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a66ccf1640c975f50a0881c41e97fb25b}{\_expanded\_class\_weight}}[1],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00748}00748\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a66ccf1640c975f50a0881c41e97fb25b}{\_expanded\_class\_weight}}[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00749}00749\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00750}00750\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00751}00751\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00752}00752\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00753}00753\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ +=\ n\_iter\_\ *\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00754}00754\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a0474f6c395235d59a0aa0ab441476639}{n\_iter\_}}\ =\ n\_iter\_}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00755}00755\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00756}00756\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ need\ to\ be\ 2d}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00757}00757\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00758}00758\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ <=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ -\/\ 1:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00759}00759\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}.reshape(1,\ -\/1)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00760}00760\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00761}00761\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00762}00762\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}.reshape(1,\ -\/1)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00763}00763\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00764}00764\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00765}00765\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00766}00766\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ coef.reshape(1,\ -\/1)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00767}00767\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ intercept\ is\ a\ float,\ need\ to\ convert\ it\ to\ an\ array\ of\ length\ 1}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00768}00768\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00769}00769\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00770}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_af1d60f803d68cdee27d5b71914bfd429}{00770}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_af1d60f803d68cdee27d5b71914bfd429}{\_fit\_multiclass}}(self,\ X,\ y,\ alpha,\ C,\ learning\_rate,\ sample\_weight,\ max\_iter):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00771}00771\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ a\ multi-\/class\ classifier\ by\ combining\ binary\ classifiers}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00772}00772\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00773}00773\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Each\ binary\ classifier\ predicts\ one\ class\ versus\ all\ others.\ This}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00774}00774\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ strategy\ is\ called\ OvA\ (One\ versus\ All)\ or\ OvR\ (One\ versus\ Rest).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00775}00775\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00776}00776\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Precompute\ the\ validation\ split\ using\ the\ multiclass\ labels}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00777}00777\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ to\ ensure\ proper\ balancing\ of\ the\ classes.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00778}00778\ \ \ \ \ \ \ \ \ validation\_mask\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a457a1068cea451fa2ce0e860900e6cbc}{\_make\_validation\_split}}(y,\ sample\_mask=sample\_weight\ >\ 0)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00779}00779\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00780}00780\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Use\ joblib\ to\ fit\ OvA\ in\ parallel.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00781}00781\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Pick\ the\ random\ seed\ for\ each\ job\ outside\ of\ fit\_binary\ to\ avoid}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00782}00782\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ sharing\ the\ estimator\ random\ state\ between\ threads\ which\ could\ lead}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00783}00783\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ to\ non-\/deterministic\ behavior}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00784}00784\ \ \ \ \ \ \ \ \ random\_state\ =\ check\_random\_state(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00785}00785\ \ \ \ \ \ \ \ \ seeds\ =\ random\_state.randint(MAX\_INT,\ size=len(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a68471036bb7f66daf0cedfd513125f45}{classes\_}}))}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00786}00786\ \ \ \ \ \ \ \ \ result\ =\ \mbox{\hyperlink{classsklearn_1_1utils_1_1parallel_1_1Parallel}{Parallel}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00787}00787\ \ \ \ \ \ \ \ \ \ \ \ \ n\_jobs=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afce3a140475bee1914231068067a37b2}{n\_jobs}},\ verbose=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a765908f4c904561675197a917fe5ca7a}{verbose}},\ require=\textcolor{stringliteral}{"{}sharedmem"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00788}00788\ \ \ \ \ \ \ \ \ )(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00789}00789\ \ \ \ \ \ \ \ \ \ \ \ \ delayed(fit\_binary)(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00790}00790\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00791}00791\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ i,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00792}00792\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00793}00793\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00794}00794\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00795}00795\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00796}00796\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00797}00797\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00798}00798\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a66ccf1640c975f50a0881c41e97fb25b}{\_expanded\_class\_weight}}[i],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00799}00799\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00800}00800\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00801}00801\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ validation\_mask=validation\_mask,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00802}00802\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=seed,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00803}00803\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00804}00804\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i,\ seed\ \textcolor{keywordflow}{in}\ enumerate(seeds)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00805}00805\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00806}00806\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00807}00807\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ take\ the\ maximum\ of\ n\_iter\_\ over\ every\ binary\ fit}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00808}00808\ \ \ \ \ \ \ \ \ n\_iter\_\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00809}00809\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i,\ (\_,\ intercept,\ n\_iter\_i)\ \textcolor{keywordflow}{in}\ enumerate(result):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00810}00810\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}[i]\ =\ intercept}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00811}00811\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_\ =\ max(n\_iter\_,\ n\_iter\_i)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00812}00812\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00813}00813\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ +=\ n\_iter\_\ *\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00814}00814\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a0474f6c395235d59a0aa0ab441476639}{n\_iter\_}}\ =\ n\_iter\_}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00815}00815\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00816}00816\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00817}00817\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ <=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a624a066b750b21fd1ac1feeb3862f3b0}{t\_}}\ -\/\ 1.0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00818}00818\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00819}00819\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00820}00820\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00821}00821\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00822}00822\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ np.atleast\_1d(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00824}00824\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00825}00825\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00826}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a8fc27c9caf1e85cda6f1702d69b487be}{00826}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a8fc27c9caf1e85cda6f1702d69b487be}{partial\_fit}}(self,\ X,\ y,\ classes=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00827}00827\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Perform\ one\ epoch\ of\ stochastic\ gradient\ descent\ on\ given\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00828}00828\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00829}00829\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Internally,\ this\ method\ uses\ \`{}\`{}max\_iter\ =\ 1\`{}\`{}.\ Therefore,\ it\ is\ not}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00830}00830\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ guaranteed\ that\ a\ minimum\ of\ the\ cost\ function\ is\ reached\ after\ calling}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00831}00831\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ it\ once.\ Matters\ such\ as\ objective\ convergence,\ early\ stopping,\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00832}00832\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learning\ rate\ adjustments\ should\ be\ handled\ by\ the\ user.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00833}00833\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00834}00834\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00835}00835\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00836}00836\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00837}00837\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Subset\ of\ the\ training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00838}00838\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00839}00839\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00840}00840\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Subset\ of\ the\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00841}00841\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00842}00842\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ classes\ :\ ndarray\ of\ shape\ (n\_classes,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00843}00843\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Classes\ across\ all\ calls\ to\ partial\_fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00844}00844\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Can\ be\ obtained\ by\ via\ \`{}np.unique(y\_all)\`{},\ where\ y\_all\ is\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00845}00845\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ target\ vector\ of\ the\ entire\ dataset.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00846}00846\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ This\ argument\ is\ required\ for\ the\ first\ call\ to\ partial\_fit}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00847}00847\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ and\ can\ be\ omitted\ in\ the\ subsequent\ calls.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00848}00848\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Note\ that\ y\ doesn't\ need\ to\ contain\ all\ labels\ in\ \`{}classes\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00849}00849\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00850}00850\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00851}00851\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00852}00852\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ not\ provided,\ uniform\ weights\ are\ assumed.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00853}00853\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00854}00854\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00855}00855\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00856}00856\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00857}00857\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ an\ instance\ of\ self.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00858}00858\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00859}00859\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}classes\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00860}00860\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}(for\_partial\_fit=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00861}00861\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00862}00862\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afcaf42a7853b47b202c1de226a462ce7}{class\_weight}}\ ==\ \textcolor{stringliteral}{"{}balanced"{}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00863}00863\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00864}00864\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}class\_weight\ '\{0\}'\ is\ not\ supported\ for\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00865}00865\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}partial\_fit.\ In\ order\ to\ use\ 'balanced'\ weights,"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00866}00866\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}\ use\ compute\_class\_weight('\{0\}',\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00867}00867\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}classes=classes,\ y=y).\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00868}00868\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}In\ place\ of\ y\ you\ can\ use\ a\ large\ enough\ sample\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00869}00869\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}of\ the\ full\ training\ set\ target\ to\ properly\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00870}00870\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}estimate\ the\ class\ frequency\ distributions.\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00871}00871\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Pass\ the\ resulting\ weights\ as\ the\ class\_weight\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00872}00872\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}parameter."{}}.format(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_afcaf42a7853b47b202c1de226a462ce7}{class\_weight}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00873}00873\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00874}00874\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a7845b268c0487dacb008c0cd198d29eb}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00878}00878\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b842303176fb0c02ee9039e7bf77ff5}{alpha}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00879}00879\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00880}00880\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00881}00881\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00882}00882\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00883}00883\ \ \ \ \ \ \ \ \ \ \ \ \ classes=classes,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00884}00884\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00885}00885\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00886}00886\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00887}00887\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00888}00888\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00889}00889\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00890}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a8f044a5df9eff3779fd38ba6c21b5123}{00890}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_a8f044a5df9eff3779fd38ba6c21b5123}{fit}}(self,\ X,\ y,\ coef\_init=None,\ intercept\_init=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00891}00891\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ linear\ model\ with\ Stochastic\ Gradient\ Descent.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00892}00892\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00893}00893\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00894}00894\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00895}00895\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00896}00896\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00897}00897\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00898}00898\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00899}00899\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Target\ values.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00900}00900\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00901}00901\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\_init\ :\ ndarray\ of\ shape\ (n\_classes,\ n\_features),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00902}00902\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ coefficients\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00903}00903\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00904}00904\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_init\ :\ ndarray\ of\ shape\ (n\_classes,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00905}00905\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ intercept\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00906}00906\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00907}00907\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00908}00908\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00909}00909\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ not\ provided,\ uniform\ weights\ are\ assumed.\ These\ weights\ will}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00910}00910\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ be\ multiplied\ with\ class\_weight\ (passed\ through\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00911}00911\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ constructor)\ if\ class\_weight\ is\ specified.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00912}00912\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00913}00913\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00914}00914\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00915}00915\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00916}00916\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ an\ instance\ of\ self.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00917}00917\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00918}00918\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00919}00919\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00920}00920\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier_af957503d3362240c9468ccbe4402f35e}{\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00921}00921\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00922}00922\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00923}00923\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b842303176fb0c02ee9039e7bf77ff5}{alpha}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00924}00924\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00925}00925\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00926}00926\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00927}00927\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00928}00928\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00929}00929\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00930}00930\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00931}00931\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00932}00932\ \ \ \ \ \textcolor{keyword}{def\ }\_\_sklearn\_tags\_\_(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00933}00933\ \ \ \ \ \ \ \ \ tags\ =\ super().\_\_sklearn\_tags\_\_()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00934}00934\ \ \ \ \ \ \ \ \ tags.input\_tags.sparse\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00935}00935\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ tags}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00936}00936\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00937}00937\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00938}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier}{00938}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier}{SGDClassifier}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDClassifier}{BaseSGDClassifier}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00939}00939\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Linear\ classifiers\ (SVM,\ logistic\ regression,\ etc.)\ with\ SGD\ training.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00940}00940\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00941}00941\ \textcolor{stringliteral}{\ \ \ \ This\ estimator\ implements\ regularized\ linear\ models\ with\ stochastic}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00942}00942\ \textcolor{stringliteral}{\ \ \ \ gradient\ descent\ (SGD)\ learning:\ the\ gradient\ of\ the\ loss\ is\ estimated}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00943}00943\ \textcolor{stringliteral}{\ \ \ \ each\ sample\ at\ a\ time\ and\ the\ model\ is\ updated\ along\ the\ way\ with\ a}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00944}00944\ \textcolor{stringliteral}{\ \ \ \ decreasing\ strength\ schedule\ (aka\ learning\ rate).\ SGD\ allows\ minibatch}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00945}00945\ \textcolor{stringliteral}{\ \ \ \ (online/out-\/of-\/core)\ learning\ via\ the\ \`{}partial\_fit\`{}\ method.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00946}00946\ \textcolor{stringliteral}{\ \ \ \ For\ best\ results\ using\ the\ default\ learning\ rate\ schedule,\ the\ data\ should}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00947}00947\ \textcolor{stringliteral}{\ \ \ \ have\ zero\ mean\ and\ unit\ variance.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00948}00948\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00949}00949\ \textcolor{stringliteral}{\ \ \ \ This\ implementation\ works\ with\ data\ represented\ as\ dense\ or\ sparse\ arrays}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00950}00950\ \textcolor{stringliteral}{\ \ \ \ of\ floating\ point\ values\ for\ the\ features.\ The\ model\ it\ fits\ can\ be}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00951}00951\ \textcolor{stringliteral}{\ \ \ \ controlled\ with\ the\ loss\ parameter;\ by\ default,\ it\ fits\ a\ linear\ support}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00952}00952\ \textcolor{stringliteral}{\ \ \ \ vector\ machine\ (SVM).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00953}00953\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00954}00954\ \textcolor{stringliteral}{\ \ \ \ The\ regularizer\ is\ a\ penalty\ added\ to\ the\ loss\ function\ that\ shrinks\ model}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00955}00955\ \textcolor{stringliteral}{\ \ \ \ parameters\ towards\ the\ zero\ vector\ using\ either\ the\ squared\ euclidean\ norm}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00956}00956\ \textcolor{stringliteral}{\ \ \ \ L2\ or\ the\ absolute\ norm\ L1\ or\ a\ combination\ of\ both\ (Elastic\ Net).\ If\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00957}00957\ \textcolor{stringliteral}{\ \ \ \ parameter\ update\ crosses\ the\ 0.0\ value\ because\ of\ the\ regularizer,\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00958}00958\ \textcolor{stringliteral}{\ \ \ \ update\ is\ truncated\ to\ 0.0\ to\ allow\ for\ learning\ sparse\ models\ and\ achieve}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00959}00959\ \textcolor{stringliteral}{\ \ \ \ online\ feature\ selection.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00960}00960\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00961}00961\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <sgd>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00962}00962\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00963}00963\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00964}00964\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00965}00965\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ \{'hinge',\ 'log\_loss',\ 'modified\_huber',\ 'squared\_hinge',\(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00966}00966\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'perceptron',\ 'squared\_error',\ 'huber',\ 'epsilon\_insensitive',\(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00967}00967\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'squared\_epsilon\_insensitive'\},\ default='hinge'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00968}00968\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ loss\ function\ to\ be\ used.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00969}00969\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00970}00970\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'hinge'\ gives\ a\ linear\ SVM.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00971}00971\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'log\_loss'\ gives\ logistic\ regression,\ a\ probabilistic\ classifier.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00972}00972\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'modified\_huber'\ is\ another\ smooth\ loss\ that\ brings\ tolerance\ to}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00973}00973\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ outliers\ as\ well\ as\ probability\ estimates.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00974}00974\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'squared\_hinge'\ is\ like\ hinge\ but\ is\ quadratically\ penalized.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00975}00975\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'perceptron'\ is\ the\ linear\ loss\ used\ by\ the\ perceptron\ algorithm.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00976}00976\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ The\ other\ losses,\ 'squared\_error',\ 'huber',\ 'epsilon\_insensitive'\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00977}00977\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ 'squared\_epsilon\_insensitive'\ are\ designed\ for\ regression\ but\ can\ be\ useful}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00978}00978\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ in\ classification\ as\ well;\ see}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00979}00979\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ :class:\`{}\string~sklearn.linear\_model.SGDRegressor\`{}\ for\ a\ description.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00980}00980\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00981}00981\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ More\ details\ about\ the\ losses\ formulas\ can\ be\ found\ in\ the\ :ref:\`{}User\ Guide}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00982}00982\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ <sgd\_mathematical\_formulation>\`{}\ and\ you\ can\ find\ a\ visualisation\ of\ the\ loss}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00983}00983\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ functions\ in}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00984}00984\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_loss\_functions.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00985}00985\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00986}00986\ \textcolor{stringliteral}{\ \ \ \ penalty\ :\ \{'l2',\ 'l1',\ 'elasticnet',\ None\},\ default='l2'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00987}00987\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ penalty\ (aka\ regularization\ term)\ to\ be\ used.\ Defaults\ to\ 'l2'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00988}00988\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ which\ is\ the\ standard\ regularizer\ for\ linear\ SVM\ models.\ 'l1'\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00989}00989\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'elasticnet'\ might\ bring\ sparsity\ to\ the\ model\ (feature\ selection)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00990}00990\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ not\ achievable\ with\ 'l2'.\ No\ penalty\ is\ added\ when\ set\ to\ \`{}None\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00991}00991\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00992}00992\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ You\ can\ see\ a\ visualisation\ of\ the\ penalties\ in}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00993}00993\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_penalties.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00994}00994\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00995}00995\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=0.0001}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00996}00996\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ regularization\ term.\ The\ higher\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00997}00997\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ value,\ the\ stronger\ the\ regularization.\ Also\ used\ to\ compute\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00998}00998\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learning\ rate\ when\ \`{}learning\_rate\`{}\ is\ set\ to\ 'optimal'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l00999}00999\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01000}01000\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01001}01001\ \textcolor{stringliteral}{\ \ \ \ l1\_ratio\ :\ float,\ default=0.15}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01002}01002\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ Elastic\ Net\ mixing\ parameter,\ with\ 0\ <=\ l1\_ratio\ <=\ 1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01003}01003\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l1\_ratio=0\ corresponds\ to\ L2\ penalty,\ l1\_ratio=1\ to\ L1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01004}01004\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}penalty\`{}\ is\ 'elasticnet'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01005}01005\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ 1.0]\`{}\ or\ can\ be\ \`{}None\`{}\ if}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01006}01006\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}penalty\`{}\ is\ not\ \`{}elasticnet\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01007}01007\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01008}01008\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 1.7}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01009}01009\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}l1\_ratio\`{}\ can\ be\ \`{}None\`{}\ when\ \`{}penalty\`{}\ is\ not\ "{}elasticnet"{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01010}01010\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01011}01011\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01012}01012\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ the\ intercept\ should\ be\ estimated\ or\ not.\ If\ False,\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01013}01013\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ data\ is\ assumed\ to\ be\ already\ centered.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01014}01014\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01015}01015\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=1000}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01016}01016\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximum\ number\ of\ passes\ over\ the\ training\ data\ (aka\ epochs).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01017}01017\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ only\ impacts\ the\ behavior\ in\ the\ \`{}\`{}fit\`{}\`{}\ method,\ and\ not\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01018}01018\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :meth:\`{}partial\_fit\`{}\ method.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01019}01019\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01020}01020\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01021}01021\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01022}01022\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01023}01023\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float\ or\ None,\ default=1e-\/3}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01024}01024\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ stopping\ criterion.\ If\ it\ is\ not\ None,\ training\ will\ stop}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01025}01025\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ when\ (loss\ >\ best\_loss\ -\/\ tol)\ for\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ consecutive}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01026}01026\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ epochs.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01027}01027\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Convergence\ is\ checked\ against\ the\ training\ loss\ or\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01028}01028\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ loss\ depending\ on\ the\ \`{}early\_stopping\`{}\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01029}01029\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01030}01030\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01031}01031\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01032}01032\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01033}01033\ \textcolor{stringliteral}{\ \ \ \ shuffle\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01034}01034\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ or\ not\ the\ training\ data\ should\ be\ shuffled\ after\ each\ epoch.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01035}01035\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01036}01036\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01037}01037\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ verbosity\ level.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01038}01038\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01039}01039\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01040}01040\ \textcolor{stringliteral}{\ \ \ \ epsilon\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01041}01041\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Epsilon\ in\ the\ epsilon-\/insensitive\ loss\ functions;\ only\ if\ \`{}loss\`{}\ is}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01042}01042\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'huber',\ 'epsilon\_insensitive',\ or\ 'squared\_epsilon\_insensitive'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01043}01043\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ 'huber',\ determines\ the\ threshold\ at\ which\ it\ becomes\ less}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01044}01044\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ important\ to\ get\ the\ prediction\ exactly\ right.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01045}01045\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ epsilon-\/insensitive,\ any\ differences\ between\ the\ current\ prediction}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01046}01046\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ the\ correct\ label\ are\ ignored\ if\ they\ are\ less\ than\ this\ threshold.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01047}01047\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01048}01048\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01049}01049\ \textcolor{stringliteral}{\ \ \ \ n\_jobs\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01050}01050\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ CPUs\ to\ use\ to\ do\ the\ OVA\ (One\ Versus\ All,\ for}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01051}01051\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ multi-\/class\ problems)\ computation.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01052}01052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}None\`{}\`{}\ means\ 1\ unless\ in\ a\ :obj:\`{}joblib.parallel\_backend\`{}\ context.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01053}01053\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}-\/1\`{}\`{}\ means\ using\ all\ processors.\ See\ :term:\`{}Glossary\ <n\_jobs>\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01054}01054\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ more\ details.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01055}01055\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01056}01056\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance,\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01057}01057\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Used\ for\ shuffling\ the\ data,\ when\ \`{}\`{}shuffle\`{}\`{}\ is\ set\ to\ \`{}\`{}True\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01058}01058\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Pass\ an\ int\ for\ reproducible\ output\ across\ multiple\ function\ calls.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01059}01059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}Glossary\ <random\_state>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01060}01060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Integer\ values\ must\ be\ in\ the\ range\ \`{}[0,\ 2**32\ -\/\ 1]\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01061}01061\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01062}01062\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ str,\ default='optimal'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01063}01063\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ learning\ rate\ schedule:}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01064}01064\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01065}01065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'constant':\ \`{}eta\ =\ eta0\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01066}01066\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'optimal':\ \`{}eta\ =\ 1.0\ /\ (alpha\ *\ (t\ +\ t0))\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01067}01067\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ where\ \`{}t0\`{}\ is\ chosen\ by\ a\ heuristic\ proposed\ by\ Leon\ Bottou.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01068}01068\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'invscaling':\ \`{}eta\ =\ eta0\ /\ pow(t,\ power\_t)\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01069}01069\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'adaptive':\ \`{}eta\ =\ eta0\`{},\ as\ long\ as\ the\ training\ keeps\ decreasing.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01070}01070\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ Each\ time\ n\_iter\_no\_change\ consecutive\ epochs\ fail\ to\ decrease\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01071}01071\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ training\ loss\ by\ tol\ or\ fail\ to\ increase\ validation\ score\ by\ tol\ if}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01072}01072\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \`{}early\_stopping\`{}\ is\ \`{}True\`{},\ the\ current\ learning\ rate\ is\ divided\ by\ 5.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01073}01073\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01074}01074\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01075}01075\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'adaptive'\ option.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01076}01076\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01077}01077\ \textcolor{stringliteral}{\ \ \ \ eta0\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01078}01078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ for\ the\ 'constant',\ 'invscaling'\ or}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01079}01079\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'adaptive'\ schedules.\ The\ default\ value\ is\ 0.0\ as\ eta0\ is\ not\ used\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01080}01080\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ default\ schedule\ 'optimal'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01081}01081\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01082}01082\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01083}01083\ \textcolor{stringliteral}{\ \ \ \ power\_t\ :\ float,\ default=0.5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01084}01084\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ exponent\ for\ inverse\ scaling\ learning\ rate.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01085}01085\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(-\/inf,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01086}01086\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01087}01087\ \textcolor{stringliteral}{\ \ \ \ early\_stopping\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01088}01088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ to\ use\ early\ stopping\ to\ terminate\ training\ when\ validation}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01089}01089\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\ is\ not\ improving.\ If\ set\ to\ \`{}True\`{},\ it\ will\ automatically\ set\ aside}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01090}01090\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ a\ stratified\ fraction\ of\ training\ data\ as\ validation\ and\ terminate}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01091}01091\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ training\ when\ validation\ score\ returned\ by\ the\ \`{}score\`{}\ method\ is\ not}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01092}01092\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ improving\ by\ at\ least\ tol\ for\ n\_iter\_no\_change\ consecutive\ epochs.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01093}01093\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01094}01094\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :ref:\`{}sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_early\_stopping.py\`{}\ for\ an}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01095}01095\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ example\ of\ the\ effects\ of\ early\ stopping.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01096}01096\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01097}01097\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01098}01098\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'early\_stopping'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01099}01099\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01100}01100\ \textcolor{stringliteral}{\ \ \ \ validation\_fraction\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01101}01101\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ proportion\ of\ training\ data\ to\ set\ aside\ as\ validation\ set\ for}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01102}01102\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ early\ stopping.\ Must\ be\ between\ 0\ and\ 1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01103}01103\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}early\_stopping\`{}\ is\ True.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01104}01104\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01105}01105\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01106}01106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01107}01107\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'validation\_fraction'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01108}01108\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01109}01109\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_no\_change\ :\ int,\ default=5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01110}01110\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ iterations\ with\ no\ improvement\ to\ wait\ before\ stopping}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01111}01111\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ fitting.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01112}01112\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Convergence\ is\ checked\ against\ the\ training\ loss\ or\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01113}01113\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ loss\ depending\ on\ the\ \`{}early\_stopping\`{}\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01114}01114\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Integer\ values\ must\ be\ in\ the\ range\ \`{}[1,\ max\_iter)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01115}01115\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01116}01116\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01117}01117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'n\_iter\_no\_change'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01118}01118\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01119}01119\ \textcolor{stringliteral}{\ \ \ \ class\_weight\ :\ dict,\ \{class\_label:\ weight\}\ or\ "{}balanced"{},\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01120}01120\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Preset\ for\ the\ class\_weight\ fit\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01121}01121\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01122}01122\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weights\ associated\ with\ classes.\ If\ not\ given,\ all\ classes}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01123}01123\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ are\ supposed\ to\ have\ weight\ one.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01124}01124\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01125}01125\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ "{}balanced"{}\ mode\ uses\ the\ values\ of\ y\ to\ automatically\ adjust}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01126}01126\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ weights\ inversely\ proportional\ to\ class\ frequencies\ in\ the\ input\ data}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01127}01127\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ as\ \`{}\`{}n\_samples\ /\ (n\_classes\ *\ np.bincount(y))\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01128}01128\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01129}01129\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01130}01130\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ True,\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ fit\ as}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01131}01131\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ initialization,\ otherwise,\ just\ erase\ the\ previous\ solution.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01132}01132\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}the\ Glossary\ <warm\_start>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01133}01133\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01134}01134\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Repeatedly\ calling\ fit\ or\ partial\_fit\ when\ warm\_start\ is\ True\ can}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01135}01135\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ result\ in\ a\ different\ solution\ than\ when\ calling\ fit\ a\ single\ time}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01136}01136\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ because\ of\ the\ way\ the\ data\ is\ shuffled.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01137}01137\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ a\ dynamic\ learning\ rate\ is\ used,\ the\ learning\ rate\ is\ adapted}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01138}01138\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ depending\ on\ the\ number\ of\ samples\ already\ seen.\ Calling\ \`{}\`{}fit\`{}\`{}\ resets}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01139}01139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ this\ counter,\ while\ \`{}\`{}partial\_fit\`{}\`{}\ will\ result\ in\ increasing\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01140}01140\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ existing\ counter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01141}01141\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01142}01142\ \textcolor{stringliteral}{\ \ \ \ average\ :\ bool\ or\ int,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01143}01143\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ \`{}True\`{},\ computes\ the\ averaged\ SGD\ weights\ across\ all}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01144}01144\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ updates\ and\ stores\ the\ result\ in\ the\ \`{}\`{}coef\_\`{}\`{}\ attribute.\ If\ set\ to}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01145}01145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ an\ int\ greater\ than\ 1,\ averaging\ will\ begin\ once\ the\ total\ number\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01146}01146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples\ seen\ reaches\ \`{}average\`{}.\ So\ \`{}\`{}average=10\`{}\`{}\ will\ begin}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01147}01147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ averaging\ after\ seeing\ 10\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01148}01148\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Integer\ values\ must\ be\ in\ the\ range\ \`{}[1,\ n\_samples]\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01149}01149\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01150}01150\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01151}01151\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01152}01152\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ ndarray\ of\ shape\ (1,\ n\_features)\ if\ n\_classes\ ==\ 2\ else\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01153}01153\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01154}01154\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weights\ assigned\ to\ the\ features.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01155}01155\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01156}01156\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ ndarray\ of\ shape\ (1,)\ if\ n\_classes\ ==\ 2\ else\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01157}01157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constants\ in\ decision\ function.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01158}01158\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01159}01159\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01160}01160\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ actual\ number\ of\ iterations\ before\ reaching\ the\ stopping\ criterion.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01161}01161\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ multiclass\ fits,\ it\ is\ the\ maximum\ over\ every\ binary\ fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01162}01162\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01163}01163\ \textcolor{stringliteral}{\ \ \ \ classes\_\ :\ array\ of\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01164}01164\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01165}01165\ \textcolor{stringliteral}{\ \ \ \ t\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01166}01166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ weight\ updates\ performed\ during\ training.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01167}01167\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Same\ as\ \`{}\`{}(n\_iter\_\ *\ n\_samples\ +\ 1)\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01168}01168\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01169}01169\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01170}01170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01171}01171\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01172}01172\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01173}01173\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01174}01174\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01175}01175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01176}01176\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01177}01177\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01178}01178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01179}01179\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01180}01180\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01181}01181\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01182}01182\ \textcolor{stringliteral}{\ \ \ \ sklearn.svm.LinearSVC\ :\ Linear\ support\ vector\ classification.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01183}01183\ \textcolor{stringliteral}{\ \ \ \ LogisticRegression\ :\ Logistic\ regression.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01184}01184\ \textcolor{stringliteral}{\ \ \ \ Perceptron\ :\ Inherits\ from\ SGDClassifier.\ \`{}\`{}Perceptron()\`{}\`{}\ is\ equivalent\ to}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01185}01185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}SGDClassifier(loss="{}perceptron"{},\ eta0=1,\ learning\_rate="{}constant"{},}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01186}01186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ penalty=None)\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01187}01187\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01188}01188\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01189}01189\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01190}01190\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01191}01191\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.linear\_model\ import\ SGDClassifier}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01192}01192\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.preprocessing\ import\ StandardScaler}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01193}01193\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.pipeline\ import\ make\_pipeline}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01194}01194\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ np.array([[-\/1,\ -\/1],\ [-\/2,\ -\/1],\ [1,\ 1],\ [2,\ 1]])}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01195}01195\ \textcolor{stringliteral}{\ \ \ \ >>>\ Y\ =\ np.array([1,\ 1,\ 2,\ 2])}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01196}01196\ \textcolor{stringliteral}{\ \ \ \ >>>\ \#\ Always\ scale\ the\ input.\ The\ most\ convenient\ way\ is\ to\ use\ a\ pipeline.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01197}01197\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ make\_pipeline(StandardScaler(),}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01198}01198\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ SGDClassifier(max\_iter=1000,\ tol=1e-\/3))}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01199}01199\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ Y)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01200}01200\ \textcolor{stringliteral}{\ \ \ \ Pipeline(steps=[('standardscaler',\ StandardScaler()),}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01201}01201\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ('sgdclassifier',\ SGDClassifier())])}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01202}01202\ \textcolor{stringliteral}{\ \ \ \ >>>\ print(clf.predict([[-\/0.8,\ -\/1]]))}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01203}01203\ \textcolor{stringliteral}{\ \ \ \ [1]}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01204}01204\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01205}01205\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01206}01206\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01207}01207\ \ \ \ \ \ \ \ \ **BaseSGDClassifier.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01208}01208\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}penalty"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}l2"{}},\ \textcolor{stringliteral}{"{}l1"{}},\ \textcolor{stringliteral}{"{}elasticnet"{}}\}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01209}01209\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}alpha"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01210}01210\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}l1\_ratio"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ 1,\ closed=\textcolor{stringliteral}{"{}both"{}}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01211}01211\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}power\_t"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01212}01212\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}epsilon"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01213}01213\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}learning\_rate"{}}:\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01214}01214\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}constant"{}},\ \textcolor{stringliteral}{"{}optimal"{}},\ \textcolor{stringliteral}{"{}invscaling"{}},\ \textcolor{stringliteral}{"{}adaptive"{}}\}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01215}01215\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Hidden}{Hidden}}(\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}pa1"{}},\ \textcolor{stringliteral}{"{}pa2"{}}\})),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01216}01216\ \ \ \ \ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01217}01217\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}eta0"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01218}01218\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01219}01219\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01220}01220\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01221}01221\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01222}01222\ \ \ \ \ \ \ \ \ loss="{}hinge"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01223}01223\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01224}01224\ \ \ \ \ \ \ \ \ penalty="{}l2"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01225}01225\ \ \ \ \ \ \ \ \ alpha=0.0001,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01226}01226\ \ \ \ \ \ \ \ \ l1\_ratio=0.15,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01227}01227\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01228}01228\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01229}01229\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01230}01230\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01231}01231\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01232}01232\ \ \ \ \ \ \ \ \ epsilon=DEFAULT\_EPSILON,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01233}01233\ \ \ \ \ \ \ \ \ n\_jobs=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01234}01234\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01235}01235\ \ \ \ \ \ \ \ \ learning\_rate="{}optimal"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01236}01236\ \ \ \ \ \ \ \ \ eta0=0.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01237}01237\ \ \ \ \ \ \ \ \ power\_t=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01238}01238\ \ \ \ \ \ \ \ \ early\_stopping=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01239}01239\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01240}01240\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01241}01241\ \ \ \ \ \ \ \ \ class\_weight=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01242}01242\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01243}01243\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01244}01244\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01245}01245\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01246}01246\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01247}01247\ \ \ \ \ \ \ \ \ \ \ \ \ penalty=penalty,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01248}01248\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01249}01249\ \ \ \ \ \ \ \ \ \ \ \ \ l1\_ratio=l1\_ratio,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01250}01250\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01251}01251\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01252}01252\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01253}01253\ \ \ \ \ \ \ \ \ \ \ \ \ shuffle=shuffle,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01254}01254\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01255}01255\ \ \ \ \ \ \ \ \ \ \ \ \ epsilon=epsilon,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01256}01256\ \ \ \ \ \ \ \ \ \ \ \ \ n\_jobs=n\_jobs,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01257}01257\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01258}01258\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01259}01259\ \ \ \ \ \ \ \ \ \ \ \ \ eta0=eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01260}01260\ \ \ \ \ \ \ \ \ \ \ \ \ power\_t=power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01261}01261\ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping=early\_stopping,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01262}01262\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01263}01263\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01264}01264\ \ \ \ \ \ \ \ \ \ \ \ \ class\_weight=class\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01265}01265\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01266}01266\ \ \ \ \ \ \ \ \ \ \ \ \ average=average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01267}01267\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01268}01268\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01269}01269\ \ \ \ \ \textcolor{keyword}{def\ }\_check\_proba(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01270}01270\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}\ \textcolor{keywordflow}{not}\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}log\_loss"{}},\ \textcolor{stringliteral}{"{}modified\_huber"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01271}01271\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classAttributeError}{AttributeError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01272}01272\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}probability\ estimates\ are\ not\ available\ for\ loss=\%r"{}}\ \%\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01273}01273\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01274}01274\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01275}01275\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01276}01276\ \ \ \ \ \textcolor{preprocessor}{@available\_if(\_check\_proba)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01277}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier_a1d9158204341405a022f6202a6d14898}{01277}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier_a1d9158204341405a022f6202a6d14898}{predict\_proba}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01278}01278\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Probability\ estimates.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01279}01279\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01280}01280\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ is\ only\ available\ for\ log\ loss\ and\ modified\ Huber\ loss.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01281}01281\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01282}01282\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Multiclass\ probability\ estimates\ are\ derived\ from\ binary\ (one-\/vs.-\/rest)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01283}01283\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ estimates\ by\ simple\ normalization,\ as\ recommended\ by\ Zadrozny\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01284}01284\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Elkan.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01285}01285\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01286}01286\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Binary\ probability\ estimates\ for\ loss="{}modified\_huber"{}\ are\ given\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01287}01287\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (clip(decision\_function(X),\ -\/1,\ 1)\ +\ 1)\ /\ 2.\ For\ other\ loss\ functions}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01288}01288\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ it\ is\ necessary\ to\ perform\ proper\ probability\ calibration\ by\ wrapping}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01289}01289\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ classifier\ with}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01290}01290\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :class:\`{}\string~sklearn.calibration.CalibratedClassifierCV\`{}\ instead.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01291}01291\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01292}01292\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01293}01293\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01294}01294\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01295}01295\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Input\ data\ for\ prediction.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01296}01296\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01297}01297\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01298}01298\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01299}01299\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ndarray\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01300}01300\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ the\ probability\ of\ the\ sample\ for\ each\ class\ in\ the\ model,}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01301}01301\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ where\ classes\ are\ ordered\ as\ they\ are\ in\ \`{}self.classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01302}01302\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01303}01303\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01304}01304\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01305}01305\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Zadrozny\ and\ Elkan,\ "{}Transforming\ classifier\ scores\ into\ multiclass}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01306}01306\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ probability\ estimates"{},\ SIGKDD'02,}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01307}01307\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ https://dl.acm.org/doi/pdf/10.1145/775047.775151}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01308}01308\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01309}01309\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ justification\ for\ the\ formula\ in\ the\ loss="{}modified\_huber"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01310}01310\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ case\ is\ in\ the\ appendix\ B\ in:}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01311}01311\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01312}01312\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01313}01313\ \ \ \ \ \ \ \ \ check\_is\_fitted(self)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01314}01314\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01315}01315\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}\ ==\ \textcolor{stringliteral}{"{}log\_loss"{}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01316}01316\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a5a266bf89399050c6d930132ded7acea}{\_predict\_proba\_lr}}(X)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01317}01317\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01318}01318\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}\ ==\ \textcolor{stringliteral}{"{}modified\_huber"{}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01319}01319\ \ \ \ \ \ \ \ \ \ \ \ \ binary\ =\ len(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a68471036bb7f66daf0cedfd513125f45}{classes\_}})\ ==\ 2}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01320}01320\ \ \ \ \ \ \ \ \ \ \ \ \ scores\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a0fa3aba9b5ea98b5dd05c1f36e792c94}{decision\_function}}(X)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01321}01321\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01322}01322\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ binary:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01323}01323\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob2\ =\ np.ones((scores.shape[0],\ 2))}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01324}01324\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ =\ prob2[:,\ 1]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01325}01325\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01326}01326\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ =\ scores}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01327}01327\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01328}01328\ \ \ \ \ \ \ \ \ \ \ \ \ np.clip(scores,\ -\/1,\ 1,\ prob)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01329}01329\ \ \ \ \ \ \ \ \ \ \ \ \ prob\ +=\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01330}01330\ \ \ \ \ \ \ \ \ \ \ \ \ prob\ /=\ 2.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01331}01331\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01332}01332\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ binary:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01333}01333\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob2[:,\ 0]\ -\/=\ prob}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01334}01334\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ =\ prob2}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01335}01335\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01336}01336\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ above\ might\ assign\ zero\ to\ all\ classes,\ which\ doesn't}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01337}01337\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ normalize\ neatly;\ work\ around\ this\ to\ produce\ uniform}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01338}01338\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ probabilities}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01339}01339\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\_sum\ =\ prob.sum(axis=1)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01340}01340\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ all\_zero\ =\ prob\_sum\ ==\ 0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01341}01341\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ np.any(all\_zero):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01342}01342\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob[all\_zero,\ :]\ =\ 1}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01343}01343\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\_sum[all\_zero]\ =\ len(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1LinearClassifierMixin_a68471036bb7f66daf0cedfd513125f45}{classes\_}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01344}01344\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01345}01345\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ normalize}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01346}01346\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ /=\ prob\_sum.reshape((prob.shape[0],\ -\/1))}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01347}01347\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01348}01348\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ prob}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01349}01349\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01350}01350\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01351}01351\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classNotImplementedError}{NotImplementedError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01352}01352\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}predict\_(log\_)proba\ only\ supported\ when"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01353}01353\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}\ loss='log\_loss'\ or\ loss='modified\_huber'\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01354}01354\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}(\%r\ given)"{}}\ \%\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01355}01355\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01356}01356\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01357}01357\ \ \ \ \ \textcolor{preprocessor}{@available\_if(\_check\_proba)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01358}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier_a58d6d79ae54f212514d813266fed82d0}{01358}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier_a58d6d79ae54f212514d813266fed82d0}{predict\_log\_proba}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01359}01359\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Log\ of\ probability\ estimates.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01360}01360\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01361}01361\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ is\ only\ available\ for\ log\ loss\ and\ modified\ Huber\ loss.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01362}01362\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01363}01363\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ loss="{}modified\_huber"{},\ probability\ estimates\ may\ be\ hard\ zeros}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01364}01364\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ ones,\ so\ taking\ the\ logarithm\ is\ not\ possible.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01365}01365\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01366}01366\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ \`{}\`{}predict\_proba\`{}\`{}\ for\ details.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01367}01367\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01368}01368\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01369}01369\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01370}01370\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01371}01371\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Input\ data\ for\ prediction.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01372}01372\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01373}01373\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01374}01374\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01375}01375\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ T\ :\ array-\/like,\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01376}01376\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ the\ log-\/probability\ of\ the\ sample\ for\ each\ class\ in\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01377}01377\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ model,\ where\ classes\ are\ ordered\ as\ they\ are\ in}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01378}01378\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}self.classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01379}01379\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01380}01380\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.log(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDClassifier_a1d9158204341405a022f6202a6d14898}{predict\_proba}}(X))}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01381}01381\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01382}01382\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01383}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor}{01383}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor}{BaseSGDRegressor}}(\mbox{\hyperlink{classsklearn_1_1base_1_1RegressorMixin}{RegressorMixin}},\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD}{BaseSGD}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01384}01384\ \ \ \ \ loss\_functions\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01385}01385\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}squared\_error"{}}:\ (CyHalfSquaredError,),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01386}01386\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}huber"{}}:\ (CyHuberLoss,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01387}01387\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}epsilon\_insensitive"{}}:\ (EpsilonInsensitive,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01388}01388\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}squared\_epsilon\_insensitive"{}}:\ (SquaredEpsilonInsensitive,\ DEFAULT\_EPSILON),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01389}01389\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01390}01390\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01391}01391\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01392}01392\ \ \ \ \ \ \ \ \ **BaseSGD.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01393}01393\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}loss"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(set(loss\_functions))],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01394}01394\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}early\_stopping"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01395}01395\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}validation\_fraction"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ 1,\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01396}01396\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_iter\_no\_change"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01397}01397\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01398}01398\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01399}01399\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01400}01400\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01401}01401\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01402}01402\ \ \ \ \ \ \ \ \ loss="{}squared\_error"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01403}01403\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01404}01404\ \ \ \ \ \ \ \ \ penalty="{}l2"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01405}01405\ \ \ \ \ \ \ \ \ alpha=0.0001,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01406}01406\ \ \ \ \ \ \ \ \ l1\_ratio=0.15,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01407}01407\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01408}01408\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01409}01409\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01410}01410\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01411}01411\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01412}01412\ \ \ \ \ \ \ \ \ epsilon=DEFAULT\_EPSILON,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01413}01413\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01414}01414\ \ \ \ \ \ \ \ \ learning\_rate="{}invscaling"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01415}01415\ \ \ \ \ \ \ \ \ eta0=0.01,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01416}01416\ \ \ \ \ \ \ \ \ power\_t=0.25,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01417}01417\ \ \ \ \ \ \ \ \ early\_stopping=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01418}01418\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01419}01419\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01420}01420\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01421}01421\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01422}01422\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01423}01423\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01424}01424\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01425}01425\ \ \ \ \ \ \ \ \ \ \ \ \ penalty=penalty,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01426}01426\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01427}01427\ \ \ \ \ \ \ \ \ \ \ \ \ l1\_ratio=l1\_ratio,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01428}01428\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01429}01429\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01430}01430\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01431}01431\ \ \ \ \ \ \ \ \ \ \ \ \ shuffle=shuffle,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01432}01432\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01433}01433\ \ \ \ \ \ \ \ \ \ \ \ \ epsilon=epsilon,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01434}01434\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01435}01435\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01436}01436\ \ \ \ \ \ \ \ \ \ \ \ \ eta0=eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01437}01437\ \ \ \ \ \ \ \ \ \ \ \ \ power\_t=power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01438}01438\ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping=early\_stopping,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01439}01439\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01440}01440\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01441}01441\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01442}01442\ \ \ \ \ \ \ \ \ \ \ \ \ average=average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01443}01443\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01444}01444\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01445}01445\ \ \ \ \ \textcolor{keyword}{def\ }\_partial\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01446}01446\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01447}01447\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01448}01448\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01449}01449\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01450}01450\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01451}01451\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01452}01452\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01453}01453\ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01454}01454\ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01455}01455\ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01456}01456\ \ \ \ \ \ \ \ \ intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01457}01457\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01458}01458\ \ \ \ \ \ \ \ \ first\_call\ =\ getattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01459}01459\ \ \ \ \ \ \ \ \ X,\ y\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01460}01460\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01461}01461\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01462}01462\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01463}01463\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01464}01464\ \ \ \ \ \ \ \ \ \ \ \ \ copy=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01465}01465\ \ \ \ \ \ \ \ \ \ \ \ \ order=\textcolor{stringliteral}{"{}C"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01466}01466\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=[np.float64,\ np.float32],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01467}01467\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_large\_sparse=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01468}01468\ \ \ \ \ \ \ \ \ \ \ \ \ reset=first\_call,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01469}01469\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01470}01470\ \ \ \ \ \ \ \ \ y\ =\ y.astype(X.dtype,\ copy=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01471}01471\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01472}01472\ \ \ \ \ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01473}01473\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01474}01474\ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01475}01475\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01476}01476\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Allocate\ datastructures\ from\ input\ arguments}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01477}01477\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ first\_call:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01478}01478\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a24c5e64c46bf66749287ae2a8f06836f}{\_allocate\_parameter\_mem}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01479}01479\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01480}01480\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01481}01481\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ input\_dtype=X.dtype,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01482}01482\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01483}01483\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01484}01484\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01485}01485\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0\ \textcolor{keywordflow}{and}\ getattr(self,\ \textcolor{stringliteral}{"{}\_average\_coef"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01486}01486\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}\ =\ np.zeros(n\_features,\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01487}01487\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ np.zeros(1,\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01488}01488\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01489}01489\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_af9aa89c983449203bc450346396636e7}{\_fit\_regressor}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01490}01490\ \ \ \ \ \ \ \ \ \ \ \ \ X,\ y,\ alpha,\ C,\ loss,\ learning\_rate,\ sample\_weight,\ max\_iter}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01491}01491\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01492}01492\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01493}01493\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01494}01494\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01495}01495\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01496}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a08a9a88f61e4912aaa928a76232bfd7a}{01496}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a08a9a88f61e4912aaa928a76232bfd7a}{partial\_fit}}(self,\ X,\ y,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01497}01497\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Perform\ one\ epoch\ of\ stochastic\ gradient\ descent\ on\ given\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01498}01498\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01499}01499\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Internally,\ this\ method\ uses\ \`{}\`{}max\_iter\ =\ 1\`{}\`{}.\ Therefore,\ it\ is\ not}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01500}01500\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ guaranteed\ that\ a\ minimum\ of\ the\ cost\ function\ is\ reached\ after\ calling}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01501}01501\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ it\ once.\ Matters\ such\ as\ objective\ convergence\ and\ early\ stopping}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01502}01502\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ should\ be\ handled\ by\ the\ user.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01503}01503\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01504}01504\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01505}01505\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01506}01506\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01507}01507\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Subset\ of\ training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01508}01508\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01509}01509\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ numpy\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01510}01510\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Subset\ of\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01511}01511\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01512}01512\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01513}01513\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01514}01514\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ not\ provided,\ uniform\ weights\ are\ assumed.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01515}01515\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01516}01516\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01517}01517\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01518}01518\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01519}01519\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ an\ instance\ of\ self.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01520}01520\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01521}01521\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01522}01522\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}(for\_partial\_fit=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01523}01523\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01524}01524\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_abeefc880facab385adb2bafc1da8887d}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01525}01525\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01526}01526\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01527}01527\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b842303176fb0c02ee9039e7bf77ff5}{alpha}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01528}01528\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01529}01529\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01530}01530\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01531}01531\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01532}01532\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01533}01533\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01534}01534\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01535}01535\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01536}01536\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01537}01537\ \ \ \ \ \textcolor{keyword}{def\ }\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01538}01538\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01539}01539\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01540}01540\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01541}01541\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01542}01542\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01543}01543\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01544}01544\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01545}01545\ \ \ \ \ \ \ \ \ coef\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01546}01546\ \ \ \ \ \ \ \ \ intercept\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01547}01547\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01548}01548\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01549}01549\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_ab915396b2d26eac3638d1d0c051e0d53}{warm\_start}}\ \textcolor{keywordflow}{and}\ getattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01550}01550\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01551}01551\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01552}01552\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ intercept\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01553}01553\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01554}01554\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01555}01555\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01556}01556\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01557}01557\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01558}01558\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Clear\ iteration\ count\ for\ multiple\ call\ to\ fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01559}01559\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a7644c65a107d69e361f58d995fd0f01f}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01560}01560\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01561}01561\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_abeefc880facab385adb2bafc1da8887d}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01562}01562\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01563}01563\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01564}01564\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01565}01565\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01566}01566\ \ \ \ \ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01567}01567\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01568}01568\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01569}01569\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01570}01570\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01571}01571\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01572}01572\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01573}01573\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01574}01574\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01575}01575\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01576}01576\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ >\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01577}01577\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a464ab3aede84c945bd81079d0723c788}{n\_iter\_}}\ ==\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01578}01578\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01579}01579\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01580}01580\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01581}01581\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Maximum\ number\ of\ iteration\ reached\ before\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01582}01582\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}convergence.\ Consider\ increasing\ max\_iter\ to\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01583}01583\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}improve\ the\ fit."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01584}01584\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01585}01585\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ConvergenceWarning,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01586}01586\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01587}01587\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01588}01588\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01589}01589\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01590}01590\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01591}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a27a24d9e9477eb95f0cd60376442c4b6}{01591}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a27a24d9e9477eb95f0cd60376442c4b6}{fit}}(self,\ X,\ y,\ coef\_init=None,\ intercept\_init=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01592}01592\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ linear\ model\ with\ Stochastic\ Gradient\ Descent.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01593}01593\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01594}01594\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01595}01595\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01596}01596\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01597}01597\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01598}01598\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01599}01599\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01600}01600\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Target\ values.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01601}01601\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01602}01602\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\_init\ :\ ndarray\ of\ shape\ (n\_features,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01603}01603\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ coefficients\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01604}01604\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01605}01605\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_init\ :\ ndarray\ of\ shape\ (1,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01606}01606\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ intercept\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01607}01607\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01608}01608\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01609}01609\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples\ (1.\ for\ unweighted).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01610}01610\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01611}01611\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01612}01612\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01613}01613\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01614}01614\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Fitted\ \`{}SGDRegressor\`{}\ estimator.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01615}01615\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01616}01616\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01617}01617\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01618}01618\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a2bbaa9b48ea4b6d3d425a2073109e6a5}{\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01619}01619\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01620}01620\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01621}01621\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b842303176fb0c02ee9039e7bf77ff5}{alpha}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01622}01622\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01623}01623\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01624}01624\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01625}01625\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01626}01626\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=intercept\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01627}01627\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01628}01628\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01629}01629\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01630}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a67f2f347192056c676fefff1742c1f03}{01630}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a67f2f347192056c676fefff1742c1f03}{\_decision\_function}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01631}01631\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ using\ the\ linear\ model}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01632}01632\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01633}01633\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01634}01634\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01635}01635\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01636}01636\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01637}01637\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01638}01638\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01639}01639\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01640}01640\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Predicted\ target\ values\ per\ element\ in\ X.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01641}01641\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01642}01642\ \ \ \ \ \ \ \ \ check\_is\_fitted(self)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01643}01643\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01644}01644\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(self,\ X,\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ reset=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01645}01645\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01646}01646\ \ \ \ \ \ \ \ \ scores\ =\ safe\_sparse\_dot(X,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.T,\ dense\_output=\textcolor{keyword}{True})\ +\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01647}01647\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ scores.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01648}01648\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01649}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a470e6ac102ac4ee2e72c7c3e1c4e736c}{01649}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a470e6ac102ac4ee2e72c7c3e1c4e736c}{predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01650}01650\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ using\ the\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01651}01651\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01652}01652\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01653}01653\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01654}01654\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01655}01655\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Input\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01656}01656\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01657}01657\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01658}01658\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01659}01659\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01660}01660\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Predicted\ target\ values\ per\ element\ in\ X.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01661}01661\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01662}01662\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a67f2f347192056c676fefff1742c1f03}{\_decision\_function}}(X)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01663}01663\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01664}01664\ \ \ \ \ \textcolor{keyword}{def\ }\_fit\_regressor(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01665}01665\ \ \ \ \ \ \ \ \ self,\ X,\ y,\ alpha,\ C,\ loss,\ learning\_rate,\ sample\_weight,\ max\_iter}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01666}01666\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01667}01667\ \ \ \ \ \ \ \ \ loss\_function\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a8273e23a4d6abfb3450dd63875cc92bf}{\_get\_loss\_function}}(loss)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01668}01668\ \ \ \ \ \ \ \ \ penalty\_type\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a5174566af6a15812a2387cda1dc63ed3}{\_get\_penalty\_type}}(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a54e5935459afe8ab0025a1a240b1306e}{penalty}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01669}01669\ \ \ \ \ \ \ \ \ learning\_rate\_type\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a92f619ac5f9b25d77a4311eb6d0f2ee6}{\_get\_learning\_rate\_type}}(learning\_rate)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01670}01670\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01671}01671\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}t\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01672}01672\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a7644c65a107d69e361f58d995fd0f01f}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01673}01673\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01674}01674\ \ \ \ \ \ \ \ \ validation\_mask\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a457a1068cea451fa2ce0e860900e6cbc}{\_make\_validation\_split}}(y,\ sample\_mask=sample\_weight\ >\ 0)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01675}01675\ \ \ \ \ \ \ \ \ validation\_score\_cb\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a6aeba82c137c7119da03e200f0abfd2b}{\_make\_validation\_score\_cb}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01676}01676\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_mask,\ X,\ y,\ sample\_weight}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01677}01677\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01678}01678\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01679}01679\ \ \ \ \ \ \ \ \ random\_state\ =\ check\_random\_state(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01680}01680\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ numpy\ mtrand\ expects\ a\ C\ long\ which\ is\ a\ signed\ 32\ bit\ integer\ under}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01681}01681\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Windows}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01682}01682\ \ \ \ \ \ \ \ \ seed\ =\ random\_state.randint(0,\ MAX\_INT)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01683}01683\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01684}01684\ \ \ \ \ \ \ \ \ dataset,\ intercept\_decay\ =\ make\_dataset(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01685}01685\ \ \ \ \ \ \ \ \ \ \ \ \ X,\ y,\ sample\_weight,\ random\_state=random\_state}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01686}01686\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01687}01687\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01688}01688\ \ \ \ \ \ \ \ \ tol\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01689}01689\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01690}01690\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01691}01691\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01692}01692\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01693}01693\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01694}01694\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01695}01695\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01696}01696\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01697}01697\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01698}01698\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ \textcolor{keywordtype}{None}\ \ \textcolor{comment}{\#\ Not\ used}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01699}01699\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ [0]\ \ \textcolor{comment}{\#\ Not\ used}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01700}01700\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01701}01701\ \ \ \ \ \ \ \ \ \_plain\_sgd\ =\ \_get\_plain\_sgd\_function(input\_dtype=coef.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01702}01702\ \ \ \ \ \ \ \ \ coef,\ intercept,\ average\_coef,\ average\_intercept,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a464ab3aede84c945bd81079d0723c788}{n\_iter\_}}\ =\ \_plain\_sgd(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01703}01703\ \ \ \ \ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01704}01704\ \ \ \ \ \ \ \ \ \ \ \ \ intercept[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01705}01705\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01706}01706\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01707}01707\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_function,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01708}01708\ \ \ \ \ \ \ \ \ \ \ \ \ penalty\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01709}01709\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01710}01710\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01711}01711\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a9f8a9195483bd8e9b178f677458dc262}{\_get\_l1\_ratio}}(),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01712}01712\ \ \ \ \ \ \ \ \ \ \ \ \ dataset,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01713}01713\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_mask,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01714}01714\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a35b3a2d80cb33a4e256b88c1854ee310}{early\_stopping}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01715}01715\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_score\_cb,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01716}01716\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a13be23dfb5b6535863d8e3e1725bd0bb}{n\_iter\_no\_change}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01717}01717\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01718}01718\ \ \ \ \ \ \ \ \ \ \ \ \ tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01719}01719\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a6fb4d8bcd04de6ef03e14478f3626b2e}{fit\_intercept}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01720}01720\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a765908f4c904561675197a917fe5ca7a}{verbose}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01721}01721\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a65505cf8c52af32b031af3b524212573}{shuffle}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01722}01722\ \ \ \ \ \ \ \ \ \ \ \ \ seed,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01723}01723\ \ \ \ \ \ \ \ \ \ \ \ \ 1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01724}01724\ \ \ \ \ \ \ \ \ \ \ \ \ 1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01725}01725\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01726}01726\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b55861c795a8d839caaa8bb556cba1c}{eta0}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01727}01727\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a634cccd56c7f3d053e23890a5b60a564}{power\_t}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01728}01728\ \ \ \ \ \ \ \ \ \ \ \ \ 0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01729}01729\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a7644c65a107d69e361f58d995fd0f01f}{t\_}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01730}01730\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_decay,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01731}01731\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01732}01732\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01733}01733\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01734}01734\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a7644c65a107d69e361f58d995fd0f01f}{t\_}}\ +=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a464ab3aede84c945bd81079d0723c788}{n\_iter\_}}\ *\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01735}01735\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01736}01736\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01737}01737\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ np.atleast\_1d(average\_intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01738}01738\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01739}01739\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01740}01740\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ <=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor_a7644c65a107d69e361f58d995fd0f01f}{t\_}}\ -\/\ 1.0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01741}01741\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ made\ enough\ updates\ for\ averaging\ to\ be\ taken\ into\ account}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01742}01742\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ average\_coef}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01743}01743\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.atleast\_1d(average\_intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01744}01744\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01745}01745\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ coef}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01746}01746\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01747}01747\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01748}01748\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01749}01749\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a94963afa5e3ca19c6d3ed9d54c10458e}{intercept\_}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01750}01750\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01751}01751\ \ \ \ \ \textcolor{keyword}{def\ }\_\_sklearn\_tags\_\_(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01752}01752\ \ \ \ \ \ \ \ \ tags\ =\ super().\_\_sklearn\_tags\_\_()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01753}01753\ \ \ \ \ \ \ \ \ tags.input\_tags.sparse\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01754}01754\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ tags}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01755}01755\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01756}01756\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01757}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDRegressor}{01757}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDRegressor}{SGDRegressor}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGDRegressor}{BaseSGDRegressor}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01758}01758\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Linear\ model\ fitted\ by\ minimizing\ a\ regularized\ empirical\ loss\ with\ SGD.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01759}01759\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01760}01760\ \textcolor{stringliteral}{\ \ \ \ SGD\ stands\ for\ Stochastic\ Gradient\ Descent:\ the\ gradient\ of\ the\ loss\ is}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01761}01761\ \textcolor{stringliteral}{\ \ \ \ estimated\ each\ sample\ at\ a\ time\ and\ the\ model\ is\ updated\ along\ the\ way\ with}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01762}01762\ \textcolor{stringliteral}{\ \ \ \ a\ decreasing\ strength\ schedule\ (aka\ learning\ rate).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01763}01763\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01764}01764\ \textcolor{stringliteral}{\ \ \ \ The\ regularizer\ is\ a\ penalty\ added\ to\ the\ loss\ function\ that\ shrinks\ model}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01765}01765\ \textcolor{stringliteral}{\ \ \ \ parameters\ towards\ the\ zero\ vector\ using\ either\ the\ squared\ euclidean\ norm}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01766}01766\ \textcolor{stringliteral}{\ \ \ \ L2\ or\ the\ absolute\ norm\ L1\ or\ a\ combination\ of\ both\ (Elastic\ Net).\ If\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01767}01767\ \textcolor{stringliteral}{\ \ \ \ parameter\ update\ crosses\ the\ 0.0\ value\ because\ of\ the\ regularizer,\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01768}01768\ \textcolor{stringliteral}{\ \ \ \ update\ is\ truncated\ to\ 0.0\ to\ allow\ for\ learning\ sparse\ models\ and\ achieve}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01769}01769\ \textcolor{stringliteral}{\ \ \ \ online\ feature\ selection.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01770}01770\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01771}01771\ \textcolor{stringliteral}{\ \ \ \ This\ implementation\ works\ with\ data\ represented\ as\ dense\ numpy\ arrays\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01772}01772\ \textcolor{stringliteral}{\ \ \ \ floating\ point\ values\ for\ the\ features.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01773}01773\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01774}01774\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <sgd>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01775}01775\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01776}01776\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01777}01777\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01778}01778\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ str,\ default='squared\_error'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01779}01779\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ loss\ function\ to\ be\ used.\ The\ possible\ values\ are\ 'squared\_error',}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01780}01780\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'huber',\ 'epsilon\_insensitive',\ or\ 'squared\_epsilon\_insensitive'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01781}01781\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01782}01782\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ 'squared\_error'\ refers\ to\ the\ ordinary\ least\ squares\ fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01783}01783\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'huber'\ modifies\ 'squared\_error'\ to\ focus\ less\ on\ getting\ outliers}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01784}01784\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ correct\ by\ switching\ from\ squared\ to\ linear\ loss\ past\ a\ distance\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01785}01785\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ epsilon.\ 'epsilon\_insensitive'\ ignores\ errors\ less\ than\ epsilon\ and\ is}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01786}01786\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ linear\ past\ that;\ this\ is\ the\ loss\ function\ used\ in\ SVR.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01787}01787\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'squared\_epsilon\_insensitive'\ is\ the\ same\ but\ becomes\ squared\ loss\ past}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01788}01788\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ a\ tolerance\ of\ epsilon.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01789}01789\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01790}01790\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ More\ details\ about\ the\ losses\ formulas\ can\ be\ found\ in\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01791}01791\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}User\ Guide\ <sgd\_mathematical\_formulation>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01792}01792\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01793}01793\ \textcolor{stringliteral}{\ \ \ \ penalty\ :\ \{'l2',\ 'l1',\ 'elasticnet',\ None\},\ default='l2'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01794}01794\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ penalty\ (aka\ regularization\ term)\ to\ be\ used.\ Defaults\ to\ 'l2'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01795}01795\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ which\ is\ the\ standard\ regularizer\ for\ linear\ SVM\ models.\ 'l1'\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01796}01796\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'elasticnet'\ might\ bring\ sparsity\ to\ the\ model\ (feature\ selection)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01797}01797\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ not\ achievable\ with\ 'l2'.\ No\ penalty\ is\ added\ when\ set\ to\ \`{}None\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01798}01798\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01799}01799\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ You\ can\ see\ a\ visualisation\ of\ the\ penalties\ in}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01800}01800\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_penalties.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01801}01801\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01802}01802\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=0.0001}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01803}01803\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ regularization\ term.\ The\ higher\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01804}01804\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ value,\ the\ stronger\ the\ regularization.\ Also\ used\ to\ compute\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01805}01805\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learning\ rate\ when\ \`{}learning\_rate\`{}\ is\ set\ to\ 'optimal'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01806}01806\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01807}01807\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01808}01808\ \textcolor{stringliteral}{\ \ \ \ l1\_ratio\ :\ float,\ default=0.15}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01809}01809\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ Elastic\ Net\ mixing\ parameter,\ with\ 0\ <=\ l1\_ratio\ <=\ 1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01810}01810\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l1\_ratio=0\ corresponds\ to\ L2\ penalty,\ l1\_ratio=1\ to\ L1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01811}01811\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}penalty\`{}\ is\ 'elasticnet'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01812}01812\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ 1.0]\`{}\ or\ can\ be\ \`{}None\`{}\ if}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01813}01813\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}penalty\`{}\ is\ not\ \`{}elasticnet\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01814}01814\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01815}01815\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 1.7}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01816}01816\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}l1\_ratio\`{}\ can\ be\ \`{}None\`{}\ when\ \`{}penalty\`{}\ is\ not\ "{}elasticnet"{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01817}01817\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01818}01818\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01819}01819\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ the\ intercept\ should\ be\ estimated\ or\ not.\ If\ False,\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01820}01820\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ data\ is\ assumed\ to\ be\ already\ centered.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01821}01821\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01822}01822\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=1000}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01823}01823\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximum\ number\ of\ passes\ over\ the\ training\ data\ (aka\ epochs).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01824}01824\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ only\ impacts\ the\ behavior\ in\ the\ \`{}\`{}fit\`{}\`{}\ method,\ and\ not\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01825}01825\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :meth:\`{}partial\_fit\`{}\ method.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01826}01826\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01827}01827\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01828}01828\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01829}01829\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01830}01830\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float\ or\ None,\ default=1e-\/3}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01831}01831\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ stopping\ criterion.\ If\ it\ is\ not\ None,\ training\ will\ stop}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01832}01832\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ when\ (loss\ >\ best\_loss\ -\/\ tol)\ for\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ consecutive}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01833}01833\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ epochs.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01834}01834\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Convergence\ is\ checked\ against\ the\ training\ loss\ or\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01835}01835\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ loss\ depending\ on\ the\ \`{}early\_stopping\`{}\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01836}01836\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01837}01837\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01838}01838\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01839}01839\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01840}01840\ \textcolor{stringliteral}{\ \ \ \ shuffle\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01841}01841\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ or\ not\ the\ training\ data\ should\ be\ shuffled\ after\ each\ epoch.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01842}01842\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01843}01843\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01844}01844\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ verbosity\ level.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01845}01845\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01846}01846\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01847}01847\ \textcolor{stringliteral}{\ \ \ \ epsilon\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01848}01848\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Epsilon\ in\ the\ epsilon-\/insensitive\ loss\ functions;\ only\ if\ \`{}loss\`{}\ is}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01849}01849\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'huber',\ 'epsilon\_insensitive',\ or\ 'squared\_epsilon\_insensitive'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01850}01850\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ 'huber',\ determines\ the\ threshold\ at\ which\ it\ becomes\ less}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01851}01851\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ important\ to\ get\ the\ prediction\ exactly\ right.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01852}01852\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ epsilon-\/insensitive,\ any\ differences\ between\ the\ current\ prediction}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01853}01853\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ the\ correct\ label\ are\ ignored\ if\ they\ are\ less\ than\ this\ threshold.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01854}01854\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01855}01855\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01856}01856\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance,\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01857}01857\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Used\ for\ shuffling\ the\ data,\ when\ \`{}\`{}shuffle\`{}\`{}\ is\ set\ to\ \`{}\`{}True\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01858}01858\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Pass\ an\ int\ for\ reproducible\ output\ across\ multiple\ function\ calls.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01859}01859\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}Glossary\ <random\_state>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01860}01860\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01861}01861\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ str,\ default='invscaling'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01862}01862\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ learning\ rate\ schedule:}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01863}01863\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01864}01864\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'constant':\ \`{}eta\ =\ eta0\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01865}01865\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'optimal':\ \`{}eta\ =\ 1.0\ /\ (alpha\ *\ (t\ +\ t0))\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01866}01866\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ where\ t0\ is\ chosen\ by\ a\ heuristic\ proposed\ by\ Leon\ Bottou.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01867}01867\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'invscaling':\ \`{}eta\ =\ eta0\ /\ pow(t,\ power\_t)\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01868}01868\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'adaptive':\ eta\ =\ eta0,\ as\ long\ as\ the\ training\ keeps\ decreasing.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01869}01869\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ Each\ time\ n\_iter\_no\_change\ consecutive\ epochs\ fail\ to\ decrease\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01870}01870\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ training\ loss\ by\ tol\ or\ fail\ to\ increase\ validation\ score\ by\ tol\ if}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01871}01871\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ early\_stopping\ is\ True,\ the\ current\ learning\ rate\ is\ divided\ by\ 5.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01872}01872\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01873}01873\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01874}01874\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'adaptive'\ option.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01875}01875\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01876}01876\ \textcolor{stringliteral}{\ \ \ \ eta0\ :\ float,\ default=0.01}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01877}01877\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ for\ the\ 'constant',\ 'invscaling'\ or}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01878}01878\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'adaptive'\ schedules.\ The\ default\ value\ is\ 0.01.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01879}01879\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01880}01880\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01881}01881\ \textcolor{stringliteral}{\ \ \ \ power\_t\ :\ float,\ default=0.25}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01882}01882\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ exponent\ for\ inverse\ scaling\ learning\ rate.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01883}01883\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(-\/inf,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01884}01884\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01885}01885\ \textcolor{stringliteral}{\ \ \ \ early\_stopping\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01886}01886\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ to\ use\ early\ stopping\ to\ terminate\ training\ when\ validation}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01887}01887\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\ is\ not\ improving.\ If\ set\ to\ True,\ it\ will\ automatically\ set\ aside}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01888}01888\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ a\ fraction\ of\ training\ data\ as\ validation\ and\ terminate}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01889}01889\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ training\ when\ validation\ score\ returned\ by\ the\ \`{}score\`{}\ method\ is\ not}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01890}01890\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ improving\ by\ at\ least\ \`{}tol\`{}\ for\ \`{}n\_iter\_no\_change\`{}\ consecutive}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01891}01891\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ epochs.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01892}01892\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01893}01893\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :ref:\`{}sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_early\_stopping.py\`{}\ for\ an}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01894}01894\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ example\ of\ the\ effects\ of\ early\ stopping.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01895}01895\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01896}01896\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01897}01897\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'early\_stopping'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01898}01898\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01899}01899\ \textcolor{stringliteral}{\ \ \ \ validation\_fraction\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01900}01900\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ proportion\ of\ training\ data\ to\ set\ aside\ as\ validation\ set\ for}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01901}01901\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ early\ stopping.\ Must\ be\ between\ 0\ and\ 1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01902}01902\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}early\_stopping\`{}\ is\ True.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01903}01903\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01904}01904\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01905}01905\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01906}01906\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'validation\_fraction'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01907}01907\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01908}01908\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_no\_change\ :\ int,\ default=5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01909}01909\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ iterations\ with\ no\ improvement\ to\ wait\ before\ stopping}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01910}01910\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ fitting.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01911}01911\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Convergence\ is\ checked\ against\ the\ training\ loss\ or\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01912}01912\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ loss\ depending\ on\ the\ \`{}early\_stopping\`{}\ parameter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01913}01913\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Integer\ values\ must\ be\ in\ the\ range\ \`{}[1,\ max\_iter)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01914}01914\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01915}01915\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01916}01916\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Added\ 'n\_iter\_no\_change'\ option}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01917}01917\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01918}01918\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01919}01919\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ True,\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ fit\ as}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01920}01920\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ initialization,\ otherwise,\ just\ erase\ the\ previous\ solution.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01921}01921\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}the\ Glossary\ <warm\_start>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01922}01922\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01923}01923\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Repeatedly\ calling\ fit\ or\ partial\_fit\ when\ warm\_start\ is\ True\ can}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01924}01924\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ result\ in\ a\ different\ solution\ than\ when\ calling\ fit\ a\ single\ time}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01925}01925\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ because\ of\ the\ way\ the\ data\ is\ shuffled.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01926}01926\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ a\ dynamic\ learning\ rate\ is\ used,\ the\ learning\ rate\ is\ adapted}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01927}01927\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ depending\ on\ the\ number\ of\ samples\ already\ seen.\ Calling\ \`{}\`{}fit\`{}\`{}\ resets}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01928}01928\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ this\ counter,\ while\ \`{}\`{}partial\_fit\`{}\`{}\ \ will\ result\ in\ increasing\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01929}01929\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ existing\ counter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01930}01930\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01931}01931\ \textcolor{stringliteral}{\ \ \ \ average\ :\ bool\ or\ int,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01932}01932\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ True,\ computes\ the\ averaged\ SGD\ weights\ across\ all}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01933}01933\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ updates\ and\ stores\ the\ result\ in\ the\ \`{}\`{}coef\_\`{}\`{}\ attribute.\ If\ set\ to}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01934}01934\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ an\ int\ greater\ than\ 1,\ averaging\ will\ begin\ once\ the\ total\ number\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01935}01935\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples\ seen\ reaches\ \`{}average\`{}.\ So\ \`{}\`{}average=10\`{}\`{}\ will\ begin}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01936}01936\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ averaging\ after\ seeing\ 10\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01937}01937\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01938}01938\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01939}01939\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01940}01940\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ ndarray\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01941}01941\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weights\ assigned\ to\ the\ features.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01942}01942\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01943}01943\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ ndarray\ of\ shape\ (1,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01944}01944\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ intercept\ term.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01945}01945\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01946}01946\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01947}01947\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ actual\ number\ of\ iterations\ before\ reaching\ the\ stopping\ criterion.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01948}01948\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01949}01949\ \textcolor{stringliteral}{\ \ \ \ t\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01950}01950\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ weight\ updates\ performed\ during\ training.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01951}01951\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Same\ as\ \`{}\`{}(n\_iter\_\ *\ n\_samples\ +\ 1)\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01952}01952\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01953}01953\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01954}01954\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01955}01955\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01956}01956\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01957}01957\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01958}01958\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01959}01959\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01960}01960\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01961}01961\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01962}01962\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01963}01963\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01964}01964\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01965}01965\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01966}01966\ \textcolor{stringliteral}{\ \ \ \ HuberRegressor\ :\ Linear\ regression\ model\ that\ is\ robust\ to\ outliers.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01967}01967\ \textcolor{stringliteral}{\ \ \ \ Lars\ :\ Least\ Angle\ Regression\ model.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01968}01968\ \textcolor{stringliteral}{\ \ \ \ Lasso\ :\ Linear\ Model\ trained\ with\ L1\ prior\ as\ regularizer.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01969}01969\ \textcolor{stringliteral}{\ \ \ \ RANSACRegressor\ :\ RANSAC\ (RANdom\ SAmple\ Consensus)\ algorithm.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01970}01970\ \textcolor{stringliteral}{\ \ \ \ Ridge\ :\ Linear\ least\ squares\ with\ l2\ regularization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01971}01971\ \textcolor{stringliteral}{\ \ \ \ sklearn.svm.SVR\ :\ Epsilon-\/Support\ Vector\ Regression.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01972}01972\ \textcolor{stringliteral}{\ \ \ \ TheilSenRegressor\ :\ Theil-\/Sen\ Estimator\ robust\ multivariate\ regression\ model.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01973}01973\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01974}01974\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01975}01975\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01976}01976\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01977}01977\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.linear\_model\ import\ SGDRegressor}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01978}01978\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.pipeline\ import\ make\_pipeline}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01979}01979\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.preprocessing\ import\ StandardScaler}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01980}01980\ \textcolor{stringliteral}{\ \ \ \ >>>\ n\_samples,\ n\_features\ =\ 10,\ 5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01981}01981\ \textcolor{stringliteral}{\ \ \ \ >>>\ rng\ =\ np.random.RandomState(0)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01982}01982\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ rng.randn(n\_samples)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01983}01983\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ rng.randn(n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01984}01984\ \textcolor{stringliteral}{\ \ \ \ >>>\ \#\ Always\ scale\ the\ input.\ The\ most\ convenient\ way\ is\ to\ use\ a\ pipeline.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01985}01985\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg\ =\ make\_pipeline(StandardScaler(),}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01986}01986\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ SGDRegressor(max\_iter=1000,\ tol=1e-\/3))}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01987}01987\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01988}01988\ \textcolor{stringliteral}{\ \ \ \ Pipeline(steps=[('standardscaler',\ StandardScaler()),}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01989}01989\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ('sgdregressor',\ SGDRegressor())])}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01990}01990\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01991}01991\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01992}01992\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01993}01993\ \ \ \ \ \ \ \ \ **BaseSGDRegressor.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01994}01994\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}penalty"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}l2"{}},\ \textcolor{stringliteral}{"{}l1"{}},\ \textcolor{stringliteral}{"{}elasticnet"{}}\}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01995}01995\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}alpha"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01996}01996\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}l1\_ratio"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ 1,\ closed=\textcolor{stringliteral}{"{}both"{}}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01997}01997\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}power\_t"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01998}01998\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}learning\_rate"{}}:\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l01999}01999\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}constant"{}},\ \textcolor{stringliteral}{"{}optimal"{}},\ \textcolor{stringliteral}{"{}invscaling"{}},\ \textcolor{stringliteral}{"{}adaptive"{}}\}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02000}02000\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Hidden}{Hidden}}(\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}pa1"{}},\ \textcolor{stringliteral}{"{}pa2"{}}\})),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02001}02001\ \ \ \ \ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02002}02002\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}epsilon"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02003}02003\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}eta0"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02004}02004\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02005}02005\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02006}02006\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02007}02007\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02008}02008\ \ \ \ \ \ \ \ \ loss="{}squared\_error"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02009}02009\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02010}02010\ \ \ \ \ \ \ \ \ penalty="{}l2"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02011}02011\ \ \ \ \ \ \ \ \ alpha=0.0001,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02012}02012\ \ \ \ \ \ \ \ \ l1\_ratio=0.15,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02013}02013\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02014}02014\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02015}02015\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02016}02016\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02017}02017\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02018}02018\ \ \ \ \ \ \ \ \ epsilon=DEFAULT\_EPSILON,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02019}02019\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02020}02020\ \ \ \ \ \ \ \ \ learning\_rate="{}invscaling"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02021}02021\ \ \ \ \ \ \ \ \ eta0=0.01,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02022}02022\ \ \ \ \ \ \ \ \ power\_t=0.25,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02023}02023\ \ \ \ \ \ \ \ \ early\_stopping=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02024}02024\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02025}02025\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02026}02026\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02027}02027\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02028}02028\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02029}02029\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02030}02030\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02031}02031\ \ \ \ \ \ \ \ \ \ \ \ \ penalty=penalty,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02032}02032\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02033}02033\ \ \ \ \ \ \ \ \ \ \ \ \ l1\_ratio=l1\_ratio,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02034}02034\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02035}02035\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02036}02036\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02037}02037\ \ \ \ \ \ \ \ \ \ \ \ \ shuffle=shuffle,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02038}02038\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02039}02039\ \ \ \ \ \ \ \ \ \ \ \ \ epsilon=epsilon,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02040}02040\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02041}02041\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02042}02042\ \ \ \ \ \ \ \ \ \ \ \ \ eta0=eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02043}02043\ \ \ \ \ \ \ \ \ \ \ \ \ power\_t=power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02044}02044\ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping=early\_stopping,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02045}02045\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02046}02046\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02047}02047\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02048}02048\ \ \ \ \ \ \ \ \ \ \ \ \ average=average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02049}02049\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02050}02050\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02051}02051\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02052}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM}{02052}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM}{SGDOneClassSVM}}(\mbox{\hyperlink{classsklearn_1_1base_1_1OutlierMixin}{OutlierMixin}},\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD}{BaseSGD}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02053}02053\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Solves\ linear\ One-\/Class\ SVM\ using\ Stochastic\ Gradient\ Descent.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02054}02054\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02055}02055\ \textcolor{stringliteral}{\ \ \ \ This\ implementation\ is\ meant\ to\ be\ used\ with\ a\ kernel\ approximation}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02056}02056\ \textcolor{stringliteral}{\ \ \ \ technique\ (e.g.\ \`{}sklearn.kernel\_approximation.Nystroem\`{})\ to\ obtain\ results}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02057}02057\ \textcolor{stringliteral}{\ \ \ \ similar\ to\ \`{}sklearn.svm.OneClassSVM\`{}\ which\ uses\ a\ Gaussian\ kernel\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02058}02058\ \textcolor{stringliteral}{\ \ \ \ default.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02059}02059\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02060}02060\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <sgd\_online\_one\_class\_svm>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02061}02061\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02062}02062\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02063}02063\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02064}02064\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02065}02065\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02066}02066\ \textcolor{stringliteral}{\ \ \ \ nu\ :\ float,\ default=0.5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02067}02067\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ nu\ parameter\ of\ the\ One\ Class\ SVM:\ an\ upper\ bound\ on\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02068}02068\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ fraction\ of\ training\ errors\ and\ a\ lower\ bound\ of\ the\ fraction\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02069}02069\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ support\ vectors.\ Should\ be\ in\ the\ interval\ (0,\ 1].\ By\ default\ 0.5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02070}02070\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ will\ be\ taken.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02071}02071\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02072}02072\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02073}02073\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ the\ intercept\ should\ be\ estimated\ or\ not.\ Defaults\ to\ True.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02074}02074\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02075}02075\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=1000}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02076}02076\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximum\ number\ of\ passes\ over\ the\ training\ data\ (aka\ epochs).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02077}02077\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ only\ impacts\ the\ behavior\ in\ the\ \`{}\`{}fit\`{}\`{}\ method,\ and\ not\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02078}02078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}partial\_fit\`{}.\ Defaults\ to\ 1000.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02079}02079\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02080}02080\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02081}02081\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float\ or\ None,\ default=1e-\/3}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02082}02082\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ stopping\ criterion.\ If\ it\ is\ not\ None,\ the\ iterations\ will\ stop}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02083}02083\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ when\ (loss\ >\ previous\_loss\ -\/\ tol).\ Defaults\ to\ 1e-\/3.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02084}02084\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02085}02085\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02086}02086\ \textcolor{stringliteral}{\ \ \ \ shuffle\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02087}02087\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ or\ not\ the\ training\ data\ should\ be\ shuffled\ after\ each\ epoch.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02088}02088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Defaults\ to\ True.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02089}02089\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02090}02090\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02091}02091\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ verbosity\ level.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02092}02092\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02093}02093\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance\ or\ None,\ default=None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02094}02094\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ seed\ of\ the\ pseudo\ random\ number\ generator\ to\ use\ when\ shuffling}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02095}02095\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ data.\ \ If\ int,\ random\_state\ is\ the\ seed\ used\ by\ the\ random\ number}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02096}02096\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ generator;\ If\ RandomState\ instance,\ random\_state\ is\ the\ random\ number}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02097}02097\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ generator;\ If\ None,\ the\ random\ number\ generator\ is\ the\ RandomState}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02098}02098\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ instance\ used\ by\ \`{}np.random\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02099}02099\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02100}02100\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ \{'constant',\ 'optimal',\ 'invscaling',\ 'adaptive'\},\ default='optimal'}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02101}02101\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ learning\ rate\ schedule\ to\ use\ with\ \`{}fit\`{}.\ (If\ using\ \`{}partial\_fit\`{},}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02102}02102\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learning\ rate\ must\ be\ controlled\ directly).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02103}02103\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02104}02104\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'constant':\ \`{}eta\ =\ eta0\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02105}02105\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'optimal':\ \`{}eta\ =\ 1.0\ /\ (alpha\ *\ (t\ +\ t0))\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02106}02106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ where\ t0\ is\ chosen\ by\ a\ heuristic\ proposed\ by\ Leon\ Bottou.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02107}02107\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'invscaling':\ \`{}eta\ =\ eta0\ /\ pow(t,\ power\_t)\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02108}02108\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'adaptive':\ eta\ =\ eta0,\ as\ long\ as\ the\ training\ keeps\ decreasing.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02109}02109\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ Each\ time\ n\_iter\_no\_change\ consecutive\ epochs\ fail\ to\ decrease\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02110}02110\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ training\ loss\ by\ tol\ or\ fail\ to\ increase\ validation\ score\ by\ tol\ if}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02111}02111\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ early\_stopping\ is\ True,\ the\ current\ learning\ rate\ is\ divided\ by\ 5.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02112}02112\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02113}02113\ \textcolor{stringliteral}{\ \ \ \ eta0\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02114}02114\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ for\ the\ 'constant',\ 'invscaling'\ or}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02115}02115\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'adaptive'\ schedules.\ The\ default\ value\ is\ 0.0\ as\ eta0\ is\ not\ used\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02116}02116\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ default\ schedule\ 'optimal'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02117}02117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02118}02118\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02119}02119\ \textcolor{stringliteral}{\ \ \ \ power\_t\ :\ float,\ default=0.5}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02120}02120\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ exponent\ for\ inverse\ scaling\ learning\ rate.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02121}02121\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(-\/inf,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02122}02122\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02123}02123\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02124}02124\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ True,\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ fit\ as}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02125}02125\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ initialization,\ otherwise,\ just\ erase\ the\ previous\ solution.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02126}02126\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}the\ Glossary\ <warm\_start>\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02127}02127\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02128}02128\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Repeatedly\ calling\ fit\ or\ partial\_fit\ when\ warm\_start\ is\ True\ can}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02129}02129\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ result\ in\ a\ different\ solution\ than\ when\ calling\ fit\ a\ single\ time}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02130}02130\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ because\ of\ the\ way\ the\ data\ is\ shuffled.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02131}02131\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ a\ dynamic\ learning\ rate\ is\ used,\ the\ learning\ rate\ is\ adapted}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02132}02132\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ depending\ on\ the\ number\ of\ samples\ already\ seen.\ Calling\ \`{}\`{}fit\`{}\`{}\ resets}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02133}02133\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ this\ counter,\ while\ \`{}\`{}partial\_fit\`{}\`{}\ \ will\ result\ in\ increasing\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02134}02134\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ existing\ counter.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02135}02135\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02136}02136\ \textcolor{stringliteral}{\ \ \ \ average\ :\ bool\ or\ int,\ default=False}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02137}02137\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ True,\ computes\ the\ averaged\ SGD\ weights\ and\ stores\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02138}02138\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ result\ in\ the\ \`{}\`{}coef\_\`{}\`{}\ attribute.\ If\ set\ to\ an\ int\ greater\ than\ 1,}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02139}02139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ averaging\ will\ begin\ once\ the\ total\ number\ of\ samples\ seen\ reaches}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02140}02140\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ average.\ So\ \`{}\`{}average=10\`{}\`{}\ will\ begin\ averaging\ after\ seeing\ 10}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02141}02141\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02142}02142\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02143}02143\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02144}02144\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02145}02145\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ ndarray\ of\ shape\ (1,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02146}02146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Weights\ assigned\ to\ the\ features.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02147}02147\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02148}02148\ \textcolor{stringliteral}{\ \ \ \ offset\_\ :\ ndarray\ of\ shape\ (1,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02149}02149\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Offset\ used\ to\ define\ the\ decision\ function\ from\ the\ raw\ scores.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02150}02150\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ We\ have\ the\ relation:\ decision\_function\ =\ score\_samples\ -\/\ offset.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02151}02151\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02152}02152\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02153}02153\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ actual\ number\ of\ iterations\ to\ reach\ the\ stopping\ criterion.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02154}02154\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02155}02155\ \textcolor{stringliteral}{\ \ \ \ t\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02156}02156\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ weight\ updates\ performed\ during\ training.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02157}02157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Same\ as\ \`{}\`{}(n\_iter\_\ *\ n\_samples\ +\ 1)\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02158}02158\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02159}02159\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02160}02160\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02161}02161\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02162}02162\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02163}02163\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02164}02164\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02165}02165\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02166}02166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02167}02167\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02168}02168\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02169}02169\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02170}02170\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02171}02171\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02172}02172\ \textcolor{stringliteral}{\ \ \ \ sklearn.svm.OneClassSVM\ :\ Unsupervised\ Outlier\ Detection.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02173}02173\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02174}02174\ \textcolor{stringliteral}{\ \ \ \ Notes}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02175}02175\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02176}02176\ \textcolor{stringliteral}{\ \ \ \ This\ estimator\ has\ a\ linear\ complexity\ in\ the\ number\ of\ training\ samples}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02177}02177\ \textcolor{stringliteral}{\ \ \ \ and\ is\ thus\ better\ suited\ than\ the\ \`{}sklearn.svm.OneClassSVM\`{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02178}02178\ \textcolor{stringliteral}{\ \ \ \ implementation\ for\ datasets\ with\ a\ large\ number\ of\ training\ samples\ (say}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02179}02179\ \textcolor{stringliteral}{\ \ \ \ >\ 10,000).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02180}02180\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02181}02181\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02182}02182\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02183}02183\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02184}02184\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn\ import\ linear\_model}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02185}02185\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ np.array([[-\/1,\ -\/1],\ [-\/2,\ -\/1],\ [1,\ 1],\ [2,\ 1]])}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02186}02186\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.SGDOneClassSVM(random\_state=42)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02187}02187\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02188}02188\ \textcolor{stringliteral}{\ \ \ \ SGDOneClassSVM(random\_state=42)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02189}02189\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02190}02190\ \textcolor{stringliteral}{\ \ \ \ >>>\ print(clf.predict([[4,\ 4]]))}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02191}02191\ \textcolor{stringliteral}{\ \ \ \ [1]}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02192}02192\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02193}02193\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02194}02194\ \ \ \ \ loss\_functions\ =\ \{\textcolor{stringliteral}{"{}hinge"{}}:\ (Hinge,\ 1.0)\}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02195}02195\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02196}02196\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02197}02197\ \ \ \ \ \ \ \ \ **BaseSGD.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02198}02198\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}nu"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ 1.0,\ closed=\textcolor{stringliteral}{"{}right"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02199}02199\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}learning\_rate"{}}:\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02200}02200\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}constant"{}},\ \textcolor{stringliteral}{"{}optimal"{}},\ \textcolor{stringliteral}{"{}invscaling"{}},\ \textcolor{stringliteral}{"{}adaptive"{}}\}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02201}02201\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Hidden}{Hidden}}(\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}pa1"{}},\ \textcolor{stringliteral}{"{}pa2"{}}\})),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02202}02202\ \ \ \ \ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02203}02203\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}eta0"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02204}02204\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}power\_t"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02205}02205\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02206}02206\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02207}02207\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02208}02208\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02209}02209\ \ \ \ \ \ \ \ \ nu=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02210}02210\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02211}02211\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02212}02212\ \ \ \ \ \ \ \ \ tol=1e-\/3,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02213}02213\ \ \ \ \ \ \ \ \ shuffle=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02214}02214\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02215}02215\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02216}02216\ \ \ \ \ \ \ \ \ learning\_rate="{}optimal"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02217}02217\ \ \ \ \ \ \ \ \ eta0=0.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02218}02218\ \ \ \ \ \ \ \ \ power\_t=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02219}02219\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02220}02220\ \ \ \ \ \ \ \ \ average=False,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02221}02221\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02222}02222\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a6209c87b6f5f715e295e71369f98c5b2}{nu}}\ =\ nu}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02223}02223\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02224}02224\ \ \ \ \ \ \ \ \ \ \ \ \ loss=\textcolor{stringliteral}{"{}hinge"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02225}02225\ \ \ \ \ \ \ \ \ \ \ \ \ penalty=\textcolor{stringliteral}{"{}l2"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02226}02226\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02227}02227\ \ \ \ \ \ \ \ \ \ \ \ \ l1\_ratio=0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02228}02228\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02229}02229\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02230}02230\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02231}02231\ \ \ \ \ \ \ \ \ \ \ \ \ shuffle=shuffle,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02232}02232\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02233}02233\ \ \ \ \ \ \ \ \ \ \ \ \ epsilon=DEFAULT\_EPSILON,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02234}02234\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02235}02235\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02236}02236\ \ \ \ \ \ \ \ \ \ \ \ \ eta0=eta0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02237}02237\ \ \ \ \ \ \ \ \ \ \ \ \ power\_t=power\_t,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02238}02238\ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02239}02239\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02240}02240\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=5,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02241}02241\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02242}02242\ \ \ \ \ \ \ \ \ \ \ \ \ average=average,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02243}02243\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02244}02244\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02245}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ac3f73864fe8b9d102598b20804fb9adc}{02245}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ac3f73864fe8b9d102598b20804fb9adc}{\_fit\_one\_class}}(self,\ X,\ alpha,\ C,\ sample\_weight,\ learning\_rate,\ max\_iter):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02246}02246\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Uses\ SGD\ implementation\ with\ X\ and\ y=np.ones(n\_samples)."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02247}02247\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02248}02248\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ One-\/Class\ SVM\ uses\ the\ SGD\ implementation\ with}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02249}02249\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ y=np.ones(n\_samples).}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02250}02250\ \ \ \ \ \ \ \ \ n\_samples\ =\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02251}02251\ \ \ \ \ \ \ \ \ y\ =\ np.ones(n\_samples,\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02252}02252\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02253}02253\ \ \ \ \ \ \ \ \ dataset,\ offset\_decay\ =\ make\_dataset(X,\ y,\ sample\_weight)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02254}02254\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02255}02255\ \ \ \ \ \ \ \ \ penalty\_type\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a5174566af6a15812a2387cda1dc63ed3}{\_get\_penalty\_type}}(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a54e5935459afe8ab0025a1a240b1306e}{penalty}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02256}02256\ \ \ \ \ \ \ \ \ learning\_rate\_type\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a92f619ac5f9b25d77a4311eb6d0f2ee6}{\_get\_learning\_rate\_type}}(learning\_rate)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02257}02257\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02258}02258\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ early\ stopping\ is\ set\ to\ False\ for\ the\ One-\/Class\ SVM.\ thus}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02259}02259\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ validation\_mask\ and\ validation\_score\_cb\ will\ be\ set\ to\ values}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02260}02260\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ associated\ to\ early\_stopping=False\ in\ \_make\_validation\_split\ and}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02261}02261\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \_make\_validation\_score\_cb\ respectively.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02262}02262\ \ \ \ \ \ \ \ \ validation\_mask\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a457a1068cea451fa2ce0e860900e6cbc}{\_make\_validation\_split}}(y,\ sample\_mask=sample\_weight\ >\ 0)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02263}02263\ \ \ \ \ \ \ \ \ validation\_score\_cb\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a6aeba82c137c7119da03e200f0abfd2b}{\_make\_validation\_score\_cb}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02264}02264\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_mask,\ X,\ y,\ sample\_weight}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02265}02265\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02266}02266\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02267}02267\ \ \ \ \ \ \ \ \ random\_state\ =\ check\_random\_state(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4f3392a0ad2ef2032a6ec5810151138a}{random\_state}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02268}02268\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ numpy\ mtrand\ expects\ a\ C\ long\ which\ is\ a\ signed\ 32\ bit\ integer\ under}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02269}02269\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Windows}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02270}02270\ \ \ \ \ \ \ \ \ seed\ =\ random\_state.randint(0,\ np.iinfo(np.int32).max)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02271}02271\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02272}02272\ \ \ \ \ \ \ \ \ tol\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02273}02273\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02274}02274\ \ \ \ \ \ \ \ \ one\_class\ =\ 1}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02275}02275\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ There\ are\ no\ class\ weights\ for\ the\ One-\/Class\ SVM\ and\ they\ are}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02276}02276\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ therefore\ set\ to\ 1.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02277}02277\ \ \ \ \ \ \ \ \ pos\_weight\ =\ 1}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02278}02278\ \ \ \ \ \ \ \ \ neg\_weight\ =\ 1}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02279}02279\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02280}02280\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02281}02281\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2a4f46a6b70ac1acc47437a79da8cf87}{\_standard\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02282}02282\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02283}02283\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02284}02284\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02285}02285\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02286}02286\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02287}02287\ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ 1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02288}02288\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef\ =\ \textcolor{keywordtype}{None}\ \ \textcolor{comment}{\#\ Not\ used}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02289}02289\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept\ =\ [0]\ \ \textcolor{comment}{\#\ Not\ used}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02290}02290\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02291}02291\ \ \ \ \ \ \ \ \ \_plain\_sgd\ =\ \_get\_plain\_sgd\_function(input\_dtype=coef.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02292}02292\ \ \ \ \ \ \ \ \ coef,\ intercept,\ average\_coef,\ average\_intercept,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ae80c8f360ccae1bb421677516165189e}{n\_iter\_}}\ =\ \_plain\_sgd(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02293}02293\ \ \ \ \ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02294}02294\ \ \ \ \ \ \ \ \ \ \ \ \ intercept[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02295}02295\ \ \ \ \ \ \ \ \ \ \ \ \ average\_coef,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02296}02296\ \ \ \ \ \ \ \ \ \ \ \ \ average\_intercept[0],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02297}02297\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a562d3ca720fa2d73d305c5c3014dc239}{\_loss\_function\_}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02298}02298\ \ \ \ \ \ \ \ \ \ \ \ \ penalty\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02299}02299\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02300}02300\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02301}02301\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_acdd6e3b5c1a5fd7a79dd170071f264a8}{l1\_ratio}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02302}02302\ \ \ \ \ \ \ \ \ \ \ \ \ dataset,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02303}02303\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_mask,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02304}02304\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a35b3a2d80cb33a4e256b88c1854ee310}{early\_stopping}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02305}02305\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_score\_cb,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02306}02306\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a13be23dfb5b6535863d8e3e1725bd0bb}{n\_iter\_no\_change}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02307}02307\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02308}02308\ \ \ \ \ \ \ \ \ \ \ \ \ tol,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02309}02309\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a6fb4d8bcd04de6ef03e14478f3626b2e}{fit\_intercept}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02310}02310\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a765908f4c904561675197a917fe5ca7a}{verbose}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02311}02311\ \ \ \ \ \ \ \ \ \ \ \ \ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a65505cf8c52af32b031af3b524212573}{shuffle}}),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02312}02312\ \ \ \ \ \ \ \ \ \ \ \ \ seed,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02313}02313\ \ \ \ \ \ \ \ \ \ \ \ \ neg\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02314}02314\ \ \ \ \ \ \ \ \ \ \ \ \ pos\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02315}02315\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate\_type,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02316}02316\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a4b55861c795a8d839caaa8bb556cba1c}{eta0}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02317}02317\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a634cccd56c7f3d053e23890a5b60a564}{power\_t}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02318}02318\ \ \ \ \ \ \ \ \ \ \ \ \ one\_class,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02319}02319\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a88108265d6d74fda0c55055aadad0f5c}{t\_}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02320}02320\ \ \ \ \ \ \ \ \ \ \ \ \ offset\_decay,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02321}02321\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02322}02322\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02323}02323\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02324}02324\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a88108265d6d74fda0c55055aadad0f5c}{t\_}}\ +=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ae80c8f360ccae1bb421677516165189e}{n\_iter\_}}\ *\ n\_samples}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02325}02325\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02326}02326\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02327}02327\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ np.atleast\_1d(average\_intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02328}02328\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a2755997ae14a4771b767d6cbcc3ea96c}{\_standard\_intercept}}\ =\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02329}02329\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02330}02330\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ <=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a88108265d6d74fda0c55055aadad0f5c}{t\_}}\ -\/\ 1.0:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02331}02331\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ made\ enough\ updates\ for\ averaging\ to\ be\ taken\ into\ account}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02332}02332\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ average\_coef}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02333}02333\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ 1\ -\/\ np.atleast\_1d(average\_intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02334}02334\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02335}02335\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ coef}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02336}02336\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ 1\ -\/\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02337}02337\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02338}02338\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02339}02339\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ 1\ -\/\ np.atleast\_1d(intercept)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02340}02340\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02341}02341\ \ \ \ \ \textcolor{keyword}{def\ }\_partial\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02342}02342\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02343}02343\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02344}02344\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02345}02345\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02346}02346\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02347}02347\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02348}02348\ \ \ \ \ \ \ \ \ max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02349}02349\ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02350}02350\ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02351}02351\ \ \ \ \ \ \ \ \ offset\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02352}02352\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02353}02353\ \ \ \ \ \ \ \ \ first\_call\ =\ getattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02354}02354\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02355}02355\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02356}02356\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02357}02357\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02358}02358\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02359}02359\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=[np.float64,\ np.float32],}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02360}02360\ \ \ \ \ \ \ \ \ \ \ \ \ order=\textcolor{stringliteral}{"{}C"{}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02361}02361\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_large\_sparse=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02362}02362\ \ \ \ \ \ \ \ \ \ \ \ \ reset=first\_call,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02363}02363\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02364}02364\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02365}02365\ \ \ \ \ \ \ \ \ n\_features\ =\ X.shape[1]}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02366}02366\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02367}02367\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Allocate\ datastructures\ from\ input\ arguments}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02368}02368\ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=X.dtype)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02369}02369\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02370}02370\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ We\ use\ intercept\ =\ 1\ -\/\ offset\ where\ intercept\ is\ the\ intercept\ of}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02371}02371\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ SGD\ implementation\ and\ offset\ is\ the\ offset\ of\ the\ One-\/Class\ SVM}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02372}02372\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ optimization\ problem.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02373}02373\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ getattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{or}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02374}02374\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a24c5e64c46bf66749287ae2a8f06836f}{\_allocate\_parameter\_mem}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02375}02375\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02376}02376\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02377}02377\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ input\_dtype=X.dtype,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02378}02378\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02379}02379\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\_init=offset\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02380}02380\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ one\_class=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02381}02381\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02382}02382\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ n\_features\ !=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.shape[-\/1]:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02383}02383\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02384}02384\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Number\ of\ features\ \%d\ does\ not\ match\ previous\ data\ \%d."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02385}02385\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ (n\_features,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.shape[-\/1])}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02386}02386\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02387}02387\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02388}02388\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a025a0f85b7c34d573ea66857d3c8dbfc}{average}}\ \textcolor{keywordflow}{and}\ getattr(self,\ \textcolor{stringliteral}{"{}\_average\_coef"{}},\ \textcolor{keywordtype}{None})\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02389}02389\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a770a68beb0f484f535122ba4497f8d41}{\_average\_coef}}\ =\ np.zeros(n\_features,\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02390}02390\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3706ed886e4150ed8d2789c783c22e6c}{\_average\_intercept}}\ =\ np.zeros(1,\ dtype=X.dtype,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02391}02391\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02392}02392\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a562d3ca720fa2d73d305c5c3014dc239}{\_loss\_function\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a8273e23a4d6abfb3450dd63875cc92bf}{\_get\_loss\_function}}(loss)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02393}02393\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}t\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02394}02394\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a88108265d6d74fda0c55055aadad0f5c}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02395}02395\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02396}02396\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ delegate\ to\ concrete\ training\ procedure}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02397}02397\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ac3f73864fe8b9d102598b20804fb9adc}{\_fit\_one\_class}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02398}02398\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02399}02399\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02400}02400\ \ \ \ \ \ \ \ \ \ \ \ \ C=C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02401}02401\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02402}02402\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02403}02403\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02404}02404\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02405}02405\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02406}02406\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02407}02407\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02408}02408\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02409}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a54136126424de8375a2dbe128624004d}{02409}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a54136126424de8375a2dbe128624004d}{partial\_fit}}(self,\ X,\ y=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02410}02410\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ linear\ One-\/Class\ SVM\ with\ Stochastic\ Gradient\ Descent.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02411}02411\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02412}02412\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02413}02413\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02414}02414\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02415}02415\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Subset\ of\ the\ training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02416}02416\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ Ignored}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02417}02417\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Not\ used,\ present\ for\ API\ consistency\ by\ convention.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02418}02418\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02419}02419\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ optional}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02420}02420\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02421}02421\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ not\ provided,\ uniform\ weights\ are\ assumed.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02422}02422\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02423}02423\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02424}02424\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02425}02425\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02426}02426\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ a\ fitted\ instance\ of\ self.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02427}02427\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02428}02428\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ hasattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02429}02429\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}(for\_partial\_fit=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02430}02430\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02431}02431\ \ \ \ \ \ \ \ \ alpha\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a6209c87b6f5f715e295e71369f98c5b2}{nu}}\ /\ 2}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02432}02432\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a70c7885dbe9361c1f91df9c28f3492b1}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02433}02433\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02434}02434\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02435}02435\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02436}02436\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02437}02437\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02438}02438\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=1,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02439}02439\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02440}02440\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02441}02441\ \ \ \ \ \ \ \ \ \ \ \ \ offset\_init=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02442}02442\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02443}02443\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02444}02444\ \ \ \ \ \textcolor{keyword}{def\ }\_fit(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02445}02445\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02446}02446\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02447}02447\ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02448}02448\ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02449}02449\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02450}02450\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02451}02451\ \ \ \ \ \ \ \ \ coef\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02452}02452\ \ \ \ \ \ \ \ \ offset\_init=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02453}02453\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02454}02454\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02455}02455\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_ab915396b2d26eac3638d1d0c051e0d53}{warm\_start}}\ \textcolor{keywordflow}{and}\ hasattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02456}02456\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02457}02457\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02458}02458\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ offset\_init\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02459}02459\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ offset\_init\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02460}02460\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02461}02461\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02462}02462\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02463}02463\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02464}02464\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Clear\ iteration\ count\ for\ multiple\ call\ to\ fit.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02465}02465\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a88108265d6d74fda0c55055aadad0f5c}{t\_}}\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02466}02466\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02467}02467\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a70c7885dbe9361c1f91df9c28f3492b1}{\_partial\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02468}02468\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02469}02469\ \ \ \ \ \ \ \ \ \ \ \ \ alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02470}02470\ \ \ \ \ \ \ \ \ \ \ \ \ C,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02471}02471\ \ \ \ \ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02472}02472\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02473}02473\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02474}02474\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02475}02475\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02476}02476\ \ \ \ \ \ \ \ \ \ \ \ \ offset\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02477}02477\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02478}02478\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02479}02479\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02480}02480\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02481}02481\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a1cc69f278cf6f9bfb5fe76023a9ba2a1}{tol}}\ >\ -\/np.inf}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02482}02482\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{and}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ae80c8f360ccae1bb421677516165189e}{n\_iter\_}}\ ==\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a0c3e3fadf1e922a20c689ebb1cb0f142}{max\_iter}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02483}02483\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02484}02484\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02485}02485\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02486}02486\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Maximum\ number\ of\ iteration\ reached\ before\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02487}02487\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}convergence.\ Consider\ increasing\ max\_iter\ to\ "{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02488}02488\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}improve\ the\ fit."{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02489}02489\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02490}02490\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ConvergenceWarning,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02491}02491\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02492}02492\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02493}02493\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02494}02494\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02495}02495\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02496}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ac30e01ac8535e3800511b278e1bbddbc}{02496}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ac30e01ac8535e3800511b278e1bbddbc}{fit}}(self,\ X,\ y=None,\ coef\_init=None,\ offset\_init=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02497}02497\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ linear\ One-\/Class\ SVM\ with\ Stochastic\ Gradient\ Descent.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02498}02498\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02499}02499\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ solves\ an\ equivalent\ optimization\ problem\ of\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02500}02500\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ One-\/Class\ SVM\ primal\ optimization\ problem\ and\ returns\ a\ weight\ vector}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02501}02501\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ w\ and\ an\ offset\ rho\ such\ that\ the\ decision\ function\ is\ given\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02502}02502\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ <w,\ x>\ -\/\ rho.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02503}02503\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02504}02504\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02505}02505\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02506}02506\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02507}02507\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02508}02508\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ Ignored}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02509}02509\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Not\ used,\ present\ for\ API\ consistency\ by\ convention.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02510}02510\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02511}02511\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\_init\ :\ array,\ shape\ (n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02512}02512\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ coefficients\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02513}02513\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02514}02514\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ offset\_init\ :\ array,\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02515}02515\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ initial\ offset\ to\ warm-\/start\ the\ optimization.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02516}02516\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02517}02517\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like,\ shape\ (n\_samples,),\ optional}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02518}02518\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weights\ applied\ to\ individual\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02519}02519\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ not\ provided,\ uniform\ weights\ are\ assumed.\ These\ weights\ will}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02520}02520\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ be\ multiplied\ with\ class\_weight\ (passed\ through\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02521}02521\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ constructor)\ if\ class\_weight\ is\ specified.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02522}02522\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02523}02523\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02524}02524\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02525}02525\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02526}02526\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ a\ fitted\ instance\ of\ self.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02527}02527\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02528}02528\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a972752d5c0061d9280377c236eaaf4df}{\_more\_validate\_params}}()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02529}02529\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02530}02530\ \ \ \ \ \ \ \ \ alpha\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a6209c87b6f5f715e295e71369f98c5b2}{nu}}\ /\ 2}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02531}02531\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_aac5f65a2c2cb844d6203ffcf2e271c40}{\_fit}}(}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02532}02532\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02533}02533\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02534}02534\ \ \ \ \ \ \ \ \ \ \ \ \ C=1.0,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02535}02535\ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3ebcb098aa87c097b93d8f208056d7cc}{loss}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02536}02536\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_a3a5679feefacc72b369bbabbe475a077}{learning\_rate}},}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02537}02537\ \ \ \ \ \ \ \ \ \ \ \ \ coef\_init=coef\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02538}02538\ \ \ \ \ \ \ \ \ \ \ \ \ offset\_init=offset\_init,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02539}02539\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02540}02540\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02541}02541\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02542}02542\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02543}02543\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02544}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ab87e3fe7a74695854ae2b66edca87f6b}{02544}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ab87e3fe7a74695854ae2b66edca87f6b}{decision\_function}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02545}02545\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Signed\ distance\ to\ the\ separating\ hyperplane.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02546}02546\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02547}02547\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Signed\ distance\ is\ positive\ for\ an\ inlier\ and\ negative\ for\ an}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02548}02548\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ outlier.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02549}02549\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02550}02550\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02551}02551\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02552}02552\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02553}02553\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Testing\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02554}02554\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02555}02555\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02556}02556\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02557}02557\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ dec\ :\ array-\/like,\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02558}02558\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Decision\ function\ values\ of\ the\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02559}02559\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02560}02560\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02561}02561\ \ \ \ \ \ \ \ \ check\_is\_fitted(self,\ \textcolor{stringliteral}{"{}coef\_"{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02562}02562\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02563}02563\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(self,\ X,\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ reset=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02564}02564\ \ \ \ \ \ \ \ \ decisions\ =\ safe\_sparse\_dot(X,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__base_1_1SparseCoefMixin_a3f469570abd7a9826ba7287dbc75f663}{coef\_}}.T,\ dense\_output=\textcolor{keyword}{True})\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02565}02565\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02566}02566\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ decisions.ravel()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02567}02567\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02568}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a7178244c75ea7d1872a0a62402575222}{02568}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a7178244c75ea7d1872a0a62402575222}{score\_samples}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02569}02569\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Raw\ scoring\ function\ of\ the\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02570}02570\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02571}02571\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02572}02572\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02573}02573\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02574}02574\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Testing\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02575}02575\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02576}02576\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02577}02577\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02578}02578\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\_samples\ :\ array-\/like,\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02579}02579\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Unshiffted\ scoring\ function\ values\ of\ the\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02580}02580\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02581}02581\ \ \ \ \ \ \ \ \ score\_samples\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ab87e3fe7a74695854ae2b66edca87f6b}{decision\_function}}(X)\ +\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1BaseSGD_aaa04504efdf60d7c2b9da76a38f2c95e}{offset\_}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02582}02582\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ score\_samples}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02583}02583\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02584}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a250aa14e88c3ca2168828c62f43d960a}{02584}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_a250aa14e88c3ca2168828c62f43d960a}{predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02585}02585\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ labels\ (1\ inlier,\ -\/1\ outlier)\ of\ the\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02586}02586\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02587}02587\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02588}02588\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02589}02589\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\},\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02590}02590\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Testing\ data.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02591}02591\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02592}02592\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02593}02593\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02594}02594\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ array,\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02595}02595\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Labels\ of\ the\ samples.}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02596}02596\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02597}02597\ \ \ \ \ \ \ \ \ y\ =\ (self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__stochastic__gradient_1_1SGDOneClassSVM_ab87e3fe7a74695854ae2b66edca87f6b}{decision\_function}}(X)\ >=\ 0).astype(np.int32)}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02598}02598\ \ \ \ \ \ \ \ \ y[y\ ==\ 0]\ =\ -\/1\ \ \textcolor{comment}{\#\ for\ consistency\ with\ outlier\ detectors}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02599}02599\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ y}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02600}02600\ }
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02601}02601\ \ \ \ \ \textcolor{keyword}{def\ }\_\_sklearn\_tags\_\_(self):}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02602}02602\ \ \ \ \ \ \ \ \ tags\ =\ super().\_\_sklearn\_tags\_\_()}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02603}02603\ \ \ \ \ \ \ \ \ tags.input\_tags.sparse\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__gradient_8py_source_l02604}02604\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ tags}

\end{DoxyCode}
