\doxysection{sklearn.\+linear\+\_\+model.\+\_\+logistic Namespace Reference}
\hypertarget{namespacesklearn_1_1linear__model_1_1__logistic}{}\label{namespacesklearn_1_1linear__model_1_1__logistic}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__logistic_1_1LogisticRegression}{Logistic\+Regression}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__logistic_1_1LogisticRegressionCV}{Logistic\+Regression\+CV}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__logistic_a8d9119fe5a2852152893dd839028f15f}{\+\_\+check\+\_\+solver}} (solver, penalty, dual)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__logistic_a345ab1f72da761ccf68e0bbec57f3c79}{\+\_\+check\+\_\+multi\+\_\+class}} (multi\+\_\+class, solver, n\+\_\+classes)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__logistic_a4bcf185546ecc3dd3272b38aede3876f}{\+\_\+logistic\+\_\+regression\+\_\+path}} (X, y, pos\+\_\+class=None, Cs=10, fit\+\_\+intercept=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+iter=100, tol=1e-\/4, verbose=0, solver="{}lbfgs"{}, coef=None, class\+\_\+weight=None, dual=False, penalty="{}l2"{}, intercept\+\_\+scaling=1.\+0, multi\+\_\+class="{}auto"{}, random\+\_\+state=None, check\+\_\+input=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+squared\+\_\+sum=None, sample\+\_\+weight=None, l1\+\_\+ratio=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__logistic_a6ed0e655a841cdb769a47cba70c13b07}{\+\_\+log\+\_\+reg\+\_\+scoring\+\_\+path}} (X, y, train, test, \texorpdfstring{$\ast$}{*}, pos\+\_\+class, Cs, scoring, fit\+\_\+intercept, max\+\_\+iter, tol, class\+\_\+weight, verbose, solver, penalty, dual, intercept\+\_\+scaling, multi\+\_\+class, random\+\_\+state, max\+\_\+squared\+\_\+sum, sample\+\_\+weight, l1\+\_\+ratio, score\+\_\+params)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
tuple \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__logistic_a59835dda5959e60841bcb83a1c6e452e}{\+\_\+\+LOGISTIC\+\_\+\+SOLVER\+\_\+\+CONVERGENCE\+\_\+\+MSG}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Logistic Regression
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1linear__model_1_1__logistic_a345ab1f72da761ccf68e0bbec57f3c79}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}!\_check\_multi\_class@{\_check\_multi\_class}}
\index{\_check\_multi\_class@{\_check\_multi\_class}!sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection{\texorpdfstring{\_check\_multi\_class()}{\_check\_multi\_class()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__logistic_a345ab1f72da761ccf68e0bbec57f3c79} 
sklearn.\+linear\+\_\+model.\+\_\+logistic.\+\_\+check\+\_\+multi\+\_\+class (\begin{DoxyParamCaption}\item[{}]{multi\+\_\+class}{, }\item[{}]{solver}{, }\item[{}]{n\+\_\+classes}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes the multi class type, either "multinomial" or "ovr".

For `n_classes` > 2 and a solver that supports it, returns "multinomial".
For all other cases, in particular binary classification, return "ovr".
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__logistic_8py_source_l00082}{82}} of file \mbox{\hyperlink{__logistic_8py_source}{\+\_\+logistic.\+py}}.



Referenced by \mbox{\hyperlink{__logistic_8py_source_l00122}{\+\_\+logistic\+\_\+regression\+\_\+path()}}, \mbox{\hyperlink{__logistic_8py_source_l01189}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression.\+fit()}}, and \mbox{\hyperlink{__logistic_8py_source_l01851}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression\+CV.\+fit()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__logistic_a8d9119fe5a2852152893dd839028f15f}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}!\_check\_solver@{\_check\_solver}}
\index{\_check\_solver@{\_check\_solver}!sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection{\texorpdfstring{\_check\_solver()}{\_check\_solver()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__logistic_a8d9119fe5a2852152893dd839028f15f} 
sklearn.\+linear\+\_\+model.\+\_\+logistic.\+\_\+check\+\_\+solver (\begin{DoxyParamCaption}\item[{}]{solver}{, }\item[{}]{penalty}{, }\item[{}]{dual}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__logistic_8py_source_l00062}{62}} of file \mbox{\hyperlink{__logistic_8py_source}{\+\_\+logistic.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__logistic_a6ed0e655a841cdb769a47cba70c13b07}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}!\_log\_reg\_scoring\_path@{\_log\_reg\_scoring\_path}}
\index{\_log\_reg\_scoring\_path@{\_log\_reg\_scoring\_path}!sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection{\texorpdfstring{\_log\_reg\_scoring\_path()}{\_log\_reg\_scoring\_path()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__logistic_a6ed0e655a841cdb769a47cba70c13b07} 
sklearn.\+linear\+\_\+model.\+\_\+logistic.\+\_\+log\+\_\+reg\+\_\+scoring\+\_\+path (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{train}{, }\item[{}]{test}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{pos\+\_\+class}{, }\item[{}]{Cs}{, }\item[{}]{scoring}{, }\item[{}]{fit\+\_\+intercept}{, }\item[{}]{max\+\_\+iter}{, }\item[{}]{tol}{, }\item[{}]{class\+\_\+weight}{, }\item[{}]{verbose}{, }\item[{}]{solver}{, }\item[{}]{penalty}{, }\item[{}]{dual}{, }\item[{}]{intercept\+\_\+scaling}{, }\item[{}]{multi\+\_\+class}{, }\item[{}]{random\+\_\+state}{, }\item[{}]{max\+\_\+squared\+\_\+sum}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{l1\+\_\+ratio}{, }\item[{}]{score\+\_\+params}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes scores across logistic_regression_path

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.

y : array-like of shape (n_samples,) or (n_samples, n_targets)
    Target labels.

train : list of indices
    The indices of the train set.

test : list of indices
    The indices of the test set.

pos_class : int
    The class with respect to which we perform a one-vs-all fit.
    If None, then it is assumed that the given problem is binary.

Cs : int or list of floats
    Each of the values in Cs describes the inverse of
    regularization strength. If Cs is as an int, then a grid of Cs
    values are chosen in a logarithmic scale between 1e-4 and 1e4.

scoring : str, callable or None
    The scoring method to use for cross-validation. Options:

    - str: see :ref:`scoring_string_names` for options.
    - callable: a scorer callable object (e.g., function) with signature
      ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.
    - `None`: :ref:`accuracy <accuracy_score>` is used.

fit_intercept : bool
    If False, then the bias term is set to zero. Else the last
    term of each coef_ gives us the intercept.

max_iter : int
    Maximum number of iterations for the solver.

tol : float
    Tolerance for stopping criteria.

class_weight : dict or 'balanced'
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

verbose : int
    For the liblinear and lbfgs solvers set verbose to any positive
    number for verbosity.

solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}
    Decides which solver to use.

penalty : {'l1', 'l2', 'elasticnet'}
    Used to specify the norm used in the penalization. The 'newton-cg',
    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
    only supported by the 'saga' solver.

dual : bool
    Dual or primal formulation. Dual formulation is only implemented for
    l2 penalty with liblinear solver. Prefer dual=False when
    n_samples > n_features.

intercept_scaling : float
    Useful only when the solver `liblinear` is used
    and `self.fit_intercept` is set to `True`. In this case, `x` becomes
    `[x, self.intercept_scaling]`,
    i.e. a "synthetic" feature with constant value equal to
    `intercept_scaling` is appended to the instance vector.
    The intercept becomes
    ``intercept_scaling * synthetic_feature_weight``.

    .. note::
        The synthetic feature weight is subject to L1 or L2
        regularization as all other features.
        To lessen the effect of regularization on synthetic feature weight
        (and therefore on the intercept) `intercept_scaling` has to be increased.

multi_class : {'auto', 'ovr', 'multinomial'}
    If the option chosen is 'ovr', then a binary problem is fit for each
    label. For 'multinomial' the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. 'multinomial' is unavailable when solver='liblinear'.

random_state : int, RandomState instance
    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the
    data. See :term:`Glossary <random_state>` for details.

max_squared_sum : float
    Maximum squared sum of X over samples. Used only in SAG solver.
    If None, it will be computed, going through all the samples.
    The value should be precomputed to speed up cross validation.

sample_weight : array-like of shape(n_samples,)
    Array of weights that are assigned to individual samples.
    If not provided, then each sample is given unit weight.

l1_ratio : float
    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only
    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent
    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent
    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a
    combination of L1 and L2.

score_params : dict
    Parameters to pass to the `score` method of the underlying scorer.

Returns
-------
coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)
    List of coefficients for the Logistic Regression model. If
    fit_intercept is set to True then the second dimension will be
    n_features + 1, where the last item represents the intercept.

Cs : ndarray
    Grid of Cs used for cross-validation.

scores : ndarray of shape (n_cs,)
    Scores obtained for each Cs.

n_iter : ndarray of shape(n_cs,)
    Actual number of iteration for each Cs.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__logistic_8py_source_l00601}{601}} of file \mbox{\hyperlink{__logistic_8py_source}{\+\_\+logistic.\+py}}.



References \mbox{\hyperlink{__logistic_8py_source_l00122}{\+\_\+logistic\+\_\+regression\+\_\+path()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__logistic_a4bcf185546ecc3dd3272b38aede3876f}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}!\_logistic\_regression\_path@{\_logistic\_regression\_path}}
\index{\_logistic\_regression\_path@{\_logistic\_regression\_path}!sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection{\texorpdfstring{\_logistic\_regression\_path()}{\_logistic\_regression\_path()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__logistic_a4bcf185546ecc3dd3272b38aede3876f} 
sklearn.\+linear\+\_\+model.\+\_\+logistic.\+\_\+logistic\+\_\+regression\+\_\+path (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{pos\+\_\+class}{ = {\ttfamily None}, }\item[{}]{Cs}{ = {\ttfamily 10}, }\item[{}]{fit\+\_\+intercept}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{solver}{ = {\ttfamily "{}lbfgs"{}}, }\item[{}]{coef}{ = {\ttfamily None}, }\item[{}]{class\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{dual}{ = {\ttfamily False}, }\item[{}]{penalty}{ = {\ttfamily "{}l2"{}}, }\item[{}]{intercept\+\_\+scaling}{ = {\ttfamily 1.0}, }\item[{}]{multi\+\_\+class}{ = {\ttfamily "{}auto"{}}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{check\+\_\+input}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+squared\+\_\+sum}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l1\+\_\+ratio}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute a Logistic Regression model for a list of regularization
parameters.

This is an implementation that uses the result of the previous model
to speed up computations along the set of solutions, making it faster
than sequentially calling LogisticRegression for the different parameters.
Note that there will be no speedup with liblinear solver, since it does
not handle warm-starting.

Read more in the :ref:`User Guide <logistic_regression>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Input data.

y : array-like of shape (n_samples,) or (n_samples, n_targets)
    Input data, target values.

pos_class : int, default=None
    The class with respect to which we perform a one-vs-all fit.
    If None, then it is assumed that the given problem is binary.

Cs : int or array-like of shape (n_cs,), default=10
    List of values for the regularization parameter or integer specifying
    the number of regularization parameters that should be used. In this
    case, the parameters will be chosen in a logarithmic scale between
    1e-4 and 1e4.

fit_intercept : bool, default=True
    Whether to fit an intercept for the model. In this case the shape of
    the returned array is (n_cs, n_features + 1).

max_iter : int, default=100
    Maximum number of iterations for the solver.

tol : float, default=1e-4
    Stopping criterion. For the newton-cg and lbfgs solvers, the iteration
    will stop when ``max{|g_i | i = 1, ..., n} <= tol``
    where ``g_i`` is the i-th component of the gradient.

verbose : int, default=0
    For the liblinear and lbfgs solvers set verbose to any positive
    number for verbosity.

solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \
        default='lbfgs'
    Numerical solver to use.

coef : array-like of shape (n_features,), default=None
    Initialization value for coefficients of logistic regression.
    Useless for liblinear solver.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

dual : bool, default=False
    Dual or primal formulation. Dual formulation is only implemented for
    l2 penalty with liblinear solver. Prefer dual=False when
    n_samples > n_features.

penalty : {'l1', 'l2', 'elasticnet'}, default='l2'
    Used to specify the norm used in the penalization. The 'newton-cg',
    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
    only supported by the 'saga' solver.

intercept_scaling : float, default=1.
    Useful only when the solver `liblinear` is used
    and `self.fit_intercept` is set to `True`. In this case, `x` becomes
    `[x, self.intercept_scaling]`,
    i.e. a "synthetic" feature with constant value equal to
    `intercept_scaling` is appended to the instance vector.
    The intercept becomes
    ``intercept_scaling * synthetic_feature_weight``.

    .. note::
        The synthetic feature weight is subject to L1 or L2
        regularization as all other features.
        To lessen the effect of regularization on synthetic feature weight
        (and therefore on the intercept) `intercept_scaling` has to be increased.

multi_class : {'ovr', 'multinomial', 'auto'}, default='auto'
    If the option chosen is 'ovr', then a binary problem is fit for each
    label. For 'multinomial' the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. 'multinomial' is unavailable when solver='liblinear'.
    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
    and otherwise selects 'multinomial'.

    .. versionadded:: 0.18
       Stochastic Average Gradient descent solver for 'multinomial' case.
    .. versionchanged:: 0.22
        Default changed from 'ovr' to 'auto' in 0.22.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the
    data. See :term:`Glossary <random_state>` for details.

check_input : bool, default=True
    If False, the input arrays X and y will not be checked.

max_squared_sum : float, default=None
    Maximum squared sum of X over samples. Used only in SAG solver.
    If None, it will be computed, going through all the samples.
    The value should be precomputed to speed up cross validation.

sample_weight : array-like of shape(n_samples,), default=None
    Array of weights that are assigned to individual samples.
    If not provided, then each sample is given unit weight.

l1_ratio : float, default=None
    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only
    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent
    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent
    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a
    combination of L1 and L2.

n_threads : int, default=1
   Number of OpenMP threads to use.

Returns
-------
coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)
    List of coefficients for the Logistic Regression model. If
    fit_intercept is set to True then the second dimension will be
    n_features + 1, where the last item represents the intercept. For
    ``multiclass='multinomial'``, the shape is (n_classes, n_cs,
    n_features) or (n_classes, n_cs, n_features + 1).

Cs : ndarray
    Grid of Cs used for cross-validation.

n_iter : array of shape (n_cs,)
    Actual number of iteration for each Cs.

Notes
-----
You might get slightly different results with the solver liblinear than
with the others since this uses LIBLINEAR which penalizes the intercept.

.. versionchanged:: 0.19
    The "copy" parameter was removed.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__logistic_8py_source_l00100}{100}} of file \mbox{\hyperlink{__logistic_8py_source}{\+\_\+logistic.\+py}}.



References \mbox{\hyperlink{__logistic_8py_source_l00082}{\+\_\+check\+\_\+multi\+\_\+class()}}.



Referenced by \mbox{\hyperlink{__logistic_8py_source_l00625}{\+\_\+log\+\_\+reg\+\_\+scoring\+\_\+path()}}, and \mbox{\hyperlink{__logistic_8py_source_l01851}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression\+CV.\+fit()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1linear__model_1_1__logistic_a59835dda5959e60841bcb83a1c6e452e}\index{sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}!\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG@{\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG}}
\index{\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG@{\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG}!sklearn.linear\_model.\_logistic@{sklearn.linear\_model.\_logistic}}
\doxysubsubsection{\texorpdfstring{\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG}{\_LOGISTIC\_SOLVER\_CONVERGENCE\_MSG}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__logistic_a59835dda5959e60841bcb83a1c6e452e} 
tuple sklearn.\+linear\+\_\+model.\+\_\+logistic.\+\_\+\+LOGISTIC\+\_\+\+SOLVER\+\_\+\+CONVERGENCE\+\_\+\+MSG\hspace{0.3cm}{\ttfamily [protected]}}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ (}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{stringliteral}{"{}Please\ also\ refer\ to\ the\ documentation\ for\ alternative\ solver\ options:\(\backslash\)n"{}}}
\DoxyCodeLine{00003\ \ \ \ \ \textcolor{stringliteral}{"{}\ \ \ \ https://scikit-\/learn.org/stable/modules/linear\_model.html"{}}}
\DoxyCodeLine{00004\ \ \ \ \ \textcolor{stringliteral}{"{}\#logistic-\/regression"{}}}
\DoxyCodeLine{00005\ )}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{__logistic_8py_source_l00055}{55}} of file \mbox{\hyperlink{__logistic_8py_source}{\+\_\+logistic.\+py}}.

