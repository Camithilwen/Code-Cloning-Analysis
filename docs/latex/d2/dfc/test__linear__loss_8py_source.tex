\doxysection{test\+\_\+linear\+\_\+loss.\+py}
\hypertarget{test__linear__loss_8py_source}{}\label{test__linear__loss_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/tests/test\_linear\_loss.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/tests/test\_linear\_loss.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00002}00002\ \textcolor{stringliteral}{Tests\ for\ LinearModelLoss}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00003}00003\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00004}00004\ \textcolor{stringliteral}{Note\ that\ correctness\ of\ losses\ (which\ compose\ LinearModelLoss)\ is\ already\ well}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00005}00005\ \textcolor{stringliteral}{covered\ in\ the\ \_loss\ module.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00006}00006\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00008}00008\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00009}00009\ \textcolor{keyword}{import}\ pytest}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00010}00010\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacenumpy_1_1testing}{numpy.testing}}\ \textcolor{keyword}{import}\ assert\_allclose}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00011}00011\ \textcolor{keyword}{from}\ scipy\ \textcolor{keyword}{import}\ linalg,\ optimize}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00012}00012\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00013}00013\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1loss}{sklearn.\_loss.loss}}\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00014}00014\ \ \ \ \ HalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00015}00015\ \ \ \ \ HalfMultinomialLoss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00016}00016\ \ \ \ \ HalfPoissonLoss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00017}00017\ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00018}00018\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1datasets}{sklearn.datasets}}\ \textcolor{keyword}{import}\ make\_low\_rank\_matrix}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00019}00019\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss}{sklearn.linear\_model.\_linear\_loss}}\ \textcolor{keyword}{import}\ LinearModelLoss}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00020}00020\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1utils_1_1extmath}{sklearn.utils.extmath}}\ \textcolor{keyword}{import}\ squared\_norm}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00021}00021\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1utils_1_1fixes}{sklearn.utils.fixes}}\ \textcolor{keyword}{import}\ CSR\_CONTAINERS}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00022}00022\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00023}00023\ \textcolor{comment}{\#\ We\ do\ not\ need\ to\ test\ all\ losses,\ just\ what\ LinearModelLoss\ does\ on\ top\ of\ the}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00024}00024\ \textcolor{comment}{\#\ base\ losses.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00025}00025\ LOSSES\ =\ [HalfBinomialLoss,\ HalfMultinomialLoss,\ HalfPoissonLoss]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00026}00026\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00027}00027\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00028}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{00028}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00029}00029\ \ \ \ \ linear\_model\_loss,\ n\_samples,\ n\_features,\ coef\_bound=(-\/2,\ 2),\ seed=42}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00030}00030\ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00031}00031\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Random\ generate\ y,\ X\ and\ coef\ in\ valid\ range."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00032}00032\ \ \ \ \ rng\ =\ np.random.RandomState(seed)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00033}00033\ \ \ \ \ n\_dof\ =\ n\_features\ +\ linear\_model\_loss.fit\_intercept}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00034}00034\ \ \ \ \ X\ =\ make\_low\_rank\_matrix(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00035}00035\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00036}00036\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00037}00037\ \ \ \ \ \ \ \ \ random\_state=rng,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00038}00038\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00039}00039\ \ \ \ \ coef\ =\ linear\_model\_loss.init\_zero\_coef(X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00040}00040\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00041}00041\ \ \ \ \ \textcolor{keywordflow}{if}\ linear\_model\_loss.base\_loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00042}00042\ \ \ \ \ \ \ \ \ n\_classes\ =\ linear\_model\_loss.base\_loss.n\_classes}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00043}00043\ \ \ \ \ \ \ \ \ coef.flat[:]\ =\ rng.uniform(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00044}00044\ \ \ \ \ \ \ \ \ \ \ \ \ low=coef\_bound[0],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00045}00045\ \ \ \ \ \ \ \ \ \ \ \ \ high=coef\_bound[1],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00046}00046\ \ \ \ \ \ \ \ \ \ \ \ \ size=n\_classes\ *\ n\_dof,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00047}00047\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00048}00048\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ linear\_model\_loss.fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00049}00049\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ coef[:,\ :-\/1].T\ +\ coef[:,\ -\/1]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00050}00050\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00051}00051\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ coef.T}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00052}00052\ \ \ \ \ \ \ \ \ proba\ =\ linear\_model\_loss.base\_loss.link.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00053}00053\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00054}00054\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ y\ =\ rng.choice(np.arange(n\_classes),\ p=proba)\ does\ not\ work.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00055}00055\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ https://stackoverflow.com/a/34190035/16761084}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00056}00056\ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }choice\_vectorized(items,\ p):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00057}00057\ \ \ \ \ \ \ \ \ \ \ \ \ s\ =\ p.cumsum(axis=1)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00058}00058\ \ \ \ \ \ \ \ \ \ \ \ \ r\ =\ rng.rand(p.shape[0])[:,\ \textcolor{keywordtype}{None}]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00059}00059\ \ \ \ \ \ \ \ \ \ \ \ \ k\ =\ (s\ <\ r).sum(axis=1)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ items[k]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00061}00061\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00062}00062\ \ \ \ \ \ \ \ \ y\ =\ choice\_vectorized(np.arange(n\_classes),\ p=proba).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00063}00063\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00064}00064\ \ \ \ \ \ \ \ \ coef.flat[:]\ =\ rng.uniform(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00065}00065\ \ \ \ \ \ \ \ \ \ \ \ \ low=coef\_bound[0],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00066}00066\ \ \ \ \ \ \ \ \ \ \ \ \ high=coef\_bound[1],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ \ \ \ \ size=n\_dof,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00069}00069\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ linear\_model\_loss.fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00070}00070\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ coef[:-\/1]\ +\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00071}00071\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00072}00072\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ coef}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ y\ =\ linear\_model\_loss.base\_loss.link.inverse(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ +\ rng.uniform(low=-\/1,\ high=1,\ size=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00076}00076\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00077}00077\ \ \ \ \ \textcolor{keywordflow}{return}\ X,\ y,\ coef}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00078}00078\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00079}00079\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00080}00080\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}base\_loss"{},\ LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00081}00081\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00082}00082\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}n\_features"{},\ [0,\ 1,\ 10])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00083}00083\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}dtype"{},\ [None,\ np.float32,\ np.float64,\ np.int64])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00084}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_ada4e0d385b593936e1cd56f4a349dece}{00084}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_ada4e0d385b593936e1cd56f4a349dece}{test\_init\_zero\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00085}00085\ \ \ \ \ base\_loss,\ fit\_intercept,\ n\_features,\ dtype,\ global\_random\_seed}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00086}00086\ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00087}00087\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ init\_zero\_coef\ initializes\ coef\ correctly."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00088}00088\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=base\_loss(),\ fit\_intercept=fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00089}00089\ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00090}00090\ \ \ \ \ X\ =\ rng.normal(size=(5,\ n\_features))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00091}00091\ \ \ \ \ coef\ =\ loss.init\_zero\_coef(X,\ dtype=dtype)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00092}00092\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.base\_loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00093}00093\ \ \ \ \ \ \ \ \ n\_classes\ =\ loss.base\_loss.n\_classes}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00094}00094\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ coef.shape\ ==\ (n\_classes,\ n\_features\ +\ fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00095}00095\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ coef.flags[\textcolor{stringliteral}{"{}F\_CONTIGUOUS"{}}]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00096}00096\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00097}00097\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ coef.shape\ ==\ (n\_features\ +\ fit\_intercept,)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00098}00098\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00099}00099\ \ \ \ \ \textcolor{keywordflow}{if}\ dtype\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00100}00100\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ coef.dtype\ ==\ X.dtype}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00101}00101\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00102}00102\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ coef.dtype\ ==\ dtype}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00103}00103\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00104}00104\ \ \ \ \ \textcolor{keyword}{assert}\ np.count\_nonzero(coef)\ ==\ 0}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00105}00105\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00106}00106\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00107}00107\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}base\_loss"{},\ LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00108}00108\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00109}00109\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00110}00110\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}l2\_reg\_strength"{},\ [0,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00111}00111\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}csr\_container"{},\ CSR\_CONTAINERS)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00112}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a5f883ca3bb26105181811613663acd28}{00112}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a5f883ca3bb26105181811613663acd28}{test\_loss\_grad\_hess\_are\_the\_same}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00113}00113\ \ \ \ \ base\_loss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00114}00114\ \ \ \ \ fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00115}00115\ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00116}00116\ \ \ \ \ l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00117}00117\ \ \ \ \ csr\_container,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00118}00118\ \ \ \ \ global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00119}00119\ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00120}00120\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ loss\ and\ gradient\ are\ the\ same\ across\ different\ functions."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00121}00121\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=base\_loss(),\ fit\_intercept=fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00122}00122\ \ \ \ \ X,\ y,\ coef\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ linear\_model\_loss=loss,\ n\_samples=10,\ n\_features=5,\ seed=global\_random\_seed}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00124}00124\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00125}00125\ \ \ \ \ X\_old,\ y\_old,\ coef\_old\ =\ X.copy(),\ y.copy(),\ coef.copy()}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00126}00126\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00127}00127\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00128}00128\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y.shape[0],\ num=y.shape[0])}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00129}00129\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00130}00130\ \ \ \ \ l1\ =\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00132}00132\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00133}00133\ \ \ \ \ g1\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00135}00135\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00136}00136\ \ \ \ \ l2,\ g2\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00138}00138\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00139}00139\ \ \ \ \ g3,\ h3\ =\ loss.gradient\_hessian\_product(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00141}00141\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00142}00142\ \ \ \ \ g4,\ h4,\ \_\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00144}00144\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00145}00145\ \ \ \ \ assert\_allclose(l1,\ l2)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00146}00146\ \ \ \ \ assert\_allclose(g1,\ g2)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00147}00147\ \ \ \ \ assert\_allclose(g1,\ g3)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00148}00148\ \ \ \ \ assert\_allclose(g1,\ g4)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00149}00149\ \ \ \ \ \textcolor{comment}{\#\ The\ ravelling\ only\ takes\ effect\ for\ multiclass.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00150}00150\ \ \ \ \ assert\_allclose(h4\ @\ g4.ravel(order=\textcolor{stringliteral}{"{}F"{}}),\ h3(g3).ravel(order=\textcolor{stringliteral}{"{}F"{}}))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00151}00151\ \ \ \ \ \textcolor{comment}{\#\ Test\ that\ gradient\_out\ and\ hessian\_out\ are\ considered\ properly.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00152}00152\ \ \ \ \ g\_out\ =\ np.empty\_like(coef)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00153}00153\ \ \ \ \ h\_out\ =\ np.empty\_like(coef,\ shape=(coef.size,\ coef.size))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00154}00154\ \ \ \ \ g5,\ h5,\ \_\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00157}00157\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ gradient\_out=g\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ hessian\_out=h\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00162}00162\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00163}00163\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(g5,\ g\_out)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00164}00164\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(h5,\ h\_out)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00165}00165\ \ \ \ \ assert\_allclose(g5,\ g\_out)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00166}00166\ \ \ \ \ assert\_allclose(h5,\ h\_out)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00167}00167\ \ \ \ \ assert\_allclose(g1,\ g5)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00168}00168\ \ \ \ \ assert\_allclose(h5,\ h4)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00169}00169\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00170}00170\ \ \ \ \ \textcolor{comment}{\#\ same\ for\ sparse\ X}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00171}00171\ \ \ \ \ Xs\ =\ csr\_container(X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00172}00172\ \ \ \ \ l1\_sp\ =\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ coef,\ Xs,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00174}00174\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00175}00175\ \ \ \ \ g1\_sp\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00176}00176\ \ \ \ \ \ \ \ \ coef,\ Xs,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00177}00177\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00178}00178\ \ \ \ \ l2\_sp,\ g2\_sp\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00179}00179\ \ \ \ \ \ \ \ \ coef,\ Xs,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00180}00180\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00181}00181\ \ \ \ \ g3\_sp,\ h3\_sp\ =\ loss.gradient\_hessian\_product(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00182}00182\ \ \ \ \ \ \ \ \ coef,\ Xs,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00183}00183\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00184}00184\ \ \ \ \ g4\_sp,\ h4\_sp,\ \_\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00185}00185\ \ \ \ \ \ \ \ \ coef,\ Xs,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00186}00186\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00187}00187\ \ \ \ \ assert\_allclose(l1,\ l1\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00188}00188\ \ \ \ \ assert\_allclose(l1,\ l2\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00189}00189\ \ \ \ \ assert\_allclose(g1,\ g1\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00190}00190\ \ \ \ \ assert\_allclose(g1,\ g2\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00191}00191\ \ \ \ \ assert\_allclose(g1,\ g3\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00192}00192\ \ \ \ \ assert\_allclose(h3(g1),\ h3\_sp(g1\_sp))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00193}00193\ \ \ \ \ assert\_allclose(g1,\ g4\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00194}00194\ \ \ \ \ assert\_allclose(h4,\ h4\_sp)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00195}00195\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00196}00196\ \ \ \ \ \textcolor{comment}{\#\ X,\ y\ and\ coef\ should\ not\ have\ changed}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00197}00197\ \ \ \ \ assert\_allclose(X,\ X\_old)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00198}00198\ \ \ \ \ assert\_allclose(Xs.toarray(),\ X\_old)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00199}00199\ \ \ \ \ assert\_allclose(y,\ y\_old)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00200}00200\ \ \ \ \ assert\_allclose(coef,\ coef\_old)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00201}00201\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00202}00202\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00203}00203\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}base\_loss"{},\ LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00204}00204\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00205}00205\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}l2\_reg\_strength"{},\ [0,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00206}00206\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}X\_container"{},\ CSR\_CONTAINERS\ +\ [None])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00207}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_ae7d1832a0beb1cd6066e5ef8f70db9c7}{00207}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_ae7d1832a0beb1cd6066e5ef8f70db9c7}{test\_loss\_gradients\_hessp\_intercept}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00208}00208\ \ \ \ \ base\_loss,\ sample\_weight,\ l2\_reg\_strength,\ X\_container,\ global\_random\_seed}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00209}00209\ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00210}00210\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ loss\ and\ gradient\ handle\ intercept\ correctly."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00211}00211\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=base\_loss(),\ fit\_intercept=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00212}00212\ \ \ \ \ loss\_inter\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=base\_loss(),\ fit\_intercept=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00213}00213\ \ \ \ \ n\_samples,\ n\_features\ =\ 10,\ 5}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00214}00214\ \ \ \ \ X,\ y,\ coef\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00215}00215\ \ \ \ \ \ \ \ \ linear\_model\_loss=loss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00216}00216\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00218}00218\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00219}00219\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00220}00220\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00221}00221\ \ \ \ \ X[:,\ -\/1]\ =\ 1\ \ \textcolor{comment}{\#\ make\ last\ column\ of\ 1\ to\ mimic\ intercept\ term}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00222}00222\ \ \ \ \ X\_inter\ =\ X[}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00223}00223\ \ \ \ \ \ \ \ \ :,\ :-\/1}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00224}00224\ \ \ \ \ ]\ \ \textcolor{comment}{\#\ exclude\ intercept\ column\ as\ it\ is\ added\ automatically\ by\ loss\_inter}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00225}00225\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00226}00226\ \ \ \ \ \textcolor{keywordflow}{if}\ X\_container\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00227}00227\ \ \ \ \ \ \ \ \ X\ =\ X\_container(X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00228}00228\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00229}00229\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00230}00230\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y.shape[0],\ num=y.shape[0])}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00231}00231\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00232}00232\ \ \ \ \ l,\ g\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00233}00233\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00234}00234\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00235}00235\ \ \ \ \ \_,\ hessp\ =\ loss.gradient\_hessian\_product(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00237}00237\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00238}00238\ \ \ \ \ l\_inter,\ g\_inter\ =\ loss\_inter.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00239}00239\ \ \ \ \ \ \ \ \ coef,\ X\_inter,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00240}00240\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00241}00241\ \ \ \ \ \_,\ hessp\_inter\ =\ loss\_inter.gradient\_hessian\_product(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ coef,\ X\_inter,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00243}00243\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00244}00244\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00245}00245\ \ \ \ \ \textcolor{comment}{\#\ Note,\ that\ intercept\ gets\ no\ L2\ penalty.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00246}00246\ \ \ \ \ \textcolor{keyword}{assert}\ l\ ==\ pytest.approx(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00247}00247\ \ \ \ \ \ \ \ \ l\_inter\ +\ 0.5\ *\ l2\_reg\_strength\ *\ squared\_norm(coef.T[-\/1])}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00248}00248\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00249}00249\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00250}00250\ \ \ \ \ g\_inter\_corrected\ =\ g\_inter}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00251}00251\ \ \ \ \ g\_inter\_corrected.T[-\/1]\ +=\ l2\_reg\_strength\ *\ coef.T[-\/1]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00252}00252\ \ \ \ \ assert\_allclose(g,\ g\_inter\_corrected)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00253}00253\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00254}00254\ \ \ \ \ s\ =\ np.random.RandomState(global\_random\_seed).randn(*coef.shape)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00255}00255\ \ \ \ \ h\ =\ hessp(s)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00256}00256\ \ \ \ \ h\_inter\ =\ hessp\_inter(s)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00257}00257\ \ \ \ \ h\_inter\_corrected\ =\ h\_inter}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00258}00258\ \ \ \ \ h\_inter\_corrected.T[-\/1]\ +=\ l2\_reg\_strength\ *\ s.T[-\/1]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00259}00259\ \ \ \ \ assert\_allclose(h,\ h\_inter\_corrected)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00260}00260\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00261}00261\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00262}00262\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}base\_loss"{},\ LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00263}00263\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00264}00264\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00265}00265\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}l2\_reg\_strength"{},\ [0,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00266}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a2146538f9ab20b014d199d6f389c6524}{00266}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a2146538f9ab20b014d199d6f389c6524}{test\_gradients\_hessians\_numerically}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00267}00267\ \ \ \ \ base\_loss,\ fit\_intercept,\ sample\_weight,\ l2\_reg\_strength,\ global\_random\_seed}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00268}00268\ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00269}00269\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ gradients\ and\ hessians\ with\ numerical\ derivatives.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00270}00270\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00271}00271\ \textcolor{stringliteral}{\ \ \ \ Gradient\ should\ equal\ the\ numerical\ derivatives\ of\ the\ loss\ function.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00272}00272\ \textcolor{stringliteral}{\ \ \ \ Hessians\ should\ equal\ the\ numerical\ derivatives\ of\ gradients.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00273}00273\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00274}00274\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=base\_loss(),\ fit\_intercept=fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00275}00275\ \ \ \ \ n\_samples,\ n\_features\ =\ 10,\ 5}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00276}00276\ \ \ \ \ X,\ y,\ coef\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00277}00277\ \ \ \ \ \ \ \ \ linear\_model\_loss=loss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00278}00278\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00281}00281\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00282}00282\ \ \ \ \ coef\ =\ coef.ravel(order=\textcolor{stringliteral}{"{}F"{}})\ \ \textcolor{comment}{\#\ this\ is\ important\ only\ for\ multinomial\ loss}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00283}00283\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00284}00284\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00285}00285\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y.shape[0],\ num=y.shape[0])}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00286}00286\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00287}00287\ \ \ \ \ \textcolor{comment}{\#\ 1.\ Check\ gradients\ numerically}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00288}00288\ \ \ \ \ eps\ =\ 1e-\/6}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00289}00289\ \ \ \ \ g,\ hessp\ =\ loss.gradient\_hessian\_product(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ coef,\ X,\ y,\ sample\_weight=sample\_weight,\ l2\_reg\_strength=l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00291}00291\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00292}00292\ \ \ \ \ \textcolor{comment}{\#\ Use\ a\ trick\ to\ get\ central\ finite\ difference\ of\ accuracy\ 4\ (five-\/point\ stencil)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00293}00293\ \ \ \ \ \textcolor{comment}{\#\ https://en.wikipedia.org/wiki/Numerical\_differentiation}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00294}00294\ \ \ \ \ \textcolor{comment}{\#\ https://en.wikipedia.org/wiki/Finite\_difference\_coefficient}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00295}00295\ \ \ \ \ \textcolor{comment}{\#\ approx\_g1\ =\ (f(x\ +\ eps)\ -\/\ f(x\ -\/\ eps))\ /\ (2*eps)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00296}00296\ \ \ \ \ approx\_g1\ =\ optimize.approx\_fprime(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00298}00298\ \ \ \ \ \ \ \ \ \textcolor{keyword}{lambda}\ coef:\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ -\/\ eps,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ 2\ *\ eps,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00306}00306\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00307}00307\ \ \ \ \ \textcolor{comment}{\#\ approx\_g2\ =\ (f(x\ +\ 2*eps)\ -\/\ f(x\ -\/\ 2*eps))\ /\ (4*eps)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00308}00308\ \ \ \ \ approx\_g2\ =\ optimize.approx\_fprime(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ \textcolor{keyword}{lambda}\ coef:\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ -\/\ 2\ *\ eps,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00315}00315\ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ 4\ *\ eps,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00318}00318\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00319}00319\ \ \ \ \ \textcolor{comment}{\#\ Five-\/point\ stencil\ approximation}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00320}00320\ \ \ \ \ \textcolor{comment}{\#\ See:\ https://en.wikipedia.org/wiki/Five-\/point\_stencil\#1D\_first\_derivative}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00321}00321\ \ \ \ \ approx\_g\ =\ (4\ *\ approx\_g1\ -\/\ approx\_g2)\ /\ 3}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00322}00322\ \ \ \ \ assert\_allclose(g,\ approx\_g,\ rtol=1e-\/2,\ atol=1e-\/8)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00323}00323\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00324}00324\ \ \ \ \ \textcolor{comment}{\#\ 2.\ Check\ hessp\ numerically\ along\ the\ second\ direction\ of\ the\ gradient}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00325}00325\ \ \ \ \ vector\ =\ np.zeros\_like(g)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00326}00326\ \ \ \ \ vector[1]\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00327}00327\ \ \ \ \ hess\_col\ =\ hessp(vector)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00328}00328\ \ \ \ \ \textcolor{comment}{\#\ Computation\ of\ the\ Hessian\ is\ particularly\ fragile\ to\ numerical\ errors\ when\ doing}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00329}00329\ \ \ \ \ \textcolor{comment}{\#\ simple\ finite\ differences.\ Here\ we\ compute\ the\ grad\ along\ a\ path\ in\ the\ direction}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00330}00330\ \ \ \ \ \textcolor{comment}{\#\ of\ the\ vector\ and\ then\ use\ a\ least-\/square\ regression\ to\ estimate\ the\ slope}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00331}00331\ \ \ \ \ eps\ =\ 1e-\/3}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00332}00332\ \ \ \ \ d\_x\ =\ np.linspace(-\/eps,\ eps,\ 30)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00333}00333\ \ \ \ \ d\_grad\ =\ np.array(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00334}00334\ \ \ \ \ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\ +\ t\ *\ vector,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ t\ \textcolor{keywordflow}{in}\ d\_x}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00344}00344\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00345}00345\ \ \ \ \ d\_grad\ -\/=\ d\_grad.mean(axis=0)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00346}00346\ \ \ \ \ approx\_hess\_col\ =\ linalg.lstsq(d\_x[:,\ np.newaxis],\ d\_grad)[0].ravel()}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00347}00347\ \ \ \ \ assert\_allclose(approx\_hess\_col,\ hess\_col,\ rtol=1e-\/3)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00348}00348\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00349}00349\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00350}00350\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00351}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a5465428a317bee7e2af1ae69ac4807ab}{00351}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a5465428a317bee7e2af1ae69ac4807ab}{test\_multinomial\_coef\_shape}}(fit\_intercept,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00352}00352\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ multinomial\ LinearModelLoss\ respects\ shape\ of\ coef."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00353}00353\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(),\ fit\_intercept=fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00354}00354\ \ \ \ \ n\_samples,\ n\_features\ =\ 10,\ 5}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00355}00355\ \ \ \ \ X,\ y,\ coef\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00356}00356\ \ \ \ \ \ \ \ \ linear\_model\_loss=loss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00357}00357\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00360}00360\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00361}00361\ \ \ \ \ s\ =\ np.random.RandomState(global\_random\_seed).randn(*coef.shape)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00362}00362\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00363}00363\ \ \ \ \ l,\ g\ =\ loss.loss\_gradient(coef,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00364}00364\ \ \ \ \ g1\ =\ loss.gradient(coef,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00365}00365\ \ \ \ \ g2,\ hessp\ =\ loss.gradient\_hessian\_product(coef,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00366}00366\ \ \ \ \ h\ =\ hessp(s)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00367}00367\ \ \ \ \ \textcolor{keyword}{assert}\ g.shape\ ==\ coef.shape}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00368}00368\ \ \ \ \ \textcolor{keyword}{assert}\ h.shape\ ==\ coef.shape}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00369}00369\ \ \ \ \ assert\_allclose(g,\ g1)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00370}00370\ \ \ \ \ assert\_allclose(g,\ g2)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00371}00371\ \ \ \ \ g3,\ hess,\ \_\ =\ loss.gradient\_hessian(coef,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00372}00372\ \ \ \ \ \textcolor{keyword}{assert}\ g3.shape\ ==\ coef.shape}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00373}00373\ \ \ \ \ \textcolor{comment}{\#\ But\ full\ hessian\ is\ always\ 2d.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00374}00374\ \ \ \ \ \textcolor{keyword}{assert}\ hess.shape\ ==\ (coef.size,\ coef.size)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00375}00375\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00376}00376\ \ \ \ \ coef\_r\ =\ coef.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00377}00377\ \ \ \ \ s\_r\ =\ s.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00378}00378\ \ \ \ \ l\_r,\ g\_r\ =\ loss.loss\_gradient(coef\_r,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00379}00379\ \ \ \ \ g1\_r\ =\ loss.gradient(coef\_r,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00380}00380\ \ \ \ \ g2\_r,\ hessp\_r\ =\ loss.gradient\_hessian\_product(coef\_r,\ X,\ y)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00381}00381\ \ \ \ \ h\_r\ =\ hessp\_r(s\_r)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00382}00382\ \ \ \ \ \textcolor{keyword}{assert}\ g\_r.shape\ ==\ coef\_r.shape}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00383}00383\ \ \ \ \ \textcolor{keyword}{assert}\ h\_r.shape\ ==\ coef\_r.shape}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00384}00384\ \ \ \ \ assert\_allclose(g\_r,\ g1\_r)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00385}00385\ \ \ \ \ assert\_allclose(g\_r,\ g2\_r)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00386}00386\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00387}00387\ \ \ \ \ assert\_allclose(g,\ g\_r.reshape(loss.base\_loss.n\_classes,\ -\/1,\ order=\textcolor{stringliteral}{"{}F"{}}))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00388}00388\ \ \ \ \ assert\_allclose(h,\ h\_r.reshape(loss.base\_loss.n\_classes,\ -\/1,\ order=\textcolor{stringliteral}{"{}F"{}}))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00389}00389\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00390}00390\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00391}00391\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00392}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a50324b92cf016067abf81649d5d0202b}{00392}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_a50324b92cf016067abf81649d5d0202b}{test\_multinomial\_hessian\_3\_classes}}(sample\_weight,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00393}00393\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ multinomial\ hessian\ for\ 3\ classes\ and\ 2\ points.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00394}00394\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00395}00395\ \textcolor{stringliteral}{\ \ \ \ For\ n\_classes\ =\ 3\ and\ n\_samples\ =\ 2,\ we\ have}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00396}00396\ \textcolor{stringliteral}{\ \ \ \ \ \ p0\ =\ [p0\_0,\ p0\_1]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00397}00397\ \textcolor{stringliteral}{\ \ \ \ \ \ p1\ =\ [p1\_0,\ p1\_1]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00398}00398\ \textcolor{stringliteral}{\ \ \ \ \ \ p2\ =\ [p2\_0,\ p2\_1]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00399}00399\ \textcolor{stringliteral}{\ \ \ \ and\ with\ 2\ x\ 2\ diagonal\ subblocks}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00400}00400\ \textcolor{stringliteral}{\ \ \ \ \ \ H\ =\ [p0\ *\ (1-\/p0),\ \ \ \ -\/p0\ *\ p1,\ \ \ \ -\/p0\ *\ p2]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00401}00401\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ [\ \ \ -\/p0\ *\ p1,\ p1\ *\ (1-\/p1),\ \ \ \ -\/p1\ *\ p2]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00402}00402\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ [\ \ \ -\/p0\ *\ p2,\ \ \ \ -\/p1\ *\ p2,\ p2\ *\ (1-\/p2)]}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00403}00403\ \textcolor{stringliteral}{\ \ \ \ \ \ hess\ =\ X'\ H\ X}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00404}00404\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00405}00405\ \ \ \ \ n\_samples,\ n\_features,\ n\_classes\ =\ 2,\ 5,\ 3}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00406}00406\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=n\_classes),\ fit\_intercept=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00408}00408\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00409}00409\ \ \ \ \ X,\ y,\ coef\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aba6de1423bd923f8d4a792fb44100f4e}{random\_X\_y\_coef}}(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ linear\_model\_loss=loss,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00414}00414\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00415}00415\ \ \ \ \ coef\ =\ coef.ravel(order=\textcolor{stringliteral}{"{}F"{}})\ \ \textcolor{comment}{\#\ this\ is\ important\ only\ for\ multinomial\ loss}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00416}00416\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00417}00417\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00418}00418\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y.shape[0],\ num=y.shape[0])}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00419}00419\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00420}00420\ \ \ \ \ grad,\ hess,\ \_\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00421}00421\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00422}00422\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00423}00423\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00424}00424\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00425}00425\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00426}00426\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00427}00427\ \ \ \ \ \textcolor{comment}{\#\ Hessian\ must\ be\ a\ symmetrix\ matrix.}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00428}00428\ \ \ \ \ assert\_allclose(hess,\ hess.T)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00429}00429\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00430}00430\ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ loss.weight\_intercept\_raw(coef,\ X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00431}00431\ \ \ \ \ grad\_pointwise,\ proba\ =\ loss.base\_loss.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00435}00435\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00436}00436\ \ \ \ \ p0d,\ p1d,\ p2d,\ oned\ =\ (}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ np.diag(proba[:,\ 0]),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00438}00438\ \ \ \ \ \ \ \ \ np.diag(proba[:,\ 1]),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ np.diag(proba[:,\ 2]),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00440}00440\ \ \ \ \ \ \ \ \ np.diag(np.ones(2)),}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00441}00441\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00442}00442\ \ \ \ \ h\ =\ np.block(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00443}00443\ \ \ \ \ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \ \ \ \ [p0d\ *\ (oned\ -\/\ p0d),\ -\/p0d\ *\ p1d,\ -\/p0d\ *\ p2d],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ \ \ \ \ [-\/p0d\ *\ p1d,\ p1d\ *\ (oned\ -\/\ p1d),\ -\/p1d\ *\ p2d],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00446}00446\ \ \ \ \ \ \ \ \ \ \ \ \ [-\/p0d\ *\ p2d,\ -\/p1d\ *\ p2d,\ p2d\ *\ (oned\ -\/\ p2d)],}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00448}00448\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00449}00449\ \ \ \ \ h\ =\ h.reshape((n\_classes,\ n\_samples,\ n\_classes,\ n\_samples))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00450}00450\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ h\ /=\ n\_samples}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00452}00452\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00453}00453\ \ \ \ \ \ \ \ \ h\ *=\ sample\_weight\ /\ np.sum(sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00454}00454\ \ \ \ \ \textcolor{comment}{\#\ hess\_expected.shape\ =\ (n\_features,\ n\_classes,\ n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00455}00455\ \ \ \ \ hess\_expected\ =\ np.einsum(\textcolor{stringliteral}{"{}ij,\ mini,\ ik-\/>jmnk"{}},\ X,\ h,\ X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00456}00456\ \ \ \ \ hess\_expected\ =\ np.moveaxis(hess\_expected,\ 2,\ 3)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00457}00457\ \ \ \ \ hess\_expected\ =\ hess\_expected.reshape(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00458}00458\ \ \ \ \ \ \ \ \ n\_classes\ *\ n\_features,\ n\_classes\ *\ n\_features,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00459}00459\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00460}00460\ \ \ \ \ assert\_allclose(hess\_expected,\ hess\_expected.T)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00461}00461\ \ \ \ \ assert\_allclose(hess,\ hess\_expected)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00462}00462\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00463}00463\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00464}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aa56641957a85c6cf5960e69c7bd54bc0}{00464}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss_aa56641957a85c6cf5960e69c7bd54bc0}{test\_linear\_loss\_gradient\_hessian\_raises\_wrong\_out\_parameters}}():}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00465}00465\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ wrong\ gradient\_out\ and\ hessian\_out\ raises\ errors."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00466}00466\ \ \ \ \ n\_samples,\ n\_features,\ n\_classes\ =\ 5,\ 2,\ 3}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00467}00467\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ fit\_intercept=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00468}00468\ \ \ \ \ X\ =\ np.ones((n\_samples,\ n\_features))}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00469}00469\ \ \ \ \ y\ =\ np.ones(n\_samples)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00470}00470\ \ \ \ \ coef\ =\ loss.init\_zero\_coef(X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00471}00471\ \ \ \ \ gradient\_out\ =\ np.zeros(1)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00472}00472\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00473}00473\ \ \ \ \ \ \ \ \ ValueError,\ match=\textcolor{stringliteral}{"{}gradient\_out\ is\ required\ to\ have\ shape\ coef.shape"{}}}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00474}00474\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00475}00475\ \ \ \ \ \ \ \ \ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00476}00476\ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00477}00477\ \ \ \ \ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00478}00478\ \ \ \ \ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00479}00479\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00480}00480\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00481}00481\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00482}00482\ \ \ \ \ hessian\_out\ =\ np.zeros(1)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00483}00483\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=\textcolor{stringliteral}{"{}hessian\_out\ is\ required\ to\ have\ shape"{}}):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00486}00486\ \ \ \ \ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00487}00487\ \ \ \ \ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00489}00489\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out=hessian\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00490}00490\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00491}00491\ }
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00492}00492\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(),\ fit\_intercept=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00493}00493\ \ \ \ \ coef\ =\ loss.init\_zero\_coef(X)}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00494}00494\ \ \ \ \ gradient\_out\ =\ np.zeros((2\ *\ n\_classes,\ n\_features))[::2]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00495}00495\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=\textcolor{stringliteral}{"{}gradient\_out\ must\ be\ F-\/contiguous"{}}):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00496}00496\ \ \ \ \ \ \ \ \ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00501}00501\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00502}00502\ \ \ \ \ hessian\_out\ =\ np.zeros((2\ *\ n\_classes\ *\ n\_features,\ n\_classes\ *\ n\_features))[::2]}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00503}00503\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=\textcolor{stringliteral}{"{}hessian\_out\ must\ be\ contiguous"{}}):}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00504}00504\ \ \ \ \ \ \ \ \ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00505}00505\ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00506}00506\ \ \ \ \ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00507}00507\ \ \ \ \ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00508}00508\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00509}00509\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out=hessian\_out,}
\DoxyCodeLine{\Hypertarget{test__linear__loss_8py_source_l00510}00510\ \ \ \ \ \ \ \ \ )}

\end{DoxyCode}
