\doxysection{sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss Class Reference}
\hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{}\label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a72a722924c5fe32413be8fe95e0f4e0e}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, base\+\_\+loss, fit\+\_\+intercept)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a90d92487a6eb759c86797dfbfe23153c}{init\+\_\+zero\+\_\+coef}} (self, X, dtype=None)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\+\_\+intercept}} (self, coef)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\+\_\+intercept\+\_\+raw}} (self, coef, X)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}{l2\+\_\+penalty}} (self, weights, l2\+\_\+reg\+\_\+strength)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f}{loss}} (self, coef, X, y, sample\+\_\+weight=None, l2\+\_\+reg\+\_\+strength=0.\+0, n\+\_\+threads=1, raw\+\_\+prediction=None)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958}{loss\+\_\+gradient}} (self, coef, X, y, sample\+\_\+weight=None, l2\+\_\+reg\+\_\+strength=0.\+0, n\+\_\+threads=1, raw\+\_\+prediction=None)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2}{gradient}} (self, coef, X, y, sample\+\_\+weight=None, l2\+\_\+reg\+\_\+strength=0.\+0, n\+\_\+threads=1, raw\+\_\+prediction=None)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}{gradient\+\_\+hessian}} (self, coef, X, y, sample\+\_\+weight=None, l2\+\_\+reg\+\_\+strength=0.\+0, n\+\_\+threads=1, gradient\+\_\+out=None, hessian\+\_\+out=None, raw\+\_\+prediction=None)
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aabd136d35f0b757f2d9d4da9a41ceabd}{gradient\+\_\+hessian\+\_\+product}} (self, coef, X, y, sample\+\_\+weight=None, l2\+\_\+reg\+\_\+strength=0.\+0, n\+\_\+threads=1)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\+\_\+loss}} = base\+\_\+loss
\item 
\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\+\_\+intercept}} = fit\+\_\+intercept
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}General class for loss functions with raw_prediction = X @ coef + intercept.

Note that raw_prediction is also known as linear predictor.

The loss is the average of per sample losses and includes a term for L2
regularization::

    loss = 1 / s_sum * sum_i s_i loss(y_i, X_i @ coef + intercept)
           + 1/2 * l2_reg_strength * ||coef||_2^2

with sample weights s_i=1 if sample_weight=None and s_sum=sum_i s_i.

Gradient and hessian, for simplicity without intercept, are::

    gradient = 1 / s_sum * X.T @ loss.gradient + l2_reg_strength * coef
    hessian = 1 / s_sum * X.T @ diag(loss.hessian) @ X
              + l2_reg_strength * identity

Conventions:
    if fit_intercept:
        n_dof =  n_features + 1
    else:
        n_dof = n_features

    if base_loss.is_multiclass:
        coef.shape = (n_classes, n_dof) or ravelled (n_classes * n_dof,)
    else:
        coef.shape = (n_dof,)

    The intercept term is at the end of the coef array:
    if base_loss.is_multiclass:
        if coef.shape (n_classes, n_dof):
            intercept = coef[:, -1]
        if coef.shape (n_classes * n_dof,)
            intercept = coef[n_features::n_dof] = coef[(n_dof-1)::n_dof]
        intercept.shape = (n_classes,)
    else:
        intercept = coef[-1]

    Shape of gradient follows shape of coef.
    gradient.shape = coef.shape

    But hessian (to make our lives simpler) are always 2-d:
    if base_loss.is_multiclass:
        hessian.shape = (n_classes * n_dof, n_classes * n_dof)
    else:
        hessian.shape = (n_dof, n_dof)

Note: If coef has shape (n_classes * n_dof,), the 2d-array can be reconstructed as

    coef.reshape((n_classes, -1), order="F")

The option order="F" makes coef[:, i] contiguous. This, in turn, makes the
coefficients without intercept, coef[:, :-1], contiguous and speeds up
matrix-vector computations.

Note: If the average loss per sample is wanted instead of the sum of the loss per
sample, one can simply use a rescaled sample_weight such that
sum(sample_weight) = 1.

Parameters
----------
base_loss : instance of class BaseLoss from sklearn._loss.
fit_intercept : bool
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00037}{37}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a72a722924c5fe32413be8fe95e0f4e0e}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a72a722924c5fe32413be8fe95e0f4e0e} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{base\+\_\+loss}{, }\item[{}]{fit\+\_\+intercept}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00104}{104}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!gradient@{gradient}}
\index{gradient@{gradient}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{gradient()}{gradient()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+gradient (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l2\+\_\+reg\+\_\+strength}{ = {\ttfamily 0.0}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}, }\item[{}]{raw\+\_\+prediction}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Computes the gradient w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00344}{344}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!gradient\_hessian@{gradient\_hessian}}
\index{gradient\_hessian@{gradient\_hessian}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{gradient\_hessian()}{gradient\_hessian()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+gradient\+\_\+hessian (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l2\+\_\+reg\+\_\+strength}{ = {\ttfamily 0.0}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}, }\item[{}]{gradient\+\_\+out}{ = {\ttfamily None}, }\item[{}]{hessian\+\_\+out}{ = {\ttfamily None}, }\item[{}]{raw\+\_\+prediction}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Computes gradient and hessian w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
gradient_out : None or ndarray of shape coef.shape
    A location into which the gradient is stored. If None, a new array
    might be created.
hessian_out : None or ndarray of shape (n_dof, n_dof) or \
    (n_classes * n_dof, n_classes * n_dof)
    A location into which the hessian is stored. If None, a new array
    might be created.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.

hessian : ndarray of shape (n_dof, n_dof) or \
    (n_classes, n_dof, n_dof, n_classes)
    Hessian matrix.

hessian_warning : bool
    True if pointwise hessian has more than 25% of its elements non-positive.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00417}{417}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00014}{sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+sandwich\+\_\+dot()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00676}{gradient\+\_\+hessian\+\_\+product()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aabd136d35f0b757f2d9d4da9a41ceabd}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!gradient\_hessian\_product@{gradient\_hessian\_product}}
\index{gradient\_hessian\_product@{gradient\_hessian\_product}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{gradient\_hessian\_product()}{gradient\_hessian\_product()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aabd136d35f0b757f2d9d4da9a41ceabd} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+gradient\+\_\+hessian\+\_\+product (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l2\+\_\+reg\+\_\+strength}{ = {\ttfamily 0.0}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Computes gradient and hessp (hessian product function) w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.

hessp : callable
    Function that takes in a vector input of shape of gradient and
    and returns matrix-vector product with hessian.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00674}{674}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a90d92487a6eb759c86797dfbfe23153c}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!init\_zero\_coef@{init\_zero\_coef}}
\index{init\_zero\_coef@{init\_zero\_coef}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{init\_zero\_coef()}{init\_zero\_coef()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a90d92487a6eb759c86797dfbfe23153c} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+init\+\_\+zero\+\_\+coef (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{X}{, }\item[{}]{dtype}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Allocate coef of correct shape with zeros.

Parameters:
-----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
dtype : data-type, default=None
    Overrides the data type of coef. With dtype=None, coef will have the same
    dtype as X.

Returns
-------
coef : ndarray of shape (n_dof,) or (n_classes, n_dof)
    Coefficients of a linear model.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00108}{108}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!l2\_penalty@{l2\_penalty}}
\index{l2\_penalty@{l2\_penalty}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{l2\_penalty()}{l2\_penalty()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+l2\+\_\+penalty (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{weights}{, }\item[{}]{l2\+\_\+reg\+\_\+strength}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute L2 penalty term l2_reg_strength/2 *||w||_2^2.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00207}{207}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!loss@{loss}}
\index{loss@{loss}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{loss()}{loss()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+loss (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l2\+\_\+reg\+\_\+strength}{ = {\ttfamily 0.0}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}, }\item[{}]{raw\+\_\+prediction}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the loss as weighted average over point-wise losses.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
loss : float
    Weighted average of losses per sample, plus penalty.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00212}{212}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{__linear__loss_8py_source_l00207}{l2\+\_\+penalty()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!loss\_gradient@{loss\_gradient}}
\index{loss\_gradient@{loss\_gradient}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{loss\_gradient()}{loss\_gradient()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+loss\+\_\+gradient (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{l2\+\_\+reg\+\_\+strength}{ = {\ttfamily 0.0}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}, }\item[{}]{raw\+\_\+prediction}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Computes the sum of loss and gradient w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
loss : float
    Weighted average of losses per sample, plus penalty.

gradient : ndarray of shape coef.shape
     The gradient of the loss.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00266}{266}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}, \mbox{\hyperlink{__linear__loss_8py_source_l00207}{l2\+\_\+penalty()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!weight\_intercept@{weight\_intercept}}
\index{weight\_intercept@{weight\_intercept}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{weight\_intercept()}{weight\_intercept()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+weight\+\_\+intercept (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Helper function to get coefficients and intercept.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").

Returns
-------
weights : ndarray of shape (n_features,) or (n_classes, n_features)
    Coefficients without intercept term.
intercept : float or ndarray of shape (n_classes,)
    Intercept terms.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00136}{136}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00305}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Model.\+fit\+\_\+intercept}}, \mbox{\hyperlink{sklearn_2linear__model_2__base_8py_source_l00584}{sklearn.\+linear\+\_\+model.\+\_\+base.\+Linear\+Regression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00631}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+ARDRegression.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__bayes_8py_source_l00212}{sklearn.\+linear\+\_\+model.\+\_\+bayes.\+Bayesian\+Ridge.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l00921}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02424}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l01550}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Linear\+Model\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02633}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l03100}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Elastic\+Net\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__coordinate__descent_8py_source_l02873}{sklearn.\+linear\+\_\+model.\+\_\+coordinate\+\_\+descent.\+Multi\+Task\+Lasso.\+fit\+\_\+intercept}}, \mbox{\hyperlink{glm_8py_source_l00164}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+\_\+\+Generalized\+Linear\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__huber_8py_source_l00274}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01058}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l01377}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02015}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+CV.\+fit\+\_\+intercept}}, \mbox{\hyperlink{__least__angle_8py_source_l02206}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit\+\_\+intercept}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00106}{fit\+\_\+intercept}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!weight\_intercept\_raw@{weight\_intercept\_raw}}
\index{weight\_intercept\_raw@{weight\_intercept\_raw}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{weight\_intercept\_raw()}{weight\_intercept\_raw()}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+weight\+\_\+intercept\+\_\+raw (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{coef}{, }\item[{}]{X}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Helper function to get coefficients, intercept and raw_prediction.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.

Returns
-------
weights : ndarray of shape (n_features,) or (n_classes, n_features)
    Coefficients without intercept term.
intercept : float or ndarray of shape (n_classes,)
    Intercept terms.
raw_prediction : ndarray of shape (n_samples,) or \
    (n_samples, n_classes)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00175}{175}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



References \mbox{\hyperlink{__linear__loss_8py_source_l00105}{base\+\_\+loss}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00676}{gradient\+\_\+hessian\+\_\+product()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}.



\doxysubsection{Member Data Documentation}
\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!base\_loss@{base\_loss}}
\index{base\_loss@{base\_loss}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{base\_loss}{base\_loss}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+base\+\_\+loss = base\+\_\+loss}



Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00105}{105}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00676}{gradient\+\_\+hessian\+\_\+product()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00108}{init\+\_\+zero\+\_\+coef()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00221}{loss()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00175}{weight\+\_\+intercept\+\_\+raw()}}.

\Hypertarget{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}\index{sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}!fit\_intercept@{fit\_intercept}}
\index{fit\_intercept@{fit\_intercept}!sklearn.linear\_model.\_linear\_loss.LinearModelLoss@{sklearn.linear\_model.\_linear\_loss.LinearModelLoss}}
\doxysubsubsection{\texorpdfstring{fit\_intercept}{fit\_intercept}}
{\footnotesize\ttfamily \label{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852} 
sklearn.\+linear\+\_\+model.\+\_\+linear\+\_\+loss.\+Linear\+Model\+Loss.\+fit\+\_\+intercept = fit\+\_\+intercept}



Definition at line \mbox{\hyperlink{__linear__loss_8py_source_l00106}{106}} of file \mbox{\hyperlink{__linear__loss_8py_source}{\+\_\+linear\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__gradient_8py_source_l02245}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+SGDOne\+Class\+SVM.\+\_\+fit\+\_\+one\+\_\+class()}}, \mbox{\hyperlink{__logistic_8py_source_l01189}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression.\+fit()}}, \mbox{\hyperlink{__logistic_8py_source_l01851}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression\+CV.\+fit()}}, \mbox{\hyperlink{svm_2__classes_8py_source_l00281}{sklearn.\+svm.\+\_\+classes.\+Linear\+SVC.\+fit()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00353}{gradient()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00428}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00676}{gradient\+\_\+hessian\+\_\+product()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00108}{init\+\_\+zero\+\_\+coef()}}, \mbox{\hyperlink{__linear__loss_8py_source_l00275}{loss\+\_\+gradient()}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l01649}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+Base\+SGDRegressor.\+predict()}}, and \mbox{\hyperlink{__linear__loss_8py_source_l00136}{weight\+\_\+intercept()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/linear\+\_\+model/\+\_\+linear\+\_\+loss.\+py\end{DoxyCompactItemize}
