\doxysection{sklearn.\+random\+\_\+projection Namespace Reference}
\hypertarget{namespacesklearn_1_1random__projection}{}\label{namespacesklearn_1_1random__projection}\index{sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1random__projection_1_1BaseRandomProjection}{Base\+Random\+Projection}}
\item 
class \mbox{\hyperlink{classsklearn_1_1random__projection_1_1GaussianRandomProjection}{Gaussian\+Random\+Projection}}
\item 
class \mbox{\hyperlink{classsklearn_1_1random__projection_1_1SparseRandomProjection}{Sparse\+Random\+Projection}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1random__projection_a128fb18565ae7cf49265e19dad340d43}{johnson\+\_\+lindenstrauss\+\_\+min\+\_\+dim}} (n\+\_\+samples, \texorpdfstring{$\ast$}{*}, eps=0.\+1)
\item 
\mbox{\hyperlink{namespacesklearn_1_1random__projection_a66706d21724ce9e236c5f0c6bb41e387}{\+\_\+check\+\_\+density}} (density, n\+\_\+features)
\item 
\mbox{\hyperlink{namespacesklearn_1_1random__projection_a72efdca9566f07c9d433955e897f9382}{\+\_\+check\+\_\+input\+\_\+size}} (n\+\_\+components, n\+\_\+features)
\item 
\mbox{\hyperlink{namespacesklearn_1_1random__projection_a7e26fab5bcaccd2ea9d1cfddd14a3ec5}{\+\_\+gaussian\+\_\+random\+\_\+matrix}} (n\+\_\+components, n\+\_\+features, random\+\_\+state=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1random__projection_a7ceeef82d1e6f864b03b1fac5a991ca5}{\+\_\+sparse\+\_\+random\+\_\+matrix}} (n\+\_\+components, n\+\_\+features, density="{}auto"{}, random\+\_\+state=None)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1random__projection_a822dde2c3c3b0cc569b8d0a9d50bff0d}{\+\_\+\+\_\+all\+\_\+\+\_\+}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Random projection transformers.

Random projections are a simple and computationally efficient way to
reduce the dimensionality of the data by trading a controlled amount
of accuracy (as additional variance) for faster processing times and
smaller model sizes.

The dimensions and distribution of random projections matrices are
controlled so as to preserve the pairwise distances between any two
samples of the dataset.

The main theoretical result behind the efficiency of random projection is the
`Johnson-Lindenstrauss lemma (quoting Wikipedia)
<https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma>`_:

  In mathematics, the Johnson-Lindenstrauss lemma is a result
  concerning low-distortion embeddings of points from high-dimensional
  into low-dimensional Euclidean space. The lemma states that a small set
  of points in a high-dimensional space can be embedded into a space of
  much lower dimension in such a way that distances between the points are
  nearly preserved. The map used for the embedding is at least Lipschitz,
  and can even be taken to be an orthogonal projection.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1random__projection_a66706d21724ce9e236c5f0c6bb41e387}\index{sklearn.random\_projection@{sklearn.random\_projection}!\_check\_density@{\_check\_density}}
\index{\_check\_density@{\_check\_density}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{\_check\_density()}{\_check\_density()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a66706d21724ce9e236c5f0c6bb41e387} 
sklearn.\+random\+\_\+projection.\+\_\+check\+\_\+density (\begin{DoxyParamCaption}\item[{}]{density}{, }\item[{}]{n\+\_\+features}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Factorize density check according to Li et al.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{random__projection_8py_source_l00149}{149}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.



Referenced by \mbox{\hyperlink{random__projection_8py_source_l00778}{sklearn.\+random\+\_\+projection.\+Sparse\+Random\+Projection.\+\_\+make\+\_\+random\+\_\+matrix()}}, and \mbox{\hyperlink{random__projection_8py_source_l00209}{\+\_\+sparse\+\_\+random\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1random__projection_a72efdca9566f07c9d433955e897f9382}\index{sklearn.random\_projection@{sklearn.random\_projection}!\_check\_input\_size@{\_check\_input\_size}}
\index{\_check\_input\_size@{\_check\_input\_size}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{\_check\_input\_size()}{\_check\_input\_size()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a72efdca9566f07c9d433955e897f9382} 
sklearn.\+random\+\_\+projection.\+\_\+check\+\_\+input\+\_\+size (\begin{DoxyParamCaption}\item[{}]{n\+\_\+components}{, }\item[{}]{n\+\_\+features}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Factorize argument checking for random matrix generation.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{random__projection_8py_source_l00159}{159}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.



Referenced by \mbox{\hyperlink{random__projection_8py_source_l00169}{\+\_\+gaussian\+\_\+random\+\_\+matrix()}}, and \mbox{\hyperlink{random__projection_8py_source_l00209}{\+\_\+sparse\+\_\+random\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1random__projection_a7e26fab5bcaccd2ea9d1cfddd14a3ec5}\index{sklearn.random\_projection@{sklearn.random\_projection}!\_gaussian\_random\_matrix@{\_gaussian\_random\_matrix}}
\index{\_gaussian\_random\_matrix@{\_gaussian\_random\_matrix}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{\_gaussian\_random\_matrix()}{\_gaussian\_random\_matrix()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a7e26fab5bcaccd2ea9d1cfddd14a3ec5} 
sklearn.\+random\+\_\+projection.\+\_\+gaussian\+\_\+random\+\_\+matrix (\begin{DoxyParamCaption}\item[{}]{n\+\_\+components}{, }\item[{}]{n\+\_\+features}{, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Generate a dense Gaussian random matrix.

The components of the random matrix are drawn from

    N(0, 1.0 / n_components).

Read more in the :ref:`User Guide <gaussian_random_matrix>`.

Parameters
----------
n_components : int,
    Dimensionality of the target projection space.

n_features : int,
    Dimensionality of the original source space.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generator used to generate the matrix
    at fit time.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
components : ndarray of shape (n_components, n_features)
    The generated Gaussian random matrix.

See Also
--------
GaussianRandomProjection
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{random__projection_8py_source_l00169}{169}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.



References \mbox{\hyperlink{random__projection_8py_source_l00159}{\+\_\+check\+\_\+input\+\_\+size()}}.



Referenced by \mbox{\hyperlink{random__projection_8py_source_l00569}{sklearn.\+random\+\_\+projection.\+Gaussian\+Random\+Projection.\+\_\+make\+\_\+random\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1random__projection_a7ceeef82d1e6f864b03b1fac5a991ca5}\index{sklearn.random\_projection@{sklearn.random\_projection}!\_sparse\_random\_matrix@{\_sparse\_random\_matrix}}
\index{\_sparse\_random\_matrix@{\_sparse\_random\_matrix}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{\_sparse\_random\_matrix()}{\_sparse\_random\_matrix()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a7ceeef82d1e6f864b03b1fac5a991ca5} 
sklearn.\+random\+\_\+projection.\+\_\+sparse\+\_\+random\+\_\+matrix (\begin{DoxyParamCaption}\item[{}]{n\+\_\+components}{, }\item[{}]{n\+\_\+features}{, }\item[{}]{density}{ = {\ttfamily "{}auto"{}}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Generalized Achlioptas random sparse matrix for random projection.

Setting density to 1 / 3 will yield the original matrix by Dimitris
Achlioptas while setting a lower value will yield the generalization
by Ping Li et al.

If we note :math:`s = 1 / density`, the components of the random matrix are
drawn from:

  - -sqrt(s) / sqrt(n_components)   with probability 1 / 2s
  -  0                              with probability 1 - 1 / s
  - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s

Read more in the :ref:`User Guide <sparse_random_matrix>`.

Parameters
----------
n_components : int,
    Dimensionality of the target projection space.

n_features : int,
    Dimensionality of the original source space.

density : float or 'auto', default='auto'
    Ratio of non-zero component in the random projection matrix in the
    range `(0, 1]`

    If density = 'auto', the value is set to the minimum density
    as recommended by Ping Li et al.: 1 / sqrt(n_features).

    Use density = 1 / 3.0 if you want to reproduce the results from
    Achlioptas, 2001.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generator used to generate the matrix
    at fit time.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
components : {ndarray, sparse matrix} of shape (n_components, n_features)
    The generated Gaussian random matrix. Sparse matrix will be of CSR
    format.

See Also
--------
SparseRandomProjection

References
----------

.. [1] Ping Li, T. Hastie and K. W. Church, 2006,
       "Very Sparse Random Projections".
       https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf

.. [2] D. Achlioptas, 2001, "Database-friendly random projections",
       https://cgi.di.uoa.gr/~optas/papers/jl.pdf
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{random__projection_8py_source_l00209}{209}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.



References \mbox{\hyperlink{random__projection_8py_source_l00149}{\+\_\+check\+\_\+density()}}, and \mbox{\hyperlink{random__projection_8py_source_l00159}{\+\_\+check\+\_\+input\+\_\+size()}}.



Referenced by \mbox{\hyperlink{random__projection_8py_source_l00778}{sklearn.\+random\+\_\+projection.\+Sparse\+Random\+Projection.\+\_\+make\+\_\+random\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1random__projection_a128fb18565ae7cf49265e19dad340d43}\index{sklearn.random\_projection@{sklearn.random\_projection}!johnson\_lindenstrauss\_min\_dim@{johnson\_lindenstrauss\_min\_dim}}
\index{johnson\_lindenstrauss\_min\_dim@{johnson\_lindenstrauss\_min\_dim}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{johnson\_lindenstrauss\_min\_dim()}{johnson\_lindenstrauss\_min\_dim()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a128fb18565ae7cf49265e19dad340d43} 
sklearn.\+random\+\_\+projection.\+johnson\+\_\+lindenstrauss\+\_\+min\+\_\+dim (\begin{DoxyParamCaption}\item[{}]{n\+\_\+samples}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{eps}{ = {\ttfamily 0.1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find a 'safe' number of components to randomly project to.

The distortion introduced by a random projection `p` only changes the
distance between two points by a factor (1 +- eps) in a euclidean space
with good probability. The projection `p` is an eps-embedding as defined
by:

.. code-block:: text

  (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2

Where u and v are any rows taken from a dataset of shape (n_samples,
n_features), eps is in ]0, 1[ and p is a projection by a random Gaussian
N(0, 1) matrix of shape (n_components, n_features) (or a sparse
Achlioptas matrix).

The minimum number of components to guarantee the eps-embedding is
given by:

.. code-block:: text

  n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)

Note that the number of dimensions is independent of the original
number of features but instead depends on the size of the dataset:
the larger the dataset, the higher is the minimal dimensionality of
an eps-embedding.

Read more in the :ref:`User Guide <johnson_lindenstrauss>`.

Parameters
----------
n_samples : int or array-like of int
    Number of samples that should be an integer greater than 0. If an array
    is given, it will compute a safe number of components array-wise.

eps : float or array-like of shape (n_components,), dtype=float, \
        default=0.1
    Maximum distortion rate in the range (0, 1) as defined by the
    Johnson-Lindenstrauss lemma. If an array is given, it will compute a
    safe number of components array-wise.

Returns
-------
n_components : int or ndarray of int
    The minimal number of components to guarantee with good probability
    an eps-embedding with n_samples.

References
----------

.. [1] https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma

.. [2] `Sanjoy Dasgupta and Anupam Gupta, 1999,
       "An elementary proof of the Johnson-Lindenstrauss Lemma."
       <https://citeseerx.ist.psu.edu/doc_view/pid/95cd464d27c25c9c8690b378b894d337cdf021f9>`_

Examples
--------
>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
>>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)
np.int64(663)

>>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01])
array([    663,   11841, 1112658])

>>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1)
array([ 7894,  9868, 11841])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{random__projection_8py_source_l00063}{63}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.



Referenced by \mbox{\hyperlink{random__projection_8py_source_l00367}{sklearn.\+random\+\_\+projection.\+Base\+Random\+Projection.\+fit()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1random__projection_a822dde2c3c3b0cc569b8d0a9d50bff0d}\index{sklearn.random\_projection@{sklearn.random\_projection}!\_\_all\_\_@{\_\_all\_\_}}
\index{\_\_all\_\_@{\_\_all\_\_}!sklearn.random\_projection@{sklearn.random\_projection}}
\doxysubsubsection{\texorpdfstring{\_\_all\_\_}{\_\_all\_\_}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1random__projection_a822dde2c3c3b0cc569b8d0a9d50bff0d} 
list sklearn.\+random\+\_\+projection.\+\_\+\+\_\+all\+\_\+\+\_\+\hspace{0.3cm}{\ttfamily [private]}}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{stringliteral}{"{}GaussianRandomProjection"{}},}
\DoxyCodeLine{00003\ \ \ \ \ \textcolor{stringliteral}{"{}SparseRandomProjection"{}},}
\DoxyCodeLine{00004\ \ \ \ \ \textcolor{stringliteral}{"{}johnson\_lindenstrauss\_min\_dim"{}},}
\DoxyCodeLine{00005\ ]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{random__projection_8py_source_l00049}{49}} of file \mbox{\hyperlink{random__projection_8py_source}{random\+\_\+projection.\+py}}.

