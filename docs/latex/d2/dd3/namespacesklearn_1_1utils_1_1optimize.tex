\doxysection{sklearn.\+utils.\+optimize Namespace Reference}
\hypertarget{namespacesklearn_1_1utils_1_1optimize}{}\label{namespacesklearn_1_1utils_1_1optimize}\index{sklearn.utils.optimize@{sklearn.utils.optimize}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1utils_1_1optimize_1_1__LineSearchError}{\+\_\+\+Line\+Search\+Error}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1utils_1_1optimize_a1b63523ac4b53d9cf55517ac928732e2}{\+\_\+line\+\_\+search\+\_\+wolfe12}} (f, fprime, xk, pk, gfk, old\+\_\+fval, old\+\_\+old\+\_\+fval, verbose=0, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}kwargs)
\item 
\mbox{\hyperlink{namespacesklearn_1_1utils_1_1optimize_a1d3a69cf2ce1bf36552401dc57af16ed}{\+\_\+cg}} (fhess\+\_\+p, fgrad, maxiter, tol, verbose=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1utils_1_1optimize_a42743d736355106c237c8297987ea4bf}{\+\_\+newton\+\_\+cg}} (grad\+\_\+hess, func, grad, x0, args=(), tol=1e-\/4, maxiter=100, maxinner=200, line\+\_\+search=\mbox{\hyperlink{classTrue}{True}}, warn=\mbox{\hyperlink{classTrue}{True}}, verbose=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1utils_1_1optimize_a5a2e6509b4cb8d5ffb9a5a8ac39ef9e9}{\+\_\+check\+\_\+optimize\+\_\+result}} (solver, result, max\+\_\+iter=None, extra\+\_\+warning\+\_\+msg=None)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Our own implementation of the Newton algorithm

Unlike the scipy.optimize version, this version of the Newton conjugate
gradient solver uses only one function call to retrieve the
func value, the gradient value and a callable for the Hessian matvec
product. If the function call is very expensive (e.g. for logistic
regression with large design matrix), this approach gives very
significant speedups.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1utils_1_1optimize_a1d3a69cf2ce1bf36552401dc57af16ed}\index{sklearn.utils.optimize@{sklearn.utils.optimize}!\_cg@{\_cg}}
\index{\_cg@{\_cg}!sklearn.utils.optimize@{sklearn.utils.optimize}}
\doxysubsubsection{\texorpdfstring{\_cg()}{\_cg()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1utils_1_1optimize_a1d3a69cf2ce1bf36552401dc57af16ed} 
sklearn.\+utils.\+optimize.\+\_\+cg (\begin{DoxyParamCaption}\item[{}]{fhess\+\_\+p}{, }\item[{}]{fgrad}{, }\item[{}]{maxiter}{, }\item[{}]{tol}{, }\item[{}]{verbose}{ = {\ttfamily 0}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Solve iteratively the linear system 'fhess_p . xsupi = fgrad'
with a conjugate gradient descent.

Parameters
----------
fhess_p : callable
    Function that takes the gradient as a parameter and returns the
    matrix product of the Hessian and gradient.

fgrad : ndarray of shape (n_features,) or (n_features + 1,)
    Gradient vector.

maxiter : int
    Number of CG iterations.

tol : float
    Stopping criterion.

Returns
-------
xsupi : ndarray of shape (n_features,) or (n_features + 1,)
    Estimated solution.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00113}{113}} of file \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source}{optimize.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00214}{\+\_\+newton\+\_\+cg()}}.

\Hypertarget{namespacesklearn_1_1utils_1_1optimize_a5a2e6509b4cb8d5ffb9a5a8ac39ef9e9}\index{sklearn.utils.optimize@{sklearn.utils.optimize}!\_check\_optimize\_result@{\_check\_optimize\_result}}
\index{\_check\_optimize\_result@{\_check\_optimize\_result}!sklearn.utils.optimize@{sklearn.utils.optimize}}
\doxysubsubsection{\texorpdfstring{\_check\_optimize\_result()}{\_check\_optimize\_result()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1utils_1_1optimize_a5a2e6509b4cb8d5ffb9a5a8ac39ef9e9} 
sklearn.\+utils.\+optimize.\+\_\+check\+\_\+optimize\+\_\+result (\begin{DoxyParamCaption}\item[{}]{solver}{, }\item[{}]{result}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{extra\+\_\+warning\+\_\+msg}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Check the OptimizeResult for successful convergence

Parameters
----------
solver : str
   Solver name. Currently only `lbfgs` is supported.

result : OptimizeResult
   Result of the scipy.optimize.minimize function.

max_iter : int, default=None
   Expected maximum number of iterations.

extra_warning_msg : str, default=None
    Extra warning message.

Returns
-------
n_iter : int
   Number of iterations.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00331}{331}} of file \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source}{optimize.\+py}}.

\Hypertarget{namespacesklearn_1_1utils_1_1optimize_a1b63523ac4b53d9cf55517ac928732e2}\index{sklearn.utils.optimize@{sklearn.utils.optimize}!\_line\_search\_wolfe12@{\_line\_search\_wolfe12}}
\index{\_line\_search\_wolfe12@{\_line\_search\_wolfe12}!sklearn.utils.optimize@{sklearn.utils.optimize}}
\doxysubsubsection{\texorpdfstring{\_line\_search\_wolfe12()}{\_line\_search\_wolfe12()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1utils_1_1optimize_a1b63523ac4b53d9cf55517ac928732e2} 
sklearn.\+utils.\+optimize.\+\_\+line\+\_\+search\+\_\+wolfe12 (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{fprime}{, }\item[{}]{xk}{, }\item[{}]{pk}{, }\item[{}]{gfk}{, }\item[{}]{old\+\_\+fval}{, }\item[{}]{old\+\_\+old\+\_\+fval}{, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{kwargs}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Same as line_search_wolfe1, but fall back to line_search_wolfe2 if
suitable step length is not found, and raise an exception if a
suitable step length is not found.

Raises
------
_LineSearchError
    If no suitable step size is found.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00031}{31}} of file \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source}{optimize.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00214}{\+\_\+newton\+\_\+cg()}}.

\Hypertarget{namespacesklearn_1_1utils_1_1optimize_a42743d736355106c237c8297987ea4bf}\index{sklearn.utils.optimize@{sklearn.utils.optimize}!\_newton\_cg@{\_newton\_cg}}
\index{\_newton\_cg@{\_newton\_cg}!sklearn.utils.optimize@{sklearn.utils.optimize}}
\doxysubsubsection{\texorpdfstring{\_newton\_cg()}{\_newton\_cg()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1utils_1_1optimize_a42743d736355106c237c8297987ea4bf} 
sklearn.\+utils.\+optimize.\+\_\+newton\+\_\+cg (\begin{DoxyParamCaption}\item[{}]{grad\+\_\+hess}{, }\item[{}]{func}{, }\item[{}]{grad}{, }\item[{}]{x0}{, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{maxiter}{ = {\ttfamily 100}, }\item[{}]{maxinner}{ = {\ttfamily 200}, }\item[{}]{line\+\_\+search}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{warn}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Minimization of scalar function of one or more variables using the
Newton-CG algorithm.

Parameters
----------
grad_hess : callable
    Should return the gradient and a callable returning the matvec product
    of the Hessian.

func : callable
    Should return the value of the function.

grad : callable
    Should return the function value and the gradient. This is used
    by the linesearch functions.

x0 : array of float
    Initial guess.

args : tuple, default=()
    Arguments passed to func_grad_hess, func and grad.

tol : float, default=1e-4
    Stopping criterion. The iteration will stop when
    ``max{|g_i | i = 1, ..., n} <= tol``
    where ``g_i`` is the i-th component of the gradient.

maxiter : int, default=100
    Number of Newton iterations.

maxinner : int, default=200
    Number of CG iterations.

line_search : bool, default=True
    Whether to use a line search or not.

warn : bool, default=True
    Whether to warn when didn't converge.

Returns
-------
xk : ndarray of float
    Estimated minimum.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00202}{202}} of file \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source}{optimize.\+py}}.



References \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00113}{\+\_\+cg()}}, and \mbox{\hyperlink{sklearn_2utils_2optimize_8py_source_l00033}{\+\_\+line\+\_\+search\+\_\+wolfe12()}}.

