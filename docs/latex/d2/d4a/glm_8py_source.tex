\doxysection{glm.\+py}
\hypertarget{glm_8py_source}{}\label{glm_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_glm/glm.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_glm/glm.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1glm}{00001}}\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00002}00002\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00004}00004\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00005}00005\ \textcolor{stringliteral}{Generalized\ Linear\ Models\ with\ Exponential\ Dispersion\ Family}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00006}00006\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00008}00008\ \textcolor{keyword}{from}\ numbers\ \textcolor{keyword}{import}\ Integral,\ Real}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00009}00009\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00010}00010\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00011}00011\ \textcolor{keyword}{import}\ \mbox{\hyperlink{namespacescipy_1_1optimize}{scipy.optimize}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00012}00012\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00013}00013\ \textcolor{keyword}{from}\ ...\_loss.loss\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00014}00014\ \ \ \ \ HalfGammaLoss,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00015}00015\ \ \ \ \ HalfPoissonLoss,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00016}00016\ \ \ \ \ HalfSquaredError,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00017}00017\ \ \ \ \ HalfTweedieLoss,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00018}00018\ \ \ \ \ HalfTweedieLossIdentity,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00019}00019\ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00020}00020\ \textcolor{keyword}{from}\ ...base\ \textcolor{keyword}{import}\ BaseEstimator,\ RegressorMixin,\ \_fit\_context}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00021}00021\ \textcolor{keyword}{from}\ ...utils\ \textcolor{keyword}{import}\ check\_array}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00022}00022\ \textcolor{keyword}{from}\ ...utils.\_openmp\_helpers\ \textcolor{keyword}{import}\ \_openmp\_effective\_n\_threads}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00023}00023\ \textcolor{keyword}{from}\ ...utils.\_param\_validation\ \textcolor{keyword}{import}\ Hidden,\ Interval,\ StrOptions}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00024}00024\ \textcolor{keyword}{from}\ ...utils.fixes\ \textcolor{keyword}{import}\ \_get\_additional\_lbfgs\_options\_dict}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00025}00025\ \textcolor{keyword}{from}\ ...utils.optimize\ \textcolor{keyword}{import}\ \_check\_optimize\_result}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00026}00026\ \textcolor{keyword}{from}\ ...utils.validation\ \textcolor{keyword}{import}\ \_check\_sample\_weight,\ check\_is\_fitted,\ validate\_data}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00027}00027\ \textcolor{keyword}{from}\ ..\_linear\_loss\ \textcolor{keyword}{import}\ LinearModelLoss}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00028}00028\ \textcolor{keyword}{from}\ .\_newton\_solver\ \textcolor{keyword}{import}\ NewtonCholeskySolver,\ NewtonSolver}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00029}00029\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00030}00030\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00031}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{00031}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}(\mbox{\hyperlink{classsklearn_1_1base_1_1RegressorMixin}{RegressorMixin}},\ \mbox{\hyperlink{classsklearn_1_1base_1_1BaseEstimator}{BaseEstimator}}):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00032}00032\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Regression\ via\ a\ penalized\ Generalized\ Linear\ Model\ (GLM).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00033}00033\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00034}00034\ \textcolor{stringliteral}{\ \ \ \ GLMs\ based\ on\ a\ reproductive\ Exponential\ Dispersion\ Model\ (EDM)\ aim\ at\ fitting\ and}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00035}00035\ \textcolor{stringliteral}{\ \ \ \ predicting\ the\ mean\ of\ the\ target\ y\ as\ y\_pred=h(X*w)\ with\ coefficients\ w.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00036}00036\ \textcolor{stringliteral}{\ \ \ \ Therefore,\ the\ fit\ minimizes\ the\ following\ objective\ function\ with\ L2\ priors\ as}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00037}00037\ \textcolor{stringliteral}{\ \ \ \ regularizer::}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00038}00038\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00039}00039\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 1/(2*sum(s\_i))\ *\ sum(s\_i\ *\ deviance(y\_i,\ h(x\_i*w))\ +\ 1/2\ *\ alpha\ *\ ||w||\_2\string^2}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00040}00040\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00041}00041\ \textcolor{stringliteral}{\ \ \ \ with\ inverse\ link\ function\ h,\ s=sample\_weight\ and\ per\ observation\ (unit)\ deviance}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00042}00042\ \textcolor{stringliteral}{\ \ \ \ deviance(y\_i,\ h(x\_i*w)).\ Note\ that\ for\ an\ EDM,\ 1/2\ *\ deviance\ is\ the\ negative}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00043}00043\ \textcolor{stringliteral}{\ \ \ \ log-\/likelihood\ up\ to\ a\ constant\ (in\ w)\ term.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00044}00044\ \textcolor{stringliteral}{\ \ \ \ The\ parameter\ \`{}\`{}alpha\`{}\`{}\ corresponds\ to\ the\ lambda\ parameter\ in\ glmnet.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00045}00045\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00046}00046\ \textcolor{stringliteral}{\ \ \ \ Instead\ of\ implementing\ the\ EDM\ family\ and\ a\ link\ function\ separately,\ we\ directly}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00047}00047\ \textcolor{stringliteral}{\ \ \ \ use\ the\ loss\ functions\ \`{}from\ sklearn.\_loss\`{}\ which\ have\ the\ link\ functions\ included}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00048}00048\ \textcolor{stringliteral}{\ \ \ \ in\ them\ for\ performance\ reasons.\ We\ pick\ the\ loss\ functions\ that\ implement}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00049}00049\ \textcolor{stringliteral}{\ \ \ \ (1/2\ times)\ EDM\ deviances.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00050}00050\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00051}00051\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <Generalized\_linear\_models>\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00052}00052\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00053}00053\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 0.23}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00054}00054\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00055}00055\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00056}00056\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00057}00057\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00058}00058\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ penalty\ term\ and\ thus\ determines\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00059}00059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ regularization\ strength.\ \`{}\`{}alpha\ =\ 0\`{}\`{}\ is\ equivalent\ to\ unpenalized}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00060}00060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ GLMs.\ In\ this\ case,\ the\ design\ matrix\ \`{}X\`{}\ must\ have\ full\ column\ rank}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00061}00061\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (no\ collinearities).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00062}00062\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00063}00063\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00064}00064\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00065}00065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Specifies\ if\ a\ constant\ (a.k.a.\ bias\ or\ intercept)\ should\ be}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00066}00066\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ added\ to\ the\ linear\ predictor\ (X\ @\ coef\ +\ intercept).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00067}00067\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00068}00068\ \textcolor{stringliteral}{\ \ \ \ solver\ :\ \{'lbfgs',\ 'newton-\/cholesky'\},\ default='lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00069}00069\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Algorithm\ to\ use\ in\ the\ optimization\ problem:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00070}00070\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00071}00071\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00072}00072\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Calls\ scipy's\ L-\/BFGS-\/B\ optimizer.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00073}00073\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00074}00074\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'newton-\/cholesky'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00075}00075\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Uses\ Newton-\/Raphson\ steps\ (in\ arbitrary\ precision\ arithmetic\ equivalent\ to}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00076}00076\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ iterated\ reweighted\ least\ squares)\ with\ an\ inner\ Cholesky\ based\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00077}00077\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ This\ solver\ is\ a\ good\ choice\ for\ \`{}n\_samples\`{}\ >>\ \`{}n\_features\`{},\ especially}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00078}00078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ with\ one-\/hot\ encoded\ categorical\ features\ with\ rare\ categories.\ Be\ aware}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00079}00079\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ that\ the\ memory\ usage\ of\ this\ solver\ has\ a\ quadratic\ dependency\ on}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00080}00080\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}n\_features\`{}\ because\ it\ explicitly\ computes\ the\ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00081}00081\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00082}00082\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ ..\ versionadded::\ 1.2}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00083}00083\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00084}00084\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00085}00085\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximal\ number\ of\ iterations\ for\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00086}00086\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00087}00087\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00088}00088\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00089}00089\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Stopping\ criterion.\ For\ the\ lbfgs\ solver,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ iteration\ will\ stop\ when\ \`{}\`{}max\{|g\_j|,\ j\ =\ 1,\ ...,\ d\}\ <=\ tol\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}g\_j\`{}\`{}\ is\ the\ j-\/th\ component\ of\ the\ gradient\ (derivative)\ of}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ objective\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00093}00093\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00094}00094\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ \`{}\`{}fit\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ as\ initialization\ for\ \`{}\`{}coef\_\`{}\`{}\ and\ \`{}\`{}intercept\_\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00098}00098\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00099}00099\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ the\ lbfgs\ solver\ set\ verbose\ to\ any\ positive\ number\ for\ verbosity.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00102}00102\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00103}00103\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ array\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Estimated\ coefficients\ for\ the\ linear\ predictor\ (\`{}X\ @\ coef\_\ +}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_\`{})\ in\ the\ GLM.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00108}00108\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00109}00109\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00110}00110\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Intercept\ (a.k.a.\ bias)\ added\ to\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00111}00111\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Actual\ number\ of\ iterations\ used\ in\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00114}00114\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ \_base\_loss\ :\ BaseLoss,\ default=HalfSquaredError()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ is\ set\ during\ fit\ via\ \`{}self.\_get\_loss()\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ A\ \`{}\_base\_loss\`{}\ contains\ a\ specific\ loss\ function\ as\ well\ as\ the\ link}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00118}00118\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ function.\ The\ loss\ to\ be\ minimized\ specifies\ the\ distributional\ assumption\ of}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00119}00119\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ GLM,\ i.e.\ the\ distribution\ from\ the\ EDM.\ Here\ are\ some\ examples:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00120}00120\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00121}00121\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ =======================\ \ ========\ \ ==========================}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00122}00122\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \_base\_loss\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Link\ \ \ \ \ \ Target\ Domain}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00123}00123\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ =======================\ \ ========\ \ ==========================}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00124}00124\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ HalfSquaredError\ \ \ \ \ \ \ \ \ identity\ \ y\ any\ real\ number}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00125}00125\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ HalfPoissonLoss\ \ \ \ \ \ \ \ \ \ log\ \ \ \ \ \ \ 0\ <=\ y}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00126}00126\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ HalfGammaLoss\ \ \ \ \ \ \ \ \ \ \ \ log\ \ \ \ \ \ \ 0\ <\ y}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00127}00127\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ HalfTweedieLoss\ \ \ \ \ \ \ \ \ \ log\ \ \ \ \ \ \ dependent\ on\ tweedie\ power}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00128}00128\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ HalfTweedieLossIdentity\ \ identity\ \ dependent\ on\ tweedie\ power}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00129}00129\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ =======================\ \ ========\ \ ==========================}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00130}00130\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00131}00131\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ link\ function\ of\ the\ GLM,\ i.e.\ mapping\ from\ linear\ predictor}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00132}00132\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}X\ @\ coeff\ +\ intercept\`{}\ to\ prediction\ \`{}y\_pred\`{}.\ For\ instance,\ with\ a\ log\ link,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00133}00133\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ we\ have\ \`{}y\_pred\ =\ exp(X\ @\ coeff\ +\ intercept)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00134}00134\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00135}00135\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00136}00136\ \ \ \ \ \textcolor{comment}{\#\ We\ allow\ for\ NewtonSolver\ classes\ for\ the\ "{}solver"{}\ parameter\ but\ do\ not}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00137}00137\ \ \ \ \ \textcolor{comment}{\#\ make\ them\ public\ in\ the\ docstrings.\ This\ facilitates\ testing\ and}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00138}00138\ \ \ \ \ \textcolor{comment}{\#\ benchmarking.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00139}00139\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}alpha"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00141}00141\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}fit\_intercept"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00142}00142\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}solver"{}}:\ [}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}lbfgs"{}},\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}\}),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00144}00144\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Hidden}{Hidden}}(type),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00145}00145\ \ \ \ \ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00146}00146\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}max\_iter"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00147}00147\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}tol"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}warm\_start"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00149}00149\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}verbose"{}}:\ [\textcolor{stringliteral}{"{}verbose"{}}],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00150}00150\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00151}00151\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00152}00152\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00153}00153\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00154}00154\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ alpha=1.0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00157}00157\ \ \ \ \ \ \ \ \ solver="{}lbfgs"{},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ max\_iter=100,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00162}00162\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00163}00163\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a10f6d6567395b5f113f99ddaacee9676}{alpha}}\ =\ alpha}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a756495aa30d25c8cd8a4c305982e0621}{fit\_intercept}}\ =\ fit\_intercept}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}}\ =\ solver}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aaa51c024e4a31cf7c4a7a771d5eff01b}{max\_iter}}\ =\ max\_iter}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00167}00167\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ad8a9cc28be8f74b3fb19bac9949866a2}{tol}}\ =\ tol}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0b3749b033ec0d3c67e79a6ae1137232}{warm\_start}}\ =\ warm\_start}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a5e33447e0137bf12e006ee9587d96070}{verbose}}\ =\ verbose}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00170}00170\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00171}00171\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context(prefer\_skip\_nested\_validation=True)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00172}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ab74a5b0bb400e4ea3ceec27d7d60b050}{00172}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ab74a5b0bb400e4ea3ceec27d7d60b050}{fit}}(self,\ X,\ y,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ a\ Generalized\ Linear\ Model.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00174}00174\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00176}00176\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00179}00179\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ array-\/like\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Target\ values.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00182}00182\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00183}00183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00184}00184\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00185}00185\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00186}00186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00187}00187\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00188}00188\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00189}00189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Fitted\ model.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00190}00190\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00191}00191\ \ \ \ \ \ \ \ \ X,\ y\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00193}00193\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00194}00194\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00195}00195\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=[\textcolor{stringliteral}{"{}csc"{}},\ \textcolor{stringliteral}{"{}csr"{}}],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00196}00196\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=[np.float64,\ np.float32],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ \ \ \ \ y\_numeric=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00198}00198\ \ \ \ \ \ \ \ \ \ \ \ \ multi\_output=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00200}00200\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ required\ by\ losses}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}}\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ lbfgs\ will\ force\ coef\ and\ therefore\ raw\_prediction\ to\ be\ float64.\ The}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00204}00204\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ base\_loss\ needs\ y,\ X\ @\ coef\ and\ sample\_weight\ all\ of\ same\ dtype}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00205}00205\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ (and\ contiguous).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00206}00206\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_dtype\ =\ np.float64}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00207}00207\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00208}00208\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_dtype\ =\ min(max(y.dtype,\ X.dtype),\ np.float64)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00209}00209\ \ \ \ \ \ \ \ \ y\ =\ check\_array(y,\ dtype=loss\_dtype,\ order=\textcolor{stringliteral}{"{}C"{}},\ ensure\_2d=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00210}00210\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00211}00211\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00212}00212\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note\ that\ \_check\_sample\_weight\ calls\ check\_array(order="{}C"{})\ required\ by}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00213}00213\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ losses.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=loss\_dtype)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00215}00215\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00216}00216\ \ \ \ \ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a6c5abf7ad8ebdf0dd6f3a85ef3c8d37c}{\_base\_loss}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aa7ad479b2a2599e7783b2a163ced77db}{\_get\_loss}}()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00218}00218\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00219}00219\ \ \ \ \ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ \ \ \ \ base\_loss=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a6c5abf7ad8ebdf0dd6f3a85ef3c8d37c}{\_base\_loss}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00221}00221\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a756495aa30d25c8cd8a4c305982e0621}{fit\_intercept}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00223}00223\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ linear\_loss.base\_loss.in\_y\_true\_range(y):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00225}00225\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00226}00226\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Some\ value(s)\ of\ y\ are\ out\ of\ the\ valid\ range\ of\ the\ loss"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00227}00227\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\ \{self.\_base\_loss.\_\_class\_\_.\_\_name\_\_!r\}."{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00228}00228\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00229}00229\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00230}00230\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ if\ alpha=0\ check\ that\ X\ is\ not\ rank\ deficient}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00231}00231\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00232}00232\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ NOTE:\ Rescaling\ of\ sample\_weight:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00233}00233\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ We\ want\ to\ minimize}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00234}00234\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ obj\ =\ 1/(2\ *\ sum(sample\_weight))\ *\ sum(sample\_weight\ *\ deviance)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00235}00235\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ +\ 1/2\ *\ alpha\ *\ L2,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ with}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00237}00237\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ deviance\ =\ 2\ *\ loss.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00238}00238\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ objective\ is\ invariant\ to\ multiplying\ sample\_weight\ by\ a\ constant.\ We}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00239}00239\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ could\ choose\ this\ constant\ such\ that\ sum(sample\_weight)\ =\ 1\ in\ order\ to\ end}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00240}00240\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ up\ with}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00241}00241\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ obj\ =\ sum(sample\_weight\ *\ loss)\ +\ 1/2\ *\ alpha\ *\ L2.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ But\ LinearModelLoss.loss()\ already\ computes}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00243}00243\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ average(loss,\ weights=sample\_weight)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Thus,\ without\ rescaling,\ we\ have}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ obj\ =\ LinearModelLoss.loss(...)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00246}00246\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00247}00247\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0b3749b033ec0d3c67e79a6ae1137232}{warm\_start}}\ \textcolor{keywordflow}{and}\ hasattr(self,\ \textcolor{stringliteral}{"{}coef\_"{}}):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a756495aa30d25c8cd8a4c305982e0621}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ LinearModelLoss\ needs\ intercept\ at\ the\ end\ of\ coefficient\ array.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ np.concatenate((self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aefae52f07a7055c3473cdf64a523a459}{coef\_}},\ np.array([self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a572d4bf12243535e34a6a33735679726}{intercept\_}}])))}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aefae52f07a7055c3473cdf64a523a459}{coef\_}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ coef.astype(loss\_dtype,\ copy=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ linear\_loss.init\_zero\_coef(X,\ dtype=loss\_dtype)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a756495aa30d25c8cd8a4c305982e0621}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef[-\/1]\ =\ linear\_loss.base\_loss.link.link(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ np.average(y,\ weights=sample\_weight)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00260}00260\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ l2\_reg\_strength\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a10f6d6567395b5f113f99ddaacee9676}{alpha}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ n\_threads\ =\ \_openmp\_effective\_n\_threads()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00263}00263\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Algorithms\ for\ optimization:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00265}00265\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note\ again\ that\ our\ losses\ implement\ 1/2\ *\ deviance.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}}\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00267}00267\ \ \ \ \ \ \ \ \ \ \ \ \ func\ =\ linear\_loss.loss\_gradient}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00268}00268\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00269}00269\ \ \ \ \ \ \ \ \ \ \ \ \ opt\_res\ =\ scipy.optimize.minimize(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00270}00270\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ func,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00271}00271\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00272}00272\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ method=\textcolor{stringliteral}{"{}L-\/BFGS-\/B"{}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ jac=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ options=\{}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00275}00275\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}maxiter"{}}:\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aaa51c024e4a31cf7c4a7a771d5eff01b}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}maxls"{}}:\ 50,\ \ \textcolor{comment}{\#\ default\ is\ 20}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00277}00277\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}gtol"{}}:\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ad8a9cc28be8f74b3fb19bac9949866a2}{tol}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00278}00278\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ constant\ 64\ was\ found\ empirically\ to\ pass\ the\ test\ suite.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ point\ is\ that\ ftol\ is\ very\ small,\ but\ a\ bit\ larger\ than}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ machine\ precision\ for\ float64,\ which\ is\ the\ dtype\ used\ by\ lbfgs.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00281}00281\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}ftol"{}}:\ 64\ *\ np.finfo(float).eps,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00282}00282\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **\_get\_additional\_lbfgs\_options\_dict(\textcolor{stringliteral}{"{}iprint"{}},\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a5e33447e0137bf12e006ee9587d96070}{verbose}}\ -\/\ 1),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00283}00283\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ args=(X,\ y,\ sample\_weight,\ l2\_reg\_strength,\ n\_threads),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00285}00285\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a23ef763c699dd428d06479aa4cde6010}{n\_iter\_}}\ =\ \_check\_optimize\_result(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}lbfgs"{}},\ opt\_res,\ max\_iter=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aaa51c024e4a31cf7c4a7a771d5eff01b}{max\_iter}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00289}00289\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ opt\_res.x}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}}\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00291}00291\ \ \ \ \ \ \ \ \ \ \ \ \ sol\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1__newton__solver_1_1NewtonCholeskySolver}{NewtonCholeskySolver}}(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00292}00292\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00293}00293\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00294}00294\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00295}00295\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tol=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ad8a9cc28be8f74b3fb19bac9949866a2}{tol}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00296}00296\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aaa51c024e4a31cf7c4a7a771d5eff01b}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00298}00298\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ verbose=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a5e33447e0137bf12e006ee9587d96070}{verbose}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ sol.solve(X,\ y,\ sample\_weight)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a23ef763c699dd428d06479aa4cde6010}{n\_iter\_}}\ =\ sol.iteration}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ issubclass(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}},\ NewtonSolver):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ \ \ \ \ sol\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_af9c5b8b7734301633b40ee7352efe037}{solver}}(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ coef=coef,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00306}00306\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00307}00307\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tol=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_ad8a9cc28be8f74b3fb19bac9949866a2}{tol}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00308}00308\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aaa51c024e4a31cf7c4a7a771d5eff01b}{max\_iter}},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ sol.solve(X,\ y,\ sample\_weight)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a23ef763c699dd428d06479aa4cde6010}{n\_iter\_}}\ =\ sol.iteration}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(f\textcolor{stringliteral}{"{}Invalid\ solver=\{self.solver\}."{}})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00315}00315\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a756495aa30d25c8cd8a4c305982e0621}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a572d4bf12243535e34a6a33735679726}{intercept\_}}\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00318}00318\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aefae52f07a7055c3473cdf64a523a459}{coef\_}}\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00320}00320\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ set\ intercept\ to\ zero\ as\ the\ other\ linear\ models\ do}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00321}00321\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a572d4bf12243535e34a6a33735679726}{intercept\_}}\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aefae52f07a7055c3473cdf64a523a459}{coef\_}}\ =\ coef}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00323}00323\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00325}00325\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00326}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0a1f7607096b5c90682db8da99d01192}{00326}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0a1f7607096b5c90682db8da99d01192}{\_linear\_predictor}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ linear\_predictor\ =\ \`{}X\ @\ coef\_\ +\ intercept\_\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00328}00328\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00329}00329\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Note\ that\ we\ often\ use\ the\ term\ raw\_prediction\ instead\ of\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00330}00330\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00331}00331\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00332}00332\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00333}00333\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00334}00334\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Samples.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00335}00335\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00336}00336\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00337}00337\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00338}00338\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_pred\ :\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00339}00339\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ predicted\ values\ of\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00340}00340\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ check\_is\_fitted(self)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00345}00345\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=[\textcolor{stringliteral}{"{}csr"{}},\ \textcolor{stringliteral}{"{}csc"{}},\ \textcolor{stringliteral}{"{}coo"{}}],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00346}00346\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=[np.float64,\ np.float32],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00347}00347\ \ \ \ \ \ \ \ \ \ \ \ \ ensure\_2d=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00348}00348\ \ \ \ \ \ \ \ \ \ \ \ \ allow\_nd=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ \ \ \ \ reset=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00351}00351\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ X\ @\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aefae52f07a7055c3473cdf64a523a459}{coef\_}}\ +\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a572d4bf12243535e34a6a33735679726}{intercept\_}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00352}00352\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00353}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_afa466bb4162b4ea4b74fe14bbb6b271c}{00353}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_afa466bb4162b4ea4b74fe14bbb6b271c}{predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00354}00354\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ using\ GLM\ with\ feature\ matrix\ X.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00355}00355\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00356}00356\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00357}00357\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00358}00358\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00359}00359\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Samples.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00360}00360\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00361}00361\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00362}00362\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00363}00363\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_pred\ :\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00364}00364\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Returns\ predicted\ values.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00365}00365\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ check\_array\ is\ done\ in\ \_linear\_predictor}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00367}00367\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0a1f7607096b5c90682db8da99d01192}{\_linear\_predictor}}(X)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ y\_pred\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a6c5abf7ad8ebdf0dd6f3a85ef3c8d37c}{\_base\_loss}}.link.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00369}00369\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ y\_pred}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00370}00370\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00371}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a99c849640998478154423c389441f686}{00371}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a99c849640998478154423c389441f686}{score}}(self,\ X,\ y,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00372}00372\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ D\string^2,\ the\ percentage\ of\ deviance\ explained.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00373}00373\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00374}00374\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ D\string^2\ is\ a\ generalization\ of\ the\ coefficient\ of\ determination\ R\string^2.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00375}00375\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ R\string^2\ uses\ squared\ error\ and\ D\string^2\ uses\ the\ deviance\ of\ this\ GLM,\ see\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00376}00376\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}User\ Guide\ <regression\_metrics>\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00377}00377\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00378}00378\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ D\string^2\ is\ defined\ as}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00379}00379\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :math:\`{}D\string^2\ =\ 1-\/\(\backslash\)\(\backslash\)frac\{D(y\_\{true\},y\_\{pred\})\}\{D\_\{null\}\}\`{},}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00380}00380\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :math:\`{}D\_\{null\}\`{}\ is\ the\ null\ deviance,\ i.e.\ the\ deviance\ of\ a\ model}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00381}00381\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ with\ intercept\ alone,\ which\ corresponds\ to\ :math:\`{}y\_\{pred\}\ =\ \(\backslash\)\(\backslash\)bar\{y\}\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00382}00382\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ mean\ :math:\`{}\(\backslash\)\(\backslash\)bar\{y\}\`{}\ is\ averaged\ by\ sample\_weight.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00383}00383\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Best\ possible\ score\ is\ 1.0\ and\ it\ can\ be\ negative\ (because\ the\ model}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00384}00384\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ can\ be\ arbitrarily\ worse).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00385}00385\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00386}00386\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00387}00387\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00388}00388\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00389}00389\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Test\ samples.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00390}00390\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00391}00391\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ array-\/like\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00392}00392\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ True\ values\ of\ target.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00393}00393\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00394}00394\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00395}00395\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00396}00396\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00397}00397\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00398}00398\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00399}00399\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\ :\ float}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00400}00400\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ D\string^2\ of\ self.predict(X)\ w.r.t.\ y.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00401}00401\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00402}00402\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ Adapt\ link\ to\ User\ Guide\ in\ the\ docstring,\ once}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00403}00403\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ https://github.com/scikit-\/learn/scikit-\/learn/pull/22118\ is\ merged.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00404}00404\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00405}00405\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note,\ default\ score\ defined\ in\ RegressorMixin\ is\ R\string^2\ score.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ make\ D\string^2\ a\ score\ function\ in\ module\ metrics\ (and\ thereby\ get}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ input\ validation\ and\ so\ on)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a0a1f7607096b5c90682db8da99d01192}{\_linear\_predictor}}(X)\ \ \textcolor{comment}{\#\ validates\ X}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ required\ by\ losses}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ y\ =\ check\_array(y,\ dtype=raw\_prediction.dtype,\ order=\textcolor{stringliteral}{"{}C"{}},\ ensure\_2d=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00411}00411\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note\ that\ \_check\_sample\_weight\ calls\ check\_array(order="{}C"{})\ required\ by}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00414}00414\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ losses.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X,\ dtype=y.dtype)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00416}00416\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00417}00417\ \ \ \ \ \ \ \ \ base\_loss\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_a6c5abf7ad8ebdf0dd6f3a85ef3c8d37c}{\_base\_loss}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00418}00418\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00419}00419\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ base\_loss.in\_y\_true\_range(y):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00420}00420\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00421}00421\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Some\ value(s)\ of\ y\ are\ out\ of\ the\ valid\ range\ of\ the\ loss"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00422}00422\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\ \{base\_loss.\_\_name\_\_\}."{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00423}00423\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00424}00424\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00425}00425\ \ \ \ \ \ \ \ \ constant\ =\ np.average(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00426}00426\ \ \ \ \ \ \ \ \ \ \ \ \ base\_loss.constant\_to\_optimal\_zero(y\_true=y,\ sample\_weight=\textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00427}00427\ \ \ \ \ \ \ \ \ \ \ \ \ weights=sample\_weight,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00428}00428\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00429}00429\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00430}00430\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Missing\ factor\ of\ 2\ in\ deviance\ cancels\ out.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00431}00431\ \ \ \ \ \ \ \ \ deviance\ =\ base\_loss(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00435}00435\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00436}00436\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ y\_mean\ =\ base\_loss.link.link(np.average(y,\ weights=sample\_weight))}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00438}00438\ \ \ \ \ \ \ \ \ deviance\_null\ =\ base\_loss(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00440}00440\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=np.tile(y\_mean,\ y.shape[0]),}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00441}00441\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00443}00443\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ 1\ -\/\ (deviance\ +\ constant)\ /\ (deviance\_null\ +\ constant)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00445}00445\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00446}00446\ \ \ \ \ \textcolor{keyword}{def\ }\_\_sklearn\_tags\_\_(self):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ tags\ =\ super().\_\_sklearn\_tags\_\_()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00448}00448\ \ \ \ \ \ \ \ \ tags.input\_tags.sparse\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00450}00450\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Create\ instance\ of\ BaseLoss\ if\ fit\ wasn't\ called\ yet.\ This\ is\ necessary\ as}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TweedieRegressor\ might\ set\ the\ used\ loss\ during\ fit\ different\ from}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ self.\_base\_loss.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00453}00453\ \ \ \ \ \ \ \ \ \ \ \ \ base\_loss\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aa7ad479b2a2599e7783b2a163ced77db}{\_get\_loss}}()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00454}00454\ \ \ \ \ \ \ \ \ \ \ \ \ tags.target\_tags.positive\_only\ =\ \textcolor{keywordflow}{not}\ base\_loss.in\_y\_true\_range(-\/1.0)}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00455}00455\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ (ValueError,\ AttributeError,\ TypeError):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00456}00456\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ This\ happens\ when\ the\ link\ or\ power\ parameter\ of\ TweedieRegressor\ is}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00457}00457\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ invalid.\ We\ fallback\ on\ the\ default\ tags\ in\ that\ case.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00458}00458\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{pass}\ \ \textcolor{comment}{\#\ pragma:\ no\ cover}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00459}00459\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ tags}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00460}00460\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00461}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aa7ad479b2a2599e7783b2a163ced77db}{00461}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor_aa7ad479b2a2599e7783b2a163ced77db}{\_get\_loss}}(self):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00462}00462\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}This\ is\ only\ necessary\ because\ of\ the\ link\ and\ power\ arguments\ of\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00463}00463\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ TweedieRegressor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00464}00464\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00465}00465\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Note\ that\ we\ do\ not\ need\ to\ pass\ sample\_weight\ to\ the\ loss\ class\ as\ this\ is}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00466}00466\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ only\ needed\ to\ set\ loss.constant\_hessian\ on\ which\ GLMs\ do\ not\ rely.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00467}00467\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00468}00468\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00469}00469\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00470}00470\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00471}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{00471}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00472}00472\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Generalized\ Linear\ Model\ with\ a\ Poisson\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00473}00473\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00474}00474\ \textcolor{stringliteral}{\ \ \ \ This\ regressor\ uses\ the\ 'log'\ link\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00475}00475\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00476}00476\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <Generalized\_linear\_models>\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00477}00477\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00478}00478\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 0.23}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00479}00479\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00480}00480\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00481}00481\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00482}00482\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00483}00483\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ L2\ penalty\ term\ and\ determines\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00484}00484\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ regularization\ strength.\ \`{}\`{}alpha\ =\ 0\`{}\`{}\ is\ equivalent\ to\ unpenalized}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00485}00485\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ GLMs.\ In\ this\ case,\ the\ design\ matrix\ \`{}X\`{}\ must\ have\ full\ column\ rank}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00486}00486\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (no\ collinearities).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00487}00487\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ of\ \`{}alpha\`{}\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00488}00488\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00489}00489\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00490}00490\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Specifies\ if\ a\ constant\ (a.k.a.\ bias\ or\ intercept)\ should\ be}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00491}00491\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ added\ to\ the\ linear\ predictor\ (\`{}X\ @\ coef\ +\ intercept\`{}).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00492}00492\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00493}00493\ \textcolor{stringliteral}{\ \ \ \ solver\ :\ \{'lbfgs',\ 'newton-\/cholesky'\},\ default='lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00494}00494\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Algorithm\ to\ use\ in\ the\ optimization\ problem:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00495}00495\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00496}00496\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00497}00497\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Calls\ scipy's\ L-\/BFGS-\/B\ optimizer.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00498}00498\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00499}00499\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'newton-\/cholesky'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00500}00500\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Uses\ Newton-\/Raphson\ steps\ (in\ arbitrary\ precision\ arithmetic\ equivalent\ to}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00501}00501\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ iterated\ reweighted\ least\ squares)\ with\ an\ inner\ Cholesky\ based\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00502}00502\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ This\ solver\ is\ a\ good\ choice\ for\ \`{}n\_samples\`{}\ >>\ \`{}n\_features\`{},\ especially}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00503}00503\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ with\ one-\/hot\ encoded\ categorical\ features\ with\ rare\ categories.\ Be\ aware}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00504}00504\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ that\ the\ memory\ usage\ of\ this\ solver\ has\ a\ quadratic\ dependency\ on}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00505}00505\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}n\_features\`{}\ because\ it\ explicitly\ computes\ the\ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00506}00506\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00507}00507\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ ..\ versionadded::\ 1.2}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00508}00508\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00509}00509\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00510}00510\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximal\ number\ of\ iterations\ for\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00511}00511\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00512}00512\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00513}00513\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00514}00514\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Stopping\ criterion.\ For\ the\ lbfgs\ solver,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00515}00515\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ iteration\ will\ stop\ when\ \`{}\`{}max\{|g\_j|,\ j\ =\ 1,\ ...,\ d\}\ <=\ tol\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00516}00516\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}g\_j\`{}\`{}\ is\ the\ j-\/th\ component\ of\ the\ gradient\ (derivative)\ of}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00517}00517\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ objective\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00518}00518\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00519}00519\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00520}00520\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00521}00521\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ \`{}\`{}fit\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00522}00522\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ as\ initialization\ for\ \`{}\`{}coef\_\`{}\`{}\ and\ \`{}\`{}intercept\_\`{}\`{}\ .}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00523}00523\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00524}00524\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00525}00525\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ the\ lbfgs\ solver\ set\ verbose\ to\ any\ positive\ number\ for\ verbosity.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00526}00526\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00527}00527\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00528}00528\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00529}00529\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00530}00530\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ array\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00531}00531\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Estimated\ coefficients\ for\ the\ linear\ predictor\ (\`{}X\ @\ coef\_\ +}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00532}00532\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_\`{})\ in\ the\ GLM.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00533}00533\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00534}00534\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00535}00535\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Intercept\ (a.k.a.\ bias)\ added\ to\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00536}00536\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00537}00537\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00538}00538\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00539}00539\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00540}00540\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00541}00541\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00542}00542\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00543}00543\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00544}00544\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00545}00545\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00546}00546\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00547}00547\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00548}00548\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00549}00549\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Actual\ number\ of\ iterations\ used\ in\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00550}00550\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00551}00551\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00552}00552\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00553}00553\ \textcolor{stringliteral}{\ \ \ \ TweedieRegressor\ :\ Generalized\ Linear\ Model\ with\ a\ Tweedie\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00554}00554\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00555}00555\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00556}00556\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00557}00557\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn\ import\ linear\_model}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00558}00558\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.PoissonRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00559}00559\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ [[1,\ 2],\ [2,\ 3],\ [3,\ 4],\ [4,\ 3]]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00560}00560\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ [12,\ 17,\ 22,\ 21]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00561}00561\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00562}00562\ \textcolor{stringliteral}{\ \ \ \ PoissonRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00563}00563\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.score(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00564}00564\ \textcolor{stringliteral}{\ \ \ \ np.float64(0.990)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00565}00565\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.coef\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00566}00566\ \textcolor{stringliteral}{\ \ \ \ array([0.121,\ 0.158])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00567}00567\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.intercept\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00568}00568\ \textcolor{stringliteral}{\ \ \ \ np.float64(2.088)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00569}00569\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.predict([[1,\ 1],\ [3,\ 4]])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00570}00570\ \textcolor{stringliteral}{\ \ \ \ array([10.676,\ 21.875])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00571}00571\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00572}00572\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00573}00573\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00574}00574\ \ \ \ \ \ \ \ \ **\_GeneralizedLinearRegressor.\_parameter\_constraints}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00575}00575\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00576}00576\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00577}00577\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00578}00578\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00579}00579\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00580}00580\ \ \ \ \ \ \ \ \ alpha=1.0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00581}00581\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00582}00582\ \ \ \ \ \ \ \ \ solver="{}lbfgs"{},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00583}00583\ \ \ \ \ \ \ \ \ max\_iter=100,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00584}00584\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00585}00585\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00586}00586\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00587}00587\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00588}00588\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00589}00589\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00590}00590\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00591}00591\ \ \ \ \ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00592}00592\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00593}00593\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00594}00594\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00595}00595\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00596}00596\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00597}00597\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00598}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor_a7e908fc8d169faa5cd15c92ac97f1aeb}{00598}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor_a7e908fc8d169faa5cd15c92ac97f1aeb}{\_get\_loss}}(self):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00599}00599\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00600}00600\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00601}00601\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00602}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{00602}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{GammaRegressor}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00603}00603\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Generalized\ Linear\ Model\ with\ a\ Gamma\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00604}00604\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00605}00605\ \textcolor{stringliteral}{\ \ \ \ This\ regressor\ uses\ the\ 'log'\ link\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00606}00606\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00607}00607\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <Generalized\_linear\_models>\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00608}00608\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00609}00609\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 0.23}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00610}00610\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00611}00611\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00612}00612\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00613}00613\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00614}00614\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ L2\ penalty\ term\ and\ determines\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00615}00615\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ regularization\ strength.\ \`{}\`{}alpha\ =\ 0\`{}\`{}\ is\ equivalent\ to\ unpenalized}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00616}00616\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ GLMs.\ In\ this\ case,\ the\ design\ matrix\ \`{}X\`{}\ must\ have\ full\ column\ rank}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00617}00617\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (no\ collinearities).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00618}00618\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ of\ \`{}alpha\`{}\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00619}00619\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00620}00620\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00621}00621\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Specifies\ if\ a\ constant\ (a.k.a.\ bias\ or\ intercept)\ should\ be}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00622}00622\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ added\ to\ the\ linear\ predictor\ \`{}X\ @\ coef\_\ +\ intercept\_\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00623}00623\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00624}00624\ \textcolor{stringliteral}{\ \ \ \ solver\ :\ \{'lbfgs',\ 'newton-\/cholesky'\},\ default='lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00625}00625\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Algorithm\ to\ use\ in\ the\ optimization\ problem:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00626}00626\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00627}00627\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00628}00628\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Calls\ scipy's\ L-\/BFGS-\/B\ optimizer.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00629}00629\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00630}00630\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'newton-\/cholesky'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00631}00631\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Uses\ Newton-\/Raphson\ steps\ (in\ arbitrary\ precision\ arithmetic\ equivalent\ to}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00632}00632\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ iterated\ reweighted\ least\ squares)\ with\ an\ inner\ Cholesky\ based\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00633}00633\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ This\ solver\ is\ a\ good\ choice\ for\ \`{}n\_samples\`{}\ >>\ \`{}n\_features\`{},\ especially}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00634}00634\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ with\ one-\/hot\ encoded\ categorical\ features\ with\ rare\ categories.\ Be\ aware}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00635}00635\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ that\ the\ memory\ usage\ of\ this\ solver\ has\ a\ quadratic\ dependency\ on}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00636}00636\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}n\_features\`{}\ because\ it\ explicitly\ computes\ the\ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00637}00637\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00638}00638\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ ..\ versionadded::\ 1.2}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00639}00639\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00640}00640\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00641}00641\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximal\ number\ of\ iterations\ for\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00642}00642\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00643}00643\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00644}00644\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00645}00645\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Stopping\ criterion.\ For\ the\ lbfgs\ solver,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00646}00646\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ iteration\ will\ stop\ when\ \`{}\`{}max\{|g\_j|,\ j\ =\ 1,\ ...,\ d\}\ <=\ tol\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00647}00647\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}g\_j\`{}\`{}\ is\ the\ j-\/th\ component\ of\ the\ gradient\ (derivative)\ of}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00648}00648\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ objective\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00649}00649\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00650}00650\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00651}00651\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00652}00652\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ \`{}\`{}fit\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00653}00653\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ as\ initialization\ for\ \`{}coef\_\`{}\ and\ \`{}intercept\_\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00654}00654\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00655}00655\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00656}00656\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ the\ lbfgs\ solver\ set\ verbose\ to\ any\ positive\ number\ for\ verbosity.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00657}00657\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00658}00658\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00659}00659\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00660}00660\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00661}00661\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ array\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00662}00662\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Estimated\ coefficients\ for\ the\ linear\ predictor\ (\`{}X\ @\ coef\_\ +}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00663}00663\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_\`{})\ in\ the\ GLM.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00664}00664\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00665}00665\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00666}00666\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Intercept\ (a.k.a.\ bias)\ added\ to\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00667}00667\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00668}00668\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00669}00669\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00670}00670\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00671}00671\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00672}00672\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00673}00673\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00674}00674\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Actual\ number\ of\ iterations\ used\ in\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00675}00675\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00676}00676\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00677}00677\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00678}00678\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00679}00679\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00680}00680\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00681}00681\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00682}00682\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00683}00683\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00684}00684\ \textcolor{stringliteral}{\ \ \ \ PoissonRegressor\ :\ Generalized\ Linear\ Model\ with\ a\ Poisson\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00685}00685\ \textcolor{stringliteral}{\ \ \ \ TweedieRegressor\ :\ Generalized\ Linear\ Model\ with\ a\ Tweedie\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00686}00686\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00687}00687\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00688}00688\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00689}00689\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn\ import\ linear\_model}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00690}00690\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.GammaRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00691}00691\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ [[1,\ 2],\ [2,\ 3],\ [3,\ 4],\ [4,\ 3]]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00692}00692\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ [19,\ 26,\ 33,\ 30]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00693}00693\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00694}00694\ \textcolor{stringliteral}{\ \ \ \ GammaRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00695}00695\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.score(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00696}00696\ \textcolor{stringliteral}{\ \ \ \ np.float64(0.773)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00697}00697\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.coef\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00698}00698\ \textcolor{stringliteral}{\ \ \ \ array([0.073,\ 0.067])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00699}00699\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.intercept\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00700}00700\ \textcolor{stringliteral}{\ \ \ \ np.float64(2.896)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00701}00701\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.predict([[1,\ 0],\ [2,\ 8]])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00702}00702\ \textcolor{stringliteral}{\ \ \ \ array([19.483,\ 35.795])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00703}00703\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00704}00704\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00705}00705\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00706}00706\ \ \ \ \ \ \ \ \ **\_GeneralizedLinearRegressor.\_parameter\_constraints}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00707}00707\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00708}00708\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00709}00709\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00710}00710\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00711}00711\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00712}00712\ \ \ \ \ \ \ \ \ alpha=1.0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ solver="{}lbfgs"{},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ max\_iter=100,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00717}00717\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00718}00718\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00719}00719\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00720}00720\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00723}00723\ \ \ \ \ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00724}00724\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00726}00726\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00727}00727\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00728}00728\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00729}00729\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00730}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor_a6174c2bae8f6d6e89dc42378474f7532}{00730}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor_a6174c2bae8f6d6e89dc42378474f7532}{\_get\_loss}}(self):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00731}00731\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}()}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00732}00732\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00733}00733\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00734}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{00734}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00735}00735\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Generalized\ Linear\ Model\ with\ a\ Tweedie\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00736}00736\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00737}00737\ \textcolor{stringliteral}{\ \ \ \ This\ estimator\ can\ be\ used\ to\ model\ different\ GLMs\ depending\ on\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00738}00738\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}power\`{}\`{}\ parameter,\ which\ determines\ the\ underlying\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00739}00739\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00740}00740\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <Generalized\_linear\_models>\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00741}00741\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00742}00742\ \textcolor{stringliteral}{\ \ \ \ ..\ versionadded::\ 0.23}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00743}00743\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00744}00744\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00745}00745\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00746}00746\ \textcolor{stringliteral}{\ \ \ \ power\ :\ float,\ default=0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00747}00747\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ power\ determines\ the\ underlying\ target\ distribution\ according}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00748}00748\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ the\ following\ table:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00749}00749\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00750}00750\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00751}00751\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ Power\ |\ Distribution\ \ \ \ \ \ \ \ \ \ \ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00752}00752\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +=======+========================+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00753}00753\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ 0\ \ \ \ \ |\ Normal\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00754}00754\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00755}00755\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ 1\ \ \ \ \ |\ Poisson\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00756}00756\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00757}00757\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ (1,2)\ |\ Compound\ Poisson\ Gamma\ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00758}00758\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00759}00759\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ 2\ \ \ \ \ |\ Gamma\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00760}00760\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00761}00761\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ |\ 3\ \ \ \ \ |\ Inverse\ Gaussian\ \ \ \ \ \ \ |}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00762}00762\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +-\/-\/-\/-\/-\/-\/-\/+-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/+}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00763}00763\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00764}00764\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ For\ \`{}\`{}0\ <\ power\ <\ 1\`{}\`{},\ no\ distribution\ exists.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00765}00765\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00766}00766\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00767}00767\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Constant\ that\ multiplies\ the\ L2\ penalty\ term\ and\ determines\ the}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00768}00768\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ regularization\ strength.\ \`{}\`{}alpha\ =\ 0\`{}\`{}\ is\ equivalent\ to\ unpenalized}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00769}00769\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ GLMs.\ In\ this\ case,\ the\ design\ matrix\ \`{}X\`{}\ must\ have\ full\ column\ rank}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00770}00770\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (no\ collinearities).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00771}00771\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ of\ \`{}alpha\`{}\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00772}00772\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00773}00773\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00774}00774\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Specifies\ if\ a\ constant\ (a.k.a.\ bias\ or\ intercept)\ should\ be}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00775}00775\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ added\ to\ the\ linear\ predictor\ (\`{}X\ @\ coef\ +\ intercept\`{}).}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00776}00776\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00777}00777\ \textcolor{stringliteral}{\ \ \ \ link\ :\ \{'auto',\ 'identity',\ 'log'\},\ default='auto'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00778}00778\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ link\ function\ of\ the\ GLM,\ i.e.\ mapping\ from\ linear\ predictor}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00779}00779\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}X\ @\ coeff\ +\ intercept\`{}\ to\ prediction\ \`{}y\_pred\`{}.\ Option\ 'auto'\ sets}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00780}00780\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ link\ depending\ on\ the\ chosen\ \`{}power\`{}\ parameter\ as\ follows:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00781}00781\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00782}00782\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'identity'\ for\ \`{}\`{}power\ <=\ 0\`{}\`{},\ e.g.\ for\ the\ Normal\ distribution}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00783}00783\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ 'log'\ for\ \`{}\`{}power\ >\ 0\`{}\`{},\ e.g.\ for\ Poisson,\ Gamma\ and\ Inverse\ Gaussian}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00784}00784\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ distributions}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00785}00785\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00786}00786\ \textcolor{stringliteral}{\ \ \ \ solver\ :\ \{'lbfgs',\ 'newton-\/cholesky'\},\ default='lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00787}00787\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Algorithm\ to\ use\ in\ the\ optimization\ problem:}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00788}00788\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00789}00789\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'lbfgs'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00790}00790\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Calls\ scipy's\ L-\/BFGS-\/B\ optimizer.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00791}00791\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00792}00792\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'newton-\/cholesky'}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00793}00793\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Uses\ Newton-\/Raphson\ steps\ (in\ arbitrary\ precision\ arithmetic\ equivalent\ to}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00794}00794\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ iterated\ reweighted\ least\ squares)\ with\ an\ inner\ Cholesky\ based\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00795}00795\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ This\ solver\ is\ a\ good\ choice\ for\ \`{}n\_samples\`{}\ >>\ \`{}n\_features\`{},\ especially}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00796}00796\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ with\ one-\/hot\ encoded\ categorical\ features\ with\ rare\ categories.\ Be\ aware}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00797}00797\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ that\ the\ memory\ usage\ of\ this\ solver\ has\ a\ quadratic\ dependency\ on}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00798}00798\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}n\_features\`{}\ because\ it\ explicitly\ computes\ the\ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00799}00799\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00800}00800\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ ..\ versionadded::\ 1.2}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00801}00801\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00802}00802\ \textcolor{stringliteral}{\ \ \ \ max\_iter\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00803}00803\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ maximal\ number\ of\ iterations\ for\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00804}00804\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00805}00805\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00806}00806\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00807}00807\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Stopping\ criterion.\ For\ the\ lbfgs\ solver,}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00808}00808\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ iteration\ will\ stop\ when\ \`{}\`{}max\{|g\_j|,\ j\ =\ 1,\ ...,\ d\}\ <=\ tol\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00809}00809\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}g\_j\`{}\`{}\ is\ the\ j-\/th\ component\ of\ the\ gradient\ (derivative)\ of}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00810}00810\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ objective\ function.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00811}00811\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00812}00812\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00813}00813\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00814}00814\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ \`{}\`{}fit\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00815}00815\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ as\ initialization\ for\ \`{}\`{}coef\_\`{}\`{}\ and\ \`{}\`{}intercept\_\`{}\`{}\ .}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00816}00816\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00817}00817\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00818}00818\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ the\ lbfgs\ solver\ set\ verbose\ to\ any\ positive\ number\ for\ verbosity.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00819}00819\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00820}00820\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00821}00821\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00822}00822\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00823}00823\ \textcolor{stringliteral}{\ \ \ \ coef\_\ :\ array\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00824}00824\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Estimated\ coefficients\ for\ the\ linear\ predictor\ (\`{}X\ @\ coef\_\ +}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00825}00825\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\_\`{})\ in\ the\ GLM.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00826}00826\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00827}00827\ \textcolor{stringliteral}{\ \ \ \ intercept\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00828}00828\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Intercept\ (a.k.a.\ bias)\ added\ to\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00829}00829\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00830}00830\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00831}00831\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Actual\ number\ of\ iterations\ used\ in\ the\ solver.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00832}00832\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00833}00833\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00834}00834\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00835}00835\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00836}00836\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00837}00837\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00838}00838\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00839}00839\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00840}00840\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00841}00841\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00842}00842\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00843}00843\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00844}00844\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00845}00845\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00846}00846\ \textcolor{stringliteral}{\ \ \ \ PoissonRegressor\ :\ Generalized\ Linear\ Model\ with\ a\ Poisson\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00847}00847\ \textcolor{stringliteral}{\ \ \ \ GammaRegressor\ :\ Generalized\ Linear\ Model\ with\ a\ Gamma\ distribution.}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00848}00848\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00849}00849\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00850}00850\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00851}00851\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn\ import\ linear\_model}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00852}00852\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ linear\_model.TweedieRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00853}00853\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\ =\ [[1,\ 2],\ [2,\ 3],\ [3,\ 4],\ [4,\ 3]]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00854}00854\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\ =\ [2,\ 3.5,\ 5,\ 5.5]}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00855}00855\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.fit(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00856}00856\ \textcolor{stringliteral}{\ \ \ \ TweedieRegressor()}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00857}00857\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.score(X,\ y)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00858}00858\ \textcolor{stringliteral}{\ \ \ \ np.float64(0.839)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00859}00859\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.coef\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00860}00860\ \textcolor{stringliteral}{\ \ \ \ array([0.599,\ 0.299])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00861}00861\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.intercept\_}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00862}00862\ \textcolor{stringliteral}{\ \ \ \ np.float64(1.600)}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00863}00863\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.predict([[1,\ 1],\ [3,\ 4]])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00864}00864\ \textcolor{stringliteral}{\ \ \ \ array([2.500,\ 4.599])}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00865}00865\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00866}00866\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00867}00867\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00868}00868\ \ \ \ \ \ \ \ \ **\_GeneralizedLinearRegressor.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00869}00869\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}power"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00870}00870\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}link"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}auto"{}},\ \textcolor{stringliteral}{"{}identity"{}},\ \textcolor{stringliteral}{"{}log"{}}\})],}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00871}00871\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00872}00872\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00873}00873\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00874}00874\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ power=0.0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ alpha=1.0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00878}00878\ \ \ \ \ \ \ \ \ fit\_intercept=True,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00879}00879\ \ \ \ \ \ \ \ \ link="{}auto"{},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00880}00880\ \ \ \ \ \ \ \ \ solver="{}lbfgs"{},}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00881}00881\ \ \ \ \ \ \ \ \ max\_iter=100,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00882}00882\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00883}00883\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00884}00884\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00885}00885\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00886}00886\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00887}00887\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00888}00888\ \ \ \ \ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00889}00889\ \ \ \ \ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00890}00890\ \ \ \ \ \ \ \ \ \ \ \ \ max\_iter=max\_iter,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00891}00891\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00892}00892\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00893}00893\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00894}00894\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00895}00895\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a959bee666354a81e87de78ba381873ae}{link}}\ =\ link}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00896}00896\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}}\ =\ power}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00897}00897\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00898}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_aba1c54307239faafc29d9eab0820d60d}{00898}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_aba1c54307239faafc29d9eab0820d60d}{\_get\_loss}}(self):}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00899}00899\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a959bee666354a81e87de78ba381873ae}{link}}\ ==\ \textcolor{stringliteral}{"{}auto"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00900}00900\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}}\ <=\ 0:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00901}00901\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ identity\ link}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00902}00902\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00903}00903\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00904}00904\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ log\ link}}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00905}00905\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00906}00906\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00907}00907\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a959bee666354a81e87de78ba381873ae}{link}}\ ==\ \textcolor{stringliteral}{"{}log"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00908}00908\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}})}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00909}00909\ }
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00910}00910\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a959bee666354a81e87de78ba381873ae}{link}}\ ==\ \textcolor{stringliteral}{"{}identity"{}}:}
\DoxyCodeLine{\Hypertarget{glm_8py_source_l00911}00911\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor_a17040590073f54882bad058c34212301}{power}})}

\end{DoxyCode}
