\doxysection{\+\_\+entropy.\+py}
\hypertarget{__entropy_8py_source}{}\label{__entropy_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/scipy/stats/\_entropy.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/scipy/stats/\_entropy.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00001}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00002}00002\ \textcolor{stringliteral}{Created\ on\ Fri\ Apr\ \ 2\ 09:06:05\ 2021}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00003}00003\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00004}00004\ \textcolor{stringliteral}{@author:\ matth}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00005}00005\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00006}00006\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00007}00007\ \textcolor{keyword}{from}\ \_\_future\_\_\ \textcolor{keyword}{import}\ annotations}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00008}00008\ \textcolor{keyword}{import}\ math}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00009}00009\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00010}00010\ \textcolor{keyword}{from}\ scipy\ \textcolor{keyword}{import}\ special}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00011}00011\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00012}00012\ \_\_all\_\_\ =\ [\textcolor{stringliteral}{'entropy'},\ \textcolor{stringliteral}{'differential\_entropy'}]}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00014}00014\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00015}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}{00015}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}{entropy}}(pk:\ np.typing.ArrayLike,}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00016}00016\ \ \ \ \ \ \ \ \ \ \ \ \ qk:\ np.typing.ArrayLike\ |\ \textcolor{keywordtype}{None}\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00017}00017\ \ \ \ \ \ \ \ \ \ \ \ \ base:\ float\ |\ \textcolor{keywordtype}{None}\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00018}00018\ \ \ \ \ \ \ \ \ \ \ \ \ axis:\ int\ =\ 0}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00019}00019\ \ \ \ \ \ \ \ \ \ \ \ \ )\ -\/>\ np.number\ |\ np.ndarray:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00020}00020\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00021}00021\ \textcolor{stringliteral}{\ \ \ \ Calculate\ the\ Shannon\ entropy/relative\ entropy\ of\ given\ distribution(s).}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00022}00022\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00023}00023\ \textcolor{stringliteral}{\ \ \ \ If\ only\ probabilities\ \`{}pk\`{}\ are\ given,\ the\ Shannon\ entropy\ is\ calculated\ as}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00024}00024\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}H\ =\ -\/sum(pk\ *\ log(pk))\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00025}00025\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00026}00026\ \textcolor{stringliteral}{\ \ \ \ If\ \`{}qk\`{}\ is\ not\ None,\ then\ compute\ the\ relative\ entropy}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00027}00027\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}D\ =\ sum(pk\ *\ log(pk\ /\ qk))\`{}\`{}.\ This\ quantity\ is\ also\ known}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00028}00028\ \textcolor{stringliteral}{\ \ \ \ as\ the\ Kullback-\/Leibler\ divergence.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00029}00029\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00030}00030\ \textcolor{stringliteral}{\ \ \ \ This\ routine\ will\ normalize\ \`{}pk\`{}\ and\ \`{}qk\`{}\ if\ they\ don't\ sum\ to\ 1.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00031}00031\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00032}00032\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00033}00033\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00034}00034\ \textcolor{stringliteral}{\ \ \ \ pk\ :\ array\_like}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00035}00035\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Defines\ the\ (discrete)\ distribution.\ Along\ each\ axis-\/slice\ of\ \`{}\`{}pk\`{}\`{},}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00036}00036\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ element\ \`{}\`{}i\`{}\`{}\ is\ the\ \ (possibly\ unnormalized)\ probability\ of\ event}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00037}00037\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}i\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00038}00038\ \textcolor{stringliteral}{\ \ \ \ qk\ :\ array\_like,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00039}00039\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Sequence\ against\ which\ the\ relative\ entropy\ is\ computed.\ Should\ be\ in}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00040}00040\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ same\ format\ as\ \`{}pk\`{}.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00041}00041\ \textcolor{stringliteral}{\ \ \ \ base\ :\ float,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00042}00042\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ logarithmic\ base\ to\ use,\ defaults\ to\ \`{}\`{}e\`{}\`{}\ (natural\ logarithm).}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00043}00043\ \textcolor{stringliteral}{\ \ \ \ axis\ :\ int,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00044}00044\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ axis\ along\ which\ the\ entropy\ is\ calculated.\ Default\ is\ 0.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00045}00045\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00046}00046\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00047}00047\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00048}00048\ \textcolor{stringliteral}{\ \ \ \ S\ :\ \{float,\ array\_like\}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00049}00049\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ calculated\ entropy.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00050}00050\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00051}00051\ \textcolor{stringliteral}{\ \ \ \ Notes}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00052}00052\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00053}00053\ \textcolor{stringliteral}{\ \ \ \ Informally,\ the\ Shannon\ entropy\ quantifies\ the\ expected\ uncertainty}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00054}00054\ \textcolor{stringliteral}{\ \ \ \ inherent\ in\ the\ possible\ outcomes\ of\ a\ discrete\ random\ variable.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00055}00055\ \textcolor{stringliteral}{\ \ \ \ For\ example,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00056}00056\ \textcolor{stringliteral}{\ \ \ \ if\ messages\ consisting\ of\ sequences\ of\ symbols\ from\ a\ set\ are\ to\ be}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00057}00057\ \textcolor{stringliteral}{\ \ \ \ encoded\ and\ transmitted\ over\ a\ noiseless\ channel,\ then\ the\ Shannon\ entropy}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00058}00058\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}H(pk)\`{}\`{}\ gives\ a\ tight\ lower\ bound\ for\ the\ average\ number\ of\ units\ of}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00059}00059\ \textcolor{stringliteral}{\ \ \ \ information\ needed\ per\ symbol\ if\ the\ symbols\ occur\ with\ frequencies}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00060}00060\ \textcolor{stringliteral}{\ \ \ \ governed\ by\ the\ discrete\ distribution\ \`{}pk\`{}\ [1]\_.\ The\ choice\ of\ base}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00061}00061\ \textcolor{stringliteral}{\ \ \ \ determines\ the\ choice\ of\ units;\ e.g.,\ \`{}\`{}e\`{}\`{}\ for\ nats,\ \`{}\`{}2\`{}\`{}\ for\ bits,\ etc.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00062}00062\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00063}00063\ \textcolor{stringliteral}{\ \ \ \ The\ relative\ entropy,\ \`{}\`{}D(pk|qk)\`{}\`{},\ quantifies\ the\ increase\ in\ the\ average}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00064}00064\ \textcolor{stringliteral}{\ \ \ \ number\ of\ units\ of\ information\ needed\ per\ symbol\ if\ the\ encoding\ is}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00065}00065\ \textcolor{stringliteral}{\ \ \ \ optimized\ for\ the\ probability\ distribution\ \`{}qk\`{}\ instead\ of\ the\ true}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00066}00066\ \textcolor{stringliteral}{\ \ \ \ distribution\ \`{}pk\`{}.\ Informally,\ the\ relative\ entropy\ quantifies\ the\ expected}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00067}00067\ \textcolor{stringliteral}{\ \ \ \ excess\ in\ surprise\ experienced\ if\ one\ believes\ the\ true\ distribution\ is}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00068}00068\ \textcolor{stringliteral}{\ \ \ \ \`{}qk\`{}\ when\ it\ is\ actually\ \`{}pk\`{}.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00069}00069\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00070}00070\ \textcolor{stringliteral}{\ \ \ \ A\ related\ quantity,\ the\ cross\ entropy\ \`{}\`{}CE(pk,\ qk)\`{}\`{},\ satisfies\ the}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00071}00071\ \textcolor{stringliteral}{\ \ \ \ equation\ \`{}\`{}CE(pk,\ qk)\ =\ H(pk)\ +\ D(pk|qk)\`{}\`{}\ and\ can\ also\ be\ calculated\ with}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00072}00072\ \textcolor{stringliteral}{\ \ \ \ the\ formula\ \`{}\`{}CE\ =\ -\/sum(pk\ *\ log(qk))\`{}\`{}.\ It\ gives\ the\ average}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00073}00073\ \textcolor{stringliteral}{\ \ \ \ number\ of\ units\ of\ information\ needed\ per\ symbol\ if\ an\ encoding\ is}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00074}00074\ \textcolor{stringliteral}{\ \ \ \ optimized\ for\ the\ probability\ distribution\ \`{}qk\`{}\ when\ the\ true\ distribution}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00075}00075\ \textcolor{stringliteral}{\ \ \ \ is\ \`{}pk\`{}.\ It\ is\ not\ computed\ directly\ by\ \`{}entropy\`{},\ but\ it\ can\ be\ computed}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00076}00076\ \textcolor{stringliteral}{\ \ \ \ using\ two\ calls\ to\ the\ function\ (see\ Examples).}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00077}00077\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00078}00078\ \textcolor{stringliteral}{\ \ \ \ See\ [2]\_\ for\ more\ information.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00079}00079\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00080}00080\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00081}00081\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00082}00082\ \textcolor{stringliteral}{\ \ \ \ ..\ [1]\ Shannon,\ C.E.\ (1948),\ A\ Mathematical\ Theory\ of\ Communication.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00083}00083\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Bell\ System\ Technical\ Journal,\ 27:\ 379-\/423.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00084}00084\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ https://doi.org/10.1002/j.1538-\/7305.1948.tb01338.x}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00085}00085\ \textcolor{stringliteral}{\ \ \ \ ..\ [2]\ Thomas\ M.\ Cover\ and\ Joy\ A.\ Thomas.\ 2006.\ Elements\ of\ Information}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00086}00086\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Theory\ (Wiley\ Series\ in\ Telecommunications\ and\ Signal\ Processing).}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00087}00087\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Wiley-\/Interscience,\ USA.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00088}00088\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00089}00089\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ The\ outcome\ of\ a\ fair\ coin\ is\ the\ most\ uncertain:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00093}00093\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00094}00094\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ scipy.stats\ import\ entropy}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ >>>\ base\ =\ 2\ \ \#\ work\ in\ units\ of\ bits}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ >>>\ pk\ =\ np.array([1/2,\ 1/2])\ \ \#\ fair\ coin}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ >>>\ H\ =\ entropy(pk,\ base=base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00099}00099\ \textcolor{stringliteral}{\ \ \ \ >>>\ H}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ 1.0}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ >>>\ H\ ==\ -\/np.sum(pk\ *\ np.log(pk))\ /\ np.log(base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ True}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00103}00103\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ The\ outcome\ of\ a\ biased\ coin\ is\ less\ uncertain:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00105}00105\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ >>>\ qk\ =\ np.array([9/10,\ 1/10])\ \ \#\ biased\ coin}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ >>>\ entropy(qk,\ base=base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00108}00108\ \textcolor{stringliteral}{\ \ \ \ 0.46899559358928117}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00109}00109\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00110}00110\ \textcolor{stringliteral}{\ \ \ \ The\ relative\ entropy\ between\ the\ fair\ coin\ and\ biased\ coin\ is\ calculated}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ as:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00112}00112\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ >>>\ D\ =\ entropy(pk,\ qk,\ base=base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ >>>\ D}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ 0.7369655941662062}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ >>>\ D\ ==\ np.sum(pk\ *\ np.log(pk/qk))\ /\ np.log(base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ True}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00118}00118\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00119}00119\ \textcolor{stringliteral}{\ \ \ \ The\ cross\ entropy\ can\ be\ calculated\ as\ the\ sum\ of\ the\ entropy\ and}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00120}00120\ \textcolor{stringliteral}{\ \ \ \ relative\ entropy\`{}:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00121}00121\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00122}00122\ \textcolor{stringliteral}{\ \ \ \ >>>\ CE\ =\ entropy(pk,\ base=base)\ +\ entropy(pk,\ qk,\ base=base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00123}00123\ \textcolor{stringliteral}{\ \ \ \ >>>\ CE}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00124}00124\ \textcolor{stringliteral}{\ \ \ \ 1.736965594166206}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00125}00125\ \textcolor{stringliteral}{\ \ \ \ >>>\ CE\ ==\ -\/np.sum(pk\ *\ np.log(qk))\ /\ np.log(base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00126}00126\ \textcolor{stringliteral}{\ \ \ \ True}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00127}00127\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00128}00128\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00129}00129\ \ \ \ \ \textcolor{keywordflow}{if}\ base\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{and}\ base\ <=\ 0:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}\`{}base`\ must\ be\ a\ positive\ number\ or\ \`{}None`."{}})}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00131}00131\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00132}00132\ \ \ \ \ pk\ =\ np.asarray(pk)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00133}00133\ \ \ \ \ pk\ =\ 1.0*pk\ /\ np.sum(pk,\ axis=axis,\ keepdims=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00134}00134\ \ \ \ \ \textcolor{keywordflow}{if}\ qk\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00135}00135\ \ \ \ \ \ \ \ \ vec\ =\ special.entr(pk)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00136}00136\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ qk\ =\ np.asarray(qk)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00138}00138\ \ \ \ \ \ \ \ \ pk,\ qk\ =\ np.broadcast\_arrays(pk,\ qk)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00139}00139\ \ \ \ \ \ \ \ \ qk\ =\ 1.0*qk\ /\ np.sum(qk,\ axis=axis,\ keepdims=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ vec\ =\ special.rel\_entr(pk,\ qk)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00141}00141\ \ \ \ \ S\ =\ np.sum(vec,\ axis=axis)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00142}00142\ \ \ \ \ \textcolor{keywordflow}{if}\ base\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ S\ /=\ np.log(base)}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00144}00144\ \ \ \ \ \textcolor{keywordflow}{return}\ S}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00145}00145\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00146}00146\ }
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00147}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}{00147}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}{differential\_entropy}}(}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00148}00148\ \ \ \ \ values:\ np.typing.ArrayLike,}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00149}00149\ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00150}00150\ \ \ \ \ window\_length:\ int\ |\ \textcolor{keywordtype}{None}\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00151}00151\ \ \ \ \ base:\ float\ |\ \textcolor{keywordtype}{None}\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00152}00152\ \ \ \ \ axis:\ int\ =\ 0,}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00153}00153\ \ \ \ \ method:\ str\ =\ \textcolor{stringliteral}{"{}auto"{}},}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00154}00154\ )\ -\/>\ np.number\ |\ np.ndarray:}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00155}00155\ \ \ \ \ \textcolor{stringliteral}{r"{}"{}"{}Given\ a\ sample\ of\ a\ distribution,\ estimate\ the\ differential\ entropy.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00156}00156\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00157}00157\ \textcolor{stringliteral}{\ \ \ \ Several\ estimation\ methods\ are\ available\ using\ the\ \`{}method\`{}\ parameter.\ By}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00158}00158\ \textcolor{stringliteral}{\ \ \ \ default,\ a\ method\ is\ selected\ based\ the\ size\ of\ the\ sample.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00159}00159\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00160}00160\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00161}00161\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00162}00162\ \textcolor{stringliteral}{\ \ \ \ values\ :\ sequence}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00163}00163\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Sample\ from\ a\ continuous\ distribution.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00164}00164\ \textcolor{stringliteral}{\ \ \ \ window\_length\ :\ int,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00165}00165\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Window\ length\ for\ computing\ Vasicek\ estimate.\ Must\ be\ an\ integer}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00166}00166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ between\ 1\ and\ half\ of\ the\ sample\ size.\ If\ \`{}\`{}None\`{}\`{}\ (the\ default),\ it}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00167}00167\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ uses\ the\ heuristic\ value}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00168}00168\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00169}00169\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ math::}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00170}00170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \(\backslash\)left\ \(\backslash\)lfloor\ \(\backslash\)sqrt\{n\}\ +\ 0.5\ \(\backslash\)right\ \(\backslash\)rfloor}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00171}00171\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00172}00172\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ :math:\`{}n\`{}\ is\ the\ sample\ size.\ This\ heuristic\ was\ originally}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00173}00173\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proposed\ in\ [2]\_\ and\ has\ become\ common\ in\ the\ literature.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00174}00174\ \textcolor{stringliteral}{\ \ \ \ base\ :\ float,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ logarithmic\ base\ to\ use,\ defaults\ to\ \`{}\`{}e\`{}\`{}\ (natural\ logarithm).}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00176}00176\ \textcolor{stringliteral}{\ \ \ \ axis\ :\ int,\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ axis\ along\ which\ the\ differential\ entropy\ is\ calculated.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Default\ is\ 0.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ method\ :\ \{'vasicek',\ 'van\ es',\ 'ebrahimi',\ 'correa',\ 'auto'\},\ optional}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ method\ used\ to\ estimate\ the\ differential\ entropy\ from\ the\ sample.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Default\ is\ \`{}\`{}'auto'\`{}\`{}.\ \ See\ Notes\ for\ more\ information.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00182}00182\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00183}00183\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00184}00184\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00185}00185\ \textcolor{stringliteral}{\ \ \ \ entropy\ :\ float}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00186}00186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ calculated\ differential\ entropy.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00187}00187\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00188}00188\ \textcolor{stringliteral}{\ \ \ \ Notes}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00189}00189\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00190}00190\ \textcolor{stringliteral}{\ \ \ \ This\ function\ will\ converge\ to\ the\ true\ differential\ entropy\ in\ the\ limit}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00191}00191\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00192}00192\ \textcolor{stringliteral}{\ \ \ \ ..\ math::}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00193}00193\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\ \(\backslash\)to\ \(\backslash\)infty,\ \(\backslash\)quad\ m\ \(\backslash\)to\ \(\backslash\)infty,\ \(\backslash\)quad\ \(\backslash\)frac\{m\}\{n\}\ \(\backslash\)to\ 0}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00194}00194\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00195}00195\ \textcolor{stringliteral}{\ \ \ \ The\ optimal\ choice\ of\ \`{}\`{}window\_length\`{}\`{}\ for\ a\ given\ sample\ size\ depends\ on}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00196}00196\ \textcolor{stringliteral}{\ \ \ \ the\ (unknown)\ distribution.\ Typically,\ the\ smoother\ the\ density\ of\ the}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00197}00197\ \textcolor{stringliteral}{\ \ \ \ distribution,\ the\ larger\ the\ optimal\ value\ of\ \`{}\`{}window\_length\`{}\`{}\ [1]\_.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00198}00198\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00199}00199\ \textcolor{stringliteral}{\ \ \ \ The\ following\ options\ are\ available\ for\ the\ \`{}method\`{}\ parameter.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00200}00200\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00201}00201\ \textcolor{stringliteral}{\ \ \ \ *\ \`{}\`{}'vasicek'\`{}\`{}\ uses\ the\ estimator\ presented\ in\ [1]\_.\ This\ is}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00202}00202\ \textcolor{stringliteral}{\ \ \ \ \ \ one\ of\ the\ first\ and\ most\ influential\ estimators\ of\ differential\ entropy.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00203}00203\ \textcolor{stringliteral}{\ \ \ \ *\ \`{}\`{}'van\ es'\`{}\`{}\ uses\ the\ bias-\/corrected\ estimator\ presented\ in\ [3]\_,\ which}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00204}00204\ \textcolor{stringliteral}{\ \ \ \ \ \ is\ not\ only\ consistent\ but,\ under\ some\ conditions,\ asymptotically\ normal.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00205}00205\ \textcolor{stringliteral}{\ \ \ \ *\ \`{}\`{}'ebrahimi'\`{}\`{}\ uses\ an\ estimator\ presented\ in\ [4]\_,\ which\ was\ shown}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00206}00206\ \textcolor{stringliteral}{\ \ \ \ \ \ in\ simulation\ to\ have\ smaller\ bias\ and\ mean\ squared\ error\ than}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00207}00207\ \textcolor{stringliteral}{\ \ \ \ \ \ the\ Vasicek\ estimator.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00208}00208\ \textcolor{stringliteral}{\ \ \ \ *\ \`{}\`{}'correa'\`{}\`{}\ uses\ the\ estimator\ presented\ in\ [5]\_\ based\ on\ local\ linear}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00209}00209\ \textcolor{stringliteral}{\ \ \ \ \ \ regression.\ In\ a\ simulation\ study,\ it\ had\ consistently\ smaller\ mean}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00210}00210\ \textcolor{stringliteral}{\ \ \ \ \ \ square\ error\ than\ the\ Vasiceck\ estimator,\ but\ it\ is\ more\ expensive\ to}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00211}00211\ \textcolor{stringliteral}{\ \ \ \ \ \ compute.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00212}00212\ \textcolor{stringliteral}{\ \ \ \ *\ \`{}\`{}'auto'\`{}\`{}\ selects\ the\ method\ automatically\ (default).\ Currently,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00213}00213\ \textcolor{stringliteral}{\ \ \ \ \ \ this\ selects\ \`{}\`{}'van\ es'\`{}\`{}\ for\ very\ small\ samples\ (<10),\ \`{}\`{}'ebrahimi'\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00214}00214\ \textcolor{stringliteral}{\ \ \ \ \ \ for\ moderate\ sample\ sizes\ (11-\/1000),\ and\ \`{}\`{}'vasicek'\`{}\`{}\ for\ larger}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00215}00215\ \textcolor{stringliteral}{\ \ \ \ \ \ samples,\ but\ this\ behavior\ is\ subject\ to\ change\ in\ future\ versions.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00216}00216\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00217}00217\ \textcolor{stringliteral}{\ \ \ \ All\ estimators\ are\ implemented\ as\ described\ in\ [6]\_.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00218}00218\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00219}00219\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00220}00220\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00221}00221\ \textcolor{stringliteral}{\ \ \ \ ..\ [1]\ Vasicek,\ O.\ (1976).\ A\ test\ for\ normality\ based\ on\ sample\ entropy.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00222}00222\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Journal\ of\ the\ Royal\ Statistical\ Society:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00223}00223\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Series\ B\ (Methodological),\ 38(1),\ 54-\/59.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00224}00224\ \textcolor{stringliteral}{\ \ \ \ ..\ [2]\ Crzcgorzewski,\ P.,\ \&\ Wirczorkowski,\ R.\ (1999).\ Entropy-\/based}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00225}00225\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ goodness-\/of-\/fit\ test\ for\ exponentiality.\ Communications\ in}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00226}00226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Statistics-\/Theory\ and\ Methods,\ 28(5),\ 1183-\/1202.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00227}00227\ \textcolor{stringliteral}{\ \ \ \ ..\ [3]\ Van\ Es,\ B.\ (1992).\ Estimating\ functionals\ related\ to\ a\ density\ by\ a}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00228}00228\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ class\ of\ statistics\ based\ on\ spacings.\ Scandinavian\ Journal\ of}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00229}00229\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Statistics,\ 61-\/72.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00230}00230\ \textcolor{stringliteral}{\ \ \ \ ..\ [4]\ Ebrahimi,\ N.,\ Pflughoeft,\ K.,\ \&\ Soofi,\ E.\ S.\ (1994).\ Two\ measures}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00231}00231\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ of\ sample\ entropy.\ Statistics\ \&\ Probability\ Letters,\ 20(3),\ 225-\/234.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00232}00232\ \textcolor{stringliteral}{\ \ \ \ ..\ [5]\ Correa,\ J.\ C.\ (1995).\ A\ new\ estimator\ of\ entropy.\ Communications}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00233}00233\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ in\ Statistics-\/Theory\ and\ Methods,\ 24(10),\ 2439-\/2449.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00234}00234\ \textcolor{stringliteral}{\ \ \ \ ..\ [6]\ Noughabi,\ H.\ A.\ (2015).\ Entropy\ Estimation\ Using\ Numerical\ Methods.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00235}00235\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Annals\ of\ Data\ Science,\ 2(2),\ 231-\/241.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00236}00236\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ https://link.springer.com/article/10.1007/s40745-\/015-\/0045-\/9}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00237}00237\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00238}00238\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00239}00239\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00240}00240\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ numpy\ as\ np}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00241}00241\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ scipy.stats\ import\ differential\_entropy,\ norm}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00242}00242\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00243}00243\ \textcolor{stringliteral}{\ \ \ \ Entropy\ of\ a\ standard\ normal\ distribution:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00244}00244\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00245}00245\ \textcolor{stringliteral}{\ \ \ \ >>>\ rng\ =\ np.random.default\_rng()}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00246}00246\ \textcolor{stringliteral}{\ \ \ \ >>>\ values\ =\ rng.standard\_normal(100)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00247}00247\ \textcolor{stringliteral}{\ \ \ \ >>>\ differential\_entropy(values)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00248}00248\ \textcolor{stringliteral}{\ \ \ \ 1.3407817436640392}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00249}00249\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00250}00250\ \textcolor{stringliteral}{\ \ \ \ Compare\ with\ the\ true\ entropy:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00251}00251\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00252}00252\ \textcolor{stringliteral}{\ \ \ \ >>>\ float(norm.entropy())}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00253}00253\ \textcolor{stringliteral}{\ \ \ \ 1.4189385332046727}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00254}00254\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00255}00255\ \textcolor{stringliteral}{\ \ \ \ For\ several\ sample\ sizes\ between\ 5\ and\ 1000,\ compare\ the\ accuracy\ of}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00256}00256\ \textcolor{stringliteral}{\ \ \ \ the\ \`{}\`{}'vasicek'\`{}\`{},\ \`{}\`{}'van\ es'\`{}\`{},\ and\ \`{}\`{}'ebrahimi'\`{}\`{}\ methods.\ Specifically,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00257}00257\ \textcolor{stringliteral}{\ \ \ \ compare\ the\ root\ mean\ squared\ error\ (over\ 1000\ trials)\ between\ the\ estimate}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00258}00258\ \textcolor{stringliteral}{\ \ \ \ and\ the\ true\ differential\ entropy\ of\ the\ distribution.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00259}00259\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00260}00260\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ scipy\ import\ stats}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00261}00261\ \textcolor{stringliteral}{\ \ \ \ >>>\ import\ matplotlib.pyplot\ as\ plt}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00262}00262\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00263}00263\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00264}00264\ \textcolor{stringliteral}{\ \ \ \ >>>\ def\ rmse(res,\ expected):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00265}00265\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ '''Root\ mean\ squared\ error'''}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00266}00266\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ return\ np.sqrt(np.mean((res\ -\/\ expected)**2))}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00267}00267\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00268}00268\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00269}00269\ \textcolor{stringliteral}{\ \ \ \ >>>\ a,\ b\ =\ np.log10(5),\ np.log10(1000)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00270}00270\ \textcolor{stringliteral}{\ \ \ \ >>>\ ns\ =\ np.round(np.logspace(a,\ b,\ 10)).astype(int)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00271}00271\ \textcolor{stringliteral}{\ \ \ \ >>>\ reps\ =\ 1000\ \ \#\ number\ of\ repetitions\ for\ each\ sample\ size}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00272}00272\ \textcolor{stringliteral}{\ \ \ \ >>>\ expected\ =\ stats.expon.entropy()}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00273}00273\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00274}00274\ \textcolor{stringliteral}{\ \ \ \ >>>\ method\_errors\ =\ \{'vasicek':\ [],\ 'van\ es':\ [],\ 'ebrahimi':\ []\}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00275}00275\ \textcolor{stringliteral}{\ \ \ \ >>>\ for\ method\ in\ method\_errors:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00276}00276\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ for\ n\ in\ ns:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00277}00277\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ rvs\ =\ stats.expon.rvs(size=(reps,\ n),\ random\_state=rng)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00278}00278\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ res\ =\ stats.differential\_entropy(rvs,\ method=method,\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00279}00279\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ error\ =\ rmse(res,\ expected)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00280}00280\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ \ \ \ method\_errors[method].append(error)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00281}00281\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00282}00282\ \textcolor{stringliteral}{\ \ \ \ >>>\ for\ method,\ errors\ in\ method\_errors.items():}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00283}00283\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ plt.loglog(ns,\ errors,\ label=method)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00284}00284\ \textcolor{stringliteral}{\ \ \ \ >>>}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00285}00285\ \textcolor{stringliteral}{\ \ \ \ >>>\ plt.legend()}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00286}00286\ \textcolor{stringliteral}{\ \ \ \ >>>\ plt.xlabel('sample\ size')}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00287}00287\ \textcolor{stringliteral}{\ \ \ \ >>>\ plt.ylabel('RMSE\ (1000\ trials)')}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00288}00288\ \textcolor{stringliteral}{\ \ \ \ >>>\ plt.title('Entropy\ Estimator\ Error\ (Exponential\ Distribution)')}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00289}00289\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00290}00290\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00291}00291\ \textcolor{stringliteral}{\ \ \ \ values\ =\ np.asarray(values)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00292}00292\ \textcolor{stringliteral}{\ \ \ \ values\ =\ np.moveaxis(values,\ axis,\ -\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00293}00293\ \textcolor{stringliteral}{\ \ \ \ n\ =\ values.shape[-\/1]\ \ \#\ number\ of\ observations}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00294}00294\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00295}00295\ \textcolor{stringliteral}{\ \ \ \ if\ window\_length\ is\ None:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00296}00296\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ window\_length\ =\ math.floor(math.sqrt(n)\ +\ 0.5)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00297}00297\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00298}00298\ \textcolor{stringliteral}{\ \ \ \ if\ not\ 2\ <=\ 2\ *\ window\_length\ <\ n:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00299}00299\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raise\ ValueError(}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00300}00300\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ f"{}Window\ length\ (\{window\_length\})\ must\ be\ positive\ and\ less\ "{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00301}00301\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ f"{}than\ half\ the\ sample\ size\ (\{n\})."{},}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00302}00302\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ )}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00303}00303\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00304}00304\ \textcolor{stringliteral}{\ \ \ \ if\ base\ is\ not\ None\ and\ base\ <=\ 0:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00305}00305\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raise\ ValueError("{}\`{}base\`{}\ must\ be\ a\ positive\ number\ or\ \`{}None\`{}."{})}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00306}00306\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00307}00307\ \textcolor{stringliteral}{\ \ \ \ sorted\_data\ =\ np.sort(values,\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00308}00308\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00309}00309\ \textcolor{stringliteral}{\ \ \ \ methods\ =\ \{"{}vasicek"{}:\ \_vasicek\_entropy,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00310}00310\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "{}van\ es"{}:\ \_van\_es\_entropy,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00311}00311\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "{}correa"{}:\ \_correa\_entropy,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00312}00312\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "{}ebrahimi"{}:\ \_ebrahimi\_entropy,}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00313}00313\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "{}auto"{}:\ \_vasicek\_entropy\}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00314}00314\ \textcolor{stringliteral}{\ \ \ \ method\ =\ method.lower()}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00315}00315\ \textcolor{stringliteral}{\ \ \ \ if\ method\ not\ in\ methods:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00316}00316\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ message\ =\ f"{}\`{}method\`{}\ must\ be\ one\ of\ \{set(methods)\}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00317}00317\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raise\ ValueError(message)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00318}00318\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00319}00319\ \textcolor{stringliteral}{\ \ \ \ if\ method\ ==\ "{}auto"{}:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00320}00320\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ n\ <=\ 10:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00321}00321\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ method\ =\ 'van\ es'}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00322}00322\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ elif\ n\ <=\ 1000:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00323}00323\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ method\ =\ 'ebrahimi'}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00324}00324\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ else:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00325}00325\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ method\ =\ 'vasicek'}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00326}00326\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00327}00327\ \textcolor{stringliteral}{\ \ \ \ res\ =\ methods[method](sorted\_data,\ window\_length)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00328}00328\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00329}00329\ \textcolor{stringliteral}{\ \ \ \ if\ base\ is\ not\ None:}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00330}00330\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ res\ /=\ np.log(base)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00331}00331\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00332}00332\ \textcolor{stringliteral}{\ \ \ \ return\ res}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00333}00333\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00334}00334\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00335}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a8da8253d95c4150183c20f4d9f9d95ea}{00335}}\ \textcolor{stringliteral}{def\ \_pad\_along\_last\_axis(X,\ m):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00336}00336\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}Pad\ the\ data\ for\ computing\ the\ rolling\ window\ difference."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00337}00337\ \textcolor{stringliteral}{\ \ \ \ \#\ scales\ a\ \ bit\ better\ than\ method\ in\ \_vasicek\_like\_entropy}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00338}00338\ \textcolor{stringliteral}{\ \ \ \ shape\ =\ np.array(X.shape)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00339}00339\ \textcolor{stringliteral}{\ \ \ \ shape[-\/1]\ =\ m}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00340}00340\ \textcolor{stringliteral}{\ \ \ \ Xl\ =\ np.broadcast\_to(X[...,\ [0]],\ shape)\ \ \#\ [0]\ vs\ 0\ to\ maintain\ shape}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00341}00341\ \textcolor{stringliteral}{\ \ \ \ Xr\ =\ np.broadcast\_to(X[...,\ [-\/1]],\ shape)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00342}00342\ \textcolor{stringliteral}{\ \ \ \ return\ np.concatenate((Xl,\ X,\ Xr),\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00343}00343\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00344}00344\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00345}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a2a01b018776843abfb9261305cd30c68}{00345}}\ \textcolor{stringliteral}{def\ \_vasicek\_entropy(X,\ m):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00346}00346\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}Compute\ the\ Vasicek\ estimator\ as\ described\ in\ [6]\ Eq.\ 1.3."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00347}00347\ \textcolor{stringliteral}{\ \ \ \ n\ =\ X.shape[-\/1]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00348}00348\ \textcolor{stringliteral}{\ \ \ \ X\ =\ \_pad\_along\_last\_axis(X,\ m)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00349}00349\ \textcolor{stringliteral}{\ \ \ \ differences\ =\ X[...,\ 2\ *\ m:]\ -\/\ X[...,\ :\ -\/2\ *\ m:]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00350}00350\ \textcolor{stringliteral}{\ \ \ \ logs\ =\ np.log(n/(2*m)\ *\ differences)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00351}00351\ \textcolor{stringliteral}{\ \ \ \ return\ np.mean(logs,\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00352}00352\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00353}00353\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00354}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ad777cd154d4b45a745d1ccd812dd3af2}{00354}}\ \textcolor{stringliteral}{def\ \_van\_es\_entropy(X,\ m):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00355}00355\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}Compute\ the\ van\ Es\ estimator\ as\ described\ in\ [6]."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00356}00356\ \textcolor{stringliteral}{\ \ \ \ \#\ No\ equation\ number,\ but\ referred\ to\ as\ HVE\_mn.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00357}00357\ \textcolor{stringliteral}{\ \ \ \ \#\ Typo:\ there\ should\ be\ a\ log\ within\ the\ summation.}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00358}00358\ \textcolor{stringliteral}{\ \ \ \ n\ =\ X.shape[-\/1]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00359}00359\ \textcolor{stringliteral}{\ \ \ \ difference\ =\ X[...,\ m:]\ -\/\ X[...,\ :-\/m]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00360}00360\ \textcolor{stringliteral}{\ \ \ \ term1\ =\ 1/(n-\/m)\ *\ np.sum(np.log((n+1)/m\ *\ difference),\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00361}00361\ \textcolor{stringliteral}{\ \ \ \ k\ =\ np.arange(m,\ n+1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00362}00362\ \textcolor{stringliteral}{\ \ \ \ return\ term1\ +\ np.sum(1/k)\ +\ np.log(m)\ -\/\ np.log(n+1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00363}00363\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00364}00364\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00365}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ac9b0d2377dc302465e2da02678b76846}{00365}}\ \textcolor{stringliteral}{def\ \_ebrahimi\_entropy(X,\ m):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00366}00366\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}Compute\ the\ Ebrahimi\ estimator\ as\ described\ in\ [6]."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00367}00367\ \textcolor{stringliteral}{\ \ \ \ \#\ No\ equation\ number,\ but\ referred\ to\ as\ HE\_mn}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00368}00368\ \textcolor{stringliteral}{\ \ \ \ n\ =\ X.shape[-\/1]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00369}00369\ \textcolor{stringliteral}{\ \ \ \ X\ =\ \_pad\_along\_last\_axis(X,\ m)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00370}00370\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00371}00371\ \textcolor{stringliteral}{\ \ \ \ differences\ =\ X[...,\ 2\ *\ m:]\ -\/\ X[...,\ :\ -\/2\ *\ m:]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00372}00372\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00373}00373\ \textcolor{stringliteral}{\ \ \ \ i\ =\ np.arange(1,\ n+1).astype(float)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00374}00374\ \textcolor{stringliteral}{\ \ \ \ ci\ =\ np.ones\_like(i)*2}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00375}00375\ \textcolor{stringliteral}{\ \ \ \ ci[i\ <=\ m]\ =\ 1\ +\ (i[i\ <=\ m]\ -\/\ 1)/m}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00376}00376\ \textcolor{stringliteral}{\ \ \ \ ci[i\ >=\ n\ -\/\ m\ +\ 1]\ =\ 1\ +\ (n\ -\/\ i[i\ >=\ n-\/m+1])/m}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00377}00377\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00378}00378\ \textcolor{stringliteral}{\ \ \ \ logs\ =\ np.log(n\ *\ differences\ /\ (ci\ *\ m))}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00379}00379\ \textcolor{stringliteral}{\ \ \ \ return\ np.mean(logs,\ axis=-\/1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00380}00380\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00381}00381\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00382}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ae2d17de2a6387d3fa0b41294c50f91be}{00382}}\ \textcolor{stringliteral}{def\ \_correa\_entropy(X,\ m):}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00383}00383\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}Compute\ the\ Correa\ estimator\ as\ described\ in\ [6]."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00384}00384\ \textcolor{stringliteral}{\ \ \ \ \#\ No\ equation\ number,\ but\ referred\ to\ as\ HC\_mn}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00385}00385\ \textcolor{stringliteral}{\ \ \ \ n\ =\ X.shape[-\/1]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00386}00386\ \textcolor{stringliteral}{\ \ \ \ X\ =\ \_pad\_along\_last\_axis(X,\ m)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00387}00387\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00388}00388\ \textcolor{stringliteral}{\ \ \ \ i\ =\ np.arange(1,\ n+1)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00389}00389\ \textcolor{stringliteral}{\ \ \ \ dj\ =\ np.arange(-\/m,\ m+1)[:,\ None]}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00390}00390\ \textcolor{stringliteral}{\ \ \ \ j\ =\ i\ +\ dj}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00391}00391\ \textcolor{stringliteral}{\ \ \ \ j0\ =\ j\ +\ m\ -\/\ 1\ \ \#\ 0-\/indexed\ version\ of\ j}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00392}00392\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00393}00393\ \textcolor{stringliteral}{\ \ \ \ Xibar\ =\ np.mean(X[...,\ j0],\ axis=-\/2,\ keepdims=True)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00394}00394\ \textcolor{stringliteral}{\ \ \ \ difference\ =\ X[...,\ j0]\ -\/\ Xibar}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00395}00395\ \textcolor{stringliteral}{\ \ \ \ num\ =\ np.sum(difference*dj,\ axis=-\/2)\ \ \#\ dj\ is\ d-\/i}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00396}00396\ \textcolor{stringliteral}{\ \ \ \ den\ =\ n*np.sum(difference**2,\ axis=-\/2)}}
\DoxyCodeLine{\Hypertarget{__entropy_8py_source_l00397}00397\ \textcolor{stringliteral}{\ \ \ \ return\ -\/np.mean(np.log(num/den),\ axis=-\/1)}}

\end{DoxyCode}
