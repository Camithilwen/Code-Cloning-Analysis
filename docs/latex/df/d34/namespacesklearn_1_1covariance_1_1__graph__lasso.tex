\doxysection{sklearn.\+covariance.\+\_\+graph\+\_\+lasso Namespace Reference}
\hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso}{}\label{namespacesklearn_1_1covariance_1_1__graph__lasso}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1covariance_1_1__graph__lasso_1_1BaseGraphicalLasso}{Base\+Graphical\+Lasso}}
\item 
class \mbox{\hyperlink{classsklearn_1_1covariance_1_1__graph__lasso_1_1GraphicalLasso}{Graphical\+Lasso}}
\item 
class \mbox{\hyperlink{classsklearn_1_1covariance_1_1__graph__lasso_1_1GraphicalLassoCV}{Graphical\+Lasso\+CV}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_a479effa92e34e45bc6fc9557f9fdf210}{\+\_\+objective}} (mle, precision\+\_\+, alpha)
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_ac4cc33c46e6b035c177fa505d0bd05a6}{\+\_\+dual\+\_\+gap}} (emp\+\_\+cov, precision\+\_\+, alpha)
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_af1ec8298412c512a529a79d53b733cc8}{\+\_\+graphical\+\_\+lasso}} (emp\+\_\+cov, alpha, \texorpdfstring{$\ast$}{*}, cov\+\_\+init=None, mode="{}cd"{}, tol=1e-\/4, enet\+\_\+tol=1e-\/4, max\+\_\+iter=100, verbose=False, eps=np.\+finfo(np.\+float64).eps)
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_adc81dbef2f3f49c902b1b137ba86266f}{alpha\+\_\+max}} (emp\+\_\+cov)
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_a0239dff9050d4c360918e1f414f45612}{graphical\+\_\+lasso}} (emp\+\_\+cov, alpha, \texorpdfstring{$\ast$}{*}, mode="{}cd"{}, tol=1e-\/4, enet\+\_\+tol=1e-\/4, max\+\_\+iter=100, verbose=False, return\+\_\+costs=False, eps=np.\+finfo(np.\+float64).eps, return\+\_\+n\+\_\+iter=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1covariance_1_1__graph__lasso_aa421fd2a1551ff01d64fc314030ce812}{graphical\+\_\+lasso\+\_\+path}} (X, alphas, cov\+\_\+init=None, X\+\_\+test=None, mode="{}cd"{}, tol=1e-\/4, enet\+\_\+tol=1e-\/4, max\+\_\+iter=100, verbose=False, eps=np.\+finfo(np.\+float64).eps)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}GraphicalLasso: sparse inverse covariance estimation with an l1-penalized
estimator.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_ac4cc33c46e6b035c177fa505d0bd05a6}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!\_dual\_gap@{\_dual\_gap}}
\index{\_dual\_gap@{\_dual\_gap}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{\_dual\_gap()}{\_dual\_gap()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_ac4cc33c46e6b035c177fa505d0bd05a6} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+\_\+dual\+\_\+gap (\begin{DoxyParamCaption}\item[{}]{emp\+\_\+cov}{, }\item[{}]{precision\+\_\+}{, }\item[{}]{alpha}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Expression of the dual gap convergence criterion

The specific definition is given in Duchi "Projected Subgradient Methods
for Learning Sparse Gaussians".
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00058}{58}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.



References \mbox{\hyperlink{__graph__lasso_8py_source_l00058}{\+\_\+dual\+\_\+gap()}}, and \mbox{\hyperlink{__graph__lasso_8py_source_l00045}{\+\_\+objective()}}.



Referenced by \mbox{\hyperlink{__graph__lasso_8py_source_l00058}{\+\_\+dual\+\_\+gap()}}.

\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_af1ec8298412c512a529a79d53b733cc8}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!\_graphical\_lasso@{\_graphical\_lasso}}
\index{\_graphical\_lasso@{\_graphical\_lasso}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{\_graphical\_lasso()}{\_graphical\_lasso()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_af1ec8298412c512a529a79d53b733cc8} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+\_\+graphical\+\_\+lasso (\begin{DoxyParamCaption}\item[{}]{emp\+\_\+cov}{, }\item[{}]{alpha}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{cov\+\_\+init}{ = {\ttfamily None}, }\item[{}]{mode}{ = {\ttfamily "{}cd"{}}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{enet\+\_\+tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{eps}{ = {\ttfamily np.finfo(np.float64).eps}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00071}{71}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.

\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_a479effa92e34e45bc6fc9557f9fdf210}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!\_objective@{\_objective}}
\index{\_objective@{\_objective}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{\_objective()}{\_objective()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_a479effa92e34e45bc6fc9557f9fdf210} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+\_\+objective (\begin{DoxyParamCaption}\item[{}]{mle}{, }\item[{}]{precision\+\_\+}{, }\item[{}]{alpha}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Evaluation of the graphical-lasso objective function

the objective function is made of a shifted scaled version of the
normalized log-likelihood (i.e. its empirical mean over the samples) and a
penalisation term to promote sparsity
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00045}{45}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.



Referenced by \mbox{\hyperlink{__graph__lasso_8py_source_l00058}{\+\_\+dual\+\_\+gap()}}.

\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_adc81dbef2f3f49c902b1b137ba86266f}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!alpha\_max@{alpha\_max}}
\index{alpha\_max@{alpha\_max}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{alpha\_max()}{alpha\_max()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_adc81dbef2f3f49c902b1b137ba86266f} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+alpha\+\_\+max (\begin{DoxyParamCaption}\item[{}]{emp\+\_\+cov}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find the maximum alpha for which there are some non-zeros off-diagonal.

Parameters
----------
emp_cov : ndarray of shape (n_features, n_features)
    The sample covariance matrix.

Notes
-----
This results from the bound for the all the Lasso that are solved
in GraphicalLasso: each time, the row of cov corresponds to Xy. As the
bound for alpha is given by `max(abs(Xy))`, the result follows.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00204}{204}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.



Referenced by \mbox{\hyperlink{__graph__lasso_8py_source_l00941}{sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+Graphical\+Lasso\+CV.\+fit()}}.

\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_a0239dff9050d4c360918e1f414f45612}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!graphical\_lasso@{graphical\_lasso}}
\index{graphical\_lasso@{graphical\_lasso}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{graphical\_lasso()}{graphical\_lasso()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_a0239dff9050d4c360918e1f414f45612} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+graphical\+\_\+lasso (\begin{DoxyParamCaption}\item[{}]{emp\+\_\+cov}{, }\item[{}]{alpha}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{mode}{ = {\ttfamily "{}cd"{}}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{enet\+\_\+tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{return\+\_\+costs}{ = {\ttfamily False}, }\item[{}]{eps}{ = {\ttfamily np.finfo(np.float64).eps}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}L1-penalized covariance estimator.

Read more in the :ref:`User Guide <sparse_inverse_covariance>`.

.. versionchanged:: v0.20
    graph_lasso has been renamed to graphical_lasso

Parameters
----------
emp_cov : array-like of shape (n_features, n_features)
    Empirical covariance from which to compute the covariance estimate.

alpha : float
    The regularization parameter: the higher alpha, the more
    regularization, the sparser the inverse covariance.
    Range is (0, inf].

mode : {'cd', 'lars'}, default='cd'
    The Lasso solver to use: coordinate descent or LARS. Use LARS for
    very sparse underlying graphs, where p > n. Elsewhere prefer cd
    which is more numerically stable.

tol : float, default=1e-4
    The tolerance to declare convergence: if the dual gap goes below
    this value, iterations are stopped. Range is (0, inf].

enet_tol : float, default=1e-4
    The tolerance for the elastic net solver used to calculate the descent
    direction. This parameter controls the accuracy of the search direction
    for a given column update, not of the overall parameter estimate. Only
    used for mode='cd'. Range is (0, inf].

max_iter : int, default=100
    The maximum number of iterations.

verbose : bool, default=False
    If verbose is True, the objective function and dual gap are
    printed at each iteration.

return_costs : bool, default=False
    If return_costs is True, the objective function and dual gap
    at each iteration are returned.

eps : float, default=eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Default is `np.finfo(np.float64).eps`.

return_n_iter : bool, default=False
    Whether or not to return the number of iterations.

Returns
-------
covariance : ndarray of shape (n_features, n_features)
    The estimated covariance matrix.

precision : ndarray of shape (n_features, n_features)
    The estimated (sparse) precision matrix.

costs : list of (objective, dual_gap) pairs
    The list of values of the objective function and the dual gap at
    each iteration. Returned only if return_costs is True.

n_iter : int
    Number of iterations. Returned only if `return_n_iter` is set to True.

See Also
--------
GraphicalLasso : Sparse inverse covariance estimation
    with an l1-penalized estimator.
GraphicalLassoCV : Sparse inverse covariance with
    cross-validated choice of the l1 penalty.

Notes
-----
The algorithm employed to solve this problem is the GLasso algorithm,
from the Friedman 2008 Biostatistics paper. It is the same algorithm
as in the R `glasso` package.

One possible difference with the `glasso` R package is that the
diagonal coefficients are not penalized.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_sparse_spd_matrix
>>> from sklearn.covariance import empirical_covariance, graphical_lasso
>>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)
>>> rng = np.random.RandomState(42)
>>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)
>>> emp_cov = empirical_covariance(X, assume_centered=True)
>>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)
>>> emp_cov
array([[ 1.687,  0.212, -0.209],
       [ 0.212,  0.221, -0.0817],
       [-0.209, -0.0817, 0.232]])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00231}{231}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.

\Hypertarget{namespacesklearn_1_1covariance_1_1__graph__lasso_aa421fd2a1551ff01d64fc314030ce812}\index{sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}!graphical\_lasso\_path@{graphical\_lasso\_path}}
\index{graphical\_lasso\_path@{graphical\_lasso\_path}!sklearn.covariance.\_graph\_lasso@{sklearn.covariance.\_graph\_lasso}}
\doxysubsubsection{\texorpdfstring{graphical\_lasso\_path()}{graphical\_lasso\_path()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1covariance_1_1__graph__lasso_aa421fd2a1551ff01d64fc314030ce812} 
sklearn.\+covariance.\+\_\+graph\+\_\+lasso.\+graphical\+\_\+lasso\+\_\+path (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{alphas}{, }\item[{}]{cov\+\_\+init}{ = {\ttfamily None}, }\item[{}]{X\+\_\+test}{ = {\ttfamily None}, }\item[{}]{mode}{ = {\ttfamily "{}cd"{}}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{enet\+\_\+tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{eps}{ = {\ttfamily np.finfo(np.float64).eps}}\end{DoxyParamCaption})}

\begin{DoxyVerb}l1-penalized covariance estimator along a path of decreasing alphas

Read more in the :ref:`User Guide <sparse_inverse_covariance>`.

Parameters
----------
X : ndarray of shape (n_samples, n_features)
    Data from which to compute the covariance estimate.

alphas : array-like of shape (n_alphas,)
    The list of regularization parameters, decreasing order.

cov_init : array of shape (n_features, n_features), default=None
    The initial guess for the covariance.

X_test : array of shape (n_test_samples, n_features), default=None
    Optional test matrix to measure generalisation error.

mode : {'cd', 'lars'}, default='cd'
    The Lasso solver to use: coordinate descent or LARS. Use LARS for
    very sparse underlying graphs, where p > n. Elsewhere prefer cd
    which is more numerically stable.

tol : float, default=1e-4
    The tolerance to declare convergence: if the dual gap goes below
    this value, iterations are stopped. The tolerance must be a positive
    number.

enet_tol : float, default=1e-4
    The tolerance for the elastic net solver used to calculate the descent
    direction. This parameter controls the accuracy of the search direction
    for a given column update, not of the overall parameter estimate. Only
    used for mode='cd'. The tolerance must be a positive number.

max_iter : int, default=100
    The maximum number of iterations. This parameter should be a strictly
    positive integer.

verbose : int or bool, default=False
    The higher the verbosity flag, the more information is printed
    during the fitting.

eps : float, default=eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Default is `np.finfo(np.float64).eps`.

    .. versionadded:: 1.3

Returns
-------
covariances_ : list of shape (n_alphas,) of ndarray of shape \
        (n_features, n_features)
    The estimated covariance matrices.

precisions_ : list of shape (n_alphas,) of ndarray of shape \
        (n_features, n_features)
    The estimated (sparse) precision matrices.

scores_ : list of shape (n_alphas,), dtype=float
    The generalisation error (log-likelihood) on the test data.
    Returned only if test data is passed.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__graph__lasso_8py_source_l00587}{587}} of file \mbox{\hyperlink{__graph__lasso_8py_source}{\+\_\+graph\+\_\+lasso.\+py}}.

