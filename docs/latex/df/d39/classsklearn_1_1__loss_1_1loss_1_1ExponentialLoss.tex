\doxysection{sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss Class Reference}
\hypertarget{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss}{}\label{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss}\index{sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}}


Inheritance diagram for sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss\+:
% FIG 0


Collaboration diagram for sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a017b14e49db8a86a50abfeb57a1c17bd}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7}{constant\+\_\+to\+\_\+optimal\+\_\+zero}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a20f1e01994f889bd868a012a180acc04}{predict\+\_\+proba}} (self, raw\+\_\+prediction)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, closs, link, n\+\_\+classes=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{in\+\_\+y\+\_\+true\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\+\_\+y\+\_\+pred\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\+\_\+gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\+\_\+hessian}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, hessian\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{fit\+\_\+intercept\+\_\+only}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\+\_\+gradient\+\_\+and\+\_\+hessian}} (self, n\+\_\+samples, dtype=np.\+float64, order="{}F"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}} = closs
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}} = link
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\+\_\+hessian}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\+\_\+hessian}} = False
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\+\_\+classes}} = n\+\_\+classes
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\+\_\+y\+\_\+true}} = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\+\_\+y\+\_\+pred}} = self.\+link.\+interval\+\_\+y\+\_\+pred
\end{DoxyCompactItemize}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}{differentiable}} = \mbox{\hyperlink{classTrue}{True}}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}{need\+\_\+update\+\_\+leaves\+\_\+values}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\+\_\+multiclass}} = False
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Exponential loss with (half) logit link, for binary classification.

This is also know as boosting loss.

Domain:
y_true in [0, 1], i.e. regression on the unit interval
y_pred in (0, 1), i.e. boundaries excluded

Link:
y_pred = expit(2 * raw_prediction)

For a given sample x_i, the exponential loss is defined as::

    loss(x_i) = y_true_i * exp(-raw_pred_i)) + (1 - y_true_i) * exp(raw_pred_i)

See:
- J. Friedman, T. Hastie, R. Tibshirani.
  "Additive logistic regression: a statistical view of boosting (With discussion
  and a rejoinder by the authors)." Ann. Statist. 28 (2) 337 - 407, April 2000.
  https://doi.org/10.1214/aos/1016218223
- A. Buja, W. Stuetzle, Y. Shen. (2005).
  "Loss Functions for Binary Class Probability Estimation and Classification:
  Structure and Applications."

Note that the formulation works for classification, y = {0, 1}, as well as
"exponential logistic" regression, y = [0, 1].
Note that this is a proper scoring rule, but without it's canonical link.

More details: Inserting the predicted probability
y_pred = expit(2 * raw_prediction) in the loss gives::

    loss(x_i) = y_true_i * sqrt((1 - y_pred_i) / y_pred_i)
        + (1 - y_true_i) * sqrt(y_pred_i / (1 - y_pred_i))
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l01097}{1097}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a017b14e49db8a86a50abfeb57a1c17bd}\index{sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a017b14e49db8a86a50abfeb57a1c17bd} 
sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{loss_8py_source_l01133}{1133}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7}\index{sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}!constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}}
\index{constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}!sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}}
\doxysubsubsection{\texorpdfstring{constant\_to\_optimal\_zero()}{constant\_to\_optimal\_zero()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7} 
sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss.\+constant\+\_\+to\+\_\+optimal\+\_\+zero (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculate term dropped in loss.

With this term added, the loss of perfect predictions is zero.
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l01141}{1141}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a20f1e01994f889bd868a012a180acc04}\index{sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}!predict\_proba@{predict\_proba}}
\index{predict\_proba@{predict\_proba}!sklearn.\_loss.loss.ExponentialLoss@{sklearn.\_loss.loss.ExponentialLoss}}
\doxysubsubsection{\texorpdfstring{predict\_proba()}{predict\_proba()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a20f1e01994f889bd868a012a180acc04} 
sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss.\+predict\+\_\+proba (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{raw\+\_\+prediction}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Predict probabilities.

Parameters
----------
raw_prediction : array of shape (n_samples,) or (n_samples, 1)
    Raw prediction values (in link space).

Returns
-------
proba : array of shape (n_samples, 2)
    Element-wise class probabilities.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l01148}{1148}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{__internal_2cache_8py_source_l00190}{pip.\+\_\+internal.\+cache.\+Cache\+Entry.\+link}}, \mbox{\hyperlink{pip_2__internal_2exceptions_8py_source_l00100}{pip.\+\_\+internal.\+exceptions.\+Diagnostic\+Pip\+Error.\+link}}, \mbox{\hyperlink{candidate_8py_source_l00015}{pip.\+\_\+internal.\+models.\+candidate.\+Installation\+Candidate.\+link}}, \mbox{\hyperlink{pip_2__internal_2req_2constructors_8py_source_l00203}{pip.\+\_\+internal.\+req.\+constructors.\+Requirement\+Parts.\+link}}, \mbox{\hyperlink{req__install_8py_source_l00113}{pip.\+\_\+internal.\+req.\+req\+\_\+install.\+Install\+Requirement.\+link}}, \mbox{\hyperlink{pip_2__vendor_2rich_2style_8py_source_l00418}{pip.\+\_\+vendor.\+rich.\+style.\+Style.\+link}}, and \mbox{\hyperlink{loss_8py_source_l00135}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+link}}.



Referenced by \mbox{\hyperlink{calibration_8py_source_l00520}{sklearn.\+calibration.\+Calibrated\+Classifier\+CV.\+predict()}}, \mbox{\hyperlink{dummy_8py_source_l00252}{sklearn.\+dummy.\+Dummy\+Classifier.\+predict()}}, \mbox{\hyperlink{__bagging_8py_source_l00937}{sklearn.\+ensemble.\+\_\+bagging.\+Bagging\+Classifier.\+predict()}}, \mbox{\hyperlink{__forest_8py_source_l00882}{sklearn.\+ensemble.\+\_\+forest.\+Forest\+Classifier.\+predict()}}, \mbox{\hyperlink{__voting_8py_source_l00407}{sklearn.\+ensemble.\+\_\+voting.\+Voting\+Classifier.\+predict()}}, \mbox{\hyperlink{neighbors_2__classification_8py_source_l00241}{sklearn.\+neighbors.\+\_\+classification.\+KNeighbors\+Classifier.\+predict()}}, \mbox{\hyperlink{neighbors_2__classification_8py_source_l00719}{sklearn.\+neighbors.\+\_\+classification.\+Radius\+Neighbors\+Classifier.\+predict()}}, \mbox{\hyperlink{__label__propagation_8py_source_l00173}{sklearn.\+semi\+\_\+supervised.\+\_\+label\+\_\+propagation.\+Base\+Label\+Propagation.\+predict()}}, \mbox{\hyperlink{discriminant__analysis_8py_source_l00784}{sklearn.\+discriminant\+\_\+analysis.\+Linear\+Discriminant\+Analysis.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{dummy_8py_source_l00402}{sklearn.\+dummy.\+Dummy\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__bagging_8py_source_l01045}{sklearn.\+ensemble.\+\_\+bagging.\+Bagging\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__forest_8py_source_l00969}{sklearn.\+ensemble.\+\_\+forest.\+Forest\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__gb_8py_source_l01685}{sklearn.\+ensemble.\+\_\+gb.\+Gradient\+Boosting\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__weight__boosting_8py_source_l00845}{sklearn.\+ensemble.\+\_\+weight\+\_\+boosting.\+Ada\+Boost\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__logistic_8py_source_l01473}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l01358}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+SGDClassifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{multioutput_8py_source_l01109}{sklearn.\+multioutput.\+Classifier\+Chain.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l01333}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+MLPClassifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00876}{sklearn.\+svm.\+\_\+base.\+Base\+SVC.\+predict\+\_\+log\+\_\+proba()}}, and \mbox{\hyperlink{tree_2__classes_8py_source_l01069}{sklearn.\+tree.\+\_\+classes.\+Decision\+Tree\+Classifier.\+predict\+\_\+log\+\_\+proba()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/\+\_\+loss/loss.\+py\end{DoxyCompactItemize}
