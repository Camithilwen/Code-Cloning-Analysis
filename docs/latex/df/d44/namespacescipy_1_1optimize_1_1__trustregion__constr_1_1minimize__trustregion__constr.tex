\doxysection{scipy.\+optimize.\+\_\+trustregion\+\_\+constr.\+minimize\+\_\+trustregion\+\_\+constr Namespace Reference}
\hypertarget{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr}{}\label{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr}\index{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_1_1HessianLinearOperator}{Hessian\+Linear\+Operator}}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_1_1LagrangianHessian}{Lagrangian\+Hessian}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_af01858cec62fc8614c2ea0b33498f1d1}{update\+\_\+state\+\_\+sqp}} (state, x, last\+\_\+iteration\+\_\+failed, objective, prepared\+\_\+constraints, start\+\_\+time, tr\+\_\+radius, constr\+\_\+penalty, cg\+\_\+info)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a6b7c8df292dcaffa9a28b6d4c3e3ddd1}{update\+\_\+state\+\_\+ip}} (state, x, last\+\_\+iteration\+\_\+failed, objective, prepared\+\_\+constraints, start\+\_\+time, tr\+\_\+radius, constr\+\_\+penalty, cg\+\_\+info, barrier\+\_\+parameter, barrier\+\_\+tolerance)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a82cc749eafe159a942bb11eeee7962b2}{\+\_\+minimize\+\_\+trustregion\+\_\+constr}} (fun, x0, args, grad, hess, hessp, bounds, constraints, xtol=1e-\/8, gtol=1e-\/8, barrier\+\_\+tol=1e-\/8, sparse\+\_\+jacobian=None, callback=None, maxiter=1000, verbose=0, finite\+\_\+diff\+\_\+rel\+\_\+step=None, initial\+\_\+constr\+\_\+penalty=1.\+0, initial\+\_\+tr\+\_\+radius=1.\+0, initial\+\_\+barrier\+\_\+parameter=0.\+1, initial\+\_\+barrier\+\_\+tolerance=0.\+1, factorization\+\_\+method=None, disp=False)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
dict \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a4ed886d824357dd1b7a5f6e84a0e6a9b}{TERMINATION\+\_\+\+MESSAGES}}
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a82cc749eafe159a942bb11eeee7962b2}\index{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}!\_minimize\_trustregion\_constr@{\_minimize\_trustregion\_constr}}
\index{\_minimize\_trustregion\_constr@{\_minimize\_trustregion\_constr}!scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}}
\doxysubsubsection{\texorpdfstring{\_minimize\_trustregion\_constr()}{\_minimize\_trustregion\_constr()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a82cc749eafe159a942bb11eeee7962b2} 
scipy.\+optimize.\+\_\+trustregion\+\_\+constr.\+minimize\+\_\+trustregion\+\_\+constr.\+\_\+minimize\+\_\+trustregion\+\_\+constr (\begin{DoxyParamCaption}\item[{}]{fun}{, }\item[{}]{x0}{, }\item[{}]{args}{, }\item[{}]{grad}{, }\item[{}]{hess}{, }\item[{}]{hessp}{, }\item[{}]{bounds}{, }\item[{}]{constraints}{, }\item[{}]{xtol}{ = {\ttfamily 1e-\/8}, }\item[{}]{gtol}{ = {\ttfamily 1e-\/8}, }\item[{}]{barrier\+\_\+tol}{ = {\ttfamily 1e-\/8}, }\item[{}]{sparse\+\_\+jacobian}{ = {\ttfamily None}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{maxiter}{ = {\ttfamily 1000}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{finite\+\_\+diff\+\_\+rel\+\_\+step}{ = {\ttfamily None}, }\item[{}]{initial\+\_\+constr\+\_\+penalty}{ = {\ttfamily 1.0}, }\item[{}]{initial\+\_\+tr\+\_\+radius}{ = {\ttfamily 1.0}, }\item[{}]{initial\+\_\+barrier\+\_\+parameter}{ = {\ttfamily 0.1}, }\item[{}]{initial\+\_\+barrier\+\_\+tolerance}{ = {\ttfamily 0.1}, }\item[{}]{factorization\+\_\+method}{ = {\ttfamily None}, }\item[{}]{disp}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Minimize a scalar function subject to constraints.

Parameters
----------
gtol : float, optional
    Tolerance for termination by the norm of the Lagrangian gradient.
    The algorithm will terminate when both the infinity norm (i.e., max
    abs value) of the Lagrangian gradient and the constraint violation
    are smaller than ``gtol``. Default is 1e-8.
xtol : float, optional
    Tolerance for termination by the change of the independent variable.
    The algorithm will terminate when ``tr_radius < xtol``, where
    ``tr_radius`` is the radius of the trust region used in the algorithm.
    Default is 1e-8.
barrier_tol : float, optional
    Threshold on the barrier parameter for the algorithm termination.
    When inequality constraints are present, the algorithm will terminate
    only when the barrier parameter is less than `barrier_tol`.
    Default is 1e-8.
sparse_jacobian : {bool, None}, optional
    Determines how to represent Jacobians of the constraints. If bool,
    then Jacobians of all the constraints will be converted to the
    corresponding format. If None (default), then Jacobians won't be
    converted, but the algorithm can proceed only if they all have the
    same format.
initial_tr_radius: float, optional
    Initial trust radius. The trust radius gives the maximum distance
    between solution points in consecutive iterations. It reflects the
    trust the algorithm puts in the local approximation of the optimization
    problem. For an accurate local approximation the trust-region should be
    large and for an  approximation valid only close to the current point it
    should be a small one. The trust radius is automatically updated throughout
    the optimization process, with ``initial_tr_radius`` being its initial value.
    Default is 1 (recommended in [1]_, p. 19).
initial_constr_penalty : float, optional
    Initial constraints penalty parameter. The penalty parameter is used for
    balancing the requirements of decreasing the objective function
    and satisfying the constraints. It is used for defining the merit function:
    ``merit_function(x) = fun(x) + constr_penalty * constr_norm_l2(x)``,
    where ``constr_norm_l2(x)`` is the l2 norm of a vector containing all
    the constraints. The merit function is used for accepting or rejecting
    trial points and ``constr_penalty`` weights the two conflicting goals
    of reducing objective function and constraints. The penalty is automatically
    updated throughout the optimization  process, with
    ``initial_constr_penalty`` being its  initial value. Default is 1
    (recommended in [1]_, p 19).
initial_barrier_parameter, initial_barrier_tolerance: float, optional
    Initial barrier parameter and initial tolerance for the barrier subproblem.
    Both are used only when inequality constraints are present. For dealing with
    optimization problems ``min_x f(x)`` subject to inequality constraints
    ``c(x) <= 0`` the algorithm introduces slack variables, solving the problem
    ``min_(x,s) f(x) + barrier_parameter*sum(ln(s))`` subject to the equality
    constraints  ``c(x) + s = 0`` instead of the original problem. This subproblem
    is solved for decreasing values of ``barrier_parameter`` and with decreasing
    tolerances for the termination, starting with ``initial_barrier_parameter``
    for the barrier parameter and ``initial_barrier_tolerance`` for the
    barrier tolerance. Default is 0.1 for both values (recommended in [1]_ p. 19).
    Also note that ``barrier_parameter`` and ``barrier_tolerance`` are updated
    with the same prefactor.
factorization_method : string or None, optional
    Method to factorize the Jacobian of the constraints. Use None (default)
    for the auto selection or one of:

        - 'NormalEquation' (requires scikit-sparse)
        - 'AugmentedSystem'
        - 'QRFactorization'
        - 'SVDFactorization'

    The methods 'NormalEquation' and 'AugmentedSystem' can be used only
    with sparse constraints. The projections required by the algorithm
    will be computed using, respectively, the normal equation  and the
    augmented system approaches explained in [1]_. 'NormalEquation'
    computes the Cholesky factorization of ``A A.T`` and 'AugmentedSystem'
    performs the LU factorization of an augmented system. They usually
    provide similar results. 'AugmentedSystem' is used by default for
    sparse matrices.

    The methods 'QRFactorization' and 'SVDFactorization' can be used
    only with dense constraints. They compute the required projections
    using, respectively, QR and SVD factorizations. The 'SVDFactorization'
    method can cope with Jacobian matrices with deficient row rank and will
    be used whenever other factorization methods fail (which may imply the
    conversion of sparse matrices to a dense format when required).
    By default, 'QRFactorization' is used for dense matrices.
finite_diff_rel_step : None or array_like, optional
    Relative step size for the finite difference approximation.
maxiter : int, optional
    Maximum number of algorithm iterations. Default is 1000.
verbose : {0, 1, 2}, optional
    Level of algorithm's verbosity:

        * 0 (default) : work silently.
        * 1 : display a termination report.
        * 2 : display progress during iterations.
        * 3 : display progress during iterations (more complete report).

disp : bool, optional
    If True (default), then `verbose` will be set to 1 if it was 0.

Returns
-------
`OptimizeResult` with the fields documented below. Note the following:

    1. All values corresponding to the constraints are ordered as they
       were passed to the solver. And values corresponding to `bounds`
       constraints are put *after* other constraints.
    2. All numbers of function, Jacobian or Hessian evaluations correspond
       to numbers of actual Python function calls. It means, for example,
       that if a Jacobian is estimated by finite differences, then the
       number of Jacobian evaluations will be zero and the number of
       function evaluations will be incremented by all calls during the
       finite difference estimation.

x : ndarray, shape (n,)
    Solution found.
optimality : float
    Infinity norm of the Lagrangian gradient at the solution.
constr_violation : float
    Maximum constraint violation at the solution.
fun : float
    Objective function at the solution.
grad : ndarray, shape (n,)
    Gradient of the objective function at the solution.
lagrangian_grad : ndarray, shape (n,)
    Gradient of the Lagrangian function at the solution.
nit : int
    Total number of iterations.
nfev : integer
    Number of the objective function evaluations.
njev : integer
    Number of the objective function gradient evaluations.
nhev : integer
    Number of the objective function Hessian evaluations.
cg_niter : int
    Total number of the conjugate gradient method iterations.
method : {'equality_constrained_sqp', 'tr_interior_point'}
    Optimization method used.
constr : list of ndarray
    List of constraint values at the solution.
jac : list of {ndarray, sparse matrix}
    List of the Jacobian matrices of the constraints at the solution.
v : list of ndarray
    List of the Lagrange multipliers for the constraints at the solution.
    For an inequality constraint a positive multiplier means that the upper
    bound is active, a negative multiplier means that the lower bound is
    active and if a multiplier is zero it means the constraint is not
    active.
constr_nfev : list of int
    Number of constraint evaluations for each of the constraints.
constr_njev : list of int
    Number of Jacobian matrix evaluations for each of the constraints.
constr_nhev : list of int
    Number of Hessian evaluations for each of the constraints.
tr_radius : float
    Radius of the trust region at the last iteration.
constr_penalty : float
    Penalty parameter at the last iteration, see `initial_constr_penalty`.
barrier_tolerance : float
    Tolerance for the barrier subproblem at the last iteration.
    Only for problems with inequality constraints.
barrier_parameter : float
    Barrier parameter at the last iteration. Only for problems
    with inequality constraints.
execution_time : float
    Total execution time.
message : str
    Termination message.
status : {0, 1, 2, 3}
    Termination status:

        * 0 : The maximum number of function evaluations is exceeded.
        * 1 : `gtol` termination condition is satisfied.
        * 2 : `xtol` termination condition is satisfied.
        * 3 : `callback` function requested termination.

cg_stop_cond : int
    Reason for CG subproblem termination at the last iteration:

        * 0 : CG subproblem not evaluated.
        * 1 : Iteration limit was reached.
        * 2 : Reached the trust-region boundary.
        * 3 : Negative curvature detected.
        * 4 : Tolerance was satisfied.

References
----------
.. [1] Conn, A. R., Gould, N. I., & Toint, P. L.
       Trust region methods. 2000. Siam. pp. 19.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{minimize__trustregion__constr_8py_source_l00114}{114}} of file \mbox{\hyperlink{minimize__trustregion__constr_8py_source}{minimize\+\_\+trustregion\+\_\+constr.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a6b7c8df292dcaffa9a28b6d4c3e3ddd1}\index{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}!update\_state\_ip@{update\_state\_ip}}
\index{update\_state\_ip@{update\_state\_ip}!scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}}
\doxysubsubsection{\texorpdfstring{update\_state\_ip()}{update\_state\_ip()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a6b7c8df292dcaffa9a28b6d4c3e3ddd1} 
scipy.\+optimize.\+\_\+trustregion\+\_\+constr.\+minimize\+\_\+trustregion\+\_\+constr.\+update\+\_\+state\+\_\+ip (\begin{DoxyParamCaption}\item[{}]{state}{, }\item[{}]{x}{, }\item[{}]{last\+\_\+iteration\+\_\+failed}{, }\item[{}]{objective}{, }\item[{}]{prepared\+\_\+constraints}{, }\item[{}]{start\+\_\+time}{, }\item[{}]{tr\+\_\+radius}{, }\item[{}]{constr\+\_\+penalty}{, }\item[{}]{cg\+\_\+info}{, }\item[{}]{barrier\+\_\+parameter}{, }\item[{}]{barrier\+\_\+tolerance}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{minimize__trustregion__constr_8py_source_l00102}{102}} of file \mbox{\hyperlink{minimize__trustregion__constr_8py_source}{minimize\+\_\+trustregion\+\_\+constr.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_af01858cec62fc8614c2ea0b33498f1d1}\index{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}!update\_state\_sqp@{update\_state\_sqp}}
\index{update\_state\_sqp@{update\_state\_sqp}!scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}}
\doxysubsubsection{\texorpdfstring{update\_state\_sqp()}{update\_state\_sqp()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_af01858cec62fc8614c2ea0b33498f1d1} 
scipy.\+optimize.\+\_\+trustregion\+\_\+constr.\+minimize\+\_\+trustregion\+\_\+constr.\+update\+\_\+state\+\_\+sqp (\begin{DoxyParamCaption}\item[{}]{state}{, }\item[{}]{x}{, }\item[{}]{last\+\_\+iteration\+\_\+failed}{, }\item[{}]{objective}{, }\item[{}]{prepared\+\_\+constraints}{, }\item[{}]{start\+\_\+time}{, }\item[{}]{tr\+\_\+radius}{, }\item[{}]{constr\+\_\+penalty}{, }\item[{}]{cg\+\_\+info}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{minimize__trustregion__constr_8py_source_l00059}{59}} of file \mbox{\hyperlink{minimize__trustregion__constr_8py_source}{minimize\+\_\+trustregion\+\_\+constr.\+py}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a4ed886d824357dd1b7a5f6e84a0e6a9b}\index{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}!TERMINATION\_MESSAGES@{TERMINATION\_MESSAGES}}
\index{TERMINATION\_MESSAGES@{TERMINATION\_MESSAGES}!scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr@{scipy.optimize.\_trustregion\_constr.minimize\_trustregion\_constr}}
\doxysubsubsection{\texorpdfstring{TERMINATION\_MESSAGES}{TERMINATION\_MESSAGES}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__trustregion__constr_1_1minimize__trustregion__constr_a4ed886d824357dd1b7a5f6e84a0e6a9b} 
dict scipy.\+optimize.\+\_\+trustregion\+\_\+constr.\+minimize\+\_\+trustregion\+\_\+constr.\+TERMINATION\+\_\+\+MESSAGES}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ \{}
\DoxyCodeLine{00002\ \ \ \ \ 0:\ \textcolor{stringliteral}{"{}The\ maximum\ number\ of\ function\ evaluations\ is\ exceeded."{}},}
\DoxyCodeLine{00003\ \ \ \ \ 1:\ \textcolor{stringliteral}{"{}\`{}gtol`\ termination\ condition\ is\ satisfied."{}},}
\DoxyCodeLine{00004\ \ \ \ \ 2:\ \textcolor{stringliteral}{"{}\`{}xtol`\ termination\ condition\ is\ satisfied."{}},}
\DoxyCodeLine{00005\ \ \ \ \ 3:\ \textcolor{stringliteral}{"{}\`{}callback`\ function\ requested\ termination."{}}}
\DoxyCodeLine{00006\ \}}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{minimize__trustregion__constr_8py_source_l00017}{17}} of file \mbox{\hyperlink{minimize__trustregion__constr_8py_source}{minimize\+\_\+trustregion\+\_\+constr.\+py}}.

