\doxysection{sklearn.\+decomposition.\+\_\+dict\+\_\+learning Namespace Reference}
\hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning}{}\label{namespacesklearn_1_1decomposition_1_1__dict__learning}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__dict__learning_1_1__BaseSparseCoding}{\+\_\+\+Base\+Sparse\+Coding}}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__dict__learning_1_1DictionaryLearning}{Dictionary\+Learning}}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__dict__learning_1_1MiniBatchDictionaryLearning}{Mini\+Batch\+Dictionary\+Learning}}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__dict__learning_1_1SparseCoder}{Sparse\+Coder}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_a4fbda6e353295e4595a13e5b76dca5ad}{\+\_\+check\+\_\+positive\+\_\+coding}} (method, positive)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_a9cd542b7864df701e067c61ae9252807}{\+\_\+sparse\+\_\+encode\+\_\+precomputed}} (X, dictionary, \texorpdfstring{$\ast$}{*}, gram=None, cov=None, algorithm="{}lasso\+\_\+lars"{}, regularization=None, copy\+\_\+cov=\mbox{\hyperlink{classTrue}{True}}, init=None, max\+\_\+iter=1000, verbose=0, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_aeb542c321171b3f83924981cd27f2ddf}{sparse\+\_\+encode}} (X, dictionary, \texorpdfstring{$\ast$}{*}, gram=None, cov=None, algorithm="{}lasso\+\_\+lars"{}, n\+\_\+nonzero\+\_\+coefs=None, alpha=None, copy\+\_\+cov=\mbox{\hyperlink{classTrue}{True}}, init=None, max\+\_\+iter=1000, n\+\_\+jobs=None, check\+\_\+input=\mbox{\hyperlink{classTrue}{True}}, verbose=0, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_aaaf551408e1635bef1f1166956f89918}{\+\_\+sparse\+\_\+encode}} (X, dictionary, \texorpdfstring{$\ast$}{*}, gram=None, cov=None, algorithm="{}lasso\+\_\+lars"{}, n\+\_\+nonzero\+\_\+coefs=None, alpha=None, copy\+\_\+cov=\mbox{\hyperlink{classTrue}{True}}, init=None, max\+\_\+iter=1000, n\+\_\+jobs=None, verbose=0, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_a530997e2d539e5bc2aa7ce8e4846d1cd}{\+\_\+update\+\_\+dict}} (dictionary, Y, code, A=None, B=None, verbose=False, random\+\_\+state=None, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_ad7a27e47d2c9f68904050a1099d3a8aa}{\+\_\+dict\+\_\+learning}} (X, n\+\_\+components, \texorpdfstring{$\ast$}{*}, alpha, max\+\_\+iter, tol, method, n\+\_\+jobs, dict\+\_\+init, code\+\_\+init, callback, verbose, random\+\_\+state, return\+\_\+n\+\_\+iter, positive\+\_\+dict, positive\+\_\+code, method\+\_\+max\+\_\+iter)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_aa1e36859322462d60dc8c67120de225a}{dict\+\_\+learning\+\_\+online}} (X, n\+\_\+components=2, \texorpdfstring{$\ast$}{*}, alpha=1, max\+\_\+iter=100, return\+\_\+code=\mbox{\hyperlink{classTrue}{True}}, dict\+\_\+init=None, callback=None, batch\+\_\+size=256, verbose=False, shuffle=\mbox{\hyperlink{classTrue}{True}}, n\+\_\+jobs=None, method="{}lars"{}, random\+\_\+state=None, positive\+\_\+dict=False, positive\+\_\+code=False, method\+\_\+max\+\_\+iter=1000, tol=1e-\/3, max\+\_\+no\+\_\+improvement=10)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__dict__learning_a5de9ec44c21f1fd75610fdfd451b5329}{dict\+\_\+learning}} (X, n\+\_\+components, \texorpdfstring{$\ast$}{*}, alpha, max\+\_\+iter=100, tol=1e-\/8, method="{}lars"{}, n\+\_\+jobs=None, dict\+\_\+init=None, code\+\_\+init=None, callback=None, verbose=False, random\+\_\+state=None, return\+\_\+n\+\_\+iter=False, positive\+\_\+dict=False, positive\+\_\+code=False, method\+\_\+max\+\_\+iter=1000)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Dictionary learning.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_a4fbda6e353295e4595a13e5b76dca5ad}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!\_check\_positive\_coding@{\_check\_positive\_coding}}
\index{\_check\_positive\_coding@{\_check\_positive\_coding}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{\_check\_positive\_coding()}{\_check\_positive\_coding()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_a4fbda6e353295e4595a13e5b76dca5ad} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+check\+\_\+positive\+\_\+coding (\begin{DoxyParamCaption}\item[{}]{method}{, }\item[{}]{positive}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00029}{29}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_ad7a27e47d2c9f68904050a1099d3a8aa}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!\_dict\_learning@{\_dict\_learning}}
\index{\_dict\_learning@{\_dict\_learning}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{\_dict\_learning()}{\_dict\_learning()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_ad7a27e47d2c9f68904050a1099d3a8aa} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+dict\+\_\+learning (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+components}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{alpha}{, }\item[{}]{max\+\_\+iter}{, }\item[{}]{tol}{, }\item[{}]{method}{, }\item[{}]{n\+\_\+jobs}{, }\item[{}]{dict\+\_\+init}{, }\item[{}]{code\+\_\+init}{, }\item[{}]{callback}{, }\item[{}]{verbose}{, }\item[{}]{random\+\_\+state}{, }\item[{}]{return\+\_\+n\+\_\+iter}{, }\item[{}]{positive\+\_\+dict}{, }\item[{}]{positive\+\_\+code}{, }\item[{}]{method\+\_\+max\+\_\+iter}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Main dictionary learning algorithm\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00553}{553}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.



References \mbox{\hyperlink{__dict__learning_8py_source_l00482}{\+\_\+update\+\_\+dict()}}, and \mbox{\hyperlink{__dict__learning_8py_source_l00241}{sparse\+\_\+encode()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_aaaf551408e1635bef1f1166956f89918}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!\_sparse\_encode@{\_sparse\_encode}}
\index{\_sparse\_encode@{\_sparse\_encode}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{\_sparse\_encode()}{\_sparse\_encode()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_aaaf551408e1635bef1f1166956f89918} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+sparse\+\_\+encode (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{dictionary}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{gram}{ = {\ttfamily None}, }\item[{}]{cov}{ = {\ttfamily None}, }\item[{}]{algorithm}{ = {\ttfamily "{}lasso\+\_\+lars"{}}, }\item[{}]{n\+\_\+nonzero\+\_\+coefs}{ = {\ttfamily None}, }\item[{}]{alpha}{ = {\ttfamily None}, }\item[{}]{copy\+\_\+cov}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{init}{ = {\ttfamily None}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 1000}, }\item[{}]{n\+\_\+jobs}{ = {\ttfamily None}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Sparse coding without input/parameter validation.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00393}{393}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.



References \mbox{\hyperlink{__dict__learning_8py_source_l00049}{\+\_\+sparse\+\_\+encode\+\_\+precomputed()}}.



Referenced by \mbox{\hyperlink{__dict__learning_8py_source_l02088}{sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+Mini\+Batch\+Dictionary\+Learning.\+\_\+minibatch\+\_\+step()}}, and \mbox{\hyperlink{__dict__learning_8py_source_l00241}{sparse\+\_\+encode()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_a9cd542b7864df701e067c61ae9252807}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!\_sparse\_encode\_precomputed@{\_sparse\_encode\_precomputed}}
\index{\_sparse\_encode\_precomputed@{\_sparse\_encode\_precomputed}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{\_sparse\_encode\_precomputed()}{\_sparse\_encode\_precomputed()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_a9cd542b7864df701e067c61ae9252807} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+sparse\+\_\+encode\+\_\+precomputed (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{dictionary}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{gram}{ = {\ttfamily None}, }\item[{}]{cov}{ = {\ttfamily None}, }\item[{}]{algorithm}{ = {\ttfamily "{}lasso\+\_\+lars"{}}, }\item[{}]{regularization}{ = {\ttfamily None}, }\item[{}]{copy\+\_\+cov}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{init}{ = {\ttfamily None}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 1000}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Generic sparse coding with precomputed Gram and/or covariance matrices.

Each row of the result is the solution to a Lasso problem.

Parameters
----------
X : ndarray of shape (n_samples, n_features)
    Data matrix.

dictionary : ndarray of shape (n_components, n_features)
    The dictionary matrix against which to solve the sparse coding of
    the data. Some of the algorithms assume normalized rows.

gram : ndarray of shape (n_components, n_components), default=None
    Precomputed Gram matrix, `dictionary * dictionary'`
    gram can be `None` if method is 'threshold'.

cov : ndarray of shape (n_components, n_samples), default=None
    Precomputed covariance, `dictionary * X'`.

algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \
        default='lasso_lars'
    The algorithm used:

    * `'lars'`: uses the least angle regression method
      (`linear_model.lars_path`);
    * `'lasso_lars'`: uses Lars to compute the Lasso solution;
    * `'lasso_cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if
      the estimated components are sparse;
    * `'omp'`: uses orthogonal matching pursuit to estimate the sparse
      solution;
    * `'threshold'`: squashes to zero all coefficients less than
      regularization from the projection `dictionary * data'`.

regularization : int or float, default=None
    The regularization parameter. It corresponds to alpha when
    algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.
    Otherwise it corresponds to `n_nonzero_coefs`.

init : ndarray of shape (n_samples, n_components), default=None
    Initialization value of the sparse code. Only used if
    `algorithm='lasso_cd'`.

max_iter : int, default=1000
    Maximum number of iterations to perform if `algorithm='lasso_cd'` or
    `'lasso_lars'`.

copy_cov : bool, default=True
    Whether to copy the precomputed covariance matrix; if `False`, it may
    be overwritten.

verbose : int, default=0
    Controls the verbosity; the higher, the more messages.

positive: bool, default=False
    Whether to enforce a positivity constraint on the sparse code.

    .. versionadded:: 0.20

Returns
-------
code : ndarray of shape (n_components, n_features)
    The sparse codes.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00036}{36}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.



Referenced by \mbox{\hyperlink{__dict__learning_8py_source_l00408}{\+\_\+sparse\+\_\+encode()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_a530997e2d539e5bc2aa7ce8e4846d1cd}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!\_update\_dict@{\_update\_dict}}
\index{\_update\_dict@{\_update\_dict}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{\_update\_dict()}{\_update\_dict()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_a530997e2d539e5bc2aa7ce8e4846d1cd} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+update\+\_\+dict (\begin{DoxyParamCaption}\item[{}]{dictionary}{, }\item[{}]{Y}{, }\item[{}]{code}{, }\item[{}]{A}{ = {\ttfamily None}, }\item[{}]{B}{ = {\ttfamily None}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Update the dense dictionary factor in place.

Parameters
----------
dictionary : ndarray of shape (n_components, n_features)
    Value of the dictionary at the previous iteration.

Y : ndarray of shape (n_samples, n_features)
    Data matrix.

code : ndarray of shape (n_samples, n_components)
    Sparse coding of the data against which to optimize the dictionary.

A : ndarray of shape (n_components, n_components), default=None
    Together with `B`, sufficient stats of the online model to update the
    dictionary.

B : ndarray of shape (n_features, n_components), default=None
    Together with `A`, sufficient stats of the online model to update the
    dictionary.

verbose: bool, default=False
    Degree of output the procedure will print.

random_state : int, RandomState instance or None, default=None
    Used for randomly initializing the dictionary. Pass an int for
    reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

positive : bool, default=False
    Whether to enforce positivity when finding the dictionary.

    .. versionadded:: 0.20
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00473}{473}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.



Referenced by \mbox{\hyperlink{__dict__learning_8py_source_l00571}{\+\_\+dict\+\_\+learning()}}, and \mbox{\hyperlink{__dict__learning_8py_source_l02088}{sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+Mini\+Batch\+Dictionary\+Learning.\+\_\+minibatch\+\_\+step()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_a5de9ec44c21f1fd75610fdfd451b5329}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!dict\_learning@{dict\_learning}}
\index{dict\_learning@{dict\_learning}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{dict\_learning()}{dict\_learning()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_a5de9ec44c21f1fd75610fdfd451b5329} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+dict\+\_\+learning (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+components}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{alpha}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{tol}{ = {\ttfamily 1e-\/8}, }\item[{}]{method}{ = {\ttfamily "{}lars"{}}, }\item[{}]{n\+\_\+jobs}{ = {\ttfamily None}, }\item[{}]{dict\+\_\+init}{ = {\ttfamily None}, }\item[{}]{code\+\_\+init}{ = {\ttfamily None}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{positive\+\_\+dict}{ = {\ttfamily False}, }\item[{}]{positive\+\_\+code}{ = {\ttfamily False}, }\item[{}]{method\+\_\+max\+\_\+iter}{ = {\ttfamily 1000}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Solve a dictionary learning matrix factorization problem.

Finds the best dictionary and the corresponding sparse code for
approximating the data matrix X by solving::

    (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1
                 (U,V)
                with || V_k ||_2 = 1 for all  0 <= k < n_components

where V is the dictionary and U is the sparse code. ||.||_Fro stands for
the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm
which is the sum of the absolute values of all the entries in the matrix.

Read more in the :ref:`User Guide <DictionaryLearning>`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Data matrix.

n_components : int
    Number of dictionary atoms to extract.

alpha : int or float
    Sparsity controlling parameter.

max_iter : int, default=100
    Maximum number of iterations to perform.

tol : float, default=1e-8
    Tolerance for the stopping condition.

method : {'lars', 'cd'}, default='lars'
    The method used:

    * `'lars'`: uses the least angle regression method to solve the lasso
       problem (`linear_model.lars_path`);
    * `'cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). Lars will be faster if
      the estimated components are sparse.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

dict_init : ndarray of shape (n_components, n_features), default=None
    Initial value for the dictionary for warm restart scenarios. Only used
    if `code_init` and `dict_init` are not None.

code_init : ndarray of shape (n_samples, n_components), default=None
    Initial value for the sparse code for warm restart scenarios. Only used
    if `code_init` and `dict_init` are not None.

callback : callable, default=None
    Callable that gets invoked every five iterations.

verbose : bool, default=False
    To control the verbosity of the procedure.

random_state : int, RandomState instance or None, default=None
    Used for randomly initializing the dictionary. Pass an int for
    reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

return_n_iter : bool, default=False
    Whether or not to return the number of iterations.

positive_dict : bool, default=False
    Whether to enforce positivity when finding the dictionary.

    .. versionadded:: 0.20

positive_code : bool, default=False
    Whether to enforce positivity when finding the code.

    .. versionadded:: 0.20

method_max_iter : int, default=1000
    Maximum number of iterations to perform.

    .. versionadded:: 0.22

Returns
-------
code : ndarray of shape (n_samples, n_components)
    The sparse code factor in the matrix factorization.

dictionary : ndarray of shape (n_components, n_features),
    The dictionary factor in the matrix factorization.

errors : array
    Vector of errors at each iteration.

n_iter : int
    Number of iterations run. Returned only if `return_n_iter` is
    set to True.

See Also
--------
dict_learning_online : Solve a dictionary learning matrix factorization
    problem online.
DictionaryLearning : Find a dictionary that sparsely encodes data.
MiniBatchDictionaryLearning : A faster, less accurate version
    of the dictionary learning algorithm.
SparsePCA : Sparse Principal Components Analysis.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_sparse_coded_signal
>>> from sklearn.decomposition import dict_learning
>>> X, _, _ = make_sparse_coded_signal(
...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,
...     random_state=42,
... )
>>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)

We can check the level of sparsity of `U`:

>>> np.mean(U == 0)
np.float64(0.62)

We can compare the average squared euclidean norm of the reconstruction
error of the sparse coded signal relative to the squared euclidean norm of
the original signal:

>>> X_hat = U @ V
>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))
np.float64(0.0192)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00894}{894}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_aa1e36859322462d60dc8c67120de225a}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!dict\_learning\_online@{dict\_learning\_online}}
\index{dict\_learning\_online@{dict\_learning\_online}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{dict\_learning\_online()}{dict\_learning\_online()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_aa1e36859322462d60dc8c67120de225a} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+dict\+\_\+learning\+\_\+online (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+components}{ = {\ttfamily 2}, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{alpha}{ = {\ttfamily 1}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 100}, }\item[{}]{return\+\_\+code}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{dict\+\_\+init}{ = {\ttfamily None}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{batch\+\_\+size}{ = {\ttfamily 256}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{shuffle}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{n\+\_\+jobs}{ = {\ttfamily None}, }\item[{}]{method}{ = {\ttfamily "{}lars"{}}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{positive\+\_\+dict}{ = {\ttfamily False}, }\item[{}]{positive\+\_\+code}{ = {\ttfamily False}, }\item[{}]{method\+\_\+max\+\_\+iter}{ = {\ttfamily 1000}, }\item[{}]{tol}{ = {\ttfamily 1e-\/3}, }\item[{}]{max\+\_\+no\+\_\+improvement}{ = {\ttfamily 10}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Solve a dictionary learning matrix factorization problem online.

Finds the best dictionary and the corresponding sparse code for
approximating the data matrix X by solving::

    (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1
                 (U,V)
                 with || V_k ||_2 = 1 for all  0 <= k < n_components

where V is the dictionary and U is the sparse code. ||.||_Fro stands for
the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm
which is the sum of the absolute values of all the entries in the matrix.
This is accomplished by repeatedly iterating over mini-batches by slicing
the input data.

Read more in the :ref:`User Guide <DictionaryLearning>`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Data matrix.

n_components : int or None, default=2
    Number of dictionary atoms to extract. If None, then ``n_components``
    is set to ``n_features``.

alpha : float, default=1
    Sparsity controlling parameter.

max_iter : int, default=100
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion heuristics.

    .. versionadded:: 1.1

return_code : bool, default=True
    Whether to also return the code U or just the dictionary `V`.

dict_init : ndarray of shape (n_components, n_features), default=None
    Initial values for the dictionary for warm restart scenarios.
    If `None`, the initial values for the dictionary are created
    with an SVD decomposition of the data via
    :func:`~sklearn.utils.extmath.randomized_svd`.

callback : callable, default=None
    A callable that gets invoked at the end of each iteration.

batch_size : int, default=256
    The number of samples to take in each batch.

    .. versionchanged:: 1.3
       The default value of `batch_size` changed from 3 to 256 in version 1.3.

verbose : bool, default=False
    To control the verbosity of the procedure.

shuffle : bool, default=True
    Whether to shuffle the data before splitting it in batches.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

method : {'lars', 'cd'}, default='lars'
    * `'lars'`: uses the least angle regression method to solve the lasso
      problem (`linear_model.lars_path`);
    * `'cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). Lars will be faster if
      the estimated components are sparse.

random_state : int, RandomState instance or None, default=None
    Used for initializing the dictionary when ``dict_init`` is not
    specified, randomly shuffling the data when ``shuffle`` is set to
    ``True``, and updating the dictionary. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

positive_dict : bool, default=False
    Whether to enforce positivity when finding the dictionary.

    .. versionadded:: 0.20

positive_code : bool, default=False
    Whether to enforce positivity when finding the code.

    .. versionadded:: 0.20

method_max_iter : int, default=1000
    Maximum number of iterations to perform when solving the lasso problem.

    .. versionadded:: 0.22

tol : float, default=1e-3
    Control early stopping based on the norm of the differences in the
    dictionary between 2 steps.

    To disable early stopping based on changes in the dictionary, set
    `tol` to 0.0.

    .. versionadded:: 1.1

max_no_improvement : int, default=10
    Control early stopping based on the consecutive number of mini batches
    that does not yield an improvement on the smoothed cost function.

    To disable convergence detection based on cost function, set
    `max_no_improvement` to None.

    .. versionadded:: 1.1

Returns
-------
code : ndarray of shape (n_samples, n_components),
    The sparse code (only returned if `return_code=True`).

dictionary : ndarray of shape (n_components, n_features),
    The solutions to the dictionary learning problem.

n_iter : int
    Number of iterations run. Returned only if `return_n_iter` is
    set to `True`.

See Also
--------
dict_learning : Solve a dictionary learning matrix factorization problem.
DictionaryLearning : Find a dictionary that sparsely encodes data.
MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary
    learning algorithm.
SparsePCA : Sparse Principal Components Analysis.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_sparse_coded_signal
>>> from sklearn.decomposition import dict_learning_online
>>> X, _, _ = make_sparse_coded_signal(
...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,
...     random_state=42,
... )
>>> U, V = dict_learning_online(
...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42
... )

We can check the level of sparsity of `U`:

>>> np.mean(U == 0)
np.float64(0.53)

We can compare the average squared euclidean norm of the reconstruction
error of the sparse coded signal relative to the squared euclidean norm of
the original signal:

>>> X_hat = U @ V
>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))
np.float64(0.053)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00675}{675}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__dict__learning_aeb542c321171b3f83924981cd27f2ddf}\index{sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}!sparse\_encode@{sparse\_encode}}
\index{sparse\_encode@{sparse\_encode}!sklearn.decomposition.\_dict\_learning@{sklearn.decomposition.\_dict\_learning}}
\doxysubsubsection{\texorpdfstring{sparse\_encode()}{sparse\_encode()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__dict__learning_aeb542c321171b3f83924981cd27f2ddf} 
sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+sparse\+\_\+encode (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{dictionary}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{gram}{ = {\ttfamily None}, }\item[{}]{cov}{ = {\ttfamily None}, }\item[{}]{algorithm}{ = {\ttfamily "{}lasso\+\_\+lars"{}}, }\item[{}]{n\+\_\+nonzero\+\_\+coefs}{ = {\ttfamily None}, }\item[{}]{alpha}{ = {\ttfamily None}, }\item[{}]{copy\+\_\+cov}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{init}{ = {\ttfamily None}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 1000}, }\item[{}]{n\+\_\+jobs}{ = {\ttfamily None}, }\item[{}]{check\+\_\+input}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Sparse coding.

Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array `code` such that::

    X ~= code * dictionary

Read more in the :ref:`User Guide <SparseCoder>`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Data matrix.

dictionary : array-like of shape (n_components, n_features)
    The dictionary matrix against which to solve the sparse coding of
    the data. Some of the algorithms assume normalized rows for meaningful
    output.

gram : array-like of shape (n_components, n_components), default=None
    Precomputed Gram matrix, `dictionary * dictionary'`.

cov : array-like of shape (n_components, n_samples), default=None
    Precomputed covariance, `dictionary' * X`.

algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \
        default='lasso_lars'
    The algorithm used:

    * `'lars'`: uses the least angle regression method
      (`linear_model.lars_path`);
    * `'lasso_lars'`: uses Lars to compute the Lasso solution;
    * `'lasso_cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if
      the estimated components are sparse;
    * `'omp'`: uses orthogonal matching pursuit to estimate the sparse
      solution;
    * `'threshold'`: squashes to zero all coefficients less than
      regularization from the projection `dictionary * data'`.

n_nonzero_coefs : int, default=None
    Number of nonzero coefficients to target in each column of the
    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`
    and is overridden by `alpha` in the `omp` case. If `None`, then
    `n_nonzero_coefs=int(n_features / 10)`.

alpha : float, default=None
    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the
    penalty applied to the L1 norm.
    If `algorithm='threshold'`, `alpha` is the absolute value of the
    threshold below which coefficients will be squashed to zero.
    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of
    the reconstruction error targeted. In this case, it overrides
    `n_nonzero_coefs`.
    If `None`, default to 1.

copy_cov : bool, default=True
    Whether to copy the precomputed covariance matrix; if `False`, it may
    be overwritten.

init : ndarray of shape (n_samples, n_components), default=None
    Initialization value of the sparse codes. Only used if
    `algorithm='lasso_cd'`.

max_iter : int, default=1000
    Maximum number of iterations to perform if `algorithm='lasso_cd'` or
    `'lasso_lars'`.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

check_input : bool, default=True
    If `False`, the input arrays X and dictionary will not be checked.

verbose : int, default=0
    Controls the verbosity; the higher, the more messages.

positive : bool, default=False
    Whether to enforce positivity when finding the encoding.

    .. versionadded:: 0.20

Returns
-------
code : ndarray of shape (n_samples, n_components)
    The sparse codes.

See Also
--------
sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.
sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.
SparseCoder : Find a sparse representation of data from a fixed precomputed
    dictionary.

Examples
--------
>>> import numpy as np
>>> from sklearn.decomposition import sparse_encode
>>> X = np.array([[-1, -1, -1], [0, 0, 3]])
>>> dictionary = np.array(
...     [[0, 1, 0],
...      [-1, -1, 2],
...      [1, 1, 1],
...      [0, 1, 1],
...      [0, 2, 1]],
...    dtype=np.float64
... )
>>> sparse_encode(X, dictionary, alpha=1e-10)
array([[ 0.,  0., -1.,  0.,  0.],
       [ 0.,  1.,  1.,  0.,  0.]])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dict__learning_8py_source_l00225}{225}} of file \mbox{\hyperlink{__dict__learning_8py_source}{\+\_\+dict\+\_\+learning.\+py}}.



References \mbox{\hyperlink{__dict__learning_8py_source_l00408}{\+\_\+sparse\+\_\+encode()}}.



Referenced by \mbox{\hyperlink{__dict__learning_8py_source_l00571}{\+\_\+dict\+\_\+learning()}}, and \mbox{\hyperlink{__dict__learning_8py_source_l01094}{sklearn.\+decomposition.\+\_\+dict\+\_\+learning.\+\_\+\+Base\+Sparse\+Coding.\+\_\+transform()}}.

