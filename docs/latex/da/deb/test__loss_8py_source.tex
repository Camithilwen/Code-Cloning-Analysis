\doxysection{test\+\_\+loss.\+py}
\hypertarget{test__loss_8py_source}{}\label{test__loss_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/\_loss/tests/test\_loss.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/\_loss/tests/test\_loss.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss}{00001}}\ \textcolor{keyword}{import}\ pickle}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00002}00002\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00003}00003\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00004}00004\ \textcolor{keyword}{import}\ pytest}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00005}00005\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacenumpy_1_1testing}{numpy.testing}}\ \textcolor{keyword}{import}\ assert\_allclose,\ assert\_array\_equal}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00006}00006\ \textcolor{keyword}{from}\ pytest\ \textcolor{keyword}{import}\ approx}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00007}00007\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacescipy_1_1optimize}{scipy.optimize}}\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00008}00008\ \ \ \ \ LinearConstraint,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00009}00009\ \ \ \ \ minimize,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00010}00010\ \ \ \ \ minimize\_scalar,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00011}00011\ \ \ \ \ newton,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00012}00012\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00013}00013\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacescipy_1_1special}{scipy.special}}\ \textcolor{keyword}{import}\ logsumexp}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00014}00014\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00015}00015\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1link}{sklearn.\_loss.link}}\ \textcolor{keyword}{import}\ IdentityLink,\ \_inclusive\_low\_high}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00016}00016\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1loss}{sklearn.\_loss.loss}}\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00017}00017\ \ \ \ \ \_LOSSES,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00018}00018\ \ \ \ \ AbsoluteError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00019}00019\ \ \ \ \ BaseLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00020}00020\ \ \ \ \ HalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00021}00021\ \ \ \ \ HalfGammaLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00022}00022\ \ \ \ \ HalfMultinomialLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00023}00023\ \ \ \ \ HalfPoissonLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00024}00024\ \ \ \ \ HalfSquaredError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00025}00025\ \ \ \ \ HalfTweedieLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00026}00026\ \ \ \ \ HalfTweedieLossIdentity,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00027}00027\ \ \ \ \ HuberLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00028}00028\ \ \ \ \ PinballLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00029}00029\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00030}00030\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1utils}{sklearn.utils}}\ \textcolor{keyword}{import}\ assert\_all\_finite}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00031}00031\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1utils_1_1__testing}{sklearn.utils.\_testing}}\ \textcolor{keyword}{import}\ create\_memmap\_backed\_data,\ skip\_if\_32bit}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00032}00032\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00033}00033\ ALL\_LOSSES\ =\ list(\_LOSSES.values())}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00034}00034\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00035}00035\ LOSS\_INSTANCES\ =\ [loss()\ \textcolor{keywordflow}{for}\ loss\ \textcolor{keywordflow}{in}\ ALL\_LOSSES]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00036}00036\ \textcolor{comment}{\#\ HalfTweedieLoss(power=1.5)\ is\ already\ there\ as\ default}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00037}00037\ LOSS\_INSTANCES\ +=\ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00038}00038\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(quantile=0.25),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00039}00039\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(quantile=0.75),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00040}00040\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=-\/1.5),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00041}00041\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00042}00042\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=1),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00043}00043\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=2),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00044}00044\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=3.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00045}00045\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00046}00046\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=1),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00047}00047\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=2),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00048}00048\ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=3.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00049}00049\ ]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00050}00050\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00051}00051\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00052}00052\ \textcolor{keyword}{def\ }loss\_instance\_name(param):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00053}00053\ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(param,\ BaseLoss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00054}00054\ \ \ \ \ \ \ \ \ loss\ =\ param}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00055}00055\ \ \ \ \ \ \ \ \ name\ =\ loss.\_\_class\_\_.\_\_name\_\_}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00056}00056\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(loss,\ PinballLoss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00057}00057\ \ \ \ \ \ \ \ \ \ \ \ \ name\ +=\ f\textcolor{stringliteral}{"{}(quantile=\{loss.closs.quantile\})"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00058}00058\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(loss,\ HuberLoss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00059}00059\ \ \ \ \ \ \ \ \ \ \ \ \ name\ +=\ f\textcolor{stringliteral}{"{}(quantile=\{loss.quantile\}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ hasattr(loss,\ \textcolor{stringliteral}{"{}closs"{}})\ \textcolor{keywordflow}{and}\ hasattr(loss.closs,\ \textcolor{stringliteral}{"{}power"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00061}00061\ \ \ \ \ \ \ \ \ \ \ \ \ name\ +=\ f\textcolor{stringliteral}{"{}(power=\{loss.closs.power\})"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00062}00062\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ name}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00063}00063\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00064}00064\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ str(param)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00065}00065\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00066}00066\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00067}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{00067}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00068}00068\ \ \ \ \ loss,\ n\_samples,\ y\_bound=(-\/100,\ 100),\ raw\_bound=(-\/5,\ 5),\ seed=42}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00069}00069\ ):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00070}00070\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Random\ generate\ y\_true\ and\ raw\_prediction\ in\ valid\ range."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00071}00071\ \ \ \ \ rng\ =\ np.random.RandomState(seed)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00072}00072\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.empty((n\_samples,\ loss.n\_classes))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ raw\_prediction.flat[:]\ =\ rng.uniform(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ \ \ \ \ low=raw\_bound[0],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \ \ \ \ high=raw\_bound[1],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ \ \ \ \ size=n\_samples\ *\ loss.n\_classes,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00079}00079\ \ \ \ \ \ \ \ \ y\_true\ =\ np.arange(n\_samples).astype(float)\ \%\ loss.n\_classes}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00080}00080\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ If\ link\ is\ identity,\ we\ must\ respect\ the\ interval\ of\ y\_pred:}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(loss.link,\ IdentityLink):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ \ \ \ \ low,\ high\ =\ \_inclusive\_low\_high(loss.interval\_y\_pred)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00084}00084\ \ \ \ \ \ \ \ \ \ \ \ \ low\ =\ np.amax([low,\ raw\_bound[0]])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00085}00085\ \ \ \ \ \ \ \ \ \ \ \ \ high\ =\ np.amin([high,\ raw\_bound[1]])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00086}00086\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_bound\ =\ (low,\ high)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00087}00087\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ rng.uniform(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00088}00088\ \ \ \ \ \ \ \ \ \ \ \ \ low=raw\_bound[0],\ high=raw\_bound[1],\ size=n\_samples}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00089}00089\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00090}00090\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ generate\ a\ y\_true\ in\ valid\ range}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00091}00091\ \ \ \ \ \ \ \ \ low,\ high\ =\ \_inclusive\_low\_high(loss.interval\_y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00092}00092\ \ \ \ \ \ \ \ \ low\ =\ max(low,\ y\_bound[0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00093}00093\ \ \ \ \ \ \ \ \ high\ =\ min(high,\ y\_bound[1])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00094}00094\ \ \ \ \ \ \ \ \ y\_true\ =\ rng.uniform(low,\ high,\ size=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00095}00095\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ set\ some\ values\ at\ special\ boundaries}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00096}00096\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.low\ ==\ 0\ \textcolor{keywordflow}{and}\ loss.interval\_y\_true.low\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00097}00097\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true[::\ (n\_samples\ //\ 3)]\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00098}00098\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.high\ ==\ 1\ \textcolor{keywordflow}{and}\ loss.interval\_y\_true.high\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00099}00099\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true[1\ ::\ (n\_samples\ //\ 3)]\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00100}00100\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00101}00101\ \ \ \ \ \textcolor{keywordflow}{return}\ y\_true,\ raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00102}00102\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00103}00103\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00104}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{00104}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\_derivative}}(func,\ x,\ eps):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00105}00105\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Helper\ function\ for\ numerical\ (first)\ derivatives."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00106}00106\ \ \ \ \ \textcolor{comment}{\#\ For\ numerical\ derivatives,\ see}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00107}00107\ \ \ \ \ \textcolor{comment}{\#\ https://en.wikipedia.org/wiki/Numerical\_differentiation}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00108}00108\ \ \ \ \ \textcolor{comment}{\#\ https://en.wikipedia.org/wiki/Finite\_difference\_coefficient}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00109}00109\ \ \ \ \ \textcolor{comment}{\#\ We\ use\ central\ finite\ differences\ of\ accuracy\ 4.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00110}00110\ \ \ \ \ h\ =\ np.full\_like(x,\ fill\_value=eps)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00111}00111\ \ \ \ \ f\_minus\_2h\ =\ func(x\ -\/\ 2\ *\ h)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00112}00112\ \ \ \ \ f\_minus\_1h\ =\ func(x\ -\/\ h)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00113}00113\ \ \ \ \ f\_plus\_1h\ =\ func(x\ +\ h)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00114}00114\ \ \ \ \ f\_plus\_2h\ =\ func(x\ +\ 2\ *\ h)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00115}00115\ \ \ \ \ \textcolor{keywordflow}{return}\ (-\/f\_plus\_2h\ +\ 8\ *\ f\_plus\_1h\ -\/\ 8\ *\ f\_minus\_1h\ +\ f\_minus\_2h)\ /\ (12.0\ *\ eps)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00116}00116\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00117}00117\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00118}00118\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00119}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a5d4a6ec724cf2afd511ada41995b3a37}{00119}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a5d4a6ec724cf2afd511ada41995b3a37}{test\_loss\_boundary}}(loss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00120}00120\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ interval\ ranges\ of\ y\_true\ and\ y\_pred\ in\ losses."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00121}00121\ \ \ \ \ \textcolor{comment}{\#\ make\ sure\ low\ and\ high\ are\ always\ within\ the\ interval,\ used\ for\ linspace}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00122}00122\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ n\_classes\ =\ 3\ \ \textcolor{comment}{\#\ default\ value}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ y\_true\ =\ np.tile(np.linspace(0,\ n\_classes\ -\/\ 1,\ num=n\_classes),\ 3)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00125}00125\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ low,\ high\ =\ \_inclusive\_low\_high(loss.interval\_y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ y\_true\ =\ np.linspace(low,\ high,\ num=10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00128}00128\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00129}00129\ \ \ \ \ \textcolor{comment}{\#\ add\ boundaries\ if\ they\ are\ included}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00130}00130\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.low\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ y\_true\ =\ np.r\_[y\_true,\ loss.interval\_y\_true.low]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00132}00132\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.high\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ y\_true\ =\ np.r\_[y\_true,\ loss.interval\_y\_true.high]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00134}00134\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00135}00135\ \ \ \ \ \textcolor{keyword}{assert}\ loss.in\_y\_true\_range(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00136}00136\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00137}00137\ \ \ \ \ n\ =\ y\_true.shape[0]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00138}00138\ \ \ \ \ low,\ high\ =\ \_inclusive\_low\_high(loss.interval\_y\_pred)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00139}00139\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ y\_pred\ =\ np.empty((n,\ n\_classes))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00141}00141\ \ \ \ \ \ \ \ \ y\_pred[:,\ 0]\ =\ np.linspace(low,\ high,\ num=n)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00142}00142\ \ \ \ \ \ \ \ \ y\_pred[:,\ 1]\ =\ 0.5\ *\ (1\ -\/\ y\_pred[:,\ 0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ y\_pred[:,\ 2]\ =\ 0.5\ *\ (1\ -\/\ y\_pred[:,\ 0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00144}00144\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00145}00145\ \ \ \ \ \ \ \ \ y\_pred\ =\ np.linspace(low,\ high,\ num=n)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00146}00146\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00147}00147\ \ \ \ \ \textcolor{keyword}{assert}\ loss.in\_y\_pred\_range(y\_pred)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00148}00148\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00149}00149\ \ \ \ \ \textcolor{comment}{\#\ calculating\ losses\ should\ not\ fail}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00150}00150\ \ \ \ \ raw\_prediction\ =\ loss.link.link(y\_pred)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00151}00151\ \ \ \ \ loss.loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00152}00152\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00153}00153\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00154}00154\ \textcolor{comment}{\#\ Fixture\ to\ test\ valid\ value\ ranges.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00155}00155\ Y\_COMMON\_PARAMS\ =\ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00156}00156\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00157}00157\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00158}00158\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError}{AbsoluteError}}(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00159}00159\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00160}00160\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00161}00161\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00162}00162\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}(),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00163}00163\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=-\/3),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00164}00164\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=0),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00165}00165\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=1.5),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00166}00166\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=2),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00167}00167\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=3),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00168}00168\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=-\/3),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00169}00169\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=0),\ [-\/3,\ -\/0.1,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00170}00170\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=1.5),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00171}00171\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=2),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00172}00172\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=3),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00173}00173\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ [0.1,\ 0.5,\ 0.9],\ [-\/np.inf,\ -\/1,\ 2,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00174}00174\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(),\ [],\ [-\/np.inf,\ -\/1,\ 1.1,\ np.inf]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00175}00175\ ]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00176}00176\ \textcolor{comment}{\#\ y\_pred\ and\ y\_true\ do\ not\ always\ have\ the\ same\ domain\ (valid\ value\ range).}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00177}00177\ \textcolor{comment}{\#\ Hence,\ we\ define\ extra\ sets\ of\ parameters\ for\ each\ of\ them.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00178}00178\ Y\_TRUE\_PARAMS\ =\ [\ \ \textcolor{comment}{\#\ type:\ ignore[var-\/annotated]}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00179}00179\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00180}00180\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ [0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00181}00181\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(),\ [0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00182}00182\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=-\/3),\ [-\/100,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00183}00183\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=0),\ [-\/100,\ 0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00184}00184\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=1.5),\ [0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00185}00185\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=-\/3),\ [-\/100,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00186}00186\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=0),\ [-\/100,\ 0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00187}00187\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=1.5),\ [0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00188}00188\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ [0,\ 1],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00189}00189\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(),\ [0.0,\ 1.0,\ 2],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00190}00190\ ]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00191}00191\ Y\_PRED\_PARAMS\ =\ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00192}00192\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00193}00193\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ [],\ [0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00194}00194\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=-\/3),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00195}00195\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=0),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00196}00196\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=1.5),\ [],\ [0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00197}00197\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=-\/3),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00198}00198\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=0),\ [-\/3,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00199}00199\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=1.5),\ [],\ [0]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00200}00200\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ [],\ [0,\ 1]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00201}00201\ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(),\ [0.1,\ 0.5],\ [0,\ 1]),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00202}00202\ ]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00203}00203\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00204}00204\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00205}00205\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00206}00206\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ y\_true\_success,\ y\_true\_fail"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00207}00207\ \ \ \ \ Y\_COMMON\_PARAMS\ +\ Y\_TRUE\_PARAMS,\ \ \textcolor{comment}{\#\ type:\ ignore[operator]}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00208}00208\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00209}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae56255afa73f8bec4bf8a84f72f047a8}{00209}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae56255afa73f8bec4bf8a84f72f047a8}{test\_loss\_boundary\_y\_true}}(loss,\ y\_true\_success,\ y\_true\_fail):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00210}00210\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ boundaries\ of\ y\_true\ for\ loss\ functions."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00211}00211\ \ \ \ \ \textcolor{keywordflow}{for}\ y\ \textcolor{keywordflow}{in}\ y\_true\_success:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00212}00212\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ loss.in\_y\_true\_range(np.array([y]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00213}00213\ \ \ \ \ \textcolor{keywordflow}{for}\ y\ \textcolor{keywordflow}{in}\ y\_true\_fail:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ \textcolor{keywordflow}{not}\ loss.in\_y\_true\_range(np.array([y]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00215}00215\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00216}00216\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00217}00217\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00218}00218\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ y\_pred\_success,\ y\_pred\_fail"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00219}00219\ \ \ \ \ Y\_COMMON\_PARAMS\ +\ Y\_PRED\_PARAMS,\ \ \textcolor{comment}{\#\ type:\ ignore[operator]}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00220}00220\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00221}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a7a99b30b2769b07502d139824291cd45}{00221}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a7a99b30b2769b07502d139824291cd45}{test\_loss\_boundary\_y\_pred}}(loss,\ y\_pred\_success,\ y\_pred\_fail):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00222}00222\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ boundaries\ of\ y\_pred\ for\ loss\ functions."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00223}00223\ \ \ \ \ \textcolor{keywordflow}{for}\ y\ \textcolor{keywordflow}{in}\ y\_pred\_success:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ loss.in\_y\_pred\_range(np.array([y]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00225}00225\ \ \ \ \ \textcolor{keywordflow}{for}\ y\ \textcolor{keywordflow}{in}\ y\_pred\_fail:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00226}00226\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ \textcolor{keywordflow}{not}\ loss.in\_y\_pred\_range(np.array([y]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00227}00227\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00228}00228\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00229}00229\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00230}00230\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ y\_true,\ raw\_prediction,\ loss\_true,\ gradient\_true,\ hessian\_true"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00231}00231\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00232}00232\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}(),\ 1.0,\ 5.0,\ 8,\ 4,\ 1),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00233}00233\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError}{AbsoluteError}}(),\ 1.0,\ 5.0,\ 4.0,\ 1.0,\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00234}00234\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(quantile=0.5),\ 1.0,\ 5.0,\ 2,\ 0.5,\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00235}00235\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(quantile=0.25),\ 1.0,\ 5.0,\ 4\ *\ (1\ -\/\ 0.25),\ 1\ -\/\ 0.25,\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(quantile=0.25),\ 5.0,\ 1.0,\ 4\ *\ 0.25,\ -\/0.25,\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00237}00237\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(quantile=0.5,\ delta=3),\ 1.0,\ 5.0,\ 3\ *\ (4\ -\/\ 3\ /\ 2),\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00238}00238\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(quantile=0.5,\ delta=3),\ 1.0,\ 3.0,\ 0.5\ *\ 2**2,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00239}00239\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ 2.0,\ np.log(4),\ 4\ -\/\ 2\ *\ np.log(4),\ 4\ -\/\ 2,\ 4),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00240}00240\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}(),\ 2.0,\ np.log(4),\ np.log(4)\ +\ 2\ /\ 4,\ 1\ -\/\ 2\ /\ 4,\ 2\ /\ 4),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00241}00241\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=3),\ 2.0,\ np.log(4),\ -\/1\ /\ 4\ +\ 1\ /\ 4**2,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=1),\ 2.0,\ 4.0,\ 2\ -\/\ 2\ *\ np.log(2),\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00243}00243\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=2),\ 2.0,\ 4.0,\ np.log(2)\ -\/\ 1\ /\ 2,\ \textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ \ \ \ \ 2.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00247}00247\ \ \ \ \ \ \ \ \ \ \ \ \ 4.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \ \ \ \ -\/1\ /\ 4\ +\ 1\ /\ 4**2\ +\ 1\ /\ 2\ /\ 2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \ \ \ \ 0.25,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ \ \ \ \ np.log(4),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \ \ \ \ np.log1p(4)\ -\/\ 0.25\ *\ np.log(4),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00260}00260\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Extreme\ log\ loss\ cases,\ checked\ with\ mpmath:}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ import\ mpmath\ as\ mp}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00263}00263\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \#\ Stolen\ from\ scipy}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ def\ mpf2float(x):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00265}00265\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ return\ float(mp.nstr(x,\ 17,\ min\_fixed=0,\ max\_fixed=0))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00267}00267\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ def\ mp\_logloss(y\_true,\ raw):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00268}00268\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ with\ mp.workdps(100):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00269}00269\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ y\_true,\ raw\ =\ mp.mpf(float(y\_true)),\ mp.mpf(float(raw))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00270}00270\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ out\ =\ mp.log1p(mp.exp(raw))\ -\/\ y\_true\ *\ raw}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00271}00271\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ return\ mpf2float(out)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00272}00272\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ def\ mp\_gradient(y\_true,\ raw):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ with\ mp.workdps(100):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00275}00275\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ y\_true,\ raw\ =\ mp.mpf(float(y\_true)),\ mp.mpf(float(raw))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ out\ =\ mp.mpf(1)\ /\ (mp.mpf(1)\ +\ mp.exp(-\/raw))\ -\/\ y\_true}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00277}00277\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ return\ mpf2float(out)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00278}00278\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ def\ mp\_hessian(y\_true,\ raw):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ with\ mp.workdps(100):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00281}00281\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ y\_true,\ raw\ =\ mp.mpf(float(y\_true)),\ mp.mpf(float(raw))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00282}00282\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ p\ =\ mp.mpf(1)\ /\ (mp.mpf(1)\ +\ mp.exp(-\/raw))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00283}00283\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ out\ =\ p\ *\ (mp.mpf(1)\ -\/\ p)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ return\ mpf2float(out)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00285}00285\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ y,\ raw\ =\ 0.0,\ 37.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ mp\_logloss(y,\ raw),\ mp\_gradient(y,\ raw),\ mp\_hessian(y,\ raw)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ -\/1e20,\ 0,\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00289}00289\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ -\/1e20,\ 1e20,\ -\/1,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ -\/1e3,\ 0,\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00291}00291\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ -\/1e3,\ 1e3,\ -\/1,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00292}00292\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ -\/37.5,\ 37.5,\ -\/1,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00293}00293\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ -\/37.0,\ 37,\ 1e-\/16\ -\/\ 1,\ 8.533047625744065e-\/17),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00294}00294\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ -\/37.0,\ *[8.533047625744065e-\/17]\ *\ 3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00295}00295\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ -\/36.9,\ 36.9,\ 1e-\/16\ -\/\ 1,\ 9.430476078526806e-\/17),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00296}00296\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ -\/36.9,\ *[9.430476078526806e-\/17]\ *\ 3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ 37.0,\ 37,\ 1\ -\/\ 1e-\/16,\ 8.533047625744065e-\/17),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00298}00298\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ 37.0,\ *[8.533047625744066e-\/17]\ *\ 3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ 37.5,\ 37.5,\ 1,\ 5.175555005801868e-\/17),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ 232.8,\ 232.8,\ 1,\ 1.4287342391028437e-\/101),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ 1e20,\ 0,\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ 1e20,\ 1e20,\ 1,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ \ \ \ \ 1.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00306}00306\ \ \ \ \ \ \ \ \ \ \ \ \ 232.8,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00307}00307\ \ \ \ \ \ \ \ \ \ \ \ \ 0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00308}00308\ \ \ \ \ \ \ \ \ \ \ \ \ -\/1.4287342391028437e-\/101,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ \ \ \ \ 1.4287342391028437e-\/101,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ 232.9,\ 0,\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 1.0,\ 1e3,\ 0,\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ 0.0,\ 1e3,\ 1e3,\ 1,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00315}00315\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ \ \ \ \ 0.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ \ \ \ \ [0.2,\ 0.5,\ 0.3],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00318}00318\ \ \ \ \ \ \ \ \ \ \ \ \ logsumexp([0.2,\ 0.5,\ 0.3])\ -\/\ 0.2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00320}00320\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00321}00321\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00323}00323\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ \ \ \ \ 1.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00325}00325\ \ \ \ \ \ \ \ \ \ \ \ \ [0.2,\ 0.5,\ 0.3],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ \ \ \ \ logsumexp([0.2,\ 0.5,\ 0.3])\ -\/\ 0.5,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00328}00328\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00329}00329\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00330}00330\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00331}00331\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00332}00332\ \ \ \ \ \ \ \ \ \ \ \ \ 2.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00333}00333\ \ \ \ \ \ \ \ \ \ \ \ \ [0.2,\ 0.5,\ 0.3],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00334}00334\ \ \ \ \ \ \ \ \ \ \ \ \ logsumexp([0.2,\ 0.5,\ 0.3])\ -\/\ 0.3,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=3),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \ \ \ \ 2.0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ \ \ \ \ [1e4,\ 0,\ 7e-\/7],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ \ \ \ \ logsumexp([1e4,\ 0,\ 7e-\/7])\ -\/\ (7e-\/7),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00345}00345\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00346}00346\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00347}00347\ \ \ \ \ ids=loss\_instance\_name,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00348}00348\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00349}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a198aa1c112a68148858fa15453e67345}{00349}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a198aa1c112a68148858fa15453e67345}{test\_loss\_on\_specific\_values}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00350}00350\ \ \ \ \ loss,\ y\_true,\ raw\_prediction,\ loss\_true,\ gradient\_true,\ hessian\_true}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00351}00351\ ):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00352}00352\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ losses,\ gradients\ and\ hessians\ at\ specific\ values."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00353}00353\ \ \ \ \ loss1\ =\ loss(y\_true=np.array([y\_true]),\ raw\_prediction=np.array([raw\_prediction]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00354}00354\ \ \ \ \ grad1\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00355}00355\ \ \ \ \ \ \ \ \ y\_true=np.array([y\_true]),\ raw\_prediction=np.array([raw\_prediction])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00356}00356\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00357}00357\ \ \ \ \ loss2,\ grad2\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ y\_true=np.array([y\_true]),\ raw\_prediction=np.array([raw\_prediction])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00359}00359\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00360}00360\ \ \ \ \ grad3,\ hess\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00361}00361\ \ \ \ \ \ \ \ \ y\_true=np.array([y\_true]),\ raw\_prediction=np.array([raw\_prediction])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00362}00362\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00363}00363\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00364}00364\ \ \ \ \ \textcolor{keyword}{assert}\ loss1\ ==\ approx(loss\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00365}00365\ \ \ \ \ \textcolor{keyword}{assert}\ loss2\ ==\ approx(loss\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00366}00366\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00367}00367\ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_true\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ grad1\ ==\ approx(gradient\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00369}00369\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ grad2\ ==\ approx(gradient\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00370}00370\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ grad3\ ==\ approx(gradient\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00371}00371\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00372}00372\ \ \ \ \ \textcolor{keywordflow}{if}\ hessian\_true\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00373}00373\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hess\ ==\ approx(hessian\_true,\ rel=1e-\/15,\ abs=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00374}00374\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00375}00375\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00376}00376\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ ALL\_LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00377}00377\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}readonly\_memmap"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00378}00378\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}dtype\_in"{},\ [np.float32,\ np.float64])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00379}00379\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}dtype\_out"{},\ [np.float32,\ np.float64])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00380}00380\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00381}00381\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}out1"{},\ [None,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00382}00382\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}out2"{},\ [None,\ 1])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00383}00383\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}n\_threads"{},\ [1,\ 2])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00384}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a244ef4411a18107728e1ac4966639003}{00384}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a244ef4411a18107728e1ac4966639003}{test\_loss\_dtype}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00385}00385\ \ \ \ \ loss,\ readonly\_memmap,\ dtype\_in,\ dtype\_out,\ sample\_weight,\ out1,\ out2,\ n\_threads}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00386}00386\ ):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00387}00387\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ acceptance\ of\ dtypes,\ readonly\ and\ writeable\ arrays\ in\ loss\ functions.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00388}00388\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00389}00389\ \textcolor{stringliteral}{\ \ \ \ Check\ that\ loss\ accepts\ if\ all\ input\ arrays\ are\ either\ all\ float32\ or\ all}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00390}00390\ \textcolor{stringliteral}{\ \ \ \ float64,\ and\ all\ output\ arrays\ are\ either\ all\ float32\ or\ all\ float64.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00391}00391\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00392}00392\ \textcolor{stringliteral}{\ \ \ \ Also\ check\ that\ input\ arrays\ can\ be\ readonly,\ e.g.\ memory\ mapped.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00393}00393\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00394}00394\ \ \ \ \ loss\ =\ loss()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00395}00395\ \ \ \ \ \textcolor{comment}{\#\ generate\ a\ y\_true\ and\ raw\_prediction\ in\ valid\ range}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00396}00396\ \ \ \ \ n\_samples\ =\ 5}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00397}00397\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00398}00398\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00399}00399\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00400}00400\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00401}00401\ \ \ \ \ \ \ \ \ raw\_bound=(-\/10,\ 10),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00402}00402\ \ \ \ \ \ \ \ \ seed=42,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00403}00403\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00404}00404\ \ \ \ \ y\_true\ =\ y\_true.astype(dtype\_in)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00405}00405\ \ \ \ \ raw\_prediction\ =\ raw\_prediction.astype(dtype\_in)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00406}00406\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00407}00407\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.array([2.0]\ *\ n\_samples,\ dtype=dtype\_in)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00409}00409\ \ \ \ \ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ out1\ =\ np.empty\_like(y\_true,\ dtype=dtype\_out)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00411}00411\ \ \ \ \ \textcolor{keywordflow}{if}\ out2\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ out2\ =\ np.empty\_like(raw\_prediction,\ dtype=dtype\_out)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00413}00413\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00414}00414\ \ \ \ \ \textcolor{keywordflow}{if}\ readonly\_memmap:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ y\_true\ =\ create\_memmap\_backed\_data(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00416}00416\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ create\_memmap\_backed\_data(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00417}00417\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00418}00418\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\ =\ create\_memmap\_backed\_data(sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00419}00419\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00420}00420\ \ \ \ \ l\ =\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00421}00421\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00422}00422\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00423}00423\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00424}00424\ \ \ \ \ \ \ \ \ loss\_out=out1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00425}00425\ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00426}00426\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00427}00427\ \ \ \ \ \textcolor{keyword}{assert}\ l\ \textcolor{keywordflow}{is}\ out1\ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00428}00428\ \ \ \ \ g\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00429}00429\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00430}00430\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00431}00431\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ gradient\_out=out2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00434}00434\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00435}00435\ \ \ \ \ \textcolor{keyword}{assert}\ g\ \textcolor{keywordflow}{is}\ out2\ \textcolor{keywordflow}{if}\ out2\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00436}00436\ \ \ \ \ l,\ g\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00438}00438\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00440}00440\ \ \ \ \ \ \ \ \ loss\_out=out1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00441}00441\ \ \ \ \ \ \ \ \ gradient\_out=out2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00443}00443\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00444}00444\ \ \ \ \ \textcolor{keyword}{assert}\ l\ \textcolor{keywordflow}{is}\ out1\ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00445}00445\ \ \ \ \ \textcolor{keyword}{assert}\ g\ \textcolor{keywordflow}{is}\ out2\ \textcolor{keywordflow}{if}\ out2\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00446}00446\ \ \ \ \ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{and}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ out1\ =\ np.empty\_like(raw\_prediction,\ dtype=dtype\_out)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00448}00448\ \ \ \ \ g,\ h\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00450}00450\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ gradient\_out=out1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00453}00453\ \ \ \ \ \ \ \ \ hessian\_out=out2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00454}00454\ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00455}00455\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00456}00456\ \ \ \ \ \textcolor{keyword}{assert}\ g\ \textcolor{keywordflow}{is}\ out1\ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00457}00457\ \ \ \ \ \textcolor{keyword}{assert}\ h\ \textcolor{keywordflow}{is}\ out2\ \textcolor{keywordflow}{if}\ out2\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00458}00458\ \ \ \ \ loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00459}00459\ \ \ \ \ loss.fit\_intercept\_only(y\_true=y\_true,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00460}00460\ \ \ \ \ loss.constant\_to\_optimal\_zero(y\_true=y\_true,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00461}00461\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(loss,\ \textcolor{stringliteral}{"{}predict\_proba"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00462}00462\ \ \ \ \ \ \ \ \ loss.predict\_proba(raw\_prediction=raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00463}00463\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(loss,\ \textcolor{stringliteral}{"{}gradient\_proba"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00464}00464\ \ \ \ \ \ \ \ \ g,\ p\ =\ loss.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00465}00465\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00466}00466\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00467}00467\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00468}00468\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=out1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00469}00469\ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out=out2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00470}00470\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00471}00471\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00472}00472\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ g\ \textcolor{keywordflow}{is}\ out1\ \textcolor{keywordflow}{if}\ out1\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00473}00473\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ p\ \textcolor{keywordflow}{is}\ out2\ \textcolor{keywordflow}{if}\ out2\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00474}00474\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00475}00475\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00476}00476\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00477}00477\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00478}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a31f0b42e1adf3846ad9d2dd01f347af9}{00478}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a31f0b42e1adf3846ad9d2dd01f347af9}{test\_loss\_same\_as\_C\_functions}}(loss,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00479}00479\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ Python\ and\ Cython\ functions\ return\ same\ results."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00480}00480\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00481}00481\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00482}00482\ \ \ \ \ \ \ \ \ n\_samples=20,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00483}00483\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ raw\_bound=(-\/10,\ 10),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ seed=42,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00486}00486\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00487}00487\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y\_true.shape[0],\ num=y\_true.shape[0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00489}00489\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00490}00490\ \ \ \ \ out\_l1\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00491}00491\ \ \ \ \ out\_l2\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00492}00492\ \ \ \ \ out\_g1\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00493}00493\ \ \ \ \ out\_g2\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00494}00494\ \ \ \ \ out\_h1\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00495}00495\ \ \ \ \ out\_h2\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00496}00496\ \ \ \ \ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ loss\_out=out\_l1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00501}00501\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00502}00502\ \ \ \ \ loss.closs.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00503}00503\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00504}00504\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00505}00505\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00506}00506\ \ \ \ \ \ \ \ \ loss\_out=out\_l2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00507}00507\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00508}00508\ \ \ \ \ assert\_allclose(out\_l1,\ out\_l2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00509}00509\ \ \ \ \ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00510}00510\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00511}00511\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00512}00512\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00513}00513\ \ \ \ \ \ \ \ \ gradient\_out=out\_g1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00514}00514\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00515}00515\ \ \ \ \ loss.closs.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00516}00516\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00517}00517\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00518}00518\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00519}00519\ \ \ \ \ \ \ \ \ gradient\_out=out\_g2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00520}00520\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00521}00521\ \ \ \ \ assert\_allclose(out\_g1,\ out\_g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00522}00522\ \ \ \ \ loss.closs.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00523}00523\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00524}00524\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00525}00525\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00526}00526\ \ \ \ \ \ \ \ \ loss\_out=out\_l1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00527}00527\ \ \ \ \ \ \ \ \ gradient\_out=out\_g1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00528}00528\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00529}00529\ \ \ \ \ loss.closs.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00530}00530\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00531}00531\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00532}00532\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00533}00533\ \ \ \ \ \ \ \ \ loss\_out=out\_l2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00534}00534\ \ \ \ \ \ \ \ \ gradient\_out=out\_g2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00535}00535\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00536}00536\ \ \ \ \ assert\_allclose(out\_l1,\ out\_l2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00537}00537\ \ \ \ \ assert\_allclose(out\_g1,\ out\_g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00538}00538\ \ \ \ \ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00540}00540\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00541}00541\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00542}00542\ \ \ \ \ \ \ \ \ gradient\_out=out\_g1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00543}00543\ \ \ \ \ \ \ \ \ hessian\_out=out\_h1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00544}00544\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00545}00545\ \ \ \ \ loss.closs.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00546}00546\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00547}00547\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00548}00548\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00549}00549\ \ \ \ \ \ \ \ \ gradient\_out=out\_g2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00550}00550\ \ \ \ \ \ \ \ \ hessian\_out=out\_h2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00551}00551\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00552}00552\ \ \ \ \ assert\_allclose(out\_g1,\ out\_g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00553}00553\ \ \ \ \ assert\_allclose(out\_h1,\ out\_h2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00554}00554\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00555}00555\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00556}00556\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00557}00557\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00558}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a00f38156d3d6c5364582a272001ae958}{00558}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a00f38156d3d6c5364582a272001ae958}{test\_loss\_gradients\_are\_the\_same}}(loss,\ sample\_weight,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00559}00559\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ loss\ and\ gradient\ are\ the\ same\ across\ different\ functions.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00560}00560\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00561}00561\ \textcolor{stringliteral}{\ \ \ \ Also\ test\ that\ output\ arguments\ contain\ correct\ results.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00562}00562\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00563}00563\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ n\_samples=20,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00567}00567\ \ \ \ \ \ \ \ \ raw\_bound=(-\/10,\ 10),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00568}00568\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00569}00569\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00570}00570\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00571}00571\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y\_true.shape[0],\ num=y\_true.shape[0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00572}00572\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00573}00573\ \ \ \ \ out\_l1\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00574}00574\ \ \ \ \ out\_l2\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00575}00575\ \ \ \ \ out\_g1\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00576}00576\ \ \ \ \ out\_g2\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00577}00577\ \ \ \ \ out\_g3\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00578}00578\ \ \ \ \ out\_h3\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00579}00579\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00580}00580\ \ \ \ \ l1\ =\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00581}00581\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00582}00582\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00583}00583\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00584}00584\ \ \ \ \ \ \ \ \ loss\_out=out\_l1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00585}00585\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00586}00586\ \ \ \ \ g1\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00587}00587\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00588}00588\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00589}00589\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00590}00590\ \ \ \ \ \ \ \ \ gradient\_out=out\_g1,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00591}00591\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00592}00592\ \ \ \ \ l2,\ g2\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00593}00593\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00594}00594\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00595}00595\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00596}00596\ \ \ \ \ \ \ \ \ loss\_out=out\_l2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00597}00597\ \ \ \ \ \ \ \ \ gradient\_out=out\_g2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00598}00598\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00599}00599\ \ \ \ \ g3,\ h3\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00600}00600\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00601}00601\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00602}00602\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00603}00603\ \ \ \ \ \ \ \ \ gradient\_out=out\_g3,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00604}00604\ \ \ \ \ \ \ \ \ hessian\_out=out\_h3,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00605}00605\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00606}00606\ \ \ \ \ assert\_allclose(l1,\ l2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00607}00607\ \ \ \ \ assert\_array\_equal(l1,\ out\_l1)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00608}00608\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(l1,\ out\_l1)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00609}00609\ \ \ \ \ assert\_array\_equal(l2,\ out\_l2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00610}00610\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(l2,\ out\_l2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00611}00611\ \ \ \ \ assert\_allclose(g1,\ g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00612}00612\ \ \ \ \ assert\_allclose(g1,\ g3)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00613}00613\ \ \ \ \ assert\_array\_equal(g1,\ out\_g1)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00614}00614\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(g1,\ out\_g1)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00615}00615\ \ \ \ \ assert\_array\_equal(g2,\ out\_g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00616}00616\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(g2,\ out\_g2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00617}00617\ \ \ \ \ assert\_array\_equal(g3,\ out\_g3)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00618}00618\ \ \ \ \ \textcolor{keyword}{assert}\ np.shares\_memory(g3,\ out\_g3)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00619}00619\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00620}00620\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(loss,\ \textcolor{stringliteral}{"{}gradient\_proba"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00621}00621\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ loss.is\_multiclass\ \ \textcolor{comment}{\#\ only\ for\ HalfMultinomialLoss}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00622}00622\ \ \ \ \ \ \ \ \ out\_g4\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00623}00623\ \ \ \ \ \ \ \ \ out\_proba\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00624}00624\ \ \ \ \ \ \ \ \ g4,\ proba\ =\ loss.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00625}00625\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00626}00626\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00627}00627\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00628}00628\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=out\_g4,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00629}00629\ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out=out\_proba,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00630}00630\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00631}00631\ \ \ \ \ \ \ \ \ assert\_allclose(g1,\ out\_g4)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00632}00632\ \ \ \ \ \ \ \ \ assert\_allclose(g1,\ g4)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00633}00633\ \ \ \ \ \ \ \ \ assert\_allclose(proba,\ out\_proba)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00634}00634\ \ \ \ \ \ \ \ \ assert\_allclose(np.sum(proba,\ axis=1),\ 1,\ rtol=1e-\/11)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00635}00635\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00636}00636\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00637}00637\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00638}00638\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ ["{}ones"{},\ "{}random"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00639}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a936efc1d6f952c5be473378a41cbf761}{00639}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a936efc1d6f952c5be473378a41cbf761}{test\_sample\_weight\_multiplies}}(loss,\ sample\_weight,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00640}00640\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ sample\ weights\ in\ loss,\ gradients\ and\ hessians.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00641}00641\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00642}00642\ \textcolor{stringliteral}{\ \ \ \ Make\ sure\ that\ passing\ sample\ weights\ to\ loss,\ gradient\ and\ hessian}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00643}00643\ \textcolor{stringliteral}{\ \ \ \ computation\ methods\ is\ equivalent\ to\ multiplying\ by\ the\ weights.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00644}00644\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00645}00645\ \ \ \ \ n\_samples\ =\ 100}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00646}00646\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00647}00647\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00648}00648\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00649}00649\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00650}00650\ \ \ \ \ \ \ \ \ raw\_bound=(-\/5,\ 5),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00651}00651\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00652}00652\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00653}00653\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00654}00654\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}ones"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00655}00655\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.ones(shape=n\_samples,\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00656}00656\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00657}00657\ \ \ \ \ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00658}00658\ \ \ \ \ \ \ \ \ sample\_weight\ =\ rng.normal(size=n\_samples).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00659}00659\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00660}00660\ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00661}00661\ \ \ \ \ \ \ \ \ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00662}00662\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00663}00663\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00664}00664\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00665}00665\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00666}00666\ \ \ \ \ \ \ \ \ sample\_weight}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00667}00667\ \ \ \ \ \ \ \ \ *\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00668}00668\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00669}00669\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00670}00670\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00671}00671\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00672}00672\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00673}00673\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00674}00674\ \ \ \ \ losses,\ gradient\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00675}00675\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00676}00676\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00677}00677\ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00678}00678\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00679}00679\ \ \ \ \ losses\_sw,\ gradient\_sw\ =\ loss.loss\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00680}00680\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00681}00681\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00682}00682\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00683}00683\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00684}00684\ \ \ \ \ assert\_allclose(losses\ *\ sample\_weight,\ losses\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00685}00685\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00686}00686\ \ \ \ \ \ \ \ \ assert\_allclose(gradient\ *\ sample\_weight,\ gradient\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00687}00687\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00688}00688\ \ \ \ \ \ \ \ \ assert\_allclose(gradient\ *\ sample\_weight[:,\ \textcolor{keywordtype}{None}],\ gradient\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00689}00689\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00690}00690\ \ \ \ \ gradient,\ hessian\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00691}00691\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00692}00692\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00693}00693\ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00694}00694\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00695}00695\ \ \ \ \ gradient\_sw,\ hessian\_sw\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00696}00696\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00697}00697\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00698}00698\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00699}00699\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00700}00700\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00701}00701\ \ \ \ \ \ \ \ \ assert\_allclose(gradient\ *\ sample\_weight,\ gradient\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00702}00702\ \ \ \ \ \ \ \ \ assert\_allclose(hessian\ *\ sample\_weight,\ hessian\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00703}00703\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00704}00704\ \ \ \ \ \ \ \ \ assert\_allclose(gradient\ *\ sample\_weight[:,\ \textcolor{keywordtype}{None}],\ gradient\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00705}00705\ \ \ \ \ \ \ \ \ assert\_allclose(hessian\ *\ sample\_weight[:,\ \textcolor{keywordtype}{None}],\ hessian\_sw)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00706}00706\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00707}00707\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00708}00708\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00709}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ac5868a699406c3aec9683359fd63ebb1}{00709}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ac5868a699406c3aec9683359fd63ebb1}{test\_graceful\_squeezing}}(loss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00710}00710\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ reshaped\ raw\_prediction\ gives\ same\ results."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00711}00711\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00712}00712\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ n\_samples=20,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ raw\_bound=(-\/10,\ 10),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ seed=42,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00717}00717\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00718}00718\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00719}00719\ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00720}00720\ \ \ \ \ \ \ \ \ raw\_prediction\_2d\ =\ raw\_prediction[:,\ \textcolor{keywordtype}{None}]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ \ \ \ \ loss.loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction\_2d),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00723}00723\ \ \ \ \ \ \ \ \ \ \ \ \ loss.loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00724}00724\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00726}00726\ \ \ \ \ \ \ \ \ \ \ \ \ loss.loss\_gradient(y\_true=y\_true,\ raw\_prediction=raw\_prediction\_2d),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00727}00727\ \ \ \ \ \ \ \ \ \ \ \ \ loss.loss\_gradient(y\_true=y\_true,\ raw\_prediction=raw\_prediction),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00728}00728\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00729}00729\ \ \ \ \ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00730}00730\ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient(y\_true=y\_true,\ raw\_prediction=raw\_prediction\_2d),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00731}00731\ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient(y\_true=y\_true,\ raw\_prediction=raw\_prediction),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00732}00732\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00733}00733\ \ \ \ \ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00734}00734\ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient\_hessian(y\_true=y\_true,\ raw\_prediction=raw\_prediction\_2d),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00735}00735\ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient\_hessian(y\_true=y\_true,\ raw\_prediction=raw\_prediction),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00736}00736\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00737}00737\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00738}00738\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00739}00739\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00740}00740\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00741}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae10d3bf88c3a64607a239fce5530e35a}{00741}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae10d3bf88c3a64607a239fce5530e35a}{test\_loss\_of\_perfect\_prediction}}(loss,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00742}00742\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ value\ of\ perfect\ predictions.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00743}00743\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00744}00744\ \textcolor{stringliteral}{\ \ \ \ Loss\ of\ y\_pred\ =\ y\_true\ plus\ constant\_to\_optimal\_zero\ should\ sums\ up\ to}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00745}00745\ \textcolor{stringliteral}{\ \ \ \ zero.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00746}00746\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00747}00747\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00748}00748\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Use\ small\ values\ such\ that\ exp(value)\ is\ not\ nan.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00749}00749\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.array([-\/10,\ -\/0.1,\ 0,\ 0.1,\ 3,\ 10])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00750}00750\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ If\ link\ is\ identity,\ we\ must\ respect\ the\ interval\ of\ y\_pred:}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00751}00751\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(loss.link,\ IdentityLink):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00752}00752\ \ \ \ \ \ \ \ \ \ \ \ \ eps\ =\ 1e-\/10}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00753}00753\ \ \ \ \ \ \ \ \ \ \ \ \ low\ =\ loss.interval\_y\_pred.low}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00754}00754\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.interval\_y\_pred.low\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00755}00755\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ low\ =\ low\ +\ eps}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00756}00756\ \ \ \ \ \ \ \ \ \ \ \ \ high\ =\ loss.interval\_y\_pred.high}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00757}00757\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.interval\_y\_pred.high\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00758}00758\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ high\ =\ high\ -\/\ eps}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00759}00759\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.clip(raw\_prediction,\ low,\ high)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00760}00760\ \ \ \ \ \ \ \ \ y\_true\ =\ loss.link.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00761}00761\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00762}00762\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ HalfMultinomialLoss}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00763}00763\ \ \ \ \ \ \ \ \ y\_true\ =\ np.arange(loss.n\_classes).astype(float)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00764}00764\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ raw\_prediction\ with\ entries\ -\/exp(10),\ but\ +exp(10)\ on\ the\ diagonal}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00765}00765\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ this\ is\ close\ enough\ to\ np.inf\ which\ would\ produce\ nan}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00766}00766\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.full(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00767}00767\ \ \ \ \ \ \ \ \ \ \ \ \ shape=(loss.n\_classes,\ loss.n\_classes),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00768}00768\ \ \ \ \ \ \ \ \ \ \ \ \ fill\_value=-\/np.exp(10),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00769}00769\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=float,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00770}00770\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00771}00771\ \ \ \ \ \ \ \ \ raw\_prediction.flat[::\ loss.n\_classes\ +\ 1]\ =\ np.exp(10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00772}00772\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00773}00773\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00774}00774\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y\_true.shape[0],\ num=y\_true.shape[0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00775}00775\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00776}00776\ \ \ \ \ loss\_value\ =\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00777}00777\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00778}00778\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00779}00779\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00780}00780\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00781}00781\ \ \ \ \ constant\_term\ =\ loss.constant\_to\_optimal\_zero(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00782}00782\ \ \ \ \ \ \ \ \ y\_true=y\_true,\ sample\_weight=sample\_weight}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00783}00783\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00784}00784\ \ \ \ \ \textcolor{comment}{\#\ Comparing\ loss\_value\ +\ constant\_term\ to\ zero\ would\ result\ in\ large}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00785}00785\ \ \ \ \ \textcolor{comment}{\#\ round-\/off\ errors.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00786}00786\ \ \ \ \ assert\_allclose(loss\_value,\ -\/constant\_term,\ atol=1e-\/14,\ rtol=1e-\/15)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00787}00787\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00788}00788\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00789}00789\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00790}00790\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00791}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_af6443b1da2b78215f19dadf136a19716}{00791}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_af6443b1da2b78215f19dadf136a19716}{test\_gradients\_hessians\_numerically}}(loss,\ sample\_weight,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00792}00792\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ gradients\ and\ hessians\ with\ numerical\ derivatives.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00793}00793\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00794}00794\ \textcolor{stringliteral}{\ \ \ \ Gradient\ should\ equal\ the\ numerical\ derivatives\ of\ the\ loss\ function.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00795}00795\ \textcolor{stringliteral}{\ \ \ \ Hessians\ should\ equal\ the\ numerical\ derivatives\ of\ gradients.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00796}00796\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00797}00797\ \ \ \ \ n\_samples\ =\ 20}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00798}00798\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00799}00799\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00800}00800\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00801}00801\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00802}00802\ \ \ \ \ \ \ \ \ raw\_bound=(-\/5,\ 5),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00803}00803\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00804}00804\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00805}00805\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00806}00806\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00807}00807\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(1,\ y\_true.shape[0],\ num=y\_true.shape[0])}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00808}00808\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00809}00809\ \ \ \ \ g,\ h\ =\ loss.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00810}00810\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00811}00811\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00812}00812\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00813}00813\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00814}00814\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00815}00815\ \ \ \ \ \textcolor{keyword}{assert}\ g.shape\ ==\ raw\_prediction.shape}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00816}00816\ \ \ \ \ \textcolor{keyword}{assert}\ h.shape\ ==\ raw\_prediction.shape}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00817}00817\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00818}00818\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00819}00819\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00820}00820\ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }loss\_func(x):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00821}00821\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00822}00822\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=x,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00824}00824\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00825}00825\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00826}00826\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00827}00827\ \ \ \ \ \ \ \ \ g\_numeric\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\_derivative}}(loss\_func,\ raw\_prediction,\ eps=1e-\/6)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00828}00828\ \ \ \ \ \ \ \ \ assert\_allclose(g,\ g\_numeric,\ rtol=5e-\/6,\ atol=1e-\/10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00829}00829\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00830}00830\ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }grad\_func(x):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00831}00831\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00832}00832\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00833}00833\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=x,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00834}00834\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00835}00835\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00836}00836\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00837}00837\ \ \ \ \ \ \ \ \ h\_numeric\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\_derivative}}(grad\_func,\ raw\_prediction,\ eps=1e-\/6)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00838}00838\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss.approx\_hessian:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00839}00839\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ What\ could\ we\ test\ if\ loss.approx\_hessian?}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00840}00840\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{pass}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00841}00841\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00842}00842\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(h,\ h\_numeric,\ rtol=5e-\/6,\ atol=1e-\/10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00843}00843\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00844}00844\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ For\ multiclass\ loss,\ we\ should\ only\ change\ the\ predictions\ of\ the}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00845}00845\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ class\ for\ which\ the\ derivative\ is\ taken\ for,\ e.g.\ offset[:,\ k]\ =\ eps}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00846}00846\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ for\ class\ k.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00847}00847\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ As\ a\ softmax\ is\ computed,\ offsetting\ the\ whole\ array\ by\ a\ constant}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00848}00848\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ would\ have\ no\ effect\ on\ the\ probabilities,\ and\ thus\ on\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00849}00849\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(loss.n\_classes):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00850}00850\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00851}00851\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }loss\_func(x):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00852}00852\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\ =\ raw\_prediction.copy()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00853}00853\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw[:,\ k]\ =\ x}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00854}00854\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00855}00855\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00856}00856\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00857}00857\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00858}00858\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00859}00859\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00860}00860\ \ \ \ \ \ \ \ \ \ \ \ \ g\_numeric\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\_derivative}}(loss\_func,\ raw\_prediction[:,\ k],\ eps=1e-\/5)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00861}00861\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(g[:,\ k],\ g\_numeric,\ rtol=5e-\/6,\ atol=1e-\/10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00862}00862\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00863}00863\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }grad\_func(x):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00864}00864\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\ =\ raw\_prediction.copy()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00865}00865\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw[:,\ k]\ =\ x}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00866}00866\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00867}00867\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00868}00868\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00869}00869\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00870}00870\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )[:,\ k]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00871}00871\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00872}00872\ \ \ \ \ \ \ \ \ \ \ \ \ h\_numeric\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\_derivative}}(grad\_func,\ raw\_prediction[:,\ k],\ eps=1e-\/6)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00873}00873\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss.approx\_hessian:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00874}00874\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ What\ could\ we\ test\ if\ loss.approx\_hessian?}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{pass}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(h[:,\ k],\ h\_numeric,\ rtol=5e-\/6,\ atol=1e-\/10)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00878}00878\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00879}00879\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00880}00880\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00881}00881\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ x0,\ y\_true"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00882}00882\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00883}00883\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}squared\_error"{}},\ -\/2.0,\ 42),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00884}00884\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}squared\_error"{}},\ 117.0,\ 1.05),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00885}00885\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}squared\_error"{}},\ 0.0,\ 0.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00886}00886\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ argmin\ of\ binomial\_loss\ for\ y\_true=0\ and\ y\_true=1\ is\ resp.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00887}00887\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ -\/inf\ and\ +inf\ due\ to\ logit,\ cf.\ "{}complete\ separation"{}.\ Therefore,\ we}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00888}00888\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ use\ 0\ <\ y\_true\ <\ 1.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00889}00889\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}binomial\_loss"{}},\ 0.3,\ 0.1),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00890}00890\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}binomial\_loss"{}},\ -\/12,\ 0.2),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00891}00891\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}binomial\_loss"{}},\ 30,\ 0.9),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00892}00892\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}poisson\_loss"{}},\ 12.0,\ 1.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00893}00893\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}poisson\_loss"{}},\ 0.0,\ 2.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00894}00894\ \ \ \ \ \ \ \ \ (\textcolor{stringliteral}{"{}poisson\_loss"{}},\ -\/22.0,\ 10.0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00895}00895\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00896}00896\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00897}00897\ \textcolor{preprocessor}{@skip\_if\_32bit}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00898}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae2cf5e4c5116a170f359167fcfeb8a54}{00898}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae2cf5e4c5116a170f359167fcfeb8a54}{test\_derivatives}}(loss,\ x0,\ y\_true):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00899}00899\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ gradients\ are\ zero\ at\ the\ minimum\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00900}00900\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00901}00901\ \textcolor{stringliteral}{\ \ \ \ We\ check\ this\ on\ a\ single\ value/sample\ using\ Halley's\ method\ with\ the}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00902}00902\ \textcolor{stringliteral}{\ \ \ \ first\ and\ second\ order\ derivatives\ computed\ by\ the\ Loss\ instance.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00903}00903\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ methods\ of\ Loss\ instances\ operate\ on\ arrays\ while\ the\ newton}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00904}00904\ \textcolor{stringliteral}{\ \ \ \ root\ finder\ expects\ a\ scalar\ or\ a\ one-\/element\ array\ for\ this\ purpose.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00905}00905\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00906}00906\ \ \ \ \ loss\ =\ \_LOSSES[loss](sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00907}00907\ \ \ \ \ y\_true\ =\ np.array([y\_true],\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00908}00908\ \ \ \ \ x0\ =\ np.array([x0],\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00909}00909\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00910}00910\ \ \ \ \ \textcolor{keyword}{def\ }func(x:\ np.ndarray)\ -\/>\ np.ndarray:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00911}00911\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ loss\ plus\ constant\ term.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00912}00912\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00913}00913\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ constant\ term\ is\ such\ that\ the\ minimum\ function\ value\ is\ zero,}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00914}00914\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ which\ is\ required\ by\ the\ Newton\ method.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00915}00915\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00916}00916\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00917}00917\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,\ raw\_prediction=x}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00918}00918\ \ \ \ \ \ \ \ \ )\ +\ loss.constant\_to\_optimal\_zero(y\_true=y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00919}00919\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00920}00920\ \ \ \ \ \textcolor{keyword}{def\ }fprime(x:\ np.ndarray)\ -\/>\ np.ndarray:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00921}00921\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.gradient(y\_true=y\_true,\ raw\_prediction=x)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00922}00922\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00923}00923\ \ \ \ \ \textcolor{keyword}{def\ }fprime2(x:\ np.ndarray)\ -\/>\ np.ndarray:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00924}00924\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.gradient\_hessian(y\_true=y\_true,\ raw\_prediction=x)[1]}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00925}00925\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00926}00926\ \ \ \ \ optimum\ =\ newton(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00927}00927\ \ \ \ \ \ \ \ \ func,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00928}00928\ \ \ \ \ \ \ \ \ x0=x0,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00929}00929\ \ \ \ \ \ \ \ \ fprime=fprime,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00930}00930\ \ \ \ \ \ \ \ \ fprime2=fprime2,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00931}00931\ \ \ \ \ \ \ \ \ maxiter=100,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00932}00932\ \ \ \ \ \ \ \ \ tol=5e-\/8,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00933}00933\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00934}00934\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00935}00935\ \ \ \ \ \textcolor{comment}{\#\ Need\ to\ ravel\ arrays\ because\ assert\_allclose\ requires\ matching}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00936}00936\ \ \ \ \ \textcolor{comment}{\#\ dimensions.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00937}00937\ \ \ \ \ y\_true\ =\ y\_true.ravel()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00938}00938\ \ \ \ \ optimum\ =\ optimum.ravel()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00939}00939\ \ \ \ \ assert\_allclose(loss.link.inverse(optimum),\ y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00940}00940\ \ \ \ \ assert\_allclose(func(optimum),\ 0,\ atol=1e-\/14)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00941}00941\ \ \ \ \ assert\_allclose(loss.gradient(y\_true=y\_true,\ raw\_prediction=optimum),\ 0,\ atol=5e-\/7)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00942}00942\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00943}00943\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00944}00944\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00945}00945\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00946}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ad18e8017ef3e06ab7571a39b5a0a626e}{00946}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ad18e8017ef3e06ab7571a39b5a0a626e}{test\_loss\_intercept\_only}}(loss,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00947}00947\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ fit\_intercept\_only\ returns\ the\ argmin\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00948}00948\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00949}00949\ \textcolor{stringliteral}{\ \ \ \ Also\ test\ that\ the\ gradient\ is\ zero\ at\ the\ minimum.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00950}00950\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00951}00951\ \ \ \ \ n\_samples\ =\ 50}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00952}00952\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00953}00953\ \ \ \ \ \ \ \ \ y\_true\ =\ loss.link.inverse(np.linspace(-\/4,\ 4,\ num=n\_samples))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00954}00954\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00955}00955\ \ \ \ \ \ \ \ \ y\_true\ =\ np.arange(n\_samples).astype(np.float64)\ \%\ loss.n\_classes}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00956}00956\ \ \ \ \ \ \ \ \ y\_true[::5]\ =\ 0\ \ \textcolor{comment}{\#\ exceedance\ of\ class\ 0}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00957}00957\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00958}00958\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00959}00959\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.linspace(0.1,\ 2,\ num=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00960}00960\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00961}00961\ \ \ \ \ a\ =\ loss.fit\_intercept\_only(y\_true=y\_true,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00962}00962\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00963}00963\ \ \ \ \ \textcolor{comment}{\#\ find\ minimum\ by\ optimization}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00964}00964\ \ \ \ \ \textcolor{keyword}{def\ }fun(x):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00965}00965\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00966}00966\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.full(shape=(n\_samples),\ fill\_value=x)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00967}00967\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00968}00968\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ np.ascontiguousarray(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00969}00969\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ np.broadcast\_to(x,\ shape=(n\_samples,\ loss.n\_classes))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00970}00970\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00971}00971\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00972}00972\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00973}00973\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00974}00974\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00975}00975\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00976}00976\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00977}00977\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00978}00978\ \ \ \ \ \ \ \ \ opt\ =\ minimize\_scalar(fun,\ tol=1e-\/7,\ options=\{\textcolor{stringliteral}{"{}maxiter"{}}:\ 100\})}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00979}00979\ \ \ \ \ \ \ \ \ grad\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00980}00980\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00981}00981\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=np.full\_like(y\_true,\ a),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00982}00982\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00983}00983\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00984}00984\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ a.shape\ ==\ tuple()\ \ \textcolor{comment}{\#\ scalar}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00985}00985\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ a.dtype\ ==\ y\_true.dtype}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00986}00986\ \ \ \ \ \ \ \ \ assert\_all\_finite(a)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00987}00987\ \ \ \ \ \ \ \ \ a\ ==\ approx(opt.x,\ rel=1e-\/7)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00988}00988\ \ \ \ \ \ \ \ \ grad.sum()\ ==\ approx(0,\ abs=1e-\/12)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00989}00989\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00990}00990\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ constraint\ corresponds\ to\ sum(raw\_prediction)\ =\ 0.\ Without\ it,\ we\ would}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00991}00991\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ need\ to\ apply\ loss.symmetrize\_raw\_prediction\ to\ opt.x\ before\ comparing.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00992}00992\ \ \ \ \ \ \ \ \ opt\ =\ minimize(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00993}00993\ \ \ \ \ \ \ \ \ \ \ \ \ fun,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00994}00994\ \ \ \ \ \ \ \ \ \ \ \ \ np.zeros((loss.n\_classes)),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00995}00995\ \ \ \ \ \ \ \ \ \ \ \ \ tol=1e-\/13,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00996}00996\ \ \ \ \ \ \ \ \ \ \ \ \ options=\{\textcolor{stringliteral}{"{}maxiter"{}}:\ 100\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00997}00997\ \ \ \ \ \ \ \ \ \ \ \ \ method=\textcolor{stringliteral}{"{}SLSQP"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00998}00998\ \ \ \ \ \ \ \ \ \ \ \ \ constraints=\mbox{\hyperlink{classscipy_1_1optimize_1_1__constraints_1_1LinearConstraint}{LinearConstraint}}(np.ones((1,\ loss.n\_classes)),\ 0,\ 0),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l00999}00999\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01000}01000\ \ \ \ \ \ \ \ \ grad\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01001}01001\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01002}01002\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=np.tile(a,\ (n\_samples,\ 1)),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01003}01003\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01004}01004\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01005}01005\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ a.dtype\ ==\ y\_true.dtype}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01006}01006\ \ \ \ \ \ \ \ \ assert\_all\_finite(a)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01007}01007\ \ \ \ \ \ \ \ \ assert\_allclose(a,\ opt.x,\ rtol=5e-\/6,\ atol=1e-\/12)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01008}01008\ \ \ \ \ \ \ \ \ assert\_allclose(grad.sum(axis=0),\ 0,\ atol=1e-\/12)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01009}01009\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01010}01010\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01011}01011\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01012}01012\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ func,\ random\_dist"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01013}01013\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01014}01014\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}(),\ np.mean,\ \textcolor{stringliteral}{"{}normal"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01015}01015\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError}{AbsoluteError}}(),\ np.median,\ \textcolor{stringliteral}{"{}normal"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01016}01016\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(quantile=0.25),\ \textcolor{keyword}{lambda}\ x:\ np.percentile(x,\ q=25),\ \textcolor{stringliteral}{"{}normal"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01017}01017\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ np.mean,\ \textcolor{stringliteral}{"{}poisson"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01018}01018\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}(),\ np.mean,\ \textcolor{stringliteral}{"{}exponential"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01019}01019\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(),\ np.mean,\ \textcolor{stringliteral}{"{}exponential"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01020}01020\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(),\ np.mean,\ \textcolor{stringliteral}{"{}binomial"{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01021}01021\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01022}01022\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01023}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a83fe0602eb695611eaee9e0cea74d7aa}{01023}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a83fe0602eb695611eaee9e0cea74d7aa}{test\_specific\_fit\_intercept\_only}}(loss,\ func,\ random\_dist,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01024}01024\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ fit\_intercept\_only\ returns\ the\ correct\ functional.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01025}01025\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01026}01026\ \textcolor{stringliteral}{\ \ \ \ We\ test\ the\ functional\ for\ specific,\ meaningful\ distributions,\ e.g.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01027}01027\ \textcolor{stringliteral}{\ \ \ \ squared\ error\ estimates\ the\ expectation\ of\ a\ probability\ distribution.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01028}01028\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01029}01029\ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01030}01030\ \ \ \ \ \textcolor{keywordflow}{if}\ random\_dist\ ==\ \textcolor{stringliteral}{"{}binomial"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01031}01031\ \ \ \ \ \ \ \ \ y\_train\ =\ rng.binomial(1,\ 0.5,\ size=100)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01032}01032\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01033}01033\ \ \ \ \ \ \ \ \ y\_train\ =\ getattr(rng,\ random\_dist)(size=100)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01034}01034\ \ \ \ \ baseline\_prediction\ =\ loss.fit\_intercept\_only(y\_true=y\_train)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01035}01035\ \ \ \ \ \textcolor{comment}{\#\ Make\ sure\ baseline\ prediction\ is\ the\ expected\ functional=func,\ e.g.\ mean}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01036}01036\ \ \ \ \ \textcolor{comment}{\#\ or\ median.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01037}01037\ \ \ \ \ assert\_all\_finite(baseline\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01038}01038\ \ \ \ \ \textcolor{keyword}{assert}\ baseline\_prediction\ ==\ approx(loss.link.link(func(y\_train)))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01039}01039\ \ \ \ \ \textcolor{keyword}{assert}\ loss.link.inverse(baseline\_prediction)\ ==\ approx(func(y\_train))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01040}01040\ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(loss,\ IdentityLink):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01041}01041\ \ \ \ \ \ \ \ \ assert\_allclose(loss.link.inverse(baseline\_prediction),\ baseline\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01042}01042\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01043}01043\ \ \ \ \ \textcolor{comment}{\#\ Test\ baseline\ at\ boundary}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01044}01044\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.low\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01045}01045\ \ \ \ \ \ \ \ \ y\_train.fill(loss.interval\_y\_true.low)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01046}01046\ \ \ \ \ \ \ \ \ baseline\_prediction\ =\ loss.fit\_intercept\_only(y\_true=y\_train)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01047}01047\ \ \ \ \ \ \ \ \ assert\_all\_finite(baseline\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01048}01048\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.interval\_y\_true.high\_inclusive:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01049}01049\ \ \ \ \ \ \ \ \ y\_train.fill(loss.interval\_y\_true.high)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01050}01050\ \ \ \ \ \ \ \ \ baseline\_prediction\ =\ loss.fit\_intercept\_only(y\_true=y\_train)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01051}01051\ \ \ \ \ \ \ \ \ assert\_all\_finite(baseline\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01052}01052\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01053}01053\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01054}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2eeea9ba800414d69a0aae766c08d9a2}{01054}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2eeea9ba800414d69a0aae766c08d9a2}{test\_multinomial\_loss\_fit\_intercept\_only}}():}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01055}01055\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ fit\_intercept\_only\ returns\ the\ mean\ functional\ for\ CCE."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01056}01056\ \ \ \ \ rng\ =\ np.random.RandomState(0)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01057}01057\ \ \ \ \ n\_classes\ =\ 4}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01058}01058\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01059}01059\ \ \ \ \ \textcolor{comment}{\#\ Same\ logic\ as\ test\_specific\_fit\_intercept\_only.\ Here\ inverse\ link}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01060}01060\ \ \ \ \ \textcolor{comment}{\#\ function\ =\ softmax\ and\ link\ function\ =\ log\ -\/\ symmetry\ term.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01061}01061\ \ \ \ \ y\_train\ =\ rng.randint(0,\ n\_classes\ +\ 1,\ size=100).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01062}01062\ \ \ \ \ baseline\_prediction\ =\ loss.fit\_intercept\_only(y\_true=y\_train)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01063}01063\ \ \ \ \ \textcolor{keyword}{assert}\ baseline\_prediction.shape\ ==\ (n\_classes,)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01064}01064\ \ \ \ \ p\ =\ np.zeros(n\_classes,\ dtype=y\_train.dtype)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01065}01065\ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(n\_classes):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01066}01066\ \ \ \ \ \ \ \ \ p[k]\ =\ (y\_train\ ==\ k).mean()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01067}01067\ \ \ \ \ assert\_allclose(baseline\_prediction,\ np.log(p)\ -\/\ np.mean(np.log(p)))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01068}01068\ \ \ \ \ assert\_allclose(baseline\_prediction[\textcolor{keywordtype}{None},\ :],\ loss.link.link(p[\textcolor{keywordtype}{None},\ :]))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01069}01069\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01070}01070\ \ \ \ \ \textcolor{keywordflow}{for}\ y\_train\ \textcolor{keywordflow}{in}\ (np.zeros(shape=10),\ np.ones(shape=10)):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01071}01071\ \ \ \ \ \ \ \ \ y\_train\ =\ y\_train.astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01072}01072\ \ \ \ \ \ \ \ \ baseline\_prediction\ =\ loss.fit\_intercept\_only(y\_true=y\_train)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01073}01073\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ baseline\_prediction.dtype\ ==\ y\_train.dtype}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01074}01074\ \ \ \ \ \ \ \ \ assert\_all\_finite(baseline\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01075}01075\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01076}01076\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01077}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a21382cbbe6025751258b4f95d7ee7bc2}{01077}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a21382cbbe6025751258b4f95d7ee7bc2}{test\_multinomial\_cy\_gradient}}(global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01078}01078\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ Multinomial\ cy\_gradient\ gives\ the\ same\ result\ as\ gradient.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01079}01079\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01080}01080\ \textcolor{stringliteral}{\ \ \ \ CyHalfMultinomialLoss\ does\ not\ inherit\ from\ CyLossFunction\ and\ has\ a\ different\ API.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01081}01081\ \textcolor{stringliteral}{\ \ \ \ As\ a\ consequence,\ the\ functions\ like\ \`{}loss\`{}\ and\ \`{}gradient\`{}\ do\ not\ rely\ on\ \`{}cy\_loss\`{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01082}01082\ \textcolor{stringliteral}{\ \ \ \ and\ \`{}cy\_gradient\`{}.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01083}01083\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01084}01084\ \ \ \ \ n\_samples\ =\ 100}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01085}01085\ \ \ \ \ n\_classes\ =\ 5}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01086}01086\ \ \ \ \ loss\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01087}01087\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01088}01088\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01089}01089\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01090}01090\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01091}01091\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01092}01092\ \ \ \ \ sample\_weight\ =\ np.linspace(0.1,\ 2,\ num=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01093}01093\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01094}01094\ \ \ \ \ grad1\ =\ loss.closs.\_test\_cy\_gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01095}01095\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01096}01096\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,\ \ \textcolor{comment}{\#\ needs\ to\ be\ C-\/contiguous}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01097}01097\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01098}01098\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01099}01099\ \ \ \ \ grad2\ =\ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01100}01100\ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01101}01101\ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01102}01102\ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01103}01103\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01104}01104\ \ \ \ \ assert\_allclose(grad1,\ grad2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01105}01105\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01106}01106\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01107}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a399c5812263096939327a59669033bb1}{01107}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a399c5812263096939327a59669033bb1}{test\_binomial\_and\_multinomial\_loss}}(global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01108}01108\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ multinomial\ loss\ with\ n\_classes\ =\ 2\ is\ the\ same\ as\ binomial\ loss."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01109}01109\ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01110}01110\ \ \ \ \ n\_samples\ =\ 20}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01111}01111\ \ \ \ \ binom\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01112}01112\ \ \ \ \ multinom\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(n\_classes=2)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01113}01113\ \ \ \ \ y\_train\ =\ rng.randint(0,\ 2,\ size=n\_samples).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01114}01114\ \ \ \ \ raw\_prediction\ =\ rng.normal(size=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01115}01115\ \ \ \ \ raw\_multinom\ =\ np.empty((n\_samples,\ 2))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01116}01116\ \ \ \ \ raw\_multinom[:,\ 0]\ =\ -\/0.5\ *\ raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01117}01117\ \ \ \ \ raw\_multinom[:,\ 1]\ =\ 0.5\ *\ raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01118}01118\ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01119}01119\ \ \ \ \ \ \ \ \ binom.loss(y\_true=y\_train,\ raw\_prediction=raw\_prediction),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01120}01120\ \ \ \ \ \ \ \ \ multinom.loss(y\_true=y\_train,\ raw\_prediction=raw\_multinom),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01121}01121\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01122}01122\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01123}01123\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01124}01124\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}y\_true"{},\ (np.array([0.0,\ 0,\ 0])},\ np.array([1.0,\ 1,\ 1])))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01125}01125\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}y\_pred"{},\ (np.array([-\/5.0,\ -\/5,\ -\/5])},\ np.array([3.0,\ 3,\ 3])))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01126}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a6f4d248074ccac0f2fcda24370abf6e9}{01126}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a6f4d248074ccac0f2fcda24370abf6e9}{test\_binomial\_vs\_alternative\_formulation}}(y\_true,\ y\_pred,\ global\_dtype):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01127}01127\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ both\ formulations\ of\ the\ binomial\ deviance\ agree.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01128}01128\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01129}01129\ \textcolor{stringliteral}{\ \ \ \ Often,\ the\ binomial\ deviance\ or\ log\ loss\ is\ written\ in\ terms\ of\ a\ variable}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01130}01130\ \textcolor{stringliteral}{\ \ \ \ z\ in\ \{-\/1,\ +1\},\ but\ we\ use\ y\ in\ \{0,\ 1\},\ hence\ z\ =\ 2\ *\ y\ -\/\ 1.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01131}01131\ \textcolor{stringliteral}{\ \ \ \ ESL\ II\ Eq.\ (10.18):}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01132}01132\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01133}01133\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/loglike(z,\ f)\ =\ log(1\ +\ exp(-\/2\ *\ z\ *\ f))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01134}01134\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01135}01135\ \textcolor{stringliteral}{\ \ \ \ Note:}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01136}01136\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ ESL\ 2*f\ =\ raw\_prediction,\ hence\ the\ factor\ 2\ of\ ESL\ disappears.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01137}01137\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ Deviance\ =\ -\/2*loglike\ +\ ..,\ but\ HalfBinomialLoss\ is\ half\ of\ the}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01138}01138\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ deviance,\ hence\ the\ factor\ of\ 2\ cancels\ in\ the\ comparison.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01139}01139\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01140}01140\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01141}01141\ \ \ \ \ \textcolor{keyword}{def\ }alt\_loss(y,\ raw\_pred):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01142}01142\ \ \ \ \ \ \ \ \ z\ =\ 2\ *\ y\ -\/\ 1}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01143}01143\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.mean(np.log(1\ +\ np.exp(-\/z\ *\ raw\_pred)))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01144}01144\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01145}01145\ \ \ \ \ \textcolor{keyword}{def\ }alt\_gradient(y,\ raw\_pred):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01146}01146\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ alternative\ gradient\ formula\ according\ to\ ESL}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01147}01147\ \ \ \ \ \ \ \ \ z\ =\ 2\ *\ y\ -\/\ 1}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01148}01148\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ -\/z\ /\ (1\ +\ np.exp(z\ *\ raw\_pred))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01149}01149\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01150}01150\ \ \ \ \ bin\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01151}01151\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01152}01152\ \ \ \ \ y\_true\ =\ y\_true.astype(global\_dtype)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01153}01153\ \ \ \ \ y\_pred\ =\ y\_pred.astype(global\_dtype)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01154}01154\ \ \ \ \ datum\ =\ (y\_true,\ y\_pred)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01155}01155\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01156}01156\ \ \ \ \ \textcolor{keyword}{assert}\ bin\_loss(*datum)\ ==\ approx(alt\_loss(*datum))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01157}01157\ \ \ \ \ assert\_allclose(bin\_loss.gradient(*datum),\ alt\_gradient(*datum))}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01158}01158\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01159}01159\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01160}01160\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01161}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a64e4f9e9c3d30c779dc1df184f70c62d}{01161}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a64e4f9e9c3d30c779dc1df184f70c62d}{test\_predict\_proba}}(loss,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01162}01162\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ predict\_proba\ and\ gradient\_proba\ work\ as\ expected."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01163}01163\ \ \ \ \ n\_samples\ =\ 20}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01164}01164\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01165}01165\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01166}01166\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01167}01167\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01168}01168\ \ \ \ \ \ \ \ \ raw\_bound=(-\/5,\ 5),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01169}01169\ \ \ \ \ \ \ \ \ seed=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01170}01170\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01171}01171\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01172}01172\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(loss,\ \textcolor{stringliteral}{"{}predict\_proba"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01173}01173\ \ \ \ \ \ \ \ \ proba\ =\ loss.predict\_proba(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01174}01174\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ proba.shape\ ==\ (n\_samples,\ loss.n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01175}01175\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ np.sum(proba,\ axis=1)\ ==\ approx(1,\ rel=1e-\/11)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01176}01176\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01177}01177\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(loss,\ \textcolor{stringliteral}{"{}gradient\_proba"{}}):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01178}01178\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ grad,\ proba\ \textcolor{keywordflow}{in}\ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01179}01179\ \ \ \ \ \ \ \ \ \ \ \ \ (\textcolor{keywordtype}{None},\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01180}01180\ \ \ \ \ \ \ \ \ \ \ \ \ (\textcolor{keywordtype}{None},\ np.empty\_like(raw\_prediction)),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01181}01181\ \ \ \ \ \ \ \ \ \ \ \ \ (np.empty\_like(raw\_prediction),\ \textcolor{keywordtype}{None}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01182}01182\ \ \ \ \ \ \ \ \ \ \ \ \ (np.empty\_like(raw\_prediction),\ np.empty\_like(raw\_prediction)),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01183}01183\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01184}01184\ \ \ \ \ \ \ \ \ \ \ \ \ grad,\ proba\ =\ loss.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01185}01185\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01186}01186\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01187}01187\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01188}01188\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=grad,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01189}01189\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out=proba,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01190}01190\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01191}01191\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ proba.shape\ ==\ (n\_samples,\ loss.n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01192}01192\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ np.sum(proba,\ axis=1)\ ==\ approx(1,\ rel=1e-\/11)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01193}01193\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01194}01194\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01195}01195\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss.gradient(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01196}01196\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01197}01197\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01198}01198\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01199}01199\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01200}01200\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01201}01201\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01202}01202\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01203}01203\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01204}01204\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ ALL\_LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01205}01205\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ "{}range"{}])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01206}01206\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}dtype"{},\ (np.float32,\ np.float64)})}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01207}01207\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}order"{},\ ("{}C"{},\ "{}F"{})})}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01208}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aed5e6b902447152bb1f9f5d398830d9d}{01208}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aed5e6b902447152bb1f9f5d398830d9d}{test\_init\_gradient\_and\_hessians}}(loss,\ sample\_weight,\ dtype,\ order):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01209}01209\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ init\_gradient\_and\_hessian\ works\ as\ expected.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01210}01210\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01211}01211\ \textcolor{stringliteral}{\ \ \ \ passing\ sample\_weight\ to\ a\ loss\ correctly\ influences\ the\ constant\_hessian}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01212}01212\ \textcolor{stringliteral}{\ \ \ \ attribute,\ and\ consequently\ the\ shape\ of\ the\ hessian\ array.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01213}01213\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01214}01214\ \ \ \ \ n\_samples\ =\ 5}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01215}01215\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ ==\ \textcolor{stringliteral}{"{}range"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01216}01216\ \ \ \ \ \ \ \ \ sample\_weight\ =\ np.ones(n\_samples)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01217}01217\ \ \ \ \ loss\ =\ loss(sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01218}01218\ \ \ \ \ gradient,\ hessian\ =\ loss.init\_gradient\_and\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01219}01219\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01220}01220\ \ \ \ \ \ \ \ \ dtype=dtype,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01221}01221\ \ \ \ \ \ \ \ \ order=order,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01222}01222\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01223}01223\ \ \ \ \ \textcolor{keywordflow}{if}\ loss.constant\_hessian:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01224}01224\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ gradient.shape\ ==\ (n\_samples,)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01225}01225\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.shape\ ==\ (1,)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01226}01226\ \ \ \ \ \textcolor{keywordflow}{elif}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01227}01227\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ gradient.shape\ ==\ (n\_samples,\ loss.n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01228}01228\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.shape\ ==\ (n\_samples,\ loss.n\_classes)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01229}01229\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01230}01230\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.shape\ ==\ (n\_samples,)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01231}01231\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.shape\ ==\ (n\_samples,)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01232}01232\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01233}01233\ \ \ \ \ \textcolor{keyword}{assert}\ gradient.dtype\ ==\ dtype}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01234}01234\ \ \ \ \ \textcolor{keyword}{assert}\ hessian.dtype\ ==\ dtype}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01235}01235\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01236}01236\ \ \ \ \ \textcolor{keywordflow}{if}\ order\ ==\ \textcolor{stringliteral}{"{}C"{}}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01237}01237\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ gradient.flags.c\_contiguous}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01238}01238\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.flags.c\_contiguous}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01239}01239\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01240}01240\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ gradient.flags.f\_contiguous}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01241}01241\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ hessian.flags.f\_contiguous}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01242}01242\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01243}01243\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01244}01244\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ ALL\_LOSSES)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01245}01245\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01246}01246\ \ \ \ \ \textcolor{stringliteral}{"{}params,\ err\_msg"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01247}01247\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01248}01248\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01249}01249\ \ \ \ \ \ \ \ \ \ \ \ \ \{\textcolor{stringliteral}{"{}dtype"{}}:\ np.int64\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01250}01250\ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}Valid\ options\ for\ 'dtype'\ are\ .*\ Got\ dtype=\{np.int64\}\ instead."{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01251}01251\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01252}01252\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01253}01253\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01254}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a225e895b285451d57c5eb2c0f92642de}{01254}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a225e895b285451d57c5eb2c0f92642de}{test\_init\_gradient\_and\_hessian\_raises}}(loss,\ params,\ err\_msg):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01255}01255\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ init\_gradient\_and\_hessian\ raises\ errors\ for\ invalid\ input."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01256}01256\ \ \ \ \ loss\ =\ loss()}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01257}01257\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises((ValueError,\ TypeError),\ match=err\_msg):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01258}01258\ \ \ \ \ \ \ \ \ gradient,\ hessian\ =\ loss.init\_gradient\_and\_hessian(n\_samples=5,\ **params)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01259}01259\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01260}01260\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01261}01261\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01262}01262\ \ \ \ \ \textcolor{stringliteral}{"{}loss,\ params,\ err\_type,\ err\_msg"{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01263}01263\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01264}01264\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01265}01265\ \ \ \ \ \ \ \ \ \ \ \ \ PinballLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01266}01266\ \ \ \ \ \ \ \ \ \ \ \ \ \{\textcolor{stringliteral}{"{}quantile"{}}:\ \textcolor{keywordtype}{None}\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01267}01267\ \ \ \ \ \ \ \ \ \ \ \ \ TypeError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01268}01268\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile\ must\ be\ an\ instance\ of\ float,\ not\ NoneType."{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01269}01269\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01270}01270\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01271}01271\ \ \ \ \ \ \ \ \ \ \ \ \ PinballLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01272}01272\ \ \ \ \ \ \ \ \ \ \ \ \ \{\textcolor{stringliteral}{"{}quantile"{}}:\ 0\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01273}01273\ \ \ \ \ \ \ \ \ \ \ \ \ ValueError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01274}01274\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile\ ==\ 0,\ must\ be\ >\ 0."{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01275}01275\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01276}01276\ \ \ \ \ \ \ \ \ (PinballLoss,\ \{\textcolor{stringliteral}{"{}quantile"{}}:\ 1.1\},\ ValueError,\ \textcolor{stringliteral}{"{}quantile\ ==\ 1.1,\ must\ be\ <\ 1."{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01277}01277\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01278}01278\ \ \ \ \ \ \ \ \ \ \ \ \ HuberLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01279}01279\ \ \ \ \ \ \ \ \ \ \ \ \ \{\textcolor{stringliteral}{"{}quantile"{}}:\ \textcolor{keywordtype}{None}\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01280}01280\ \ \ \ \ \ \ \ \ \ \ \ \ TypeError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01281}01281\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile\ must\ be\ an\ instance\ of\ float,\ not\ NoneType."{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01282}01282\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01283}01283\ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01284}01284\ \ \ \ \ \ \ \ \ \ \ \ \ HuberLoss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01285}01285\ \ \ \ \ \ \ \ \ \ \ \ \ \{\textcolor{stringliteral}{"{}quantile"{}}:\ 0\},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01286}01286\ \ \ \ \ \ \ \ \ \ \ \ \ ValueError,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01287}01287\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile\ ==\ 0,\ must\ be\ >\ 0."{}},}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01288}01288\ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01289}01289\ \ \ \ \ \ \ \ \ (HuberLoss,\ \{\textcolor{stringliteral}{"{}quantile"{}}:\ 1.1\},\ ValueError,\ \textcolor{stringliteral}{"{}quantile\ ==\ 1.1,\ must\ be\ <\ 1."{}}),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01290}01290\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01291}01291\ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01292}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9d437e26e291830a6f7f3b2107f1842}{01292}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9d437e26e291830a6f7f3b2107f1842}{test\_loss\_init\_parameter\_validation}}(loss,\ params,\ err\_type,\ err\_msg):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01293}01293\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ loss\ raises\ errors\ for\ invalid\ input."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01294}01294\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(err\_type,\ match=err\_msg):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01295}01295\ \ \ \ \ \ \ \ \ loss(**params)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01296}01296\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01297}01297\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01298}01298\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}loss"{},\ LOSS\_INSTANCES,\ ids=loss\_instance\_name)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01299}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_afdc7d3349965971783f33103c7debb69}{01299}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_afdc7d3349965971783f33103c7debb69}{test\_loss\_pickle}}(loss):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01300}01300\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ losses\ can\ be\ pickled."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01301}01301\ \ \ \ \ n\_samples\ =\ 20}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01302}01302\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01303}01303\ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01304}01304\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01305}01305\ \ \ \ \ \ \ \ \ y\_bound=(-\/100,\ 100),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01306}01306\ \ \ \ \ \ \ \ \ raw\_bound=(-\/5,\ 5),}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01307}01307\ \ \ \ \ \ \ \ \ seed=42,}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01308}01308\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01309}01309\ \ \ \ \ pickled\_loss\ =\ pickle.dumps(loss)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01310}01310\ \ \ \ \ unpickled\_loss\ =\ pickle.loads(pickled\_loss)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01311}01311\ \ \ \ \ \textcolor{keyword}{assert}\ loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction)\ ==\ approx(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01312}01312\ \ \ \ \ \ \ \ \ unpickled\_loss(y\_true=y\_true,\ raw\_prediction=raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01313}01313\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01314}01314\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01315}01315\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01316}01316\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}p"{},\ [-\/1.5,\ 0,\ 1,\ 1.5,\ 2,\ 3])}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01317}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9bb886015d6009257e9a601b5be95ae}{01317}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9bb886015d6009257e9a601b5be95ae}{test\_tweedie\_log\_identity\_consistency}}(p):}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01318}01318\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ for\ identical\ losses\ when\ only\ the\ link\ function\ is\ different."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01319}01319\ \ \ \ \ half\_tweedie\_log\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=p)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01320}01320\ \ \ \ \ half\_tweedie\_identity\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(power=p)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01321}01321\ \ \ \ \ n\_samples\ =\ 10}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01322}01322\ \ \ \ \ y\_true,\ raw\_prediction\ =\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\_y\_true\_raw\_prediction}}(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01323}01323\ \ \ \ \ \ \ \ \ loss=half\_tweedie\_log,\ n\_samples=n\_samples,\ seed=42}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01324}01324\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01325}01325\ \ \ \ \ y\_pred\ =\ half\_tweedie\_log.link.inverse(raw\_prediction)\ \ \textcolor{comment}{\#\ exp(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01326}01326\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01327}01327\ \ \ \ \ \textcolor{comment}{\#\ Let's\ compare\ the\ loss\ values,\ up\ to\ some\ constant\ term\ that\ is\ dropped}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01328}01328\ \ \ \ \ \textcolor{comment}{\#\ in\ HalfTweedieLoss\ but\ not\ in\ HalfTweedieLossIdentity.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01329}01329\ \ \ \ \ loss\_log\ =\ half\_tweedie\_log.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01330}01330\ \ \ \ \ \ \ \ \ y\_true=y\_true,\ raw\_prediction=raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01331}01331\ \ \ \ \ )\ +\ half\_tweedie\_log.constant\_to\_optimal\_zero(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01332}01332\ \ \ \ \ loss\_identity\ =\ half\_tweedie\_identity.loss(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01333}01333\ \ \ \ \ \ \ \ \ y\_true=y\_true,\ raw\_prediction=y\_pred}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01334}01334\ \ \ \ \ )\ +\ half\_tweedie\_identity.constant\_to\_optimal\_zero(y\_true)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01335}01335\ \ \ \ \ \textcolor{comment}{\#\ Note\ that\ HalfTweedieLoss\ ignores\ different\ constant\ terms\ than}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01336}01336\ \ \ \ \ \textcolor{comment}{\#\ HalfTweedieLossIdentity.\ Constant\ terms\ means\ terms\ not\ depending\ on}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01337}01337\ \ \ \ \ \textcolor{comment}{\#\ raw\_prediction.\ By\ adding\ these\ terms,\ \`{}constant\_to\_optimal\_zero`,\ both\ losses}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01338}01338\ \ \ \ \ \textcolor{comment}{\#\ give\ the\ same\ values.}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01339}01339\ \ \ \ \ assert\_allclose(loss\_log,\ loss\_identity)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01340}01340\ }
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01341}01341\ \ \ \ \ \textcolor{comment}{\#\ For\ gradients\ and\ hessians,\ the\ constant\ terms\ do\ not\ matter.\ We\ have,\ however,}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01342}01342\ \ \ \ \ \textcolor{comment}{\#\ to\ account\ for\ the\ chain\ rule,\ i.e.\ with\ x=raw\_prediction}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01343}01343\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ gradient\_log(x)\ =\ d/dx\ loss\_log(x)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01344}01344\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\ d/dx\ loss\_identity(exp(x))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01345}01345\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\ exp(x)\ *\ gradient\_identity(exp(x))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01346}01346\ \ \ \ \ \textcolor{comment}{\#\ Similarly,}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01347}01347\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ hessian\_log(x)\ =\ exp(x)\ *\ gradient\_identity(exp(x))}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01348}01348\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ exp(x)**2\ *\ hessian\_identity(x)}}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01349}01349\ \ \ \ \ gradient\_log,\ hessian\_log\ =\ half\_tweedie\_log.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01350}01350\ \ \ \ \ \ \ \ \ y\_true=y\_true,\ raw\_prediction=raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01351}01351\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01352}01352\ \ \ \ \ gradient\_identity,\ hessian\_identity\ =\ half\_tweedie\_identity.gradient\_hessian(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01353}01353\ \ \ \ \ \ \ \ \ y\_true=y\_true,\ raw\_prediction=y\_pred}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01354}01354\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01355}01355\ \ \ \ \ assert\_allclose(gradient\_log,\ y\_pred\ *\ gradient\_identity)}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01356}01356\ \ \ \ \ assert\_allclose(}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01357}01357\ \ \ \ \ \ \ \ \ hessian\_log,\ y\_pred\ *\ gradient\_identity\ +\ y\_pred**2\ *\ hessian\_identity}
\DoxyCodeLine{\Hypertarget{test__loss_8py_source_l01358}01358\ \ \ \ \ )}

\end{DoxyCode}
