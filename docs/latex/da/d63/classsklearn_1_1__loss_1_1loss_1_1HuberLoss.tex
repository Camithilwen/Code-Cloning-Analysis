\doxysection{sklearn.\+\_\+loss.\+loss.\+Huber\+Loss Class Reference}
\hypertarget{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{}\label{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}\index{sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}}


Inheritance diagram for sklearn.\+\_\+loss.\+loss.\+Huber\+Loss\+:
% FIG 0


Collaboration diagram for sklearn.\+\_\+loss.\+loss.\+Huber\+Loss\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a21ab1ae5274176d3a942baa30358d5e8}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, sample\+\_\+weight=None, quantile=0.\+9, delta=0.\+5)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01}{fit\+\_\+intercept\+\_\+only}} (self, y\+\_\+true, sample\+\_\+weight=None)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, closs, link, n\+\_\+classes=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{in\+\_\+y\+\_\+true\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\+\_\+y\+\_\+pred\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\+\_\+gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\+\_\+hessian}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, hessian\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{constant\+\_\+to\+\_\+optimal\+\_\+zero}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\+\_\+gradient\+\_\+and\+\_\+hessian}} (self, n\+\_\+samples, dtype=np.\+float64, order="{}F"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a23d23387c19fe941d5bdd690fa3a4c61}{quantile}} = quantile
\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}} = closs
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}} = link
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\+\_\+hessian}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\+\_\+hessian}} = False
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\+\_\+classes}} = n\+\_\+classes
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\+\_\+y\+\_\+true}} = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\+\_\+y\+\_\+pred}} = self.\+link.\+interval\+\_\+y\+\_\+pred
\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}{differentiable}} = \mbox{\hyperlink{classTrue}{True}}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}{need\+\_\+update\+\_\+leaves\+\_\+values}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\+\_\+multiclass}} = False
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Huber loss, for regression.

Domain:
y_true and y_pred all real numbers
quantile in (0, 1)

Link:
y_pred = raw_prediction

For a given sample x_i, the Huber loss is defined as::

    loss(x_i) = 1/2 * abserr**2            if abserr <= delta
                delta * (abserr - delta/2) if abserr > delta

    abserr = |y_true_i - raw_prediction_i|
    delta = quantile(abserr, self.quantile)

Note: HuberLoss(quantile=1) equals HalfSquaredError and HuberLoss(quantile=0)
equals delta * (AbsoluteError() - delta/2).

Additional Attributes
---------------------
quantile : float
    The quantile level which defines the breaking point `delta` to distinguish
    between absolute error and squared error. Must be in range (0, 1).

 Reference
---------
.. [1] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient
  boosting machine <10.1214/aos/1013203451>`.
  Annals of Statistics, 29, 1189-1232.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00643}{643}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a21ab1ae5274176d3a942baa30358d5e8}\index{sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a21ab1ae5274176d3a942baa30358d5e8} 
sklearn.\+\_\+loss.\+loss.\+Huber\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{quantile}{ = {\ttfamily 0.9}, }\item[{}]{delta}{ = {\ttfamily 0.5}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{loss_8py_source_l00680}{680}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01}\index{sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}!fit\_intercept\_only@{fit\_intercept\_only}}
\index{fit\_intercept\_only@{fit\_intercept\_only}!sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}}
\doxysubsubsection{\texorpdfstring{fit\_intercept\_only()}{fit\_intercept\_only()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01} 
sklearn.\+\_\+loss.\+loss.\+Huber\+Loss.\+fit\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute raw_prediction of an intercept-only model.

This is the weighted median of the target, i.e. over the samples
axis=0.
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l00697}{697}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+closs}}.



\doxysubsection{Member Data Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a23d23387c19fe941d5bdd690fa3a4c61}\index{sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}!quantile@{quantile}}
\index{quantile@{quantile}!sklearn.\_loss.loss.HuberLoss@{sklearn.\_loss.loss.HuberLoss}}
\doxysubsubsection{\texorpdfstring{quantile}{quantile}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a23d23387c19fe941d5bdd690fa3a4c61} 
sklearn.\+\_\+loss.\+loss.\+Huber\+Loss.\+quantile = quantile}



Definition at line \mbox{\hyperlink{loss_8py_source_l00689}{689}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{dummy_8py_source_l00543}{sklearn.\+dummy.\+Dummy\+Regressor.\+fit()}}, \mbox{\hyperlink{__quantile_8py_source_l00143}{sklearn.\+linear\+\_\+model.\+\_\+quantile.\+Quantile\+Regressor.\+fit()}}, \mbox{\hyperlink{frame_8py_source_l12062}{pandas.\+core.\+frame.\+Data\+Frame.\+quantile()}}, and \mbox{\hyperlink{gradient__boosting_8py_source_l01824}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Hist\+Gradient\+Boosting\+Regressor.\+staged\+\_\+predict()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/\+\_\+loss/loss.\+py\end{DoxyCompactItemize}
