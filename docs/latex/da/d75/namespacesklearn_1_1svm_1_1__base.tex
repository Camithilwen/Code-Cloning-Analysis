\doxysection{sklearn.\+svm.\+\_\+base Namespace Reference}
\hypertarget{namespacesklearn_1_1svm_1_1__base}{}\label{namespacesklearn_1_1svm_1_1__base}\index{sklearn.svm.\_base@{sklearn.svm.\_base}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1svm_1_1__base_1_1BaseLibSVM}{Base\+Lib\+SVM}}
\item 
class \mbox{\hyperlink{classsklearn_1_1svm_1_1__base_1_1BaseSVC}{Base\+SVC}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1svm_1_1__base_aa06fd7aa93a7c1c101edce53218d69a9}{\+\_\+one\+\_\+vs\+\_\+one\+\_\+coef}} (dual\+\_\+coef, n\+\_\+support, support\+\_\+vectors)
\item 
\mbox{\hyperlink{namespacesklearn_1_1svm_1_1__base_a744f8c0d87a44ff5827c8e9d0b451acd}{\+\_\+get\+\_\+liblinear\+\_\+solver\+\_\+type}} (multi\+\_\+class, penalty, loss, dual)
\item 
\mbox{\hyperlink{namespacesklearn_1_1svm_1_1__base_ab8528bba3e87685ee2351da306eca528}{\+\_\+fit\+\_\+liblinear}} (X, y, \mbox{\hyperlink{classC}{C}}, fit\+\_\+intercept, intercept\+\_\+scaling, class\+\_\+weight, penalty, dual, verbose, max\+\_\+iter, tol, random\+\_\+state=None, multi\+\_\+class="{}ovr"{}, loss="{}logistic\+\_\+regression"{}, epsilon=0.\+1, sample\+\_\+weight=None)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1svm_1_1__base_a0b80d7c5e324f912550f18fd5fa74608}{LIBSVM\+\_\+\+IMPL}} = \mbox{[}"{}c\+\_\+svc"{}, "{}nu\+\_\+svc"{}, "{}one\+\_\+class"{}, "{}epsilon\+\_\+svr"{}, "{}nu\+\_\+svr"{}\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1svm_1_1__base_ab8528bba3e87685ee2351da306eca528}\index{sklearn.svm.\_base@{sklearn.svm.\_base}!\_fit\_liblinear@{\_fit\_liblinear}}
\index{\_fit\_liblinear@{\_fit\_liblinear}!sklearn.svm.\_base@{sklearn.svm.\_base}}
\doxysubsubsection{\texorpdfstring{\_fit\_liblinear()}{\_fit\_liblinear()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1svm_1_1__base_ab8528bba3e87685ee2351da306eca528} 
sklearn.\+svm.\+\_\+base.\+\_\+fit\+\_\+liblinear (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{C}{, }\item[{}]{fit\+\_\+intercept}{, }\item[{}]{intercept\+\_\+scaling}{, }\item[{}]{class\+\_\+weight}{, }\item[{}]{penalty}{, }\item[{}]{dual}{, }\item[{}]{verbose}{, }\item[{}]{max\+\_\+iter}{, }\item[{}]{tol}{, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{multi\+\_\+class}{ = {\ttfamily "{}ovr"{}}, }\item[{}]{loss}{ = {\ttfamily "{}logistic\+\_\+regression"{}}, }\item[{}]{epsilon}{ = {\ttfamily 0.1}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.

Preprocessing is done in this function before supplying it to liblinear.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training vector, where `n_samples` is the number of samples and
    `n_features` is the number of features.

y : array-like of shape (n_samples,)
    Target vector relative to X

C : float
    Inverse of cross-validation parameter. The lower the C, the higher
    the penalization.

fit_intercept : bool
    Whether or not to fit an intercept. If set to True, the feature vector
    is extended to include an intercept term: ``[x_1, ..., x_n, 1]``, where
    1 corresponds to the intercept. If set to False, no intercept will be
    used in calculations (i.e. data is expected to be already centered).

intercept_scaling : float
    Liblinear internally penalizes the intercept, treating it like any
    other term in the feature vector. To reduce the impact of the
    regularization on the intercept, the `intercept_scaling` parameter can
    be set to a value greater than 1; the higher the value of
    `intercept_scaling`, the lower the impact of regularization on it.
    Then, the weights become `[w_x_1, ..., w_x_n,
    w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent
    the feature weights and the intercept weight is scaled by
    `intercept_scaling`. This scaling allows the intercept term to have a
    different regularization behavior compared to the other features.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

penalty : {'l1', 'l2'}
    The norm of the penalty used in regularization.

dual : bool
    Dual or primal formulation,

verbose : int
    Set verbose to any positive number for verbosity.

max_iter : int
    Number of iterations.

tol : float
    Stopping condition.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

multi_class : {'ovr', 'crammer_singer'}, default='ovr'
    `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`
    optimizes a joint objective over all classes.
    While `crammer_singer` is interesting from an theoretical perspective
    as it is consistent it is seldom used in practice and rarely leads to
    better accuracy and is more expensive to compute.
    If `crammer_singer` is chosen, the options loss, penalty and dual will
    be ignored.

loss : {'logistic_regression', 'hinge', 'squared_hinge', \
        'epsilon_insensitive', 'squared_epsilon_insensitive}, \
        default='logistic_regression'
    The loss function used to fit the model.

epsilon : float, default=0.1
    Epsilon parameter in the epsilon-insensitive loss function. Note
    that the value of this parameter depends on the scale of the target
    variable y. If unsure, set epsilon=0.

sample_weight : array-like of shape (n_samples,), default=None
    Weights assigned to each sample.

Returns
-------
coef_ : ndarray of shape (n_features, n_features + 1)
    The coefficient vector got by minimizing the objective function.

intercept_ : float
    The intercept term added to the vector.

n_iter_ : array of int
    Number of iterations run across for each class.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l01066}{1066}} of file \mbox{\hyperlink{sklearn_2svm_2__base_8py_source}{\+\_\+base.\+py}}.



References \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l01009}{\+\_\+get\+\_\+liblinear\+\_\+solver\+\_\+type()}}.

\Hypertarget{namespacesklearn_1_1svm_1_1__base_a744f8c0d87a44ff5827c8e9d0b451acd}\index{sklearn.svm.\_base@{sklearn.svm.\_base}!\_get\_liblinear\_solver\_type@{\_get\_liblinear\_solver\_type}}
\index{\_get\_liblinear\_solver\_type@{\_get\_liblinear\_solver\_type}!sklearn.svm.\_base@{sklearn.svm.\_base}}
\doxysubsubsection{\texorpdfstring{\_get\_liblinear\_solver\_type()}{\_get\_liblinear\_solver\_type()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1svm_1_1__base_a744f8c0d87a44ff5827c8e9d0b451acd} 
sklearn.\+svm.\+\_\+base.\+\_\+get\+\_\+liblinear\+\_\+solver\+\_\+type (\begin{DoxyParamCaption}\item[{}]{multi\+\_\+class}{, }\item[{}]{penalty}{, }\item[{}]{loss}{, }\item[{}]{dual}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Find the liblinear magic number for the solver.

This number depends on the values of the following attributes:
  - multi_class
  - penalty
  - loss
  - dual

The same number is also internally used by LibLinear to determine
which solver to use.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l01009}{1009}} of file \mbox{\hyperlink{sklearn_2svm_2__base_8py_source}{\+\_\+base.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l01083}{\+\_\+fit\+\_\+liblinear()}}.

\Hypertarget{namespacesklearn_1_1svm_1_1__base_aa06fd7aa93a7c1c101edce53218d69a9}\index{sklearn.svm.\_base@{sklearn.svm.\_base}!\_one\_vs\_one\_coef@{\_one\_vs\_one\_coef}}
\index{\_one\_vs\_one\_coef@{\_one\_vs\_one\_coef}!sklearn.svm.\_base@{sklearn.svm.\_base}}
\doxysubsubsection{\texorpdfstring{\_one\_vs\_one\_coef()}{\_one\_vs\_one\_coef()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1svm_1_1__base_aa06fd7aa93a7c1c101edce53218d69a9} 
sklearn.\+svm.\+\_\+base.\+\_\+one\+\_\+vs\+\_\+one\+\_\+coef (\begin{DoxyParamCaption}\item[{}]{dual\+\_\+coef}{, }\item[{}]{n\+\_\+support}{, }\item[{}]{support\+\_\+vectors}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Generate primal coefficients from dual coefficients
for the one-vs-one multi class LibSVM in the case
of a linear kernel.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00037}{37}} of file \mbox{\hyperlink{sklearn_2svm_2__base_8py_source}{\+\_\+base.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00876}{sklearn.\+svm.\+\_\+base.\+Base\+SVC.\+predict\+\_\+log\+\_\+proba()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1svm_1_1__base_a0b80d7c5e324f912550f18fd5fa74608}\index{sklearn.svm.\_base@{sklearn.svm.\_base}!LIBSVM\_IMPL@{LIBSVM\_IMPL}}
\index{LIBSVM\_IMPL@{LIBSVM\_IMPL}!sklearn.svm.\_base@{sklearn.svm.\_base}}
\doxysubsubsection{\texorpdfstring{LIBSVM\_IMPL}{LIBSVM\_IMPL}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1svm_1_1__base_a0b80d7c5e324f912550f18fd5fa74608} 
list sklearn.\+svm.\+\_\+base.\+LIBSVM\+\_\+\+IMPL = \mbox{[}"{}c\+\_\+svc"{}, "{}nu\+\_\+svc"{}, "{}one\+\_\+class"{}, "{}epsilon\+\_\+svr"{}, "{}nu\+\_\+svr"{}\mbox{]}}



Definition at line \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00034}{34}} of file \mbox{\hyperlink{sklearn_2svm_2__base_8py_source}{\+\_\+base.\+py}}.

