\doxysection{sklearn.\+metrics.\+\_\+classification Namespace Reference}
\hypertarget{namespacesklearn_1_1metrics_1_1__classification}{}\label{namespacesklearn_1_1metrics_1_1__classification}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a346ed1eea7dc119562be2c93704eb299}{\+\_\+check\+\_\+zero\+\_\+division}} (zero\+\_\+division)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_ab052693e5a75a63b200115e2c3949926}{\+\_\+check\+\_\+targets}} (y\+\_\+true, y\+\_\+pred)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a9e8a783c3bb698a466cf5f0f2b4d39b2}{\+\_\+validate\+\_\+multiclass\+\_\+probabilistic\+\_\+prediction}} (y\+\_\+true, y\+\_\+prob, sample\+\_\+weight, labels)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a14760381108ce762d9681e2878d4da3b}{accuracy\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, normalize=\mbox{\hyperlink{classTrue}{True}}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a9bdecf060bbdd6ebe26f442d9d3f1382}{confusion\+\_\+matrix}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, sample\+\_\+weight=None, normalize=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a441d970c6097ed3b4af7dbb919e53813}{multilabel\+\_\+confusion\+\_\+matrix}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, labels=None, samplewise=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a7430b47501eea60d89bf4abec2eef197}{cohen\+\_\+kappa\+\_\+score}} (y1, y2, \texorpdfstring{$\ast$}{*}, labels=None, weights=None, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_aba21abda11d69893f8ae3727cb64472f}{jaccard\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, pos\+\_\+label=1, average="{}binary"{}, sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a68e73cd770f722164235c466adbf5a0c}{matthews\+\_\+corrcoef}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_adf01048251d2b41d3fb5ee881300fc73}{zero\+\_\+one\+\_\+loss}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, normalize=\mbox{\hyperlink{classTrue}{True}}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a5d31c24f718a99decc9441f1e82a7329}{f1\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, pos\+\_\+label=1, average="{}binary"{}, sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_ada551803819b8de72f08d36b10c2775b}{fbeta\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, beta, labels=None, pos\+\_\+label=1, average="{}binary"{}, sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a0aebe969f1ab1fec83528c39979d699a}{\+\_\+prf\+\_\+divide}} (numerator, denominator, metric, modifier, average, warn\+\_\+for, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a0b2a6e4f3795b2fbc6aaa89025757de0}{\+\_\+warn\+\_\+prf}} (average, modifier, msg\+\_\+start, result\+\_\+size)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a3c8f32774ecf6aeebfa0b0b44cd33ec3}{\+\_\+check\+\_\+set\+\_\+wise\+\_\+labels}} (y\+\_\+true, y\+\_\+pred, average, labels, pos\+\_\+label)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a15142b0e8df592903125d73338098b62}{precision\+\_\+recall\+\_\+fscore\+\_\+support}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, beta=1.\+0, labels=None, pos\+\_\+label=1, average=None, warn\+\_\+for=("{}precision"{}, "{}recall"{}, "{}f-\/score"{}), sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a09b445b22ee35994b69bb8f2b0c3fcea}{class\+\_\+likelihood\+\_\+ratios}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, sample\+\_\+weight=None, raise\+\_\+warning="{}deprecated"{}, replace\+\_\+undefined\+\_\+by=np.\+nan)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_aa7da5219ebca6807f6a19706be6f05ba}{precision\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, pos\+\_\+label=1, average="{}binary"{}, sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a5557c71c9e924d3e1d9e3679d1e0e710}{recall\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, pos\+\_\+label=1, average="{}binary"{}, sample\+\_\+weight=None, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a78df17d784f34f119fa6ccfe6ce72d2b}{balanced\+\_\+accuracy\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, adjusted=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_aa3d62594d97dfc77db45e359f91ba6b7}{classification\+\_\+report}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, labels=None, target\+\_\+names=None, sample\+\_\+weight=None, digits=2, output\+\_\+dict=False, zero\+\_\+division="{}warn"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a1726217c03db123e1bbc0ae220edabe4}{hamming\+\_\+loss}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a04f7198fff7b380c1a136c9d69c883ef}{log\+\_\+loss}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, normalize=\mbox{\hyperlink{classTrue}{True}}, sample\+\_\+weight=None, labels=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a8b103f5754e2cfae479b2d92e59f53fd}{hinge\+\_\+loss}} (y\+\_\+true, pred\+\_\+decision, \texorpdfstring{$\ast$}{*}, labels=None, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a31b149c6b885fafbe21f5de6871f4070}{\+\_\+validate\+\_\+binary\+\_\+probabilistic\+\_\+prediction}} (y\+\_\+true, y\+\_\+prob, sample\+\_\+weight, pos\+\_\+label)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a223bfbbb1d1efa0ad89868253dee9c7e}{brier\+\_\+score\+\_\+loss}} (y\+\_\+true, y\+\_\+proba, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, pos\+\_\+label=None, labels=None, scale\+\_\+by\+\_\+half="{}auto"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__classification_a457f569cc9a5bb6f2172dde936d3be7f}{d2\+\_\+log\+\_\+loss\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, labels=None)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Metrics to assess performance on classification task given class prediction.

Functions named as ``*_score`` return a scalar value to maximize: the higher
the better.

Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:
the lower the better.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a3c8f32774ecf6aeebfa0b0b44cd33ec3}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_check\_set\_wise\_labels@{\_check\_set\_wise\_labels}}
\index{\_check\_set\_wise\_labels@{\_check\_set\_wise\_labels}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_check\_set\_wise\_labels()}{\_check\_set\_wise\_labels()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a3c8f32774ecf6aeebfa0b0b44cd33ec3} 
sklearn.\+metrics.\+\_\+classification.\+\_\+check\+\_\+set\+\_\+wise\+\_\+labels (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{}]{average}{, }\item[{}]{labels}{, }\item[{}]{pos\+\_\+label}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Validation associated with set-wise metrics.

Returns identified labels.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01752}{1752}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l00928}{jaccard\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_ab052693e5a75a63b200115e2c3949926}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_check\_targets@{\_check\_targets}}
\index{\_check\_targets@{\_check\_targets}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_check\_targets()}{\_check\_targets()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_ab052693e5a75a63b200115e2c3949926} 
sklearn.\+metrics.\+\_\+classification.\+\_\+check\+\_\+targets (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Check that y_true and y_pred belong to the same classification task.

This converts multiclass or binary types to a common shape, and raises a
ValueError for a mix of multilabel and multiclass targets, a mix of
multilabel formats, for the presence of continuous-valued or multioutput
targets, or for targets of different lengths.

Column vectors are squeezed to 1d, while multilabel formats are returned
as CSR sparse label indicators.

Parameters
----------
y_true : array-like

y_pred : array-like

Returns
-------
type_true : one of {'multilabel-indicator', 'multiclass', 'binary'}
    The type of the true target data, as output by
    ``utils.multiclass.type_of_target``.

y_true : array or indicator matrix

y_pred : array or indicator matrix
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{69}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l01752}{\+\_\+check\+\_\+set\+\_\+wise\+\_\+labels()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l00296}{accuracy\+\_\+score()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l02095}{class\+\_\+likelihood\+\_\+ratios()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l02839}{classification\+\_\+report()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l00383}{confusion\+\_\+matrix()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l03061}{hamming\+\_\+loss()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l01110}{matthews\+\_\+corrcoef()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l00558}{multilabel\+\_\+confusion\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a346ed1eea7dc119562be2c93704eb299}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_check\_zero\_division@{\_check\_zero\_division}}
\index{\_check\_zero\_division@{\_check\_zero\_division}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_check\_zero\_division()}{\_check\_zero\_division()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a346ed1eea7dc119562be2c93704eb299} 
sklearn.\+metrics.\+\_\+classification.\+\_\+check\+\_\+zero\+\_\+division (\begin{DoxyParamCaption}\item[{}]{zero\+\_\+division}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00060}{60}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a0aebe969f1ab1fec83528c39979d699a}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_prf\_divide@{\_prf\_divide}}
\index{\_prf\_divide@{\_prf\_divide}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_prf\_divide()}{\_prf\_divide()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a0aebe969f1ab1fec83528c39979d699a} 
sklearn.\+metrics.\+\_\+classification.\+\_\+prf\+\_\+divide (\begin{DoxyParamCaption}\item[{}]{numerator}{, }\item[{}]{denominator}{, }\item[{}]{metric}{, }\item[{}]{modifier}{, }\item[{}]{average}{, }\item[{}]{warn\+\_\+for}{, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Performs division and handles divide-by-zero.

On zero-division, sets the corresponding result elements equal to
0, 1 or np.nan (according to ``zero_division``). Plus, if
``zero_division != "warn"`` raises a warning.

The metric, modifier and average arguments are used only for determining
an appropriate warning.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01697}{1697}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l00928}{jaccard\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a31b149c6b885fafbe21f5de6871f4070}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_validate\_binary\_probabilistic\_prediction@{\_validate\_binary\_probabilistic\_prediction}}
\index{\_validate\_binary\_probabilistic\_prediction@{\_validate\_binary\_probabilistic\_prediction}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_validate\_binary\_probabilistic\_prediction()}{\_validate\_binary\_probabilistic\_prediction()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a31b149c6b885fafbe21f5de6871f4070} 
sklearn.\+metrics.\+\_\+classification.\+\_\+validate\+\_\+binary\+\_\+probabilistic\+\_\+prediction (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+prob}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{pos\+\_\+label}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Convert y_true and y_prob in binary classification to shape (n_samples, 2)

Parameters
----------
y_true : array-like of shape (n_samples,)
    True labels.

y_prob : array-like of shape (n_samples,)
    Probabilities of the positive class.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

pos_label : int, float, bool or str, default=None
    Label of the positive class. If None, `pos_label` will be inferred
    in the following manner:

    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
    * else if `y_true` contains string, an error will be raised and
      `pos_label` should be explicitly specified;
    * otherwise, `pos_label` defaults to the greater label,
      i.e. `np.unique(y_true)[-1]`.

Returns
-------
transformed_labels : array of shape (n_samples, 2)

y_prob : array of shape (n_samples, 2)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03405}{3405}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l03495}{brier\+\_\+score\+\_\+loss()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a9e8a783c3bb698a466cf5f0f2b4d39b2}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_validate\_multiclass\_probabilistic\_prediction@{\_validate\_multiclass\_probabilistic\_prediction}}
\index{\_validate\_multiclass\_probabilistic\_prediction@{\_validate\_multiclass\_probabilistic\_prediction}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_validate\_multiclass\_probabilistic\_prediction()}{\_validate\_multiclass\_probabilistic\_prediction()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a9e8a783c3bb698a466cf5f0f2b4d39b2} 
sklearn.\+metrics.\+\_\+classification.\+\_\+validate\+\_\+multiclass\+\_\+probabilistic\+\_\+prediction (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+prob}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{labels}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Convert y_true and y_prob to shape (n_samples, n_classes)

1. Verify that y_true, y_prob, and sample_weights have the same first dim
2. Ensure 2 or more classes in y_true i.e. valid classification task. The
classes are provided by the labels argument, or inferred using y_true.
When inferring y_true is assumed binary if it has shape (n_samples, ).
3. Validate y_true, and y_prob have the same number of classes. Convert to
shape (n_samples, n_classes)

Parameters
----------
y_true : array-like or label indicator matrix
Ground truth (correct) labels for n_samples samples.

y_prob : array-like of float, shape=(n_samples, n_classes) or (n_samples,)
Predicted probabilities, as returned by a classifier's
predict_proba method. If `y_prob.shape = (n_samples,)`
the probabilities provided are assumed to be that of the
positive class. The labels in `y_prob` are assumed to be
ordered lexicographically, as done by
:class:`preprocessing.LabelBinarizer`.

sample_weight : array-like of shape (n_samples,), default=None
Sample weights.

labels : array-like, default=None
If not provided, labels will be inferred from y_true. If `labels`
is `None` and `y_prob` has shape `(n_samples,)` the labels are
assumed to be binary and are inferred from `y_true`.

Returns
-------
transformed_labels : array of shape (n_samples, n_classes)

y_prob : array of shape (n_samples, n_classes)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00154}{154}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l03495}{brier\+\_\+score\+\_\+loss()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l03172}{log\+\_\+loss()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a0b2a6e4f3795b2fbc6aaa89025757de0}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!\_warn\_prf@{\_warn\_prf}}
\index{\_warn\_prf@{\_warn\_prf}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{\_warn\_prf()}{\_warn\_prf()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a0b2a6e4f3795b2fbc6aaa89025757de0} 
sklearn.\+metrics.\+\_\+classification.\+\_\+warn\+\_\+prf (\begin{DoxyParamCaption}\item[{}]{average}{, }\item[{}]{modifier}{, }\item[{}]{msg\+\_\+start}{, }\item[{}]{result\+\_\+size}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01736}{1736}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a14760381108ce762d9681e2878d4da3b}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!accuracy\_score@{accuracy\_score}}
\index{accuracy\_score@{accuracy\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{accuracy\_score()}{accuracy\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a14760381108ce762d9681e2878d4da3b} 
sklearn.\+metrics.\+\_\+classification.\+accuracy\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{normalize}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Accuracy classification score.

In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must *exactly* match the
corresponding set of labels in y_true.

Read more in the :ref:`User Guide <accuracy_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of correctly classified samples.
    Otherwise, return the fraction of correctly classified samples.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
score : float or int
    If ``normalize == True``, return the fraction of correctly
    classified samples (float), else returns the number of correctly
    classified samples (int).

    The best performance is 1 with ``normalize == True`` and the number
    of samples with ``normalize == False``.

See Also
--------
balanced_accuracy_score : Compute the balanced accuracy to deal with
    imbalanced datasets.
jaccard_score : Compute the Jaccard similarity coefficient score.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Examples
--------
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00296}{296}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l01209}{zero\+\_\+one\+\_\+loss()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a78df17d784f34f119fa6ccfe6ce72d2b}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!balanced\_accuracy\_score@{balanced\_accuracy\_score}}
\index{balanced\_accuracy\_score@{balanced\_accuracy\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{balanced\_accuracy\_score()}{balanced\_accuracy\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a78df17d784f34f119fa6ccfe6ce72d2b} 
sklearn.\+metrics.\+\_\+classification.\+balanced\+\_\+accuracy\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{adjusted}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the balanced accuracy.

The balanced accuracy in binary and multiclass classification problems to
deal with imbalanced datasets. It is defined as the average of recall
obtained on each class.

The best value is 1 and the worst value is 0 when ``adjusted=False``.

Read more in the :ref:`User Guide <balanced_accuracy_score>`.

.. versionadded:: 0.20

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

adjusted : bool, default=False
    When true, the result is adjusted for chance, so that random
    performance would score 0, while keeping perfect performance at a score
    of 1.

Returns
-------
balanced_accuracy : float
    Balanced accuracy score.

See Also
--------
average_precision_score : Compute average precision (AP) from prediction
    scores.
precision_score : Compute the precision score.
recall_score : Compute the recall score.
roc_auc_score : Compute Area Under the Receiver Operating Characteristic
    Curve (ROC AUC) from prediction scores.

Notes
-----
Some literature promotes alternative definitions of balanced accuracy. Our
definition is equivalent to :func:`accuracy_score` with class-balanced
sample weights, and shares desirable properties with the binary case.
See the :ref:`User Guide <balanced_accuracy_score>`.

References
----------
.. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
       The balanced accuracy and its posterior distribution.
       Proceedings of the 20th International Conference on Pattern
       Recognition, 3121-24.
.. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).
       `Fundamentals of Machine Learning for Predictive Data Analytics:
       Algorithms, Worked Examples, and Case Studies
       <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.

Examples
--------
>>> from sklearn.metrics import balanced_accuracy_score
>>> y_true = [0, 1, 0, 0, 1, 0]
>>> y_pred = [0, 1, 0, 0, 0, 1]
>>> balanced_accuracy_score(y_true, y_pred)
0.625
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l02728}{2728}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a223bfbbb1d1efa0ad89868253dee9c7e}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!brier\_score\_loss@{brier\_score\_loss}}
\index{brier\_score\_loss@{brier\_score\_loss}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{brier\_score\_loss()}{brier\_score\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a223bfbbb1d1efa0ad89868253dee9c7e} 
sklearn.\+metrics.\+\_\+classification.\+brier\+\_\+score\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+proba}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily None}, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{scale\+\_\+by\+\_\+half}{ = {\ttfamily "{}auto"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the Brier score loss.

The smaller the Brier score loss, the better, hence the naming with "loss".
The Brier score measures the mean squared difference between the predicted
probability and the actual outcome. The Brier score is a strictly proper scoring
rule.

Read more in the :ref:`User Guide <brier_score_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True targets.

y_proba : array-like of shape (n_samples,) or (n_samples, n_classes)
    Predicted probabilities. If `y_proba.shape = (n_samples,)`
    the probabilities provided are assumed to be that of the
    positive class. If `y_proba.shape = (n_samples, n_classes)`
    the columns in `y_proba` are assumed to correspond to the
    labels in alphabetical order, as done by
    :class:`~sklearn.preprocessing.LabelBinarizer`.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

pos_label : int, float, bool or str, default=None
    Label of the positive class when `y_proba.shape = (n_samples,)`.
    If not provided, `pos_label` will be inferred in the
    following manner:

    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
    * else if `y_true` contains string, an error will be raised and
      `pos_label` should be explicitly specified;
    * otherwise, `pos_label` defaults to the greater label,
      i.e. `np.unique(y_true)[-1]`.

labels : array-like of shape (n_classes,), default=None
    Class labels when `y_proba.shape = (n_samples, n_classes)`.
    If not provided, labels will be inferred from `y_true`.

    .. versionadded:: 1.7

scale_by_half : bool or "auto", default="auto"
    When True, scale the Brier score by 1/2 to lie in the [0, 1] range instead
    of the [0, 2] range. The default "auto" option implements the rescaling to
    [0, 1] only for binary classification (as customary) but keeps the
    original [0, 2] range for multiclass classification.

    .. versionadded:: 1.7

Returns
-------
score : float
    Brier score loss.

Notes
-----

For :math:`N` observations labeled from :math:`C` possible classes, the Brier
score is defined as:

.. math::
    \frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}(y_{ic} - \hat{p}_{ic})^{2}

where :math:`y_{ic}` is 1 if observation `i` belongs to class `c`,
otherwise 0 and :math:`\hat{p}_{ic}` is the predicted probability for
observation `i` to belong to class `c`.
The Brier score then ranges between :math:`[0, 2]`.

In binary classification tasks the Brier score is usually divided by
two and then ranges between :math:`[0, 1]`. It can be alternatively
written as:

.. math::
    \frac{1}{N}\sum_{i=1}^{N}(y_{i} - \hat{p}_{i})^{2}

where :math:`y_{i}` is the binary target and :math:`\hat{p}_{i}`
is the predicted probability of the positive class.

References
----------
.. [1] `Wikipedia entry for the Brier score
        <https://en.wikipedia.org/wiki/Brier_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])
>>> brier_score_loss(y_true, y_prob)
0.0375
>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.0375
>>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
0.0375
>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0
>>> brier_score_loss(y_true, y_prob, scale_by_half=False)
0.075
>>> brier_score_loss(
...    ["eggs", "ham", "spam"],
...    [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]],
...    labels=["eggs", "ham", "spam"]
... )
0.146
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03487}{3487}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l03405}{\+\_\+validate\+\_\+binary\+\_\+probabilistic\+\_\+prediction()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l00156}{\+\_\+validate\+\_\+multiclass\+\_\+probabilistic\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a09b445b22ee35994b69bb8f2b0c3fcea}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!class\_likelihood\_ratios@{class\_likelihood\_ratios}}
\index{class\_likelihood\_ratios@{class\_likelihood\_ratios}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{class\_likelihood\_ratios()}{class\_likelihood\_ratios()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a09b445b22ee35994b69bb8f2b0c3fcea} 
sklearn.\+metrics.\+\_\+classification.\+class\+\_\+likelihood\+\_\+ratios (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{raise\+\_\+warning}{ = {\ttfamily "{}deprecated"{}}, }\item[{}]{replace\+\_\+undefined\+\_\+by}{ = {\ttfamily np.nan}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute binary classification positive and negative likelihood ratios.

The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`
where the sensitivity or recall is the ratio `tp / (tp + fn)` and the
specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1
- sensitivity) / specificity`. Here `tp` is the number of true positives,
`fp` the number of false positives, `tn` is the number of true negatives and
`fn` the number of false negatives. Both class likelihood ratios can be used
to obtain post-test probabilities given a pre-test probability.

`LR+` ranges from 1.0 to infinity. A `LR+` of 1.0 indicates that the probability
of predicting the positive class is the same for samples belonging to either
class; therefore, the test is useless. The greater `LR+` is, the more a
positive prediction is likely to be a true positive when compared with the
pre-test probability. A value of `LR+` lower than 1.0 is invalid as it would
indicate that the odds of a sample being a true positive decrease with
respect to the pre-test odds.

`LR-` ranges from 0.0 to 1.0. The closer it is to 0.0, the lower the probability
of a given sample to be a false negative. A `LR-` of 1.0 means the test is
useless because the odds of having the condition did not change after the
test. A value of `LR-` greater than 1.0 invalidates the classifier as it
indicates an increase in the odds of a sample belonging to the positive
class after being classified as negative. This is the case when the
classifier systematically predicts the opposite of the true label.

A typical application in medicine is to identify the positive/negative class
to the presence/absence of a disease, respectively; the classifier being a
diagnostic test; the pre-test probability of an individual having the
disease can be the prevalence of such disease (proportion of a particular
population found to be affected by a medical condition); and the post-test
probabilities would be the probability that the condition is truly present
given a positive test result.

Read more in the :ref:`User Guide <class_likelihood_ratios>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    List of labels to index the matrix. This may be used to select the
    positive and negative classes with the ordering `labels=[negative_class,
    positive_class]`. If `None` is given, those that appear at least once in
    `y_true` or `y_pred` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

raise_warning : bool, default=True
    Whether or not a case-specific warning message is raised when there is division
    by zero.

    .. deprecated:: 1.7
        `raise_warning` was deprecated in version 1.7 and will be removed in 1.9,
        when an :class:`~sklearn.exceptions.UndefinedMetricWarning` will always
        raise in case of a division by zero.

replace_undefined_by : np.nan, 1.0, or dict, default=np.nan
    Sets the return values for LR+ and LR- when there is a division by zero. Can
    take the following values:

    - `np.nan` to return `np.nan` for both `LR+` and `LR-`
    - `1.0` to return the worst possible scores: `{"LR+": 1.0, "LR-": 1.0}`
    - a dict in the format `{"LR+": value_1, "LR-": value_2}` where the values can
      be non-negative floats, `np.inf` or `np.nan` in the range of the
      likelihood ratios. For example, `{"LR+": 1.0, "LR-": 1.0}` can be used for
      returning the worst scores, indicating a useless model, and `{"LR+": np.inf,
      "LR-": 0.0}` can be used for returning the best scores, indicating a useful
      model.

    If a division by zero occurs, only the affected metric is replaced with the set
    value; the other metric is calculated as usual.

    .. versionadded:: 1.7

Returns
-------
(positive_likelihood_ratio, negative_likelihood_ratio) : tuple
    A tuple of two floats, the first containing the positive likelihood ratio (LR+)
    and the second the negative likelihood ratio (LR-).

Warns
-----
Raises :class:`~sklearn.exceptions.UndefinedMetricWarning` when `y_true` and
`y_pred` lead to the following conditions:

    - The number of false positives is 0 and `raise_warning` is set to `True`
      (default): positive likelihood ratio is undefined.
    - The number of true negatives is 0 and `raise_warning` is set to `True`
      (default): negative likelihood ratio is undefined.
    - The sum of true positives and false negatives is 0 (no samples of the positive
      class are present in `y_true`): both likelihood ratios are undefined.

    For the first two cases, an undefined metric can be defined by setting the
    `replace_undefined_by` param.

References
----------
.. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing
       <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import class_likelihood_ratios
>>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])
(1.5, 0.75)
>>> y_true = np.array(["non-cat", "cat", "non-cat", "cat", "non-cat"])
>>> y_pred = np.array(["cat", "cat", "non-cat", "non-cat", "non-cat"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.33, 0.66)
>>> y_true = np.array(["non-zebra", "zebra", "non-zebra", "zebra", "non-zebra"])
>>> y_pred = np.array(["zebra", "zebra", "non-zebra", "non-zebra", "non-zebra"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.5, 0.75)

To avoid ambiguities, use the notation `labels=[negative_class,
positive_class]`

>>> y_true = np.array(["non-cat", "cat", "non-cat", "cat", "non-cat"])
>>> y_pred = np.array(["cat", "cat", "non-cat", "non-cat", "non-cat"])
>>> class_likelihood_ratios(y_true, y_pred, labels=["non-cat", "cat"])
(1.5, 0.75)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l02087}{2087}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_aa3d62594d97dfc77db45e359f91ba6b7}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!classification\_report@{classification\_report}}
\index{classification\_report@{classification\_report}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{classification\_report()}{classification\_report()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_aa3d62594d97dfc77db45e359f91ba6b7} 
sklearn.\+metrics.\+\_\+classification.\+classification\+\_\+report (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{target\+\_\+names}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{digits}{ = {\ttfamily 2}, }\item[{}]{output\+\_\+dict}{ = {\ttfamily False}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Build a text report showing the main classification metrics.

Read more in the :ref:`User Guide <classification_report>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_labels,), default=None
    Optional list of label indices to include in the report.

target_names : array-like of shape (n_labels,), default=None
    Optional display names matching the labels (same order).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

digits : int, default=2
    Number of digits for formatting output floating point values.
    When ``output_dict`` is ``True``, this will be ignored and the
    returned values will not be rounded.

output_dict : bool, default=False
    If True, return output as dict.

    .. versionadded:: 0.20

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division. If set to
    "warn", this acts as 0, but warnings are also raised.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
report : str or dict
    Text summary of the precision, recall, F1 score for each class.
    Dictionary returned if output_dict is True. Dictionary has the
    following structure::

        {'label 1': {'precision':0.5,
                     'recall':1.0,
                     'f1-score':0.67,
                     'support':1},
         'label 2': { ... },
          ...
        }

    The reported averages include macro average (averaging the unweighted
    mean per label), weighted average (averaging the support-weighted mean
    per label), and sample average (only for multilabel classification).
    Micro average (averaging the total true positives, false negatives and
    false positives) is only shown for multi-label or multi-class
    with a subset of classes, because it corresponds to accuracy
    otherwise and would be the same for all metrics.
    See also :func:`precision_recall_fscore_support` for more details
    on averages.

    Note that in binary classification, recall of the positive class
    is also known as "sensitivity"; recall of the negative class is
    "specificity".

See Also
--------
precision_recall_fscore_support: Compute precision, recall, F-measure and
    support for each class.
confusion_matrix: Compute confusion matrix to evaluate the accuracy of a
    classification.
multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.

Examples
--------
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 2]
>>> y_pred = [0, 0, 2, 2, 1]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
              precision    recall  f1-score   support
<BLANKLINE>
     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3
<BLANKLINE>
    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
<BLANKLINE>
>>> y_pred = [1, 1, 0]
>>> y_true = [1, 1, 1]
>>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))
              precision    recall  f1-score   support
<BLANKLINE>
           1       1.00      0.67      0.80         3
           2       0.00      0.00      0.00         0
           3       0.00      0.00      0.00         0
<BLANKLINE>
   micro avg       1.00      0.67      0.80         3
   macro avg       0.33      0.22      0.27         3
weighted avg       1.00      0.67      0.80         3
<BLANKLINE>
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l02829}{2829}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a7430b47501eea60d89bf4abec2eef197}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!cohen\_kappa\_score@{cohen\_kappa\_score}}
\index{cohen\_kappa\_score@{cohen\_kappa\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{cohen\_kappa\_score()}{cohen\_kappa\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a7430b47501eea60d89bf4abec2eef197} 
sklearn.\+metrics.\+\_\+classification.\+cohen\+\_\+kappa\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y1}{, }\item[{}]{y2}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{weights}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute Cohen's kappa: a statistic that measures inter-annotator agreement.

This function computes Cohen's kappa [1]_, a score that expresses the level
of agreement between two annotators on a classification problem. It is
defined as

.. math::
    \kappa = (p_o - p_e) / (1 - p_e)

where :math:`p_o` is the empirical probability of agreement on the label
assigned to any sample (the observed agreement ratio), and :math:`p_e` is
the expected agreement when both annotators assign labels randomly.
:math:`p_e` is estimated using a per-annotator empirical prior over the
class labels [2]_.

Read more in the :ref:`User Guide <cohen_kappa>`.

Parameters
----------
y1 : array-like of shape (n_samples,)
    Labels assigned by the first annotator.

y2 : array-like of shape (n_samples,)
    Labels assigned by the second annotator. The kappa statistic is
    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.

labels : array-like of shape (n_classes,), default=None
    List of labels to index the matrix. This may be used to select a
    subset of labels. If `None`, all labels that appear at least once in
    ``y1`` or ``y2`` are used. Note that at least one label in `labels` must be
    present in `y1`, even though this function is otherwise agnostic to the order
    of `y1` and `y2`.

weights : {'linear', 'quadratic'}, default=None
    Weighting type to calculate the score. `None` means not weighted;
    "linear" means linear weighting; "quadratic" means quadratic weighting.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
kappa : float
    The kappa statistic, which is a number between -1 and 1. The maximum
    value means complete agreement; zero or lower means chance agreement.

References
----------
.. [1] :doi:`J. Cohen (1960). "A coefficient of agreement for nominal scales".
       Educational and Psychological Measurement 20(1):37-46.
       <10.1177/001316446002000104>`
.. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for
       computational linguistics". Computational Linguistics 34(4):555-596
       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.
.. [3] `Wikipedia entry for the Cohen's kappa
        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.

Examples
--------
>>> from sklearn.metrics import cohen_kappa_score
>>> y1 = ["negative", "positive", "negative", "neutral", "positive"]
>>> y2 = ["negative", "positive", "negative", "neutral", "negative"]
>>> cohen_kappa_score(y1, y2)
0.6875
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00803}{803}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a9bdecf060bbdd6ebe26f442d9d3f1382}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!confusion\_matrix@{confusion\_matrix}}
\index{confusion\_matrix@{confusion\_matrix}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{confusion\_matrix()}{confusion\_matrix()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a9bdecf060bbdd6ebe26f442d9d3f1382} 
sklearn.\+metrics.\+\_\+classification.\+confusion\+\_\+matrix (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{normalize}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute confusion matrix to evaluate the accuracy of a classification.

By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`
is equal to the number of observations known to be in group :math:`i` and
predicted to be in group :math:`j`.

Thus in binary classification, the count of true negatives is
:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is
:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.

Read more in the :ref:`User Guide <confusion_matrix>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_classes), default=None
    List of labels to index the matrix. This may be used to reorder
    or select a subset of labels.
    If ``None`` is given, those that appear at least once
    in ``y_true`` or ``y_pred`` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

normalize : {'true', 'pred', 'all'}, default=None
    Normalizes confusion matrix over the true (rows), predicted (columns)
    conditions or all the population. If None, confusion matrix will not be
    normalized.

Returns
-------
C : ndarray of shape (n_classes, n_classes)
    Confusion matrix whose i-th row and j-th
    column entry indicates the number of
    samples with true label being i-th class
    and predicted label being j-th class.

See Also
--------
ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix
    given an estimator, the data, and the label.
ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix
    given the true and predicted labels.
ConfusionMatrixDisplay : Confusion Matrix visualization.

References
----------
.. [1] `Wikipedia entry for the Confusion matrix
       <https://en.wikipedia.org/wiki/Confusion_matrix>`_
       (Wikipedia and other references may use a different
       convention for axes).

Examples
--------
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> confusion_matrix(y_true, y_pred, labels=["ant", "bird", "cat"])
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

In the binary case, we can extract true positives, etc. as follows:

>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel().tolist()
>>> (tn, fp, fn, tp)
(0, 2, 1, 1)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00381}{381}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a457f569cc9a5bb6f2172dde936d3be7f}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!d2\_log\_loss\_score@{d2\_log\_loss\_score}}
\index{d2\_log\_loss\_score@{d2\_log\_loss\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{d2\_log\_loss\_score()}{d2\_log\_loss\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a457f569cc9a5bb6f2172dde936d3be7f} 
sklearn.\+metrics.\+\_\+classification.\+d2\+\_\+log\+\_\+loss\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{labels}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}:math:`D^2` score function, fraction of log loss explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always predicts the per-class proportions
of `y_true`, disregarding the input features, gets a D^2 score of 0.0.

Read more in the :ref:`User Guide <d2_score_classification>`.

.. versionadded:: 1.5

Parameters
----------
y_true : array-like or label indicator matrix
    The actuals labels for the n_samples samples.

y_pred : array-like of shape (n_samples, n_classes) or (n_samples,)
    Predicted probabilities, as returned by a classifier's
    predict_proba method. If ``y_pred.shape = (n_samples,)``
    the probabilities provided are assumed to be that of the
    positive class. The labels in ``y_pred`` are assumed to be
    ordered alphabetically, as done by
    :class:`~sklearn.preprocessing.LabelBinarizer`.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like, default=None
    If not provided, labels will be inferred from y_true. If ``labels``
    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
    assumed to be binary and are inferred from ``y_true``.

Returns
-------
d2 : float or ndarray of floats
    The D^2 score.

Notes
-----
This is not a symmetric function.

Like R^2, D^2 score may be negative (it need not actually be the square of
a quantity D).

This metric is not well-defined for a single sample and will return a NaN
value if n_samples is less than two.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03638}{3638}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l03172}{log\+\_\+loss()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a5d31c24f718a99decc9441f1e82a7329}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!f1\_score@{f1\_score}}
\index{f1\_score@{f1\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{f1\_score()}{f1\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a5d31c24f718a99decc9441f1e82a7329} 
sklearn.\+metrics.\+\_\+classification.\+f1\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily "{}binary"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the F1 score, also known as balanced F-score or F-measure.

The F1 score can be interpreted as a harmonic mean of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:

.. math::
    \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}

Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the
number of false negatives, and :math:`\\text{FP}` is the number of false positives.
F1 is by default
calculated as 0.0 when there are no true positives, false negatives, or
false positives.

Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and F1 score for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
F1 score for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate F1 score
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \
        default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
f1_score : float or array of float, shape = [n_unique_labels]
    F1 score of the positive class in binary classification or weighted
    average of the F1 scores of each class for the multiclass task.

See Also
--------
fbeta_score : Compute the F-beta score.
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
jaccard_score : Compute the Jaccard similarity coefficient score.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0`` (i.e. a class
is completely absent from both ``y_true`` or ``y_pred``), f-score is
undefined. In such cases, by default f-score will be set to 0.0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified by
setting the ``zero_division`` parameter.

References
----------
.. [1] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import f1_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> f1_score(y_true, y_pred, average='macro')
0.267
>>> f1_score(y_true, y_pred, average='micro')
0.33
>>> f1_score(y_true, y_pred, average='weighted')
0.267
>>> f1_score(y_true, y_pred, average=None)
array([0.8, 0. , 0. ])

>>> # binary classification
>>> y_true_empty = [0, 0, 0, 0, 0, 0]
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> f1_score(y_true_empty, y_pred_empty)
0.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)
1.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)
nan...

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> f1_score(y_true, y_pred, average=None)
array([0.66666667, 1.        , 0.66666667])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01304}{1304}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01507}{fbeta\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_ada551803819b8de72f08d36b10c2775b}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!fbeta\_score@{fbeta\_score}}
\index{fbeta\_score@{fbeta\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{fbeta\_score()}{fbeta\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_ada551803819b8de72f08d36b10c2775b} 
sklearn.\+metrics.\+\_\+classification.\+fbeta\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{beta}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily "{}binary"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the F-beta score.

The F-beta score is the weighted harmonic mean of precision and recall,
reaching its optimal value at 1 and its worst value at 0.

The `beta` parameter represents the ratio of recall importance to
precision importance. `beta > 1` gives more weight to recall, while
`beta < 1` favors precision. For example, `beta = 2` makes recall twice
as important as precision, while `beta = 0.5` does the opposite.
Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`
only precision.

The formula for F-beta score is:

.. math::

   F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}
                    {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}

Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the
number of false positives, and :math:`\\text{fn}` is the number of false negatives.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is
ignored and F-beta score for both classes are computed, then averaged or both
returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, F-beta score for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate F-beta score for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float
    Determines the weight of recall in the combined score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \
        default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:

    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
fbeta_score : float (if average is not None) or array of float, shape =\
    [n_unique_labels]
    F-beta score of the positive class in binary classification or weighted
    average of the F-beta score of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0``, f-score
returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be
modified by setting ``zero_division``.

F-beta score is not implemented as a named scorer that can be passed to
the `scoring` parameter of cross-validation tools directly: it requires to be
wrapped with :func:`make_scorer` so as to specify the value of `beta`. See
examples for details.

References
----------
.. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).
       Modern Information Retrieval. Addison Wesley, pp. 327-328.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import fbeta_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.238
>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)
0.33
>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
0.238
>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)
array([0.71, 0.        , 0.        ])
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> fbeta_score(
...     y_true,
...     y_pred_empty,
...     average="macro",
...     zero_division=np.nan,
...     beta=0.5,
... )
0.128

In order to use :func:`fbeta_scorer` as a scorer, a callable
scorer objects needs to be created first with :func:`make_scorer`,
passing the value for the `beta` parameter.

>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(
...     LinearSVC(dual="auto"),
...     param_grid={'C': [1, 10]},
...     scoring=ftwo_scorer,
...     cv=5
... )
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01497}{1497}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l01313}{f1\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a1726217c03db123e1bbc0ae220edabe4}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!hamming\_loss@{hamming\_loss}}
\index{hamming\_loss@{hamming\_loss}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{hamming\_loss()}{hamming\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a1726217c03db123e1bbc0ae220edabe4} 
sklearn.\+metrics.\+\_\+classification.\+hamming\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the average Hamming loss.

The Hamming loss is the fraction of labels that are incorrectly predicted.

Read more in the :ref:`User Guide <hamming_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
loss : float or int
    Return the average Hamming loss between element of ``y_true`` and
    ``y_pred``.

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
jaccard_score : Compute the Jaccard similarity coefficient score.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Notes
-----
In multiclass classification, the Hamming loss corresponds to the Hamming
distance between ``y_true`` and ``y_pred`` which is equivalent to the
subset ``zero_one_loss`` function, when `normalize` parameter is set to
True.

In multilabel classification, the Hamming loss is different from the
subset zero-one loss. The zero-one loss considers the entire set of labels
for a given sample incorrect if it does not entirely match the true set of
labels. Hamming loss is more forgiving in that it penalizes only the
individual labels.

The Hamming loss is upperbounded by the subset zero-one loss, when
`normalize` parameter is set to True. It is always between 0 and 1,
lower being better.

References
----------
.. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:
       An Overview. International Journal of Data Warehousing & Mining,
       3(3), 1-13, July-September 2007.

.. [2] `Wikipedia entry on the Hamming distance
       <https://en.wikipedia.org/wiki/Hamming_distance>`_.

Examples
--------
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03061}{3061}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a8b103f5754e2cfae479b2d92e59f53fd}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!hinge\_loss@{hinge\_loss}}
\index{hinge\_loss@{hinge\_loss}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{hinge\_loss()}{hinge\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a8b103f5754e2cfae479b2d92e59f53fd} 
sklearn.\+metrics.\+\_\+classification.\+hinge\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{pred\+\_\+decision}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Average hinge loss (non-regularized).

In binary class case, assuming labels in y_true are encoded with +1 and -1,
when a prediction mistake is made, ``margin = y_true * pred_decision`` is
always negative (since the signs disagree), implying ``1 - margin`` is
always greater than 1.  The cumulated hinge loss is therefore an upper
bound of the number of mistakes made by the classifier.

In multiclass case, the function expects that either all the labels are
included in y_true or an optional labels argument is provided which
contains all the labels. The multilabel margin is calculated according
to Crammer-Singer's method. As in the binary case, the cumulated hinge loss
is an upper bound of the number of mistakes made by the classifier.

Read more in the :ref:`User Guide <hinge_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True target, consisting of integers of two values. The positive label
    must be greater than the negative label.

pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)
    Predicted decisions, as output by decision_function (floats).

labels : array-like, default=None
    Contains all the labels for the problem. Used in multiclass hinge loss.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    Average hinge loss.

References
----------
.. [1] `Wikipedia entry on the Hinge loss
       <https://en.wikipedia.org/wiki/Hinge_loss>`_.

.. [2] Koby Crammer, Yoram Singer. On the Algorithmic
       Implementation of Multiclass Kernel-based Vector
       Machines. Journal of Machine Learning Research 2,
       (2001), 265-292.

.. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models
       by Robert C. Moore, John DeNero
       <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.

Examples
--------
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(random_state=0)
>>> est.fit(X, y)
LinearSVC(random_state=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18,  2.36,  0.09])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.30

In the multiclass case:

>>> import numpy as np
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC()
>>> est.fit(X, Y)
LinearSVC()
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels=labels)
0.56
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03262}{3262}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_aba21abda11d69893f8ae3727cb64472f}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!jaccard\_score@{jaccard\_score}}
\index{jaccard\_score@{jaccard\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{jaccard\_score()}{jaccard\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_aba21abda11d69893f8ae3727cb64472f} 
sklearn.\+metrics.\+\_\+classification.\+jaccard\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily "{}binary"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Jaccard similarity coefficient score.

The Jaccard index [1], or Jaccard similarity coefficient, defined as
the size of the intersection divided by the size of the union of two label
sets, is used to compare set of predicted labels for a sample to the
corresponding set of labels in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return the
Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,
`pos_label` is ignored and scores for both classes are computed, then averaged or
both returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, scores for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate the score for.

Read more in the :ref:`User Guide <jaccard_similarity_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

labels : array-like of shape (n_classes,), default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', \
        'binary'} or None, default='binary'
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : "warn", {0.0, 1.0}, default="warn"
    Sets the value to return when there is a zero division, i.e. when there
    there are no negative values in predictions and labels. If set to
    "warn", this acts like 0, but a warning is also raised.

    .. versionadded:: 0.24

Returns
-------
score : float or ndarray of shape (n_unique_labels,), dtype=np.float64
    The Jaccard score. When `average` is not `None`, a single scalar is
    returned.

See Also
--------
accuracy_score : Function for calculating the accuracy score.
f1_score : Function for calculating the F1 score.
multilabel_confusion_matrix : Function for computing a confusion matrix\
                              for each class or sample.

Notes
-----
:func:`jaccard_score` may be a poor metric if there are no
positives for some samples or classes. Jaccard is undefined if there are
no true or predicted labels, and our implementation will return a score
of 0 with a warning.

References
----------
.. [1] `Wikipedia entry for the Jaccard index
       <https://en.wikipedia.org/wiki/Jaccard_index>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import jaccard_score
>>> y_true = np.array([[0, 1, 1],
...                    [1, 1, 0]])
>>> y_pred = np.array([[1, 1, 1],
...                    [1, 0, 0]])

In the binary case:

>>> jaccard_score(y_true[0], y_pred[0])
0.6666

In the 2D comparison case (e.g. image similarity):

>>> jaccard_score(y_true, y_pred, average="micro")
0.6

In the multilabel case:

>>> jaccard_score(y_true, y_pred, average='samples')
0.5833
>>> jaccard_score(y_true, y_pred, average='macro')
0.6666
>>> jaccard_score(y_true, y_pred, average=None)
array([0.5, 0.5, 1. ])

In the multiclass case:

>>> y_pred = [0, 2, 1, 2]
>>> y_true = [0, 1, 2, 2]
>>> jaccard_score(y_true, y_pred, average=None)
array([1. , 0. , 0.33])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00919}{919}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01752}{\+\_\+check\+\_\+set\+\_\+wise\+\_\+labels()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l01699}{\+\_\+prf\+\_\+divide()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l00558}{multilabel\+\_\+confusion\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a04f7198fff7b380c1a136c9d69c883ef}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!log\_loss@{log\_loss}}
\index{log\_loss@{log\_loss}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{log\_loss()}{log\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a04f7198fff7b380c1a136c9d69c883ef} 
sklearn.\+metrics.\+\_\+classification.\+log\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{normalize}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{labels}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Log loss, aka logistic loss or cross-entropy loss.

This is the loss function used in (multinomial) logistic regression
and extensions of it such as neural networks, defined as the negative
log-likelihood of a logistic model that returns ``y_pred`` probabilities
for its training data ``y_true``.
The log loss is only defined for two or more labels.
For a single sample with true label :math:`y \in \{0,1\}` and
a probability estimate :math:`p = \operatorname{Pr}(y = 1)`, the log
loss is:

.. math::
    L_{\log}(y, p) = -(y \log (p) + (1 - y) \log (1 - p))

Read more in the :ref:`User Guide <log_loss>`.

Parameters
----------
y_true : array-like or label indicator matrix
    Ground truth (correct) labels for n_samples samples.

y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)
    Predicted probabilities, as returned by a classifier's
    predict_proba method. If ``y_pred.shape = (n_samples,)``
    the probabilities provided are assumed to be that of the
    positive class. The labels in ``y_pred`` are assumed to be
    ordered alphabetically, as done by
    :class:`~sklearn.preprocessing.LabelBinarizer`.

    `y_pred` values are clipped to `[eps, 1-eps]` where `eps` is the machine
    precision for `y_pred`'s dtype.

normalize : bool, default=True
    If true, return the mean loss per sample.
    Otherwise, return the sum of the per-sample losses.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like, default=None
    If not provided, labels will be inferred from y_true. If ``labels``
    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
    assumed to be binary and are inferred from ``y_true``.

    .. versionadded:: 0.18

Returns
-------
loss : float
    Log loss, aka logistic loss or cross-entropy loss.

Notes
-----
The logarithm used is the natural logarithm (base-e).

References
----------
C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,
p. 209.

Examples
--------
>>> from sklearn.metrics import log_loss
>>> log_loss(["spam", "ham", "ham", "spam"],
...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
0.21616
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l03172}{3172}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00156}{\+\_\+validate\+\_\+multiclass\+\_\+probabilistic\+\_\+prediction()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l03638}{d2\+\_\+log\+\_\+loss\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a68e73cd770f722164235c466adbf5a0c}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!matthews\_corrcoef@{matthews\_corrcoef}}
\index{matthews\_corrcoef@{matthews\_corrcoef}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{matthews\_corrcoef()}{matthews\_corrcoef()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a68e73cd770f722164235c466adbf5a0c} 
sklearn.\+metrics.\+\_\+classification.\+matthews\+\_\+corrcoef (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the Matthews correlation coefficient (MCC).

The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary and multiclass classifications. It takes
into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are of
very different sizes. The MCC is in essence a correlation coefficient value
between -1 and +1. A coefficient of +1 represents a perfect prediction, 0
an average random prediction and -1 an inverse prediction.  The statistic
is also known as the phi coefficient. [source: Wikipedia]

Binary and multiclass labels are supported.  Only in the binary case does
this relate to information about true and false positives and negatives.
See references below.

Read more in the :ref:`User Guide <matthews_corrcoef>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
mcc : float
    The Matthews correlation coefficient (+1 represents a perfect
    prediction, 0 an average random prediction and -1 and inverse
    prediction).

References
----------
.. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the
   accuracy of prediction algorithms for classification: an overview.
   <10.1093/bioinformatics/16.5.412>`

.. [2] `Wikipedia entry for the Matthews Correlation Coefficient (phi coefficient)
   <https://en.wikipedia.org/wiki/Phi_coefficient>`_.

.. [3] `Gorodkin, (2004). Comparing two K-category assignments by a
    K-category correlation coefficient
    <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.

.. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN
    Error Measures in MultiClass Prediction
    <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.

Examples
--------
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01110}{1110}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a441d970c6097ed3b4af7dbb919e53813}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!multilabel\_confusion\_matrix@{multilabel\_confusion\_matrix}}
\index{multilabel\_confusion\_matrix@{multilabel\_confusion\_matrix}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{multilabel\_confusion\_matrix()}{multilabel\_confusion\_matrix()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a441d970c6097ed3b4af7dbb919e53813} 
sklearn.\+metrics.\+\_\+classification.\+multilabel\+\_\+confusion\+\_\+matrix (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{samplewise}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute a confusion matrix for each class or sample.

.. versionadded:: 0.21

Compute class-wise (default) or sample-wise (samplewise=True) multilabel
confusion matrix to evaluate the accuracy of a classification, and output
confusion matrices for each class or sample.

In multilabel confusion matrix :math:`MCM`, the count of true negatives
is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,
true positives is :math:`MCM_{:,1,1}` and false positives is
:math:`MCM_{:,0,1}`.

Multiclass data will be treated as if binarized under a one-vs-rest
transformation. Returned confusion matrices will be in the order of
sorted unique labels in the union of (y_true, y_pred).

Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or \
        (n_samples,)
    Ground truth (correct) target values.

y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or \
        (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like of shape (n_classes,), default=None
    A list of classes or column indices to select some (or to force
    inclusion of classes absent from the data).

samplewise : bool, default=False
    In the multilabel case, this calculates a confusion matrix per sample.

Returns
-------
multi_confusion : ndarray of shape (n_outputs, 2, 2)
    A 2x2 confusion matrix corresponding to each output in the input.
    When calculating class-wise multi_confusion (default), then
    n_outputs = n_labels; when calculating sample-wise multi_confusion
    (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,
    the results will be returned in the order specified in ``labels``,
    otherwise the results will be returned in sorted order by default.

See Also
--------
confusion_matrix : Compute confusion matrix to evaluate the accuracy of a
    classifier.

Notes
-----
The `multilabel_confusion_matrix` calculates class-wise or sample-wise
multilabel confusion matrices, and in multiclass tasks, labels are
binarized under a one-vs-rest way; while
:func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix
for confusion between every two classes.

Examples
--------
Multilabel-indicator case:

>>> import numpy as np
>>> from sklearn.metrics import multilabel_confusion_matrix
>>> y_true = np.array([[1, 0, 1],
...                    [0, 1, 0]])
>>> y_pred = np.array([[1, 0, 0],
...                    [0, 1, 1]])
>>> multilabel_confusion_matrix(y_true, y_pred)
array([[[1, 0],
        [0, 1]],
<BLANKLINE>
       [[1, 0],
        [0, 1]],
<BLANKLINE>
       [[0, 1],
        [1, 0]]])

Multiclass case:

>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> multilabel_confusion_matrix(y_true, y_pred,
...                             labels=["ant", "bird", "cat"])
array([[[3, 1],
        [0, 2]],
<BLANKLINE>
       [[5, 0],
        [1, 0]],
<BLANKLINE>
       [[2, 1],
        [1, 2]]])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l00556}{556}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00069}{\+\_\+check\+\_\+targets()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l00928}{jaccard\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a15142b0e8df592903125d73338098b62}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!precision\_recall\_fscore\_support@{precision\_recall\_fscore\_support}}
\index{precision\_recall\_fscore\_support@{precision\_recall\_fscore\_support}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{precision\_recall\_fscore\_support()}{precision\_recall\_fscore\_support()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a15142b0e8df592903125d73338098b62} 
sklearn.\+metrics.\+\_\+classification.\+precision\+\_\+recall\+\_\+fscore\+\_\+support (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{beta}{ = {\ttfamily 1.0}, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily None}, }\item[{}]{warn\+\_\+for}{ = {\ttfamily ("{}precision"{},~"{}recall"{},~"{}f-\/score"{})}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute precision, recall, F-measure and support for each class.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label a negative sample as
positive.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The F-beta score can be interpreted as a weighted harmonic mean of
the precision and recall, where an F-beta score reaches its best
value at 1 and worst score at 0.

The F-beta score weights recall more than precision by a factor of
``beta``. ``beta == 1.0`` means recall and precision are equally important.

The support is the number of occurrences of each class in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and metrics for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
metrics for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate metrics for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float, default=1.0
    The strength of recall versus precision in the F-score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \
        default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

warn_for : list, tuple or set, for internal use
    This determines which warnings will be made in the case that this
    function is being used to return only one of its metrics.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division:

    - recall: when there are no positive labels
    - precision: when there are no positive predictions
    - f-score: both

    Notes:

    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float, shape =\
    [n_unique_labels]
    Precision score.

recall : float (if average is not None) or array of float, shape =\
    [n_unique_labels]
    Recall score.

fbeta_score : float (if average is not None) or array of float, shape =\
    [n_unique_labels]
    F-beta score.

support : None (if average is not None) or array of int, shape =\
    [n_unique_labels]
    The number of occurrences of each label in ``y_true``.

Notes
-----
When ``true positive + false positive == 0``, precision is undefined.
When ``true positive + false negative == 0``, recall is undefined. When
``true positive + false negative + false positive == 0``, f-score is
undefined. In such cases, by default the metric will be set to 0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified
with ``zero_division``.

References
----------
.. [1] `Wikipedia entry for the Precision and recall
       <https://en.wikipedia.org/wiki/Precision_and_recall>`_.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

.. [3] `Discriminative Methods for Multi-labeled Classification Advances
       in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
       Godbole, Sunita Sarawagi
       <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_fscore_support
>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
>>> precision_recall_fscore_support(y_true, y_pred, average='macro')
(0.222, 0.333, 0.267, None)
>>> precision_recall_fscore_support(y_true, y_pred, average='micro')
(0.33, 0.33, 0.33, None)
>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')
(0.222, 0.333, 0.267, None)

It is possible to compute per-label precisions, recalls, F1-scores and
supports instead of averaging:

>>> precision_recall_fscore_support(y_true, y_pred, average=None,
... labels=['pig', 'dog', 'cat'])
(array([0.        , 0.        , 0.66]),
 array([0., 0., 1.]), array([0. , 0. , 0.8]),
 array([2, 2, 2]))
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01815}{1815}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01752}{\+\_\+check\+\_\+set\+\_\+wise\+\_\+labels()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l01699}{\+\_\+prf\+\_\+divide()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l00558}{multilabel\+\_\+confusion\+\_\+matrix()}}.



Referenced by \mbox{\hyperlink{metrics_2__classification_8py_source_l02839}{classification\+\_\+report()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l01507}{fbeta\+\_\+score()}}, \mbox{\hyperlink{metrics_2__classification_8py_source_l02384}{precision\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__classification_8py_source_l02565}{recall\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_aa7da5219ebca6807f6a19706be6f05ba}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!precision\_score@{precision\_score}}
\index{precision\_score@{precision\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{precision\_score()}{precision\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_aa7da5219ebca6807f6a19706be6f05ba} 
sklearn.\+metrics.\+\_\+classification.\+precision\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily "{}binary"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the precision.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and precision for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
precision for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate precision
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \
        default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division.

    Notes:

    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float of shape \
            (n_unique_labels,)
    Precision of the positive class in binary classification or weighted
    average of the precision of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the
    number of true positives and ``fn`` the number of false negatives.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive == 0``, precision returns 0 and
raises ``UndefinedMetricWarning``. This behavior can be
modified with ``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> precision_score(y_true, y_pred, average='macro')
0.22
>>> precision_score(y_true, y_pred, average='micro')
0.33
>>> precision_score(y_true, y_pred, average='weighted')
0.22
>>> precision_score(y_true, y_pred, average=None)
array([0.66, 0.        , 0.        ])
>>> y_pred = [0, 0, 0, 0, 0, 0]
>>> precision_score(y_true, y_pred, average=None)
array([0.33, 0.        , 0.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=1)
array([0.33, 1.        , 1.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.33,        nan,        nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> precision_score(y_true, y_pred, average=None)
array([0.5, 1. , 1. ])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l02375}{2375}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_a5557c71c9e924d3e1d9e3679d1e0e710}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!recall\_score@{recall\_score}}
\index{recall\_score@{recall\_score}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{recall\_score()}{recall\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_a5557c71c9e924d3e1d9e3679d1e0e710} 
sklearn.\+metrics.\+\_\+classification.\+recall\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{pos\+\_\+label}{ = {\ttfamily 1}, }\item[{}]{average}{ = {\ttfamily "{}binary"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{zero\+\_\+division}{ = {\ttfamily "{}warn"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the recall.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and recall for both classes are computed then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
recall for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate recall for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \
        default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall. Weighted recall
        is equal to accuracy.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division.

    Notes:

    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
recall : float (if average is not None) or array of float of shape \
         (n_unique_labels,)
    Recall of the positive class in binary classification or weighted
    average of the recall of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the
    number of true positives and ``fp`` the number of false positives.
balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced
    datasets.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.

Notes
-----
When ``true positive + false negative == 0``, recall returns 0 and raises
``UndefinedMetricWarning``. This behavior can be modified with
``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import recall_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> recall_score(y_true, y_pred, average='macro')
0.33
>>> recall_score(y_true, y_pred, average='micro')
0.33
>>> recall_score(y_true, y_pred, average='weighted')
0.33
>>> recall_score(y_true, y_pred, average=None)
array([1., 0., 0.])
>>> y_true = [0, 0, 0, 0, 0, 0]
>>> recall_score(y_true, y_pred, average=None)
array([0.5, 0. , 0. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=1)
array([0.5, 1. , 1. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.5, nan, nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> recall_score(y_true, y_pred, average=None)
array([1. , 1. , 0.5])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l02556}{2556}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l01826}{precision\+\_\+recall\+\_\+fscore\+\_\+support()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__classification_adf01048251d2b41d3fb5ee881300fc73}\index{sklearn.metrics.\_classification@{sklearn.metrics.\_classification}!zero\_one\_loss@{zero\_one\_loss}}
\index{zero\_one\_loss@{zero\_one\_loss}!sklearn.metrics.\_classification@{sklearn.metrics.\_classification}}
\doxysubsubsection{\texorpdfstring{zero\_one\_loss()}{zero\_one\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__classification_adf01048251d2b41d3fb5ee881300fc73} 
sklearn.\+metrics.\+\_\+classification.\+zero\+\_\+one\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{normalize}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Zero-one classification loss.

If normalize is ``True``, return the fraction of misclassifications
(float), else it returns the number of misclassifications (int). The best
performance is 0.

Read more in the :ref:`User Guide <zero_one_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of misclassifications.
    Otherwise, return the fraction of misclassifications.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float or int,
    If ``normalize == True``, return the fraction of misclassifications
    (float), else it returns the number of misclassifications (int).

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
jaccard_score : Compute the Jaccard similarity coefficient score.

Notes
-----
In multilabel classification, the zero_one_loss function corresponds to
the subset zero-one loss: for each sample, the entire set of labels must be
correctly predicted, otherwise the loss for that sample is equal to one.

Examples
--------
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__classification_8py_source_l01209}{1209}} of file \mbox{\hyperlink{metrics_2__classification_8py_source}{\+\_\+classification.\+py}}.



References \mbox{\hyperlink{metrics_2__classification_8py_source_l00296}{accuracy\+\_\+score()}}.

