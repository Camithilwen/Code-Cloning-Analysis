\doxysection{sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss Class Reference}
\hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{}\label{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}\index{sklearn.\_loss.loss.HalfTweedieLoss@{sklearn.\_loss.loss.HalfTweedieLoss}}


Inheritance diagram for sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss\+:
% FIG 0


Collaboration diagram for sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a166e9602006020897da538420926b7d9}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, sample\+\_\+weight=None, power=1.\+5)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{constant\+\_\+to\+\_\+optimal\+\_\+zero}} (self, y\+\_\+true, sample\+\_\+weight=None)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, closs, link, n\+\_\+classes=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{in\+\_\+y\+\_\+true\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\+\_\+y\+\_\+pred\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\+\_\+gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\+\_\+hessian}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, hessian\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{fit\+\_\+intercept\+\_\+only}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\+\_\+gradient\+\_\+and\+\_\+hessian}} (self, n\+\_\+samples, dtype=np.\+float64, order="{}F"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}} = closs
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}} = link
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\+\_\+hessian}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\+\_\+hessian}} = False
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\+\_\+classes}} = n\+\_\+classes
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\+\_\+y\+\_\+true}} = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\+\_\+y\+\_\+pred}} = self.\+link.\+interval\+\_\+y\+\_\+pred
\end{DoxyCompactItemize}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}{differentiable}} = \mbox{\hyperlink{classTrue}{True}}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}{need\+\_\+update\+\_\+leaves\+\_\+values}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\+\_\+multiclass}} = False
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Half Tweedie deviance loss with log-link, for regression.

Domain:
y_true in real numbers for power <= 0
y_true in non-negative real numbers for 0 < power < 2
y_true in positive real numbers for 2 <= power
y_pred in positive real numbers
power in real numbers

Link:
y_pred = exp(raw_prediction)

For a given sample x_i, half Tweedie deviance loss with p=power is defined
as::

    loss(x_i) = max(y_true_i, 0)**(2-p) / (1-p) / (2-p)
                - y_true_i * exp(raw_prediction_i)**(1-p) / (1-p)
                + exp(raw_prediction_i)**(2-p) / (2-p)

Taking the limits for p=0, 1, 2 gives HalfSquaredError with a log link,
HalfPoissonLoss and HalfGammaLoss.

We also skip constant terms, but those are different for p=0, 1, 2.
Therefore, the loss is not continuous in `power`.

Note furthermore that although no Tweedie distribution exists for
0 < power < 1, it still gives a strictly consistent scoring function for
the expectation.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00779}{779}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a166e9602006020897da538420926b7d9}\index{sklearn.\_loss.loss.HalfTweedieLoss@{sklearn.\_loss.loss.HalfTweedieLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.\_loss.loss.HalfTweedieLoss@{sklearn.\_loss.loss.HalfTweedieLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a166e9602006020897da538420926b7d9} 
sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{power}{ = {\ttfamily 1.5}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{loss_8py_source_l00810}{810}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}\index{sklearn.\_loss.loss.HalfTweedieLoss@{sklearn.\_loss.loss.HalfTweedieLoss}!constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}}
\index{constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}!sklearn.\_loss.loss.HalfTweedieLoss@{sklearn.\_loss.loss.HalfTweedieLoss}}
\doxysubsubsection{\texorpdfstring{constant\_to\_optimal\_zero()}{constant\_to\_optimal\_zero()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283} 
sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss.\+constant\+\_\+to\+\_\+optimal\+\_\+zero (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculate term dropped in loss.

With this term added, the loss of perfect predictions is zero.
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l00822}{822}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+closs}}, and \mbox{\hyperlink{loss_8py_source_l00822}{constant\+\_\+to\+\_\+optimal\+\_\+zero()}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00822}{constant\+\_\+to\+\_\+optimal\+\_\+zero()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/\+\_\+loss/loss.\+py\end{DoxyCompactItemize}
