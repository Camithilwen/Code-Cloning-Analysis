\doxysection{scipy.\+optimize.\+\_\+linesearch Namespace Reference}
\hypertarget{namespacescipy_1_1optimize_1_1__linesearch}{}\label{namespacescipy_1_1optimize_1_1__linesearch}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__linesearch_1_1LineSearchWarning}{Line\+Search\+Warning}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_aeccc455ff7d031dcb3e11045b705bf9c}{line\+\_\+search\+\_\+wolfe1}} (f, fprime, xk, pk, gfk=None, old\+\_\+fval=None, old\+\_\+old\+\_\+fval=None, args=(), c1=1e-\/4, c2=0.\+9, amax=50, amin=1e-\/8, xtol=1e-\/14)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a3b9a884bb40e777b2adaa77260e74c06}{scalar\+\_\+search\+\_\+wolfe1}} (phi, derphi, phi0=None, old\+\_\+phi0=None, derphi0=None, c1=1e-\/4, c2=0.\+9, amax=50, amin=1e-\/8, xtol=1e-\/14)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_aff851f2e3ee59ebac46a702b52918a39}{line\+\_\+search\+\_\+wolfe2}} (f, myfprime, xk, pk, gfk=None, old\+\_\+fval=None, old\+\_\+old\+\_\+fval=None, args=(), c1=1e-\/4, c2=0.\+9, amax=None, extra\+\_\+condition=None, maxiter=10)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_adee20da3d04d3e1ba764760b8ff1156a}{scalar\+\_\+search\+\_\+wolfe2}} (phi, derphi, phi0=None, old\+\_\+phi0=None, derphi0=None, c1=1e-\/4, c2=0.\+9, amax=None, extra\+\_\+condition=None, maxiter=10)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a1f3dadc643d57e3b02069fe225c359aa}{\+\_\+cubicmin}} (a, fa, fpa, b, fb, c, fc)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a6042aa7a38c75b77fe5b84c621ed5730}{\+\_\+quadmin}} (a, fa, fpa, b, fb)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_aac4ebdb188f7a368ae0558496cf04747}{\+\_\+zoom}} (a\+\_\+lo, a\+\_\+hi, phi\+\_\+lo, phi\+\_\+hi, derphi\+\_\+lo, phi, derphi, phi0, derphi0, c1, c2, extra\+\_\+condition)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a3c97a0b6f27822f3286a54ec5c2c984d}{line\+\_\+search\+\_\+armijo}} (f, xk, pk, gfk, old\+\_\+fval, args=(), c1=1e-\/4, alpha0=1)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a56e9c46ec2e3464a0706ea79cd2ac743}{line\+\_\+search\+\_\+\+BFGS}} (f, xk, pk, gfk, old\+\_\+fval, args=(), c1=1e-\/4, alpha0=1)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a51e8a46d842d0c35a45ff56d578458d9}{scalar\+\_\+search\+\_\+armijo}} (phi, phi0, derphi0, c1=1e-\/4, alpha0=1, amin=0)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_ae767c99fcdea23771d6935e6dc34aa51}{\+\_\+nonmonotone\+\_\+line\+\_\+search\+\_\+cruz}} (f, x\+\_\+k, d, prev\+\_\+fs, eta, gamma=1e-\/4, tau\+\_\+min=0.\+1, tau\+\_\+max=0.\+5)
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_ae7623710a6430053c52ffddd67912923}{\+\_\+nonmonotone\+\_\+line\+\_\+search\+\_\+cheng}} (f, x\+\_\+k, d, f\+\_\+k, \mbox{\hyperlink{classC}{C}}, Q, eta, gamma=1e-\/4, tau\+\_\+min=0.\+1, tau\+\_\+max=0.\+5, nu=0.\+85)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_ade7bbc705f2d647b07bfa055c28aba99}{\+\_\+\+\_\+all\+\_\+\+\_\+}}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_a974f515a245b8bfecca6d3425f4a5c42}{line\+\_\+search}} = \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_aeccc455ff7d031dcb3e11045b705bf9c}{line\+\_\+search\+\_\+wolfe1}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Functions
---------
.. autosummary::
   :toctree: generated/

    line_search_armijo
    line_search_wolfe1
    line_search_wolfe2
    scalar_search_wolfe1
    scalar_search_wolfe2
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a1f3dadc643d57e3b02069fe225c359aa}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_cubicmin@{\_cubicmin}}
\index{\_cubicmin@{\_cubicmin}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_cubicmin()}{\_cubicmin()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a1f3dadc643d57e3b02069fe225c359aa} 
scipy.\+optimize.\+\_\+linesearch.\+\_\+cubicmin (\begin{DoxyParamCaption}\item[{}]{a}{, }\item[{}]{fa}{, }\item[{}]{fpa}{, }\item[{}]{b}{, }\item[{}]{fb}{, }\item[{}]{c}{, }\item[{}]{fc}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Finds the minimizer for a cubic polynomial that goes through the
points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.

If no minimizer can be found, return None.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00471}{471}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00527}{\+\_\+zoom()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_ae7623710a6430053c52ffddd67912923}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_nonmonotone\_line\_search\_cheng@{\_nonmonotone\_line\_search\_cheng}}
\index{\_nonmonotone\_line\_search\_cheng@{\_nonmonotone\_line\_search\_cheng}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_nonmonotone\_line\_search\_cheng()}{\_nonmonotone\_line\_search\_cheng()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_ae7623710a6430053c52ffddd67912923} 
scipy.\+optimize.\+\_\+linesearch.\+\_\+nonmonotone\+\_\+line\+\_\+search\+\_\+cheng (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{x\+\_\+k}{, }\item[{}]{d}{, }\item[{}]{f\+\_\+k}{, }\item[{}]{C}{, }\item[{}]{Q}{, }\item[{}]{eta}{, }\item[{}]{gamma}{ = {\ttfamily 1e-\/4}, }\item[{}]{tau\+\_\+min}{ = {\ttfamily 0.1}, }\item[{}]{tau\+\_\+max}{ = {\ttfamily 0.5}, }\item[{}]{nu}{ = {\ttfamily 0.85}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Nonmonotone line search from [1]

Parameters
----------
f : callable
    Function returning a tuple ``(f, F)`` where ``f`` is the value
    of a merit function and ``F`` the residual.
x_k : ndarray
    Initial position.
d : ndarray
    Search direction.
f_k : float
    Initial merit function value.
C, Q : float
    Control parameters. On the first iteration, give values
    Q=1.0, C=f_k
eta : float
    Allowed merit function increase, see [1]_
nu, gamma, tau_min, tau_max : float, optional
    Search parameters, see [1]_

Returns
-------
alpha : float
    Step length
xp : ndarray
    Next position
fp : float
    Merit function value at next position
Fp : ndarray
    Residual at next position
C : float
    New value for the control parameter C
Q : float
    New value for the control parameter Q

References
----------
.. [1] W. Cheng & D.-H. Li, ''A derivative-free nonmonotone line
       search and its application to the spectral residual
       method'', IMA J. Numer. Anal. 29, 814 (2009).
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00812}{812}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_ae767c99fcdea23771d6935e6dc34aa51}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_nonmonotone\_line\_search\_cruz@{\_nonmonotone\_line\_search\_cruz}}
\index{\_nonmonotone\_line\_search\_cruz@{\_nonmonotone\_line\_search\_cruz}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_nonmonotone\_line\_search\_cruz()}{\_nonmonotone\_line\_search\_cruz()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_ae767c99fcdea23771d6935e6dc34aa51} 
scipy.\+optimize.\+\_\+linesearch.\+\_\+nonmonotone\+\_\+line\+\_\+search\+\_\+cruz (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{x\+\_\+k}{, }\item[{}]{d}{, }\item[{}]{prev\+\_\+fs}{, }\item[{}]{eta}{, }\item[{}]{gamma}{ = {\ttfamily 1e-\/4}, }\item[{}]{tau\+\_\+min}{ = {\ttfamily 0.1}, }\item[{}]{tau\+\_\+max}{ = {\ttfamily 0.5}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Nonmonotone backtracking line search as described in [1]_

Parameters
----------
f : callable
    Function returning a tuple ``(f, F)`` where ``f`` is the value
    of a merit function and ``F`` the residual.
x_k : ndarray
    Initial position.
d : ndarray
    Search direction.
prev_fs : float
    List of previous merit function values. Should have ``len(prev_fs) <= M``
    where ``M`` is the nonmonotonicity window parameter.
eta : float
    Allowed merit function increase, see [1]_
gamma, tau_min, tau_max : float, optional
    Search parameters, see [1]_

Returns
-------
alpha : float
    Step length
xp : ndarray
    Next position
fp : float
    Merit function value at next position
Fp : ndarray
    Residual at next position

References
----------
[1] "Spectral residual method without gradient information for solving
    large-scale nonlinear systems of equations." W. La Cruz,
    J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00740}{740}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a6042aa7a38c75b77fe5b84c621ed5730}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_quadmin@{\_quadmin}}
\index{\_quadmin@{\_quadmin}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_quadmin()}{\_quadmin()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a6042aa7a38c75b77fe5b84c621ed5730} 
scipy.\+optimize.\+\_\+linesearch.\+\_\+quadmin (\begin{DoxyParamCaption}\item[{}]{a}{, }\item[{}]{fa}{, }\item[{}]{fpa}{, }\item[{}]{b}{, }\item[{}]{fb}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Finds the minimizer for a quadratic polynomial that goes through
the points (a,fa), (b,fb) with derivative at a of fpa.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00505}{505}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00527}{\+\_\+zoom()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_aac4ebdb188f7a368ae0558496cf04747}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_zoom@{\_zoom}}
\index{\_zoom@{\_zoom}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_zoom()}{\_zoom()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_aac4ebdb188f7a368ae0558496cf04747} 
scipy.\+optimize.\+\_\+linesearch.\+\_\+zoom (\begin{DoxyParamCaption}\item[{}]{a\+\_\+lo}{, }\item[{}]{a\+\_\+hi}{, }\item[{}]{phi\+\_\+lo}{, }\item[{}]{phi\+\_\+hi}{, }\item[{}]{derphi\+\_\+lo}{, }\item[{}]{phi}{, }\item[{}]{derphi}{, }\item[{}]{phi0}{, }\item[{}]{derphi0}{, }\item[{}]{c1}{, }\item[{}]{c2}{, }\item[{}]{extra\+\_\+condition}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Zoom stage of approximate linesearch satisfying strong Wolfe conditions.

Part of the optimization algorithm in `scalar_search_wolfe2`.

Notes
-----
Implements Algorithm 3.6 (zoom) in Wright and Nocedal,
'Numerical Optimization', 1999, pp. 61.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00526}{526}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



References \mbox{\hyperlink{__linesearch_8py_source_l00471}{\+\_\+cubicmin()}}, and \mbox{\hyperlink{__linesearch_8py_source_l00505}{\+\_\+quadmin()}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00328}{scalar\+\_\+search\+\_\+wolfe2()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a3c97a0b6f27822f3286a54ec5c2c984d}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!line\_search\_armijo@{line\_search\_armijo}}
\index{line\_search\_armijo@{line\_search\_armijo}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{line\_search\_armijo()}{line\_search\_armijo()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a3c97a0b6f27822f3286a54ec5c2c984d} 
scipy.\+optimize.\+\_\+linesearch.\+line\+\_\+search\+\_\+armijo (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{xk}{, }\item[{}]{pk}{, }\item[{}]{gfk}{, }\item[{}]{old\+\_\+fval}{, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{alpha0}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Minimize over alpha, the function ``f(xk+alpha pk)``.

Parameters
----------
f : callable
    Function to be minimized.
xk : array_like
    Current point.
pk : array_like
    Search direction.
gfk : array_like
    Gradient of `f` at point `xk`.
old_fval : float
    Value of `f` at point `xk`.
args : tuple, optional
    Optional arguments.
c1 : float, optional
    Value to control stopping criterion.
alpha0 : scalar, optional
    Value of `alpha` at start of the optimization.

Returns
-------
alpha
f_count
f_val_at_alpha

Notes
-----
Uses the interpolation algorithm (Armijo backtracking) as suggested by
Wright and Nocedal in 'Numerical Optimization', 1999, pp. 56-57
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00617}{617}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



References \mbox{\hyperlink{__linesearch_8py_source_l00678}{scalar\+\_\+search\+\_\+armijo()}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00669}{line\+\_\+search\+\_\+\+BFGS()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a56e9c46ec2e3464a0706ea79cd2ac743}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!line\_search\_BFGS@{line\_search\_BFGS}}
\index{line\_search\_BFGS@{line\_search\_BFGS}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{line\_search\_BFGS()}{line\_search\_BFGS()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a56e9c46ec2e3464a0706ea79cd2ac743} 
scipy.\+optimize.\+\_\+linesearch.\+line\+\_\+search\+\_\+\+BFGS (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{xk}{, }\item[{}]{pk}{, }\item[{}]{gfk}{, }\item[{}]{old\+\_\+fval}{, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{alpha0}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compatibility wrapper for `line_search_armijo`
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00669}{669}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



References \mbox{\hyperlink{__linesearch_8py_source_l00617}{line\+\_\+search\+\_\+armijo()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_aeccc455ff7d031dcb3e11045b705bf9c}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!line\_search\_wolfe1@{line\_search\_wolfe1}}
\index{line\_search\_wolfe1@{line\_search\_wolfe1}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{line\_search\_wolfe1()}{line\_search\_wolfe1()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_aeccc455ff7d031dcb3e11045b705bf9c} 
scipy.\+optimize.\+\_\+linesearch.\+line\+\_\+search\+\_\+wolfe1 (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{fprime}{, }\item[{}]{xk}{, }\item[{}]{pk}{, }\item[{}]{gfk}{ = {\ttfamily None}, }\item[{}]{old\+\_\+fval}{ = {\ttfamily None}, }\item[{}]{old\+\_\+old\+\_\+fval}{ = {\ttfamily None}, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{c2}{ = {\ttfamily 0.9}, }\item[{}]{amax}{ = {\ttfamily 50}, }\item[{}]{amin}{ = {\ttfamily 1e-\/8}, }\item[{}]{xtol}{ = {\ttfamily 1e-\/14}}\end{DoxyParamCaption})}

\begin{DoxyVerb}As `scalar_search_wolfe1` but do a line search to direction `pk`

Parameters
----------
f : callable
    Function `f(x)`
fprime : callable
    Gradient of `f`
xk : array_like
    Current point
pk : array_like
    Search direction

gfk : array_like, optional
    Gradient of `f` at point `xk`
old_fval : float, optional
    Value of `f` at point `xk`
old_old_fval : float, optional
    Value of `f` at point preceding `xk`

The rest of the parameters are the same as for `scalar_search_wolfe1`.

Returns
-------
stp, f_count, g_count, fval, old_fval
    As in `line_search_wolfe1`
gval : array
    Gradient of `f` at the final point
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00031}{31}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_aff851f2e3ee59ebac46a702b52918a39}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!line\_search\_wolfe2@{line\_search\_wolfe2}}
\index{line\_search\_wolfe2@{line\_search\_wolfe2}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{line\_search\_wolfe2()}{line\_search\_wolfe2()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_aff851f2e3ee59ebac46a702b52918a39} 
scipy.\+optimize.\+\_\+linesearch.\+line\+\_\+search\+\_\+wolfe2 (\begin{DoxyParamCaption}\item[{}]{f}{, }\item[{}]{myfprime}{, }\item[{}]{xk}{, }\item[{}]{pk}{, }\item[{}]{gfk}{ = {\ttfamily None}, }\item[{}]{old\+\_\+fval}{ = {\ttfamily None}, }\item[{}]{old\+\_\+old\+\_\+fval}{ = {\ttfamily None}, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{c2}{ = {\ttfamily 0.9}, }\item[{}]{amax}{ = {\ttfamily None}, }\item[{}]{extra\+\_\+condition}{ = {\ttfamily None}, }\item[{}]{maxiter}{ = {\ttfamily 10}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find alpha that satisfies strong Wolfe conditions.

Parameters
----------
f : callable f(x,*args)
    Objective function.
myfprime : callable f'(x,*args)
    Objective function gradient.
xk : ndarray
    Starting point.
pk : ndarray
    Search direction. The search direction must be a descent direction
    for the algorithm to converge.
gfk : ndarray, optional
    Gradient value for x=xk (xk being the current parameter
    estimate). Will be recomputed if omitted.
old_fval : float, optional
    Function value for x=xk. Will be recomputed if omitted.
old_old_fval : float, optional
    Function value for the point preceding x=xk.
args : tuple, optional
    Additional arguments passed to objective function.
c1 : float, optional
    Parameter for Armijo condition rule.
c2 : float, optional
    Parameter for curvature condition rule.
amax : float, optional
    Maximum step size
extra_condition : callable, optional
    A callable of the form ``extra_condition(alpha, x, f, g)``
    returning a boolean. Arguments are the proposed step ``alpha``
    and the corresponding ``x``, ``f`` and ``g`` values. The line search
    accepts the value of ``alpha`` only if this
    callable returns ``True``. If the callable returns ``False``
    for the step length, the algorithm will continue with
    new iterates. The callable is only called for iterates
    satisfying the strong Wolfe conditions.
maxiter : int, optional
    Maximum number of iterations to perform.

Returns
-------
alpha : float or None
    Alpha for which ``x_new = x0 + alpha * pk``,
    or None if the line search algorithm did not converge.
fc : int
    Number of function evaluations made.
gc : int
    Number of gradient evaluations made.
new_fval : float or None
    New function value ``f(x_new)=f(x0+alpha*pk)``,
    or None if the line search algorithm did not converge.
old_fval : float
    Old function value ``f(x0)``.
new_slope : float or None
    The local slope along the search direction at the
    new value ``<myfprime(x_new), pk>``,
    or None if the line search algorithm did not converge.


Notes
-----
Uses the line search algorithm to enforce strong Wolfe
conditions. See Wright and Nocedal, 'Numerical Optimization',
1999, pp. 59-61.

The search direction `pk` must be a descent direction (e.g.
``-myfprime(xk)``) to find a step length that satisfies the strong Wolfe
conditions. If the search direction is not a descent direction (e.g.
``myfprime(xk)``), then `alpha`, `new_fval`, and `new_slope` will be None.

Examples
--------
>>> import numpy as np
>>> from scipy.optimize import line_search

A objective function and its gradient are defined.

>>> def obj_func(x):
...     return (x[0])**2+(x[1])**2
>>> def obj_grad(x):
...     return [2*x[0], 2*x[1]]

We can find alpha that satisfies strong Wolfe conditions.

>>> start_point = np.array([1.8, 1.7])
>>> search_gradient = np.array([-1.0, -1.0])
>>> line_search(obj_func, obj_grad, start_point, search_gradient)
(1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00183}{183}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



References \mbox{\hyperlink{__linesearch_8py_source_l00328}{scalar\+\_\+search\+\_\+wolfe2()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a51e8a46d842d0c35a45ff56d578458d9}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!scalar\_search\_armijo@{scalar\_search\_armijo}}
\index{scalar\_search\_armijo@{scalar\_search\_armijo}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{scalar\_search\_armijo()}{scalar\_search\_armijo()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a51e8a46d842d0c35a45ff56d578458d9} 
scipy.\+optimize.\+\_\+linesearch.\+scalar\+\_\+search\+\_\+armijo (\begin{DoxyParamCaption}\item[{}]{phi}{, }\item[{}]{phi0}{, }\item[{}]{derphi0}{, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{alpha0}{ = {\ttfamily 1}, }\item[{}]{amin}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Minimize over alpha, the function ``phi(alpha)``.

Uses the interpolation algorithm (Armijo backtracking) as suggested by
Wright and Nocedal in 'Numerical Optimization', 1999, pp. 56-57

alpha > 0 is assumed to be a descent direction.

Returns
-------
alpha
phi1
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00678}{678}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00617}{line\+\_\+search\+\_\+armijo()}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a3b9a884bb40e777b2adaa77260e74c06}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!scalar\_search\_wolfe1@{scalar\_search\_wolfe1}}
\index{scalar\_search\_wolfe1@{scalar\_search\_wolfe1}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{scalar\_search\_wolfe1()}{scalar\_search\_wolfe1()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a3b9a884bb40e777b2adaa77260e74c06} 
scipy.\+optimize.\+\_\+linesearch.\+scalar\+\_\+search\+\_\+wolfe1 (\begin{DoxyParamCaption}\item[{}]{phi}{, }\item[{}]{derphi}{, }\item[{}]{phi0}{ = {\ttfamily None}, }\item[{}]{old\+\_\+phi0}{ = {\ttfamily None}, }\item[{}]{derphi0}{ = {\ttfamily None}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{c2}{ = {\ttfamily 0.9}, }\item[{}]{amax}{ = {\ttfamily 50}, }\item[{}]{amin}{ = {\ttfamily 1e-\/8}, }\item[{}]{xtol}{ = {\ttfamily 1e-\/14}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Scalar function search for alpha that satisfies strong Wolfe conditions

alpha > 0 is assumed to be a descent direction.

Parameters
----------
phi : callable phi(alpha)
    Function at point `alpha`
derphi : callable phi'(alpha)
    Objective function derivative. Returns a scalar.
phi0 : float, optional
    Value of phi at 0
old_phi0 : float, optional
    Value of phi at previous point
derphi0 : float, optional
    Value derphi at 0
c1 : float, optional
    Parameter for Armijo condition rule.
c2 : float, optional
    Parameter for curvature condition rule.
amax, amin : float, optional
    Maximum and minimum step size
xtol : float, optional
    Relative tolerance for an acceptable step.

Returns
-------
alpha : float
    Step size, or None if no suitable step was found
phi : float
    Value of `phi` at the new point `alpha`
phi0 : float
    Value of `phi` at `alpha=0`

Notes
-----
Uses routine DCSRCH from MINPACK.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00091}{91}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_adee20da3d04d3e1ba764760b8ff1156a}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!scalar\_search\_wolfe2@{scalar\_search\_wolfe2}}
\index{scalar\_search\_wolfe2@{scalar\_search\_wolfe2}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{scalar\_search\_wolfe2()}{scalar\_search\_wolfe2()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_adee20da3d04d3e1ba764760b8ff1156a} 
scipy.\+optimize.\+\_\+linesearch.\+scalar\+\_\+search\+\_\+wolfe2 (\begin{DoxyParamCaption}\item[{}]{phi}{, }\item[{}]{derphi}{, }\item[{}]{phi0}{ = {\ttfamily None}, }\item[{}]{old\+\_\+phi0}{ = {\ttfamily None}, }\item[{}]{derphi0}{ = {\ttfamily None}, }\item[{}]{c1}{ = {\ttfamily 1e-\/4}, }\item[{}]{c2}{ = {\ttfamily 0.9}, }\item[{}]{amax}{ = {\ttfamily None}, }\item[{}]{extra\+\_\+condition}{ = {\ttfamily None}, }\item[{}]{maxiter}{ = {\ttfamily 10}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find alpha that satisfies strong Wolfe conditions.

alpha > 0 is assumed to be a descent direction.

Parameters
----------
phi : callable phi(alpha)
    Objective scalar function.
derphi : callable phi'(alpha)
    Objective function derivative. Returns a scalar.
phi0 : float, optional
    Value of phi at 0.
old_phi0 : float, optional
    Value of phi at previous point.
derphi0 : float, optional
    Value of derphi at 0
c1 : float, optional
    Parameter for Armijo condition rule.
c2 : float, optional
    Parameter for curvature condition rule.
amax : float, optional
    Maximum step size.
extra_condition : callable, optional
    A callable of the form ``extra_condition(alpha, phi_value)``
    returning a boolean. The line search accepts the value
    of ``alpha`` only if this callable returns ``True``.
    If the callable returns ``False`` for the step length,
    the algorithm will continue with new iterates.
    The callable is only called for iterates satisfying
    the strong Wolfe conditions.
maxiter : int, optional
    Maximum number of iterations to perform.

Returns
-------
alpha_star : float or None
    Best alpha, or None if the line search algorithm did not converge.
phi_star : float
    phi at alpha_star.
phi0 : float
    phi at 0.
derphi_star : float or None
    derphi at alpha_star, or None if the line search algorithm
    did not converge.

Notes
-----
Uses the line search algorithm to enforce strong Wolfe
conditions. See Wright and Nocedal, 'Numerical Optimization',
1999, pp. 59-61.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00325}{325}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.



References \mbox{\hyperlink{__linesearch_8py_source_l00527}{\+\_\+zoom()}}.



Referenced by \mbox{\hyperlink{__linesearch_8py_source_l00185}{line\+\_\+search\+\_\+wolfe2()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_ade7bbc705f2d647b07bfa055c28aba99}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!\_\_all\_\_@{\_\_all\_\_}}
\index{\_\_all\_\_@{\_\_all\_\_}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{\_\_all\_\_}{\_\_all\_\_}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_ade7bbc705f2d647b07bfa055c28aba99} 
list scipy.\+optimize.\+\_\+linesearch.\+\_\+\+\_\+all\+\_\+\+\_\+\hspace{0.3cm}{\ttfamily [private]}}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [\textcolor{stringliteral}{'LineSearchWarning'},\ \textcolor{stringliteral}{'line\_search\_wolfe1'},\ \textcolor{stringliteral}{'line\_search\_wolfe2'},}
\DoxyCodeLine{00002\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{'scalar\_search\_wolfe1'},\ \textcolor{stringliteral}{'scalar\_search\_wolfe2'},}
\DoxyCodeLine{00003\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{'line\_search\_armijo'}]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00019}{19}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

\Hypertarget{namespacescipy_1_1optimize_1_1__linesearch_a974f515a245b8bfecca6d3425f4a5c42}\index{scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}!line\_search@{line\_search}}
\index{line\_search@{line\_search}!scipy.optimize.\_linesearch@{scipy.optimize.\_linesearch}}
\doxysubsubsection{\texorpdfstring{line\_search}{line\_search}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__linesearch_a974f515a245b8bfecca6d3425f4a5c42} 
scipy.\+optimize.\+\_\+linesearch.\+line\+\_\+search = \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__linesearch_aeccc455ff7d031dcb3e11045b705bf9c}{line\+\_\+search\+\_\+wolfe1}}}



Definition at line \mbox{\hyperlink{__linesearch_8py_source_l00174}{174}} of file \mbox{\hyperlink{__linesearch_8py_source}{\+\_\+linesearch.\+py}}.

