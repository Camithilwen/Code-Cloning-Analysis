\doxysection{scipy.\+optimize.\+\_\+dual\+\_\+annealing Namespace Reference}
\hypertarget{namespacescipy_1_1optimize_1_1__dual__annealing}{}\label{namespacescipy_1_1optimize_1_1__dual__annealing}\index{scipy.optimize.\_dual\_annealing@{scipy.optimize.\_dual\_annealing}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__dual__annealing_1_1EnergyState}{Energy\+State}}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__dual__annealing_1_1LocalSearchWrapper}{Local\+Search\+Wrapper}}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__dual__annealing_1_1ObjectiveFunWrapper}{Objective\+Fun\+Wrapper}}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__dual__annealing_1_1StrategyChain}{Strategy\+Chain}}
\item 
class \mbox{\hyperlink{classscipy_1_1optimize_1_1__dual__annealing_1_1VisitingDistribution}{Visiting\+Distribution}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__dual__annealing_a74b11a06f5c4ff98596433b936026ced}{dual\+\_\+annealing}} (func, bounds, args=(), maxiter=1000, minimizer\+\_\+kwargs=None, initial\+\_\+temp=5230., restart\+\_\+temp\+\_\+ratio=2.e-\/5, visit=2.\+62, accept=-\/5.\+0, maxfun=1e7, seed=None, no\+\_\+local\+\_\+search=False, callback=None, x0=None)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacescipy_1_1optimize_1_1__dual__annealing_a0b782f8e4f46f73e871f86f9f52d857a}{\+\_\+\+\_\+all\+\_\+\+\_\+}} = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__dual__annealing_a74b11a06f5c4ff98596433b936026ced}{dual\+\_\+annealing}}\textquotesingle{}\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}A Dual Annealing global optimization algorithm
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__dual__annealing_a74b11a06f5c4ff98596433b936026ced}\index{scipy.optimize.\_dual\_annealing@{scipy.optimize.\_dual\_annealing}!dual\_annealing@{dual\_annealing}}
\index{dual\_annealing@{dual\_annealing}!scipy.optimize.\_dual\_annealing@{scipy.optimize.\_dual\_annealing}}
\doxysubsubsection{\texorpdfstring{dual\_annealing()}{dual\_annealing()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__dual__annealing_a74b11a06f5c4ff98596433b936026ced} 
scipy.\+optimize.\+\_\+dual\+\_\+annealing.\+dual\+\_\+annealing (\begin{DoxyParamCaption}\item[{}]{func}{, }\item[{}]{bounds}{, }\item[{}]{args}{ = {\ttfamily ()}, }\item[{}]{maxiter}{ = {\ttfamily 1000}, }\item[{}]{minimizer\+\_\+kwargs}{ = {\ttfamily None}, }\item[{}]{initial\+\_\+temp}{ = {\ttfamily 5230.}, }\item[{}]{restart\+\_\+temp\+\_\+ratio}{ = {\ttfamily 2.e-\/5}, }\item[{}]{visit}{ = {\ttfamily 2.62}, }\item[{}]{accept}{ = {\ttfamily -\/5.0}, }\item[{}]{maxfun}{ = {\ttfamily 1e7}, }\item[{}]{seed}{ = {\ttfamily None}, }\item[{}]{no\+\_\+local\+\_\+search}{ = {\ttfamily False}, }\item[{}]{callback}{ = {\ttfamily None}, }\item[{}]{x0}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find the global minimum of a function using Dual Annealing.

Parameters
----------
func : callable
    The objective function to be minimized. Must be in the form
    ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array
    and ``args`` is a  tuple of any additional fixed parameters needed to
    completely specify the function.
bounds : sequence or `Bounds`
    Bounds for variables. There are two ways to specify the bounds:

    1. Instance of `Bounds` class.
    2. Sequence of ``(min, max)`` pairs for each element in `x`.

args : tuple, optional
    Any additional fixed parameters needed to completely specify the
    objective function.
maxiter : int, optional
    The maximum number of global search iterations. Default value is 1000.
minimizer_kwargs : dict, optional
    Extra keyword arguments to be passed to the local minimizer
    (`minimize`). Some important options could be:
    ``method`` for the minimizer method to use and ``args`` for
    objective function additional arguments.
initial_temp : float, optional
    The initial temperature, use higher values to facilitates a wider
    search of the energy landscape, allowing dual_annealing to escape
    local minima that it is trapped in. Default value is 5230. Range is
    (0.01, 5.e4].
restart_temp_ratio : float, optional
    During the annealing process, temperature is decreasing, when it
    reaches ``initial_temp * restart_temp_ratio``, the reannealing process
    is triggered. Default value of the ratio is 2e-5. Range is (0, 1).
visit : float, optional
    Parameter for visiting distribution. Default value is 2.62. Higher
    values give the visiting distribution a heavier tail, this makes
    the algorithm jump to a more distant region. The value range is (1, 3].
accept : float, optional
    Parameter for acceptance distribution. It is used to control the
    probability of acceptance. The lower the acceptance parameter, the
    smaller the probability of acceptance. Default value is -5.0 with
    a range (-1e4, -5].
maxfun : int, optional
    Soft limit for the number of objective function calls. If the
    algorithm is in the middle of a local search, this number will be
    exceeded, the algorithm will stop just after the local search is
    done. Default value is 1e7.
seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional
    If `seed` is None (or `np.random`), the `numpy.random.RandomState`
    singleton is used.
    If `seed` is an int, a new ``RandomState`` instance is used,
    seeded with `seed`.
    If `seed` is already a ``Generator`` or ``RandomState`` instance then
    that instance is used.
    Specify `seed` for repeatable minimizations. The random numbers
    generated with this seed only affect the visiting distribution function
    and new coordinates generation.
no_local_search : bool, optional
    If `no_local_search` is set to True, a traditional Generalized
    Simulated Annealing will be performed with no local search
    strategy applied.
callback : callable, optional
    A callback function with signature ``callback(x, f, context)``,
    which will be called for all minima found.
    ``x`` and ``f`` are the coordinates and function value of the
    latest minimum found, and ``context`` has value in [0, 1, 2], with the
    following meaning:

        - 0: minimum detected in the annealing process.
        - 1: detection occurred in the local search process.
        - 2: detection done in the dual annealing process.

    If the callback implementation returns True, the algorithm will stop.
x0 : ndarray, shape(n,), optional
    Coordinates of a single N-D starting point.

Returns
-------
res : OptimizeResult
    The optimization result represented as a `OptimizeResult` object.
    Important attributes are: ``x`` the solution array, ``fun`` the value
    of the function at the solution, and ``message`` which describes the
    cause of the termination.
    See `OptimizeResult` for a description of other attributes.

Notes
-----
This function implements the Dual Annealing optimization. This stochastic
approach derived from [3]_ combines the generalization of CSA (Classical
Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled
to a strategy for applying a local search on accepted locations [4]_.
An alternative implementation of this same algorithm is described in [5]_
and benchmarks are presented in [6]_. This approach introduces an advanced
method to refine the solution found by the generalized annealing
process. This algorithm uses a distorted Cauchy-Lorentz visiting
distribution, with its shape controlled by the parameter :math:`q_{v}`

.. math::

    g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\
    \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\
    \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\
    \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\
    \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}

Where :math:`t` is the artificial time. This visiting distribution is used
to generate a trial jump distance :math:`\\Delta x(t)` of variable
:math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.

From the starting point, after calling the visiting distribution
function, the acceptance probability is computed as follows:

.. math::

    p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\
    \\frac{1}{1-q_{a}}}\\}}

Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero
acceptance probability is assigned to the cases where

.. math::

    [1-(1-q_{a}) \\beta \\Delta E] < 0

The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to

.. math::

    T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\
    1 + t\\right)^{q_{v}-1}-1}

Where :math:`q_{v}` is the visiting parameter.

.. versionadded:: 1.2.0

References
----------
.. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs
    statistics. Journal of Statistical Physics, 52, 479-487 (1998).
.. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.
    Physica A, 233, 395-406 (1996).
.. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated
    Annealing Algorithm and Its Application to the Thomson Model.
    Physics Letters A, 233, 216-220 (1997).
.. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated
    Annealing. Physical Review E, 62, 4473 (2000).
.. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized
    Simulated Annealing for Efficient Global Optimization: the GenSA
    Package for R. The R Journal, Volume 5/1 (2013).
.. [6] Mullen, K. Continuous Global Optimization in R. Journal of
    Statistical Software, 60(6), 1 - 45, (2014).
    :doi:`10.18637/jss.v060.i06`

Examples
--------
The following example is a 10-D problem, with many local minima.
The function involved is called Rastrigin
(https://en.wikipedia.org/wiki/Rastrigin_function)

>>> import numpy as np
>>> from scipy.optimize import dual_annealing
>>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)
>>> lw = [-5.12] * 10
>>> up = [5.12] * 10
>>> ret = dual_annealing(func, bounds=list(zip(lw, up)))
>>> ret.x
array([-4.26437714e-09, -3.91699361e-09, -1.86149218e-09, -3.97165720e-09,
       -6.29151648e-09, -6.53145322e-09, -3.93616815e-09, -6.55623025e-09,
       -6.05775280e-09, -5.00668935e-09]) # random
>>> ret.fun
0.000000
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__dual__annealing_8py_source_l00440}{440}} of file \mbox{\hyperlink{__dual__annealing_8py_source}{\+\_\+dual\+\_\+annealing.\+py}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacescipy_1_1optimize_1_1__dual__annealing_a0b782f8e4f46f73e871f86f9f52d857a}\index{scipy.optimize.\_dual\_annealing@{scipy.optimize.\_dual\_annealing}!\_\_all\_\_@{\_\_all\_\_}}
\index{\_\_all\_\_@{\_\_all\_\_}!scipy.optimize.\_dual\_annealing@{scipy.optimize.\_dual\_annealing}}
\doxysubsubsection{\texorpdfstring{\_\_all\_\_}{\_\_all\_\_}}
{\footnotesize\ttfamily \label{namespacescipy_1_1optimize_1_1__dual__annealing_a0b782f8e4f46f73e871f86f9f52d857a} 
list scipy.\+optimize.\+\_\+dual\+\_\+annealing.\+\_\+\+\_\+all\+\_\+\+\_\+ = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1optimize_1_1__dual__annealing_a74b11a06f5c4ff98596433b936026ced}{dual\+\_\+annealing}}\textquotesingle{}\mbox{]}\hspace{0.3cm}{\ttfamily [private]}}



Definition at line \mbox{\hyperlink{__dual__annealing_8py_source_l00017}{17}} of file \mbox{\hyperlink{__dual__annealing_8py_source}{\+\_\+dual\+\_\+annealing.\+py}}.

