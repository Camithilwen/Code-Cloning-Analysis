\doxysection{\+\_\+linear\+\_\+loss.\+py}
\hypertarget{__linear__loss_8py_source}{}\label{__linear__loss_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_linear\_loss.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_linear\_loss.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00002}00002\ \textcolor{stringliteral}{Loss\ functions\ for\ linear\ models\ with\ raw\_prediction\ =\ X\ @\ coef}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00003}00003\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00004}00004\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00005}00005\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00006}00006\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00008}00008\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00009}00009\ \textcolor{keyword}{from}\ scipy\ \textcolor{keyword}{import}\ sparse}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00010}00010\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00011}00011\ \textcolor{keyword}{from}\ ..utils.extmath\ \textcolor{keyword}{import}\ squared\_norm}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00012}00012\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00014}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss_ad6b0bcabf47bf18c3233e3173fd41156}{00014}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss_ad6b0bcabf47bf18c3233e3173fd41156}{sandwich\_dot}}(X,\ W):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00015}00015\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ sandwich\ product\ X.T\ @\ diag(W)\ @\ X."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00016}00016\ \ \ \ \ \textcolor{comment}{\#\ TODO:\ This\ "{}sandwich\ product"{}\ is\ the\ main\ computational\ bottleneck\ for\ solvers}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00017}00017\ \ \ \ \ \textcolor{comment}{\#\ that\ use\ the\ full\ hessian\ matrix.\ Here,\ thread\ parallelism\ would\ pay-\/off\ the}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00018}00018\ \ \ \ \ \textcolor{comment}{\#\ most.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00019}00019\ \ \ \ \ \textcolor{comment}{\#\ While\ a\ dedicated\ Cython\ routine\ could\ exploit\ the\ symmetry,\ it\ is\ very\ hard\ to}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00020}00020\ \ \ \ \ \textcolor{comment}{\#\ beat\ BLAS\ GEMM,\ even\ thought\ the\ latter\ cannot\ exploit\ the\ symmetry,\ unless\ one}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00021}00021\ \ \ \ \ \textcolor{comment}{\#\ pays\ the\ price\ of\ taking\ square\ roots\ and\ implements}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00022}00022\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ sqrtWX\ =\ sqrt(W)[:\ None]\ *\ X}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00023}00023\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ return\ sqrtWX.T\ @\ sqrtWX}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00024}00024\ \ \ \ \ \textcolor{comment}{\#\ which\ (might)\ detect\ the\ symmetry\ and\ use\ BLAS\ SYRK\ under\ the\ hood.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00025}00025\ \ \ \ \ n\_samples\ =\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00026}00026\ \ \ \ \ \textcolor{keywordflow}{if}\ sparse.issparse(X):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00027}00027\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00028}00028\ \ \ \ \ \ \ \ \ \ \ \ \ X.T\ @\ \mbox{\hyperlink{classscipy_1_1sparse_1_1__dia_1_1dia__matrix}{sparse.dia\_matrix}}((W,\ 0),\ shape=(n\_samples,\ n\_samples))\ @\ X}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00029}00029\ \ \ \ \ \ \ \ \ ).toarray()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00030}00030\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00031}00031\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ np.einsum\ may\ use\ less\ memory\ but\ the\ following,\ using\ BLAS\ matrix}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00032}00032\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ multiplication\ (gemm),\ is\ by\ far\ faster.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00033}00033\ \ \ \ \ \ \ \ \ WX\ =\ W[:,\ \textcolor{keywordtype}{None}]\ *\ X}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00034}00034\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ X.T\ @\ WX}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00035}00035\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00036}00036\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00037}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{00037}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00038}00038\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}General\ class\ for\ loss\ functions\ with\ raw\_prediction\ =\ X\ @\ coef\ +\ intercept.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00039}00039\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00040}00040\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ raw\_prediction\ is\ also\ known\ as\ linear\ predictor.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00041}00041\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00042}00042\ \textcolor{stringliteral}{\ \ \ \ The\ loss\ is\ the\ average\ of\ per\ sample\ losses\ and\ includes\ a\ term\ for\ L2}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00043}00043\ \textcolor{stringliteral}{\ \ \ \ regularization::}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00044}00044\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00045}00045\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ =\ 1\ /\ s\_sum\ *\ sum\_i\ s\_i\ loss(y\_i,\ X\_i\ @\ coef\ +\ intercept)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00046}00046\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ 1/2\ *\ l2\_reg\_strength\ *\ ||coef||\_2\string^2}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00047}00047\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00048}00048\ \textcolor{stringliteral}{\ \ \ \ with\ sample\ weights\ s\_i=1\ if\ sample\_weight=None\ and\ s\_sum=sum\_i\ s\_i.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00049}00049\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00050}00050\ \textcolor{stringliteral}{\ \ \ \ Gradient\ and\ hessian,\ for\ simplicity\ without\ intercept,\ are::}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00051}00051\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00052}00052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ =\ 1\ /\ s\_sum\ *\ X.T\ @\ loss.gradient\ +\ l2\_reg\_strength\ *\ coef}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00053}00053\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\ =\ 1\ /\ s\_sum\ *\ X.T\ @\ diag(loss.hessian)\ @\ X}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00054}00054\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ l2\_reg\_strength\ *\ identity}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00055}00055\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00056}00056\ \textcolor{stringliteral}{\ \ \ \ Conventions:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00057}00057\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ fit\_intercept:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00058}00058\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ n\_dof\ =\ \ n\_features\ +\ 1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00059}00059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ else:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00060}00060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00061}00061\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00062}00062\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ base\_loss.is\_multiclass:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00063}00063\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.shape\ =\ (n\_classes,\ n\_dof)\ or\ ravelled\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00064}00064\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ else:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00065}00065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.shape\ =\ (n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00066}00066\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00067}00067\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ intercept\ term\ is\ at\ the\ end\ of\ the\ coef\ array:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00068}00068\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ base\_loss.is\_multiclass:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00069}00069\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ if\ coef.shape\ (n\_classes,\ n\_dof):}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00070}00070\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ coef[:,\ -\/1]}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00071}00071\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ if\ coef.shape\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00072}00072\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ coef[n\_features::n\_dof]\ =\ coef[(n\_dof-\/1)::n\_dof]}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00073}00073\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ intercept.shape\ =\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00074}00074\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ else:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00075}00075\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00076}00076\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00077}00077\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Shape\ of\ gradient\ follows\ shape\ of\ coef.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00078}00078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient.shape\ =\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00079}00079\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00080}00080\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ But\ hessian\ (to\ make\ our\ lives\ simpler)\ are\ always\ 2-\/d:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00081}00081\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ base\_loss.is\_multiclass:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00082}00082\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ hessian.shape\ =\ (n\_classes\ *\ n\_dof,\ n\_classes\ *\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00083}00083\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ else:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00084}00084\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ hessian.shape\ =\ (n\_dof,\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00085}00085\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00086}00086\ \textcolor{stringliteral}{\ \ \ \ Note:\ If\ coef\ has\ shape\ (n\_classes\ *\ n\_dof,),\ the\ 2d-\/array\ can\ be\ reconstructed\ as}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00087}00087\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00088}00088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{})}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00089}00089\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ The\ option\ order="{}F"{}\ makes\ coef[:,\ i]\ contiguous.\ This,\ in\ turn,\ makes\ the}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ coefficients\ without\ intercept,\ coef[:,\ :-\/1],\ contiguous\ and\ speeds\ up}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ matrix-\/vector\ computations.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00093}00093\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00094}00094\ \textcolor{stringliteral}{\ \ \ \ Note:\ If\ the\ average\ loss\ per\ sample\ is\ wanted\ instead\ of\ the\ sum\ of\ the\ loss\ per}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ sample,\ one\ can\ simply\ use\ a\ rescaled\ sample\_weight\ such\ that}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ sum(sample\_weight)\ =\ 1.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00097}00097\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00099}00099\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ base\_loss\ :\ instance\ of\ class\ BaseLoss\ from\ sklearn.\_loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ fit\_intercept\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00103}00103\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00104}00104\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ base\_loss,\ fit\_intercept):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00105}00105\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}\ =\ base\_loss}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00106}00106\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}\ =\ fit\_intercept}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00107}00107\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00108}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a90d92487a6eb759c86797dfbfe23153c}{00108}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a90d92487a6eb759c86797dfbfe23153c}{init\_zero\_coef}}(self,\ X,\ dtype=None):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00109}00109\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Allocate\ coef\ of\ correct\ shape\ with\ zeros.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00110}00110\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ dtype\ :\ data-\/type,\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Overrides\ the\ data\ type\ of\ coef.\ With\ dtype=None,\ coef\ will\ have\ the\ same}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ dtype\ as\ X.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00118}00118\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00119}00119\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00120}00120\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00121}00121\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,)\ or\ (n\_classes,\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00122}00122\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00123}00123\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ n\_features\ =\ X.shape[1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ n\_classes\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features\ +\ 1}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00128}00128\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ np.zeros\_like(X,\ shape=(n\_classes,\ n\_dof),\ dtype=dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00132}00132\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ \ \ \ \ coef\ =\ np.zeros\_like(X,\ shape=n\_dof,\ dtype=dtype)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ coef}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00135}00135\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00136}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{00136}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(self,\ coef):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Helper\ function\ to\ get\ coefficients\ and\ intercept.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00138}00138\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00139}00139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00140}00140\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00141}00141\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00142}00142\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00143}00143\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00144}00144\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00145}00145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00146}00146\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00147}00147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00148}00148\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00149}00149\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ weights\ :\ ndarray\ of\ shape\ (n\_features,)\ or\ (n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00150}00150\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ without\ intercept\ term.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00151}00151\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\ :\ float\ or\ ndarray\ of\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00152}00152\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Intercept\ terms.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00153}00153\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00154}00154\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00157}00157\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weights\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weights\ =\ coef}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00162}00162\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ reshape\ to\ (n\_classes,\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00163}00163\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weights\ =\ coef.reshape((self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes,\ -\/1),\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weights\ =\ coef}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00167}00167\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ weights[:,\ -\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weights\ =\ weights[:,\ :-\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00170}00170\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00171}00171\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00172}00172\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ weights,\ intercept}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00174}00174\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00175}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{00175}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(self,\ coef,\ X):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00176}00176\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Helper\ function\ to\ get\ coefficients,\ intercept\ and\ raw\_prediction.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00177}00177\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00182}00182\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00183}00183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00184}00184\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00185}00185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00186}00186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00187}00187\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00188}00188\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00189}00189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00190}00190\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ weights\ :\ ndarray\ of\ shape\ (n\_features,)\ or\ (n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00191}00191\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ without\ intercept\ term.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00192}00192\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ intercept\ :\ float\ or\ ndarray\ of\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00193}00193\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Intercept\ terms.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00194}00194\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ ndarray\ of\ shape\ (n\_samples,)\ or\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00195}00195\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00196}00196\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ weights,\ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(coef)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00198}00198\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ weights\ +\ intercept}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ weights\ has\ shape\ (n\_classes,\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ weights.T\ +\ intercept\ \ \textcolor{comment}{\#\ ndarray,\ likely\ C-\/contiguous}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00204}00204\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00205}00205\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ weights,\ intercept,\ raw\_prediction}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00206}00206\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00207}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}{00207}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}{l2\_penalty}}(self,\ weights,\ l2\_reg\_strength):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00208}00208\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ L2\ penalty\ term\ l2\_reg\_strength/2\ *||w||\_2\string^2."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00209}00209\ \ \ \ \ \ \ \ \ norm2\_w\ =\ weights\ @\ weights\ \textcolor{keywordflow}{if}\ weights.ndim\ ==\ 1\ \textcolor{keywordflow}{else}\ squared\_norm(weights)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00210}00210\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ 0.5\ *\ l2\_reg\_strength\ *\ norm2\_w}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00211}00211\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00212}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f}{00212}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f}{loss}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00213}00213\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00215}00215\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00216}00216\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00218}00218\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0.0,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00219}00219\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ raw\_prediction=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00221}00221\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ loss\ as\ weighted\ average\ over\ point-\/wise\ losses.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00223}00223\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00224}00224\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00225}00225\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00226}00226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00227}00227\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00228}00228\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00229}00229\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00230}00230\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00231}00231\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00232}00232\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00233}00233\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00234}00234\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00235}00235\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ contiguous\ array\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00236}00236\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00237}00237\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l2\_reg\_strength\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00238}00238\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ L2\ regularization\ strength}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00239}00239\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00240}00240\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Number\ of\ OpenMP\ threads\ to\ use.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00241}00241\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00242}00242\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00243}00243\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).\ If\ provided,\ these\ are\ used.\ If}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00244}00244\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ None,\ then\ raw\_prediction\ =\ X\ @\ coef\ +\ intercept\ is\ calculated.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00245}00245\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00246}00246\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00247}00247\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00248}00248\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ :\ float}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00249}00249\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weighted\ average\ of\ losses\ per\ sample,\ plus\ penalty.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00250}00250\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(coef,\ X)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(coef)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00255}00255\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ loss\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a502d1c80fd8bf5c9f2fefeb71c12d15f}{loss}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00260}00260\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ loss\ =\ np.average(loss,\ weights=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00263}00263\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss\ +\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}{l2\_penalty}}(weights,\ l2\_reg\_strength)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00265}00265\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00266}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958}{00266}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958}{loss\_gradient}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00267}00267\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00268}00268\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00269}00269\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00270}00270\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00271}00271\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00272}00272\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0.0,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ raw\_prediction=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00275}00275\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Computes\ the\ sum\ of\ loss\ and\ gradient\ w.r.t.\ coef.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00277}00277\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00278}00278\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00279}00279\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00280}00280\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00281}00281\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00282}00282\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00283}00283\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00284}00284\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00285}00285\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00286}00286\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00287}00287\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00288}00288\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00289}00289\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ contiguous\ array\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00290}00290\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00291}00291\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l2\_reg\_strength\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00292}00292\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ L2\ regularization\ strength}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00293}00293\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00294}00294\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Number\ of\ OpenMP\ threads\ to\ use.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00295}00295\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00296}00296\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00297}00297\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).\ If\ provided,\ these\ are\ used.\ If}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00298}00298\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ None,\ then\ raw\_prediction\ =\ X\ @\ coef\ +\ intercept\ is\ calculated.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00299}00299\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00300}00300\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00301}00301\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00302}00302\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ :\ float}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00303}00303\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Weighted\ average\ of\ losses\ per\ sample,\ plus\ penalty.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00304}00304\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00305}00305\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ ndarray\ of\ shape\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00306}00306\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ The\ gradient\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00307}00307\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00308}00308\ \ \ \ \ \ \ \ \ (n\_samples,\ n\_features),\ n\_classes\ =\ X.shape,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features\ +\ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00310}00310\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(coef,\ X)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(coef)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00315}00315\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ loss,\ grad\_pointwise\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aadedf74d59c6f362cac8adef57efc958}{loss\_gradient}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00318}00318\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00320}00320\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00321}00321\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ sw\_sum\ =\ n\_samples\ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ np.sum(sample\_weight)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00323}00323\ \ \ \ \ \ \ \ \ loss\ =\ loss.sum()\ /\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ loss\ +=\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a73116d9f130017dab7f045eab9a27dd6}{l2\_penalty}}(weights,\ l2\_reg\_strength)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00325}00325\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00327}00327\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00328}00328\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00329}00329\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty\_like(coef,\ dtype=weights.dtype)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00330}00330\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:n\_features]\ =\ X.T\ @\ grad\_pointwise\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00331}00331\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00332}00332\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[-\/1]\ =\ grad\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00333}00333\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00334}00334\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty((n\_classes,\ n\_dof),\ dtype=weights.dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ grad\_pointwise.shape\ =\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ :n\_features]\ =\ grad\_pointwise.T\ @\ X\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ -\/1]\ =\ grad\_pointwise.sum(axis=0)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ grad.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00341}00341\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss,\ grad}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00343}00343\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00344}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2}{00344}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2}{gradient}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00345}00345\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00346}00346\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00347}00347\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00348}00348\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0.0,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00351}00351\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00352}00352\ \ \ \ \ \ \ \ \ raw\_prediction=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00353}00353\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00354}00354\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Computes\ the\ gradient\ w.r.t.\ coef.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00355}00355\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00356}00356\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00357}00357\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00358}00358\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00359}00359\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00360}00360\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00361}00361\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00362}00362\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00363}00363\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00364}00364\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00365}00365\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00366}00366\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00367}00367\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ contiguous\ array\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00368}00368\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00369}00369\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l2\_reg\_strength\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00370}00370\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ L2\ regularization\ strength}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00371}00371\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00372}00372\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Number\ of\ OpenMP\ threads\ to\ use.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00373}00373\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00374}00374\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00375}00375\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).\ If\ provided,\ these\ are\ used.\ If}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00376}00376\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ None,\ then\ raw\_prediction\ =\ X\ @\ coef\ +\ intercept\ is\ calculated.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00377}00377\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00378}00378\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00379}00379\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00380}00380\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ ndarray\ of\ shape\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00381}00381\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ The\ gradient\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00382}00382\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00383}00383\ \ \ \ \ \ \ \ \ (n\_samples,\ n\_features),\ n\_classes\ =\ X.shape,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00384}00384\ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features\ +\ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00385}00385\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00386}00386\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00387}00387\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(coef,\ X)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00388}00388\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00389}00389\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(coef)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00390}00390\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00391}00391\ \ \ \ \ \ \ \ \ grad\_pointwise\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad058c2c80b2430c6cece8b6e5f37a3c2}{gradient}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00392}00392\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00393}00393\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00394}00394\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00395}00395\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00396}00396\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00397}00397\ \ \ \ \ \ \ \ \ sw\_sum\ =\ n\_samples\ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ np.sum(sample\_weight)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00398}00398\ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00399}00399\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00400}00400\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00401}00401\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty\_like(coef,\ dtype=weights.dtype)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00402}00402\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:n\_features]\ =\ X.T\ @\ grad\_pointwise\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00403}00403\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00404}00404\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[-\/1]\ =\ grad\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00405}00405\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty((n\_classes,\ n\_dof),\ dtype=weights.dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ gradient.shape\ =\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ :n\_features]\ =\ grad\_pointwise.T\ @\ X\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ -\/1]\ =\ grad\_pointwise.sum(axis=0)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00414}00414\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00416}00416\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00417}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}{00417}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}{gradient\_hessian}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00418}00418\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00419}00419\ \ \ \ \ \ \ \ \ coef,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00420}00420\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00421}00421\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00422}00422\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00423}00423\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0.0,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00424}00424\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00425}00425\ \ \ \ \ \ \ \ \ gradient\_out=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00426}00426\ \ \ \ \ \ \ \ \ hessian\_out=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00427}00427\ \ \ \ \ \ \ \ \ raw\_prediction=None,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00428}00428\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00429}00429\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Computes\ gradient\ and\ hessian\ w.r.t.\ coef.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00430}00430\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00431}00431\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00432}00432\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00433}00433\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00434}00434\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00435}00435\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00436}00436\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00437}00437\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00438}00438\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00439}00439\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00440}00440\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00441}00441\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00442}00442\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ contiguous\ array\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00443}00443\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00444}00444\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l2\_reg\_strength\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00445}00445\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ L2\ regularization\ strength}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00446}00446\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00447}00447\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Number\ of\ OpenMP\ threads\ to\ use.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00448}00448\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\_out\ :\ None\ or\ ndarray\ of\ shape\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00449}00449\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ gradient\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00450}00450\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00451}00451\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\_out\ :\ None\ or\ ndarray\ of\ shape\ (n\_dof,\ n\_dof)\ or\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00452}00452\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_classes\ *\ n\_dof,\ n\_classes\ *\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00453}00453\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ hessian\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00454}00454\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00455}00455\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00456}00456\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00457}00457\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).\ If\ provided,\ these\ are\ used.\ If}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00458}00458\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ None,\ then\ raw\_prediction\ =\ X\ @\ coef\ +\ intercept\ is\ calculated.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00459}00459\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00460}00460\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00461}00461\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00462}00462\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ ndarray\ of\ shape\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00463}00463\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ The\ gradient\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00464}00464\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00465}00465\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\ :\ ndarray\ of\ shape\ (n\_dof,\ n\_dof)\ or\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00466}00466\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_classes,\ n\_dof,\ n\_dof,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00467}00467\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00468}00468\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00469}00469\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\_warning\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00470}00470\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ True\ if\ pointwise\ hessian\ has\ more\ than\ 25\%\ of\ its\ elements\ non-\/positive.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00471}00471\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00472}00472\ \ \ \ \ \ \ \ \ (n\_samples,\ n\_features),\ n\_classes\ =\ X.shape,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00473}00473\ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features\ +\ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00474}00474\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00475}00475\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(coef,\ X)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00476}00476\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00477}00477\ \ \ \ \ \ \ \ \ \ \ \ \ weights,\ intercept\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ad6aa682bba552f631193aeb838bbb4fb}{weight\_intercept}}(coef)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00478}00478\ \ \ \ \ \ \ \ \ sw\_sum\ =\ n\_samples\ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ np.sum(sample\_weight)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00479}00479\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00480}00480\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Allocate\ gradient.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00481}00481\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00482}00482\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty\_like(coef,\ dtype=weights.dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00483}00483\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ gradient\_out.shape\ !=\ coef.shape:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}gradient\_out\ is\ required\ to\ have\ shape\ coef.shape\ =\ \{coef.shape\};\ "{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00486}00486\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}got\ \{gradient\_out.shape\}."{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00487}00487\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass\ \textcolor{keywordflow}{and}\ \textcolor{keywordflow}{not}\ gradient\_out.flags.f\_contiguous:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00489}00489\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}gradient\_out\ must\ be\ F-\/contiguous."{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00490}00490\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00491}00491\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ gradient\_out}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00492}00492\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Allocate\ hessian.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00493}00493\ \ \ \ \ \ \ \ \ n\ =\ coef.size\ \ \textcolor{comment}{\#\ for\ multinomial\ this\ equals\ n\_dof\ *\ n\_classes}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00494}00494\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hessian\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00495}00495\ \ \ \ \ \ \ \ \ \ \ \ \ hess\ =\ np.empty((n,\ n),\ dtype=weights.dtype)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00496}00496\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ hessian\_out.shape\ !=\ (n,\ n):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}hessian\_out\ is\ required\ to\ have\ shape\ (\{n,\ n\});\ got\ "{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\{hessian\_out.shape=\}."{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00501}00501\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass\ \textcolor{keywordflow}{and}\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00502}00502\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{not}\ hessian\_out.flags.c\_contiguous\ \textcolor{keywordflow}{and}\ \textcolor{keywordflow}{not}\ hessian\_out.flags.f\_contiguous}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00503}00503\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00504}00504\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(\textcolor{stringliteral}{"{}hessian\_out\ must\ be\ contiguous."{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00505}00505\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00506}00506\ \ \ \ \ \ \ \ \ \ \ \ \ hess\ =\ hessian\_out}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00507}00507\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00508}00508\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00509}00509\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise,\ hess\_pointwise\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}{gradient\_hessian}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00510}00510\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00511}00511\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00512}00512\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00513}00513\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00514}00514\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00515}00515\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00516}00516\ \ \ \ \ \ \ \ \ \ \ \ \ hess\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00517}00517\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00518}00518\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ For\ non-\/canonical\ link\ functions\ and\ far\ away\ from\ the\ optimum,\ the}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00519}00519\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ pointwise\ hessian\ can\ be\ negative.\ We\ take\ care\ that\ 75\%\ of\ the\ hessian}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00520}00520\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ entries\ are\ positive.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00521}00521\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_warning\ =\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00522}00522\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ np.average(hess\_pointwise\ <=\ 0,\ weights=sample\_weight)\ >\ 0.25}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00523}00523\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00524}00524\ \ \ \ \ \ \ \ \ \ \ \ \ hess\_pointwise\ =\ np.abs(hess\_pointwise)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00525}00525\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00526}00526\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:n\_features]\ =\ X.T\ @\ grad\_pointwise\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00527}00527\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00528}00528\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[-\/1]\ =\ grad\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00529}00529\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00530}00530\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hessian\_warning:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00531}00531\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Exit\ early\ without\ computing\ the\ hessian.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00532}00532\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad,\ hess,\ hessian\_warning}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00533}00533\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00534}00534\ \ \ \ \ \ \ \ \ \ \ \ \ hess[:n\_features,\ :n\_features]\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss_ad6b0bcabf47bf18c3233e3173fd41156}{sandwich\_dot}}(X,\ hess\_pointwise)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00535}00535\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00536}00536\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ l2\_reg\_strength\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00537}00537\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ L2\ penalty\ enters\ the\ Hessian\ on\ the\ diagonal\ only.\ To\ add\ those}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00538}00538\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ terms,\ we\ use\ a\ flattened\ view\ of\ the\ array.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ order\ =\ \textcolor{stringliteral}{"{}C"{}}\ \textcolor{keywordflow}{if}\ hess.flags.c\_contiguous\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{"{}F"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00540}00540\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess.reshape(-\/1,\ order=order)[:\ (n\_features\ *\ n\_dof)\ :\ (n\_dof\ +\ 1)]\ +=\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00541}00541\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00542}00542\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00543}00543\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00544}00544\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00545}00545\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ With\ intercept\ included\ as\ added\ column\ to\ X,\ the\ hessian\ becomes}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00546}00546\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ hess\ =\ (X,\ 1)'\ @\ diag(h)\ @\ (X,\ 1)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00547}00547\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ =\ (X'\ @\ diag(h)\ @\ X,\ X'\ @\ h)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00548}00548\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ (\ \ \ \ \ \ \ \ \ \ \ h\ @\ X,\ sum(h))}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00549}00549\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ left\ upper\ part\ has\ already\ been\ filled,\ it\ remains\ to\ compute}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00550}00550\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ last\ row\ and\ the\ last\ column.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00551}00551\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Xh\ =\ X.T\ @\ hess\_pointwise}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00552}00552\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[:-\/1,\ -\/1]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00553}00553\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[-\/1,\ :-\/1]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00554}00554\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[-\/1,\ -\/1]\ =\ hess\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00555}00555\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00556}00556\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Here\ we\ may\ safely\ assume\ HalfMultinomialLoss\ aka\ categorical}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00557}00557\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cross-\/entropy.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00558}00558\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ HalfMultinomialLoss\ computes\ only\ the\ diagonal\ part\ of\ the\ hessian,\ i.e.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00559}00559\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ diagonal\ in\ the\ classes.\ Here,\ we\ want\ the\ full\ hessian.\ Therefore,\ we}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00560}00560\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ call\ gradient\_proba.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00561}00561\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise,\ proba\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00562}00562\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00563}00563\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00567}00567\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00568}00568\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ grad.reshape((n\_classes,\ n\_dof),\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00569}00569\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ :n\_features]\ =\ grad\_pointwise.T\ @\ X\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00570}00570\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00571}00571\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ -\/1]\ =\ grad\_pointwise.sum(axis=0)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00572}00572\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00573}00573\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ grad.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00574}00574\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00575}00575\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ full\ hessian\ matrix,\ i.e.\ not\ only\ the\ diagonal\ part,\ dropping\ most}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00576}00576\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ indices,\ is\ given\ by:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00577}00577\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00578}00578\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ hess\ =\ X'\ @\ h\ @\ X}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00579}00579\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00580}00580\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Here,\ h\ is\ a\ priori\ a\ 4-\/dimensional\ matrix\ of\ shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00581}00581\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ (n\_samples,\ n\_samples,\ n\_classes,\ n\_classes).\ It\ is\ diagonal\ its\ first}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00582}00582\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ two\ dimensions\ (the\ ones\ with\ n\_samples),\ i.e.\ it\ is}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00583}00583\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ effectively\ a\ 3-\/dimensional\ matrix\ (n\_samples,\ n\_classes,\ n\_classes).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00584}00584\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00585}00585\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ h\ =\ diag(p)\ -\/\ p'\ p}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00586}00586\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00587}00587\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ or\ with\ indices\ k\ and\ l\ for\ classes}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00588}00588\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00589}00589\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ h\_kl\ =\ p\_k\ *\ delta\_kl\ -\/\ p\_k\ *\ p\_l}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00590}00590\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00591}00591\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ with\ p\_k\ the\ (predicted)\ probability\ for\ class\ k.\ Only\ the\ dimension\ in}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00592}00592\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ n\_samples\ multiplies\ with\ X.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00593}00593\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ For\ 3\ classes\ and\ n\_samples\ =\ 1,\ this\ looks\ like\ ("{}@"{}\ is\ a\ bit\ misused}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00594}00594\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ here):}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00595}00595\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00596}00596\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ hess\ =\ X'\ @\ (h00\ h10\ h20)\ @\ X}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00597}00597\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (h10\ h11\ h12)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00598}00598\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (h20\ h12\ h22)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00599}00599\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ =\ (X'\ @\ diag(h00)\ @\ X,\ X'\ @\ diag(h10),\ X'\ @\ diag(h20))}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00600}00600\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ (X'\ @\ diag(h10)\ @\ X,\ X'\ @\ diag(h11),\ X'\ @\ diag(h12))}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00601}00601\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ (X'\ @\ diag(h20)\ @\ X,\ X'\ @\ diag(h12),\ X'\ @\ diag(h22))}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00602}00602\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00603}00603\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Now\ coef\ of\ shape\ (n\_classes\ *\ n\_dof)\ is\ contiguous\ in\ n\_classes.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00604}00604\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Therefore,\ we\ want\ the\ hessian\ to\ follow\ this\ convention,\ too,\ i.e.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00605}00605\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ hess[:n\_classes,\ :n\_classes]\ =\ (x0'\ @\ h00\ @\ x0,\ x0'\ @\ h10\ @\ x0,\ ..)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00606}00606\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (x0'\ @\ h10\ @\ x0,\ x0'\ @\ h11\ @\ x0,\ ..)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00607}00607\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (x0'\ @\ h20\ @\ x0,\ x0'\ @\ h12\ @\ x0,\ ..)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00608}00608\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ is\ the\ first\ feature,\ x0,\ for\ all\ classes.\ In\ our\ implementation,\ we}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00609}00609\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ still\ want\ to\ take\ advantage\ of\ BLAS\ "{}X.T\ @\ X"{}.\ Therefore,\ we\ have\ some}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00610}00610\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ index/slicing\ battle\ to\ fight.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00611}00611\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00612}00612\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sw\ =\ sample\_weight\ /\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00613}00613\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00614}00614\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sw\ =\ 1.0\ /\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00615}00615\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00616}00616\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(n\_classes):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00617}00617\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Diagonal\ terms\ (in\ classes)\ hess\_kk.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00618}00618\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note\ that\ this\ also\ writes\ to\ some\ of\ the\ lower\ triangular\ part.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00619}00619\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ h\ =\ proba[:,\ k]\ *\ (1\ -\/\ proba[:,\ k])\ *\ sw}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00620}00620\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00621}00621\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00622}00622\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00623}00623\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss_ad6b0bcabf47bf18c3233e3173fd41156}{sandwich\_dot}}(X,\ h)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00624}00624\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00625}00625\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ above\ in\ the\ non\ multiclass\ case.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00626}00626\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Xh\ =\ X.T\ @\ h}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00627}00627\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00628}00628\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00629}00629\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes\ *\ n\_features\ +\ k,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00630}00630\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00631}00631\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00632}00632\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes\ *\ n\_features\ +\ k,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00633}00633\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00634}00634\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00635}00635\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[n\_classes\ *\ n\_features\ +\ k,\ n\_classes\ *\ n\_features\ +\ k]\ =\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00636}00636\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ h.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00637}00637\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00638}00638\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Off\ diagonal\ terms\ (in\ classes)\ hess\_kl.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00639}00639\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ l\ \textcolor{keywordflow}{in}\ range(k\ +\ 1,\ n\_classes):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00640}00640\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Upper\ triangle\ (in\ classes).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00641}00641\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ h\ =\ -\/proba[:,\ k]\ *\ proba[:,\ l]\ *\ sw}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00642}00642\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00643}00643\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00644}00644\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00645}00645\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss_ad6b0bcabf47bf18c3233e3173fd41156}{sandwich\_dot}}(X,\ h)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00646}00646\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00647}00647\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Xh\ =\ X.T\ @\ h}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00648}00648\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00649}00649\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00650}00650\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes\ *\ n\_features\ +\ l,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00651}00651\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00652}00652\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00653}00653\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes\ *\ n\_features\ +\ k,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00654}00654\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ l\ :\ n\_classes\ *\ n\_features\ :\ n\_classes,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00655}00655\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ =\ Xh}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00656}00656\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[n\_classes\ *\ n\_features\ +\ k,\ n\_classes\ *\ n\_features\ +\ l]\ =\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00657}00657\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ h.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00658}00658\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00659}00659\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Fill\ lower\ triangle\ (in\ classes).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00660}00660\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess[l::n\_classes,\ k::n\_classes]\ =\ hess[k::n\_classes,\ l::n\_classes]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00661}00661\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00662}00662\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ l2\_reg\_strength\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00663}00663\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ above\ in\ the\ non\ multiclass\ case.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00664}00664\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ order\ =\ \textcolor{stringliteral}{"{}C"{}}\ \textcolor{keywordflow}{if}\ hess.flags.c\_contiguous\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{"{}F"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00665}00665\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess.reshape(-\/1,\ order=order)[}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00666}00666\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ :\ (n\_classes**2\ *\ n\_features\ *\ n\_dof)\ :\ (n\_classes\ *\ n\_dof\ +\ 1)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00667}00667\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ]\ +=\ l2\_reg\_strength}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00668}00668\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00669}00669\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ pointwise\ hessian\ is\ always\ non-\/negative\ for\ the\ multinomial\ loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00670}00670\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_warning\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00671}00671\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00672}00672\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad,\ hess,\ hessian\_warning}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00673}00673\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00674}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aabd136d35f0b757f2d9d4da9a41ceabd}{00674}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_aabd136d35f0b757f2d9d4da9a41ceabd}{gradient\_hessian\_product}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00675}00675\ \ \ \ \ \ \ \ \ self,\ coef,\ X,\ y,\ sample\_weight=None,\ l2\_reg\_strength=0.0,\ n\_threads=1}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00676}00676\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00677}00677\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Computes\ gradient\ and\ hessp\ (hessian\ product\ function)\ w.r.t.\ coef.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00678}00678\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00679}00679\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00680}00680\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00681}00681\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ coef\ :\ ndarray\ of\ shape\ (n\_dof,),\ (n\_classes,\ n\_dof)\ or\ (n\_classes\ *\ n\_dof,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00682}00682\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Coefficients\ of\ a\ linear\ model.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00683}00683\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ shape\ (n\_classes\ *\ n\_dof,),\ the\ classes\ of\ one\ feature\ are\ contiguous,}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00684}00684\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ i.e.\ one\ reconstructs\ the\ 2d-\/array\ via}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00685}00685\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ coef.reshape((n\_classes,\ -\/1),\ order="{}F"{}).}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00686}00686\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00687}00687\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Training\ data.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00688}00688\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00689}00689\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00690}00690\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ contiguous\ array\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00691}00691\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00692}00692\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ l2\_reg\_strength\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00693}00693\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ L2\ regularization\ strength}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00694}00694\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00695}00695\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Number\ of\ OpenMP\ threads\ to\ use.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00696}00696\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00697}00697\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00698}00698\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00699}00699\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ ndarray\ of\ shape\ coef.shape}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00700}00700\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ The\ gradient\ of\ the\ loss.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00701}00701\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00702}00702\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessp\ :\ callable}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00703}00703\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Function\ that\ takes\ in\ a\ vector\ input\ of\ shape\ of\ gradient\ and}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00704}00704\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ and\ returns\ matrix-\/vector\ product\ with\ hessian.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00705}00705\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00706}00706\ \ \ \ \ \ \ \ \ (n\_samples,\ n\_features),\ n\_classes\ =\ X.shape,\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.n\_classes}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00707}00707\ \ \ \ \ \ \ \ \ n\_dof\ =\ n\_features\ +\ int(self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00708}00708\ \ \ \ \ \ \ \ \ weights,\ intercept,\ raw\_prediction\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a2c1020b46d5e61ae8bda153835bdaa2b}{weight\_intercept\_raw}}(coef,\ X)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00709}00709\ \ \ \ \ \ \ \ \ sw\_sum\ =\ n\_samples\ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ np.sum(sample\_weight)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00710}00710\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00711}00711\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00712}00712\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise,\ hess\_pointwise\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_adfe9025aefcc02979e18a6ef2dd52a32}{gradient\_hessian}}(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00717}00717\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00718}00718\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00719}00719\ \ \ \ \ \ \ \ \ \ \ \ \ hess\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00720}00720\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty\_like(coef,\ dtype=weights.dtype)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:n\_features]\ =\ X.T\ @\ grad\_pointwise\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00723}00723\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[-\/1]\ =\ grad\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00724}00724\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Precompute\ as\ much\ as\ possible:\ hX,\ hX\_sum\ and\ hessian\_sum}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00726}00726\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_sum\ =\ hess\_pointwise.sum()}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00727}00727\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sparse.issparse(X):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00728}00728\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hX\ =\ (}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00729}00729\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classscipy_1_1sparse_1_1__dia_1_1dia__matrix}{sparse.dia\_matrix}}((hess\_pointwise,\ 0),\ shape=(n\_samples,\ n\_samples))}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00730}00730\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ @\ X}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00731}00731\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00732}00732\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00733}00733\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hX\ =\ hess\_pointwise[:,\ np.newaxis]\ *\ X}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00734}00734\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00735}00735\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00736}00736\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Calculate\ the\ double\ derivative\ with\ respect\ to\ intercept.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00737}00737\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note:\ In\ case\ hX\ is\ sparse,\ hX.sum\ is\ a\ matrix\ object.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00738}00738\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hX\_sum\ =\ np.squeeze(np.asarray(hX.sum(axis=0)))}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00739}00739\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ prevent\ squeezing\ to\ zero-\/dim\ array\ if\ n\_features\ ==\ 1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00740}00740\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hX\_sum\ =\ np.atleast\_1d(hX\_sum)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00741}00741\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00742}00742\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ With\ intercept\ included\ and\ l2\_reg\_strength\ =\ 0,\ hessp\ returns}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00743}00743\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ res\ =\ (X,\ 1)'\ @\ diag(h)\ @\ (X,\ 1)\ @\ s}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00744}00744\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ =\ (X,\ 1)'\ @\ (hX\ @\ s[:n\_features],\ sum(h)\ *\ s[-\/1])}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00745}00745\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ res[:n\_features]\ =\ X'\ @\ hX\ @\ s[:n\_features]\ +\ sum(h)\ *\ s[-\/1]}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00746}00746\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ res[-\/1]\ =\ 1'\ @\ hX\ @\ s[:n\_features]\ +\ sum(h)\ *\ s[-\/1]}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00747}00747\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }hessp(s):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00748}00748\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret\ =\ np.empty\_like(s)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00749}00749\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sparse.issparse(X):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00750}00750\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret[:n\_features]\ =\ X.T\ @\ (hX\ @\ s[:n\_features])}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00751}00751\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00752}00752\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret[:n\_features]\ =\ np.linalg.multi\_dot([X.T,\ hX,\ s[:n\_features]])}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00753}00753\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret[:n\_features]\ +=\ l2\_reg\_strength\ *\ s[:n\_features]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00754}00754\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00755}00755\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00756}00756\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret[:n\_features]\ +=\ s[-\/1]\ *\ hX\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00757}00757\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ret[-\/1]\ =\ hX\_sum\ @\ s[:n\_features]\ +\ hessian\_sum\ *\ s[-\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00758}00758\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ ret}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00759}00759\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00760}00760\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00761}00761\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Here\ we\ may\ safely\ assume\ HalfMultinomialLoss\ aka\ categorical}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00762}00762\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cross-\/entropy.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00763}00763\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ HalfMultinomialLoss\ computes\ only\ the\ diagonal\ part\ of\ the\ hessian,\ i.e.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00764}00764\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ diagonal\ in\ the\ classes.\ Here,\ we\ want\ the\ matrix-\/vector\ product\ of\ the}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00765}00765\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ full\ hessian.\ Therefore,\ we\ call\ gradient\_proba.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00766}00766\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise,\ proba\ =\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_ae45960a0f1e675ef15d88e7cc4d459b9}{base\_loss}}.gradient\_proba(}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00767}00767\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00768}00768\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00769}00769\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00770}00770\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00771}00771\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00772}00772\ \ \ \ \ \ \ \ \ \ \ \ \ grad\_pointwise\ /=\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00773}00773\ \ \ \ \ \ \ \ \ \ \ \ \ grad\ =\ np.empty((n\_classes,\ n\_dof),\ dtype=weights.dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00774}00774\ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ :n\_features]\ =\ grad\_pointwise.T\ @\ X\ +\ l2\_reg\_strength\ *\ weights}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00775}00775\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00776}00776\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grad[:,\ -\/1]\ =\ grad\_pointwise.sum(axis=0)}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00777}00777\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00778}00778\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Full\ hessian-\/vector\ product,\ i.e.\ not\ only\ the\ diagonal\ part\ of\ the}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00779}00779\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ hessian.\ Derivation\ with\ some\ index\ battle\ for\ input\ vector\ s:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00780}00780\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ sample\ index\ i}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00781}00781\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ feature\ indices\ j,\ m}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00782}00782\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ class\ indices\ k,\ l}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00783}00783\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ 1\_\{k=l\}\ is\ one\ if\ k=l\ else\ 0}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00784}00784\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ p\_i\_k\ is\ the\ (predicted)\ probability\ that\ sample\ i\ belongs\ to\ class\ k}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00785}00785\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ for\ all\ i:\ sum\_k\ p\_i\_k\ =\ 1}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00786}00786\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ s\_l\_m\ is\ input\ vector\ for\ class\ l\ and\ feature\ m}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00787}00787\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ -\/\ X'\ =\ X\ transposed}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00788}00788\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00789}00789\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note:\ Hessian\ with\ dropping\ most\ indices\ is\ just:}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00790}00790\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ X'\ @\ p\_k\ (1(k=l)\ -\/\ p\_l)\ @\ X}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00791}00791\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00792}00792\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ result\_\{k\ j\}\ =\ sum\_\{i,\ l,\ m\}\ Hessian\_\{i,\ k\ j,\ m\ l\}\ *\ s\_l\_m}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00793}00793\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ =\ sum\_\{i,\ l,\ m\}\ (X')\_\{ji\}\ *\ p\_i\_k\ *\ (1\_\{k=l\}\ -\/\ p\_i\_l)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00794}00794\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *\ X\_\{im\}\ s\_l\_m}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00795}00795\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ =\ sum\_\{i,\ m\}\ (X')\_\{ji\}\ *\ p\_i\_k}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00796}00796\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *\ (X\_\{im\}\ *\ s\_k\_m\ -\/\ sum\_l\ p\_i\_l\ *\ X\_\{im\}\ *\ s\_l\_m)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00797}00797\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00798}00798\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ also\ https://github.com/scikit-\/learn/scikit-\/learn/pull/3646\#discussion\_r17461411}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00799}00799\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }hessp(s):}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00800}00800\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s\ =\ s.reshape((n\_classes,\ -\/1),\ order=\textcolor{stringliteral}{"{}F"{}})\ \ \textcolor{comment}{\#\ shape\ =\ (n\_classes,\ n\_dof)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00801}00801\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00802}00802\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s\_intercept\ =\ s[:,\ -\/1]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00803}00803\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s\ =\ s[:,\ :-\/1]\ \ \textcolor{comment}{\#\ shape\ =\ (n\_classes,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00804}00804\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00805}00805\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s\_intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00806}00806\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tmp\ =\ X\ @\ s.T\ +\ s\_intercept\ \ \textcolor{comment}{\#\ X\_\{im\}\ *\ s\_k\_m}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00807}00807\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tmp\ +=\ (-\/proba\ *\ tmp).sum(axis=1)[:,\ np.newaxis]\ \ \textcolor{comment}{\#\ -\/\ sum\_l\ ..}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00808}00808\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tmp\ *=\ proba\ \ \textcolor{comment}{\#\ *\ p\_i\_k}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00809}00809\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00810}00810\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tmp\ *=\ sample\_weight[:,\ np.newaxis]}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00811}00811\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ hess\_prod\ =\ empty\_like(grad),\ but\ we\ ravel\ grad\ below\ and\ this}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00812}00812\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ function\ is\ run\ after\ that.}}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00813}00813\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess\_prod\ =\ np.empty((n\_classes,\ n\_dof),\ dtype=weights.dtype,\ order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00814}00814\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess\_prod[:,\ :n\_features]\ =\ (tmp.T\ @\ X)\ /\ sw\_sum\ +\ l2\_reg\_strength\ *\ s}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00815}00815\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss_a9ffd044562d8ae3655156076c529a852}{fit\_intercept}}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00816}00816\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hess\_prod[:,\ -\/1]\ =\ tmp.sum(axis=0)\ /\ sw\_sum}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00817}00817\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00818}00818\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ hess\_prod.ravel(order=\textcolor{stringliteral}{"{}F"{}})}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00819}00819\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00820}00820\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ hess\_prod}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00821}00821\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00822}00822\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ coef.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad.ravel(order=\textcolor{stringliteral}{"{}F"{}}),\ hessp}
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00824}00824\ }
\DoxyCodeLine{\Hypertarget{__linear__loss_8py_source_l00825}00825\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ grad,\ hessp}

\end{DoxyCode}
