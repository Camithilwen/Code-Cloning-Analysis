\doxysection{sklearn.\+metrics.\+\_\+regression Namespace Reference}
\hypertarget{namespacesklearn_1_1metrics_1_1__regression}{}\label{namespacesklearn_1_1metrics_1_1__regression}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a8a0adc8939aa9ea3c161976a620b51ae}{\+\_\+check\+\_\+reg\+\_\+targets}} (y\+\_\+true, y\+\_\+pred, sample\+\_\+weight, multioutput, dtype="{}numeric"{}, xp=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_aa3306e99310c34ec7ce1b404e7fcc5fd}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype}} (y\+\_\+true, y\+\_\+pred, sample\+\_\+weight, multioutput, xp=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_aaec3c61c1b82e8d59a40538ede0078a1}{mean\+\_\+absolute\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a52b065184ea6cbae21705ed9d52274cc}{mean\+\_\+pinball\+\_\+loss}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, alpha=0.\+5, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a88d510dd9448e50897b2d6b782aaa3e9}{mean\+\_\+absolute\+\_\+percentage\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_aedcc4d8c35bd9f7a94cfe42a4385b811}{mean\+\_\+squared\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a050618f4151586fbb5b37ec0adb3f781}{root\+\_\+mean\+\_\+squared\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a0edc8d8123806c6ae197fc5873140471}{mean\+\_\+squared\+\_\+log\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a9a6aa63d276c5a8cbd23bac9710c275a}{root\+\_\+mean\+\_\+squared\+\_\+log\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_afd54ada5db92a642aa67f0d0a0898226}{median\+\_\+absolute\+\_\+error}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, multioutput="{}uniform\+\_\+average"{}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a75894e02c97ae591a826f48e8c0c00e1}{\+\_\+assemble\+\_\+r2\+\_\+explained\+\_\+variance}} (numerator, denominator, n\+\_\+outputs, multioutput, force\+\_\+finite, xp, device)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a09b4ca71939627b9fee2ab7a2f23cfa1}{explained\+\_\+variance\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{}, force\+\_\+finite=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a79c9c5c3f6b021490ec72a80db4634af}{r2\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{}, force\+\_\+finite=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a9193a97a2feef27a89742321e496da3b}{max\+\_\+error}} (y\+\_\+true, y\+\_\+pred)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a04b1ea8f3154a641738e0cfe021622df}{\+\_\+mean\+\_\+tweedie\+\_\+deviance}} (y\+\_\+true, y\+\_\+pred, sample\+\_\+weight, power)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a3fc598efc062cc08821e0d5e497479d6}{mean\+\_\+tweedie\+\_\+deviance}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, power=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_ab2aa3e1e0e0d902d23a64422bae5a6b3}{mean\+\_\+poisson\+\_\+deviance}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a19a57af17e31f7c9374159a7ee108e67}{mean\+\_\+gamma\+\_\+deviance}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a4f3e7f4ffcdd824441c82fb17dafe9ca}{d2\+\_\+tweedie\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, power=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_a1241eb268f7a82a5e57f3feeb6578931}{d2\+\_\+pinball\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, alpha=0.\+5, multioutput="{}uniform\+\_\+average"{})
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_ac4bee44f8b7996e88ddcef5b32b04340}{d2\+\_\+absolute\+\_\+error\+\_\+score}} (y\+\_\+true, y\+\_\+pred, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, multioutput="{}uniform\+\_\+average"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__regression_aabc697ede8125bd36432ba29e5124a7d}{\+\_\+\+\_\+\+ALL\+\_\+\+\_\+}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Metrics to assess performance on regression task.

Functions named as ``*_score`` return a scalar value to maximize: the higher
the better.

Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:
the lower the better.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a75894e02c97ae591a826f48e8c0c00e1}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!\_assemble\_r2\_explained\_variance@{\_assemble\_r2\_explained\_variance}}
\index{\_assemble\_r2\_explained\_variance@{\_assemble\_r2\_explained\_variance}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{\_assemble\_r2\_explained\_variance()}{\_assemble\_r2\_explained\_variance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a75894e02c97ae591a826f48e8c0c00e1} 
sklearn.\+metrics.\+\_\+regression.\+\_\+assemble\+\_\+r2\+\_\+explained\+\_\+variance (\begin{DoxyParamCaption}\item[{}]{numerator}{, }\item[{}]{denominator}{, }\item[{}]{n\+\_\+outputs}{, }\item[{}]{multioutput}{, }\item[{}]{force\+\_\+finite}{, }\item[{}]{xp}{, }\item[{}]{device}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Common part used by explained variance score and :math:`R^2` score.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00937}{937}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01007}{explained\+\_\+variance\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l01151}{r2\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a8a0adc8939aa9ea3c161976a620b51ae}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!\_check\_reg\_targets@{\_check\_reg\_targets}}
\index{\_check\_reg\_targets@{\_check\_reg\_targets}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{\_check\_reg\_targets()}{\_check\_reg\_targets()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a8a0adc8939aa9ea3c161976a620b51ae} 
sklearn.\+metrics.\+\_\+regression.\+\_\+check\+\_\+reg\+\_\+targets (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{multioutput}{, }\item[{}]{dtype}{ = {\ttfamily "{}numeric"{}}, }\item[{}]{xp}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Check that y_true, y_pred and sample_weight belong to the same regression task.

To reduce redundancy when calling `_find_matching_floating_dtype`,
please use `_check_reg_targets_with_floating_dtype` instead.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,) or None
    Sample weights.

multioutput : array-like or string in ['raw_values', uniform_average',
    'variance_weighted'] or None
    None is accepted due to backward compatibility of r2_score().

dtype : str or list, default="numeric"
    the dtype argument passed to check_array.

xp : module, default=None
    Precomputed array namespace module. When passed, typically from a caller
    that has already performed inspection of its own inputs, skips array
    namespace inspection.

Returns
-------
type_true : one of {'continuous', continuous-multioutput'}
    The type of the true target data, as output by
    'utils.multiclass.type_of_target'.

y_true : array-like of shape (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,) or None
    Sample weights.

multioutput : array-like of shape (n_outputs) or string in ['raw_values',
    uniform_average', 'variance_weighted'] or None
    Custom output weights if ``multioutput`` is array-like or
    just the corresponding argument if ``multioutput`` is a
    correct keyword.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00060}{60}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01698}{d2\+\_\+pinball\+\_\+score()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01316}{max\+\_\+error()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00864}{median\+\_\+absolute\+\_\+error()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_aa3306e99310c34ec7ce1b404e7fcc5fd}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!\_check\_reg\_targets\_with\_floating\_dtype@{\_check\_reg\_targets\_with\_floating\_dtype}}
\index{\_check\_reg\_targets\_with\_floating\_dtype@{\_check\_reg\_targets\_with\_floating\_dtype}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{\_check\_reg\_targets\_with\_floating\_dtype()}{\_check\_reg\_targets\_with\_floating\_dtype()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_aa3306e99310c34ec7ce1b404e7fcc5fd} 
sklearn.\+metrics.\+\_\+regression.\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{multioutput}{, }\item[{}]{xp}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Ensures y_true, y_pred, and sample_weight correspond to same regression task.

Extends `_check_reg_targets` by automatically selecting a suitable floating-point
data type for inputs using `_find_matching_floating_dtype`.

Use this private method only when converting inputs to array API-compatibles.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,)

multioutput : array-like or string in ['raw_values', 'uniform_average', \
    'variance_weighted'] or None
    None is accepted due to backward compatibility of r2_score().

xp : module, default=None
    Precomputed array namespace module. When passed, typically from a caller
    that has already performed inspection of its own inputs, skips array
    namespace inspection.

Returns
-------
type_true : one of {'continuous', 'continuous-multioutput'}
    The type of the true target data, as output by
    'utils.multiclass.type_of_target'.

y_true : array-like of shape (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : array-like of shape (n_outputs) or string in ['raw_values', \
    'uniform_average', 'variance_weighted'] or None
    Custom output weights if ``multioutput`` is array-like or
    just the corresponding argument if ``multioutput`` is a
    correct keyword.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00157}{157}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00062}{\+\_\+check\+\_\+reg\+\_\+targets()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01580}{d2\+\_\+tweedie\+\_\+score()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01007}{explained\+\_\+variance\+\_\+score()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l00227}{mean\+\_\+absolute\+\_\+error()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l00419}{mean\+\_\+absolute\+\_\+percentage\+\_\+error()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l00321}{mean\+\_\+pinball\+\_\+loss()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l00529}{mean\+\_\+squared\+\_\+error()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l00702}{mean\+\_\+squared\+\_\+log\+\_\+error()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{mean\+\_\+tweedie\+\_\+deviance()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01151}{r2\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00786}{root\+\_\+mean\+\_\+squared\+\_\+log\+\_\+error()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a04b1ea8f3154a641738e0cfe021622df}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!\_mean\_tweedie\_deviance@{\_mean\_tweedie\_deviance}}
\index{\_mean\_tweedie\_deviance@{\_mean\_tweedie\_deviance}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{\_mean\_tweedie\_deviance()}{\_mean\_tweedie\_deviance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a04b1ea8f3154a641738e0cfe021622df} 
sklearn.\+metrics.\+\_\+regression.\+\_\+mean\+\_\+tweedie\+\_\+deviance (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{power}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Mean Tweedie deviance regression loss.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01352}{1352}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01580}{d2\+\_\+tweedie\+\_\+score()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{mean\+\_\+tweedie\+\_\+deviance()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_ac4bee44f8b7996e88ddcef5b32b04340}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!d2\_absolute\_error\_score@{d2\_absolute\_error\_score}}
\index{d2\_absolute\_error\_score@{d2\_absolute\_error\_score}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{d2\_absolute\_error\_score()}{d2\_absolute\_error\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_ac4bee44f8b7996e88ddcef5b32b04340} 
sklearn.\+metrics.\+\_\+regression.\+d2\+\_\+absolute\+\_\+error\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}:math:`D^2` regression score function, fraction of absolute error explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical median of `y_true`
as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with an absolute error deviance
    or ndarray of scores if 'multioutput' is 'raw_values'.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_absolute_error_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> d2_absolute_error_score(y_true, y_pred)
0.764...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')
0.691...
>>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')
array([0.8125    , 0.57142857])
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> d2_absolute_error_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> d2_absolute_error_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> d2_absolute_error_score(y_true, y_pred)
-1.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01845}{1845}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l01698}{d2\+\_\+pinball\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a1241eb268f7a82a5e57f3feeb6578931}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!d2\_pinball\_score@{d2\_pinball\_score}}
\index{d2\_pinball\_score@{d2\_pinball\_score}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{d2\_pinball\_score()}{d2\_pinball\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a1241eb268f7a82a5e57f3feeb6578931} 
sklearn.\+metrics.\+\_\+regression.\+d2\+\_\+pinball\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{alpha}{ = {\ttfamily 0.5}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}:math:`D^2` regression score function, fraction of pinball loss explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical alpha-quantile of
`y_true` as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, default=0.5
    Slope of the pinball deviance. It determines the quantile level alpha
    for which the pinball deviance and also D2 are optimal.
    The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with a pinball deviance
    or ndarray of scores if `multioutput='raw_values'`.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for a single point and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (7) of `Koenker, Roger; Machado, Jos√© A. F. (1999).
       "Goodness of Fit and Related Inference Processes for Quantile Regression"
       <https://doi.org/10.1080/01621459.1999.10473882>`_
.. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_pinball_score
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 3, 3]
>>> d2_pinball_score(y_true, y_pred)
0.5
>>> d2_pinball_score(y_true, y_pred, alpha=0.9)
0.772...
>>> d2_pinball_score(y_true, y_pred, alpha=0.1)
-1.045...
>>> d2_pinball_score(y_true, y_true, alpha=0.1)
1.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01696}{1696}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00062}{\+\_\+check\+\_\+reg\+\_\+targets()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00321}{mean\+\_\+pinball\+\_\+loss()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01847}{d2\+\_\+absolute\+\_\+error\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a4f3e7f4ffcdd824441c82fb17dafe9ca}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!d2\_tweedie\_score@{d2\_tweedie\_score}}
\index{d2\_tweedie\_score@{d2\_tweedie\_score}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{d2\_tweedie\_score()}{d2\_tweedie\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a4f3e7f4ffcdd824441c82fb17dafe9ca} 
sklearn.\+metrics.\+\_\+regression.\+d2\+\_\+tweedie\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{power}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}:math:`D^2` regression score function, fraction of Tweedie deviance explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical mean of `y_true` as
constant prediction, disregarding the input features, gets a D^2 score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.0

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to r2_score.
      y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
z : float
    The D^2 score.

Notes
-----
This is not a symmetric function.

Like R^2, D^2 score may be negative (it need not actually be the square of
a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_tweedie_score
>>> y_true = [0.5, 1, 2.5, 7]
>>> y_pred = [1, 1, 5, 3.5]
>>> d2_tweedie_score(y_true, y_pred)
0.285...
>>> d2_tweedie_score(y_true, y_pred, power=1)
0.487...
>>> d2_tweedie_score(y_true, y_pred, power=2)
0.630...
>>> d2_tweedie_score(y_true, y_true, power=2)
1.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01580}{1580}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01352}{\+\_\+mean\+\_\+tweedie\+\_\+deviance()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{mean\+\_\+tweedie\+\_\+deviance()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a09b4ca71939627b9fee2ab7a2f23cfa1}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!explained\_variance\_score@{explained\_variance\_score}}
\index{explained\_variance\_score@{explained\_variance\_score}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{explained\_variance\_score()}{explained\_variance\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a09b4ca71939627b9fee2ab7a2f23cfa1} 
sklearn.\+metrics.\+\_\+regression.\+explained\+\_\+variance\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}, }\item[{}]{force\+\_\+finite}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Explained variance regression score function.

Best possible score is 1.0, lower values are worse.

In the particular case when ``y_true`` is constant, the explained variance
score is not finite: it is either ``NaN`` (perfect predictions) or
``-Inf`` (imperfect predictions). To prevent such non-finite numbers to
pollute higher-level experiments such as a grid search cross-validation,
by default these cases are replaced with 1.0 (perfect predictions) or 0.0
(imperfect predictions) respectively. If ``force_finite``
is set to ``False``, this score falls back on the original :math:`R^2`
definition.

.. note::
   The Explained Variance score is similar to the
   :func:`R^2 score <r2_score>`, with the notable difference that it
   does not account for systematic offsets in the prediction. Most often
   the :func:`R^2 score <r2_score>` should be preferred.

Read more in the :ref:`User Guide <explained_variance_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or \
        array-like of shape (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
score : float or ndarray of floats
    The explained variance or ndarray if 'multioutput' is 'raw_values'.

See Also
--------
r2_score :
    Similar metric, but accounting for systematic offsets in
    prediction.

Notes
-----
This is not a symmetric function.

Examples
--------
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')
0.983...
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> explained_variance_score(y_true, y_pred)
1.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> explained_variance_score(y_true, y_pred)
0.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
-inf
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01000}{1000}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00939}{\+\_\+assemble\+\_\+r2\+\_\+explained\+\_\+variance()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a9193a97a2feef27a89742321e496da3b}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!max\_error@{max\_error}}
\index{max\_error@{max\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{max\_error()}{max\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a9193a97a2feef27a89742321e496da3b} 
sklearn.\+metrics.\+\_\+regression.\+max\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}The max_error metric calculates the maximum residual error.

Read more in the :ref:`User Guide <max_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

Returns
-------
max_error : float
    A positive floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import max_error
>>> y_true = [3, 2, 7, 1]
>>> y_pred = [4, 2, 7, 1]
>>> max_error(y_true, y_pred)
1.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01316}{1316}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00062}{\+\_\+check\+\_\+reg\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_aaec3c61c1b82e8d59a40538ede0078a1}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_absolute\_error@{mean\_absolute\_error}}
\index{mean\_absolute\_error@{mean\_absolute\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_absolute\_error()}{mean\_absolute\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_aaec3c61c1b82e8d59a40538ede0078a1} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+absolute\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean absolute error regression loss.

The mean absolute error is a non-negative floating point value, where best value
is 0.0. Read more in the :ref:`User Guide <mean_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or array of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAE output is non-negative floating point. The best value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00225}{225}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a88d510dd9448e50897b2d6b782aaa3e9}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_absolute\_percentage\_error@{mean\_absolute\_percentage\_error}}
\index{mean\_absolute\_percentage\_error@{mean\_absolute\_percentage\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_absolute\_percentage\_error()}{mean\_absolute\_percentage\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a88d510dd9448e50897b2d6b782aaa3e9} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+absolute\+\_\+percentage\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean absolute percentage error (MAPE) regression loss.

Note that we are not using the common "percentage" definition: the percentage
in the range [0, 100] is converted to a relative value in the range [0, 1]
by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.

Read more in the :ref:`User Guide <mean_absolute_percentage_error>`.

.. versionadded:: 0.24

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.
    If input is list then the shape must be (n_outputs,).

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute percentage error
    is returned for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAPE output is non-negative floating point. The best value is 0.0.
    But note that bad predictions can lead to arbitrarily large
    MAPE values, especially if some `y_true` values are very close to zero.
    Note that we return a large value instead of `inf` when `y_true` is zero.

Examples
--------
>>> from sklearn.metrics import mean_absolute_percentage_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.3273...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.5515...
>>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.6198...
>>> # the value when some element of the y_true is zero is arbitrarily high because
>>> # of the division by epsilon
>>> y_true = [1., 0., 2.4, 7.]
>>> y_pred = [1.2, 0.1, 2.4, 8.]
>>> mean_absolute_percentage_error(y_true, y_pred)
112589990684262.48
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00417}{417}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a19a57af17e31f7c9374159a7ee108e67}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_gamma\_deviance@{mean\_gamma\_deviance}}
\index{mean\_gamma\_deviance@{mean\_gamma\_deviance}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_gamma\_deviance()}{mean\_gamma\_deviance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a19a57af17e31f7c9374159a7ee108e67} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+gamma\+\_\+deviance (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean Gamma deviance regression loss.

Gamma deviance is equivalent to the Tweedie deviance with
the power parameter `power=2`. It is invariant to scaling of
the target variable, and measures relative errors.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true > 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_gamma_deviance
>>> y_true = [2, 0.5, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_gamma_deviance(y_true, y_pred)
1.0568...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01532}{1532}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{mean\+\_\+tweedie\+\_\+deviance()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a52b065184ea6cbae21705ed9d52274cc}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_pinball\_loss@{mean\_pinball\_loss}}
\index{mean\_pinball\_loss@{mean\_pinball\_loss}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_pinball\_loss()}{mean\_pinball\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a52b065184ea6cbae21705ed9d52274cc} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+pinball\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{alpha}{ = {\ttfamily 0.5}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Pinball loss for quantile regression.

Read more in the :ref:`User Guide <pinball_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, slope of the pinball loss, default=0.5,
    This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,
    `alpha=0.95` is minimized by estimators of the 95th percentile.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    The pinball loss output is a non-negative floating point. The best
    value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_pinball_loss
>>> y_true = [1, 2, 3]
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)
0.03...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)
0.3...
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)
0.3...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)
0.03...
>>> mean_pinball_loss(y_true, y_true, alpha=0.1)
0.0
>>> mean_pinball_loss(y_true, y_true, alpha=0.9)
0.0
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00319}{319}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01698}{d2\+\_\+pinball\+\_\+score()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_ab2aa3e1e0e0d902d23a64422bae5a6b3}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_poisson\_deviance@{mean\_poisson\_deviance}}
\index{mean\_poisson\_deviance@{mean\_poisson\_deviance}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_poisson\_deviance()}{mean\_poisson\_deviance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_ab2aa3e1e0e0d902d23a64422bae5a6b3} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+poisson\+\_\+deviance (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean Poisson deviance regression loss.

Poisson deviance is equivalent to the Tweedie deviance with
the power parameter `power=1`.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true >= 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_poisson_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_poisson_deviance(y_true, y_pred)
1.4260...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01489}{1489}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{mean\+\_\+tweedie\+\_\+deviance()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_aedcc4d8c35bd9f7a94cfe42a4385b811}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_squared\_error@{mean\_squared\_error}}
\index{mean\_squared\_error@{mean\_squared\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_squared\_error()}{mean\_squared\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_aedcc4d8c35bd9f7a94cfe42a4385b811} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+squared\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or array of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.708...
>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')
array([0.41666667, 1.        ])
>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.825...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00523}{523}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l00702}{mean\+\_\+squared\+\_\+log\+\_\+error()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00614}{root\+\_\+mean\+\_\+squared\+\_\+error()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a0edc8d8123806c6ae197fc5873140471}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_squared\_log\_error@{mean\_squared\_log\_error}}
\index{mean\_squared\_log\_error@{mean\_squared\_log\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_squared\_log\_error()}{mean\_squared\_log\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a0edc8d8123806c6ae197fc5873140471} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+squared\+\_\+log\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> mean_squared_log_error(y_true, y_pred)
0.039...
>>> y_true = [[0.5, 1], [1, 2], [7, 6]]
>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
>>> mean_squared_log_error(y_true, y_pred)
0.044...
>>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')
array([0.00462428, 0.08377444])
>>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.060...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00696}{696}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00529}{mean\+\_\+squared\+\_\+error()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a3fc598efc062cc08821e0d5e497479d6}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!mean\_tweedie\_deviance@{mean\_tweedie\_deviance}}
\index{mean\_tweedie\_deviance@{mean\_tweedie\_deviance}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{mean\_tweedie\_deviance()}{mean\_tweedie\_deviance()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a3fc598efc062cc08821e0d5e497479d6} 
sklearn.\+metrics.\+\_\+regression.\+mean\+\_\+tweedie\+\_\+deviance (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{power}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mean Tweedie deviance regression loss.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to
      mean_squared_error. y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_tweedie_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_tweedie_deviance(y_true, y_pred, power=1)
1.4260...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01397}{1397}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l01352}{\+\_\+mean\+\_\+tweedie\+\_\+deviance()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l01580}{d2\+\_\+tweedie\+\_\+score()}}, \mbox{\hyperlink{metrics_2__regression_8py_source_l01532}{mean\+\_\+gamma\+\_\+deviance()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l01489}{mean\+\_\+poisson\+\_\+deviance()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_afd54ada5db92a642aa67f0d0a0898226}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!median\_absolute\_error@{median\_absolute\_error}}
\index{median\_absolute\_error@{median\_absolute\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{median\_absolute\_error()}{median\_absolute\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_afd54ada5db92a642aa67f0d0a0898226} 
sklearn.\+metrics.\+\_\+regression.\+median\+\_\+absolute\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Median absolute error regression loss.

Median absolute error output is non-negative floating point. The best value
is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values. Array-like value defines
    weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.24

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

Examples
--------
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> median_absolute_error(y_true, y_pred)
0.75
>>> median_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00862}{862}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00062}{\+\_\+check\+\_\+reg\+\_\+targets()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a79c9c5c3f6b021490ec72a80db4634af}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!r2\_score@{r2\_score}}
\index{r2\_score@{r2\_score}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{r2\_score()}{r2\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a79c9c5c3f6b021490ec72a80db4634af} 
sklearn.\+metrics.\+\_\+regression.\+r2\+\_\+score (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}, }\item[{}]{force\+\_\+finite}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}:math:`R^2` (coefficient of determination) regression score function.

Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). In the general case when the true y is
non-constant, a constant model that always predicts the average y
disregarding the input features would get a :math:`R^2` score of 0.0.

In the particular case when ``y_true`` is constant, the :math:`R^2` score
is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``
(imperfect predictions). To prevent such non-finite numbers to pollute
higher-level experiments such as a grid search cross-validation, by default
these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect
predictions) respectively. You can set ``force_finite`` to ``False`` to
prevent this fix from happening.

Note: when the prediction residuals have zero mean, the :math:`R^2` score
is identical to the
:func:`Explained Variance score <explained_variance_score>`.

Read more in the :ref:`User Guide <r2_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'}, \
        array-like of shape (n_outputs,) or None, default='uniform_average'

    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.
    Default is "uniform_average".

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

    .. versionchanged:: 0.19
        Default value of multioutput is 'uniform_average'.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
z : float or ndarray of floats
    The :math:`R^2` score or ndarray of scores if 'multioutput' is
    'raw_values'.

Notes
-----
This is not a symmetric function.

Unlike most other scores, :math:`R^2` score may be negative (it need not
actually be the square of a quantity R).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] `Wikipedia entry on the Coefficient of determination
        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_

Examples
--------
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred,
...          multioutput='variance_weighted')
0.938...
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> r2_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> r2_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> r2_score(y_true, y_pred)
-3.0
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> r2_score(y_true, y_pred)
1.0
>>> r2_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> r2_score(y_true, y_pred)
0.0
>>> r2_score(y_true, y_pred, force_finite=False)
-inf
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l01144}{1144}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00939}{\+\_\+assemble\+\_\+r2\+\_\+explained\+\_\+variance()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a050618f4151586fbb5b37ec0adb3f781}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!root\_mean\_squared\_error@{root\_mean\_squared\_error}}
\index{root\_mean\_squared\_error@{root\_mean\_squared\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{root\_mean\_squared\_error()}{root\_mean\_squared\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a050618f4151586fbb5b37ec0adb3f781} 
sklearn.\+metrics.\+\_\+regression.\+root\+\_\+mean\+\_\+squared\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Root mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> root_mean_squared_error(y_true, y_pred)
0.612...
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> root_mean_squared_error(y_true, y_pred)
0.822...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00612}{612}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00529}{mean\+\_\+squared\+\_\+error()}}.



Referenced by \mbox{\hyperlink{metrics_2__regression_8py_source_l00786}{root\+\_\+mean\+\_\+squared\+\_\+log\+\_\+error()}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_a9a6aa63d276c5a8cbd23bac9710c275a}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!root\_mean\_squared\_log\_error@{root\_mean\_squared\_log\_error}}
\index{root\_mean\_squared\_log\_error@{root\_mean\_squared\_log\_error}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{root\_mean\_squared\_log\_error()}{root\_mean\_squared\_log\_error()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_a9a6aa63d276c5a8cbd23bac9710c275a} 
sklearn.\+metrics.\+\_\+regression.\+root\+\_\+mean\+\_\+squared\+\_\+log\+\_\+error (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{multioutput}{ = {\ttfamily "{}uniform\+\_\+average"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Root mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
        (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> root_mean_squared_log_error(y_true, y_pred)
0.199...
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00784}{784}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.



References \mbox{\hyperlink{metrics_2__regression_8py_source_l00159}{\+\_\+check\+\_\+reg\+\_\+targets\+\_\+with\+\_\+floating\+\_\+dtype()}}, and \mbox{\hyperlink{metrics_2__regression_8py_source_l00614}{root\+\_\+mean\+\_\+squared\+\_\+error()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1metrics_1_1__regression_aabc697ede8125bd36432ba29e5124a7d}\index{sklearn.metrics.\_regression@{sklearn.metrics.\_regression}!\_\_ALL\_\_@{\_\_ALL\_\_}}
\index{\_\_ALL\_\_@{\_\_ALL\_\_}!sklearn.metrics.\_regression@{sklearn.metrics.\_regression}}
\doxysubsubsection{\texorpdfstring{\_\_ALL\_\_}{\_\_ALL\_\_}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__regression_aabc697ede8125bd36432ba29e5124a7d} 
list sklearn.\+metrics.\+\_\+regression.\+\_\+\+\_\+\+ALL\+\_\+\+\_\+\hspace{0.3cm}{\ttfamily [private]}}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{stringliteral}{"{}max\_error"{}},}
\DoxyCodeLine{00003\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_absolute\_error"{}},}
\DoxyCodeLine{00004\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_squared\_error"{}},}
\DoxyCodeLine{00005\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_squared\_log\_error"{}},}
\DoxyCodeLine{00006\ \ \ \ \ \textcolor{stringliteral}{"{}median\_absolute\_error"{}},}
\DoxyCodeLine{00007\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_absolute\_percentage\_error"{}},}
\DoxyCodeLine{00008\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_pinball\_loss"{}},}
\DoxyCodeLine{00009\ \ \ \ \ \textcolor{stringliteral}{"{}r2\_score"{}},}
\DoxyCodeLine{00010\ \ \ \ \ \textcolor{stringliteral}{"{}root\_mean\_squared\_log\_error"{}},}
\DoxyCodeLine{00011\ \ \ \ \ \textcolor{stringliteral}{"{}root\_mean\_squared\_error"{}},}
\DoxyCodeLine{00012\ \ \ \ \ \textcolor{stringliteral}{"{}explained\_variance\_score"{}},}
\DoxyCodeLine{00013\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_tweedie\_deviance"{}},}
\DoxyCodeLine{00014\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_poisson\_deviance"{}},}
\DoxyCodeLine{00015\ \ \ \ \ \textcolor{stringliteral}{"{}mean\_gamma\_deviance"{}},}
\DoxyCodeLine{00016\ \ \ \ \ \textcolor{stringliteral}{"{}d2\_tweedie\_score"{}},}
\DoxyCodeLine{00017\ \ \ \ \ \textcolor{stringliteral}{"{}d2\_pinball\_score"{}},}
\DoxyCodeLine{00018\ \ \ \ \ \textcolor{stringliteral}{"{}d2\_absolute\_error\_score"{}},}
\DoxyCodeLine{00019\ ]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{metrics_2__regression_8py_source_l00039}{39}} of file \mbox{\hyperlink{metrics_2__regression_8py_source}{\+\_\+regression.\+py}}.

