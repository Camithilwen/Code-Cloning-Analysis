\doxysection{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer Class Reference}
\hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer}{}\label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}


Inheritance diagram for sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer\+:
% FIG 0


Collaboration diagram for sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a2af9cd2488f63f9300258c552acf0cd2}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, params, learning\+\_\+rate\+\_\+init=0.\+001, beta\+\_\+1=0.\+9, beta\+\_\+2=0.\+999, epsilon=1e-\/8)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Base\+Optimizer}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a5695074d302d0ce580863d1cba1731aa}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, learning\+\_\+rate\+\_\+init=0.\+1)
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a840bd3275bac4a6db219ca7370977baf}{update\+\_\+params}} (self, params, grads)
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a23867e22d6b166756b2cf4faebc12c9d}{iteration\+\_\+ends}} (self, time\+\_\+step)
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_aeeb3ca05e9540cba44a543cd9a39cc87}{trigger\+\_\+stopping}} (self, msg, verbose)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}{beta\+\_\+1}} = beta\+\_\+1
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}{beta\+\_\+2}} = beta\+\_\+2
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af38071d35a96bdf6d266c3095c74c5da}{epsilon}} = epsilon
\item 
int \mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}{t}} = 0
\item 
list \mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}{ms}} = \mbox{[}np.\+zeros\+\_\+like(param) for param in params\mbox{]}
\item 
list \mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}{vs}} = \mbox{[}np.\+zeros\+\_\+like(param) for param in params\mbox{]}
\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Base\+Optimizer}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_ab5689b36eec135b26ea1378a0c1e0ac2}{learning\+\_\+rate\+\_\+init}} = learning\+\_\+rate\+\_\+init
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\+\_\+rate}} = float(learning\+\_\+rate\+\_\+init)
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_acc5a97518444bd40d2446bd2f12c7968}{\+\_\+get\+\_\+updates}} (self, grads)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Stochastic gradient descent optimizer with Adam

Note: All default values are from the original Adam paper

Parameters
----------
params : list, length = len(coefs_) + len(intercepts_)
    The concatenated list containing coefs_ and intercepts_ in MLP model.
    Used for initializing velocities and updating params

learning_rate_init : float, default=0.001
    The initial learning rate used. It controls the step-size in updating
    the weights

beta_1 : float, default=0.9
    Exponential decay rate for estimates of first moment vector, should be
    in [0, 1)

beta_2 : float, default=0.999
    Exponential decay rate for estimates of second moment vector, should be
    in [0, 1)

epsilon : float, default=1e-8
    Value for numerical stability

Attributes
----------
learning_rate : float
    The current learning rate

t : int
    Timestep

ms : list, length = len(params)
    First moment vectors

vs : list, length = len(params)
    Second moment vectors

References
----------
:arxiv:`Kingma, Diederik, and Jimmy Ba (2014) "Adam: A method for
    stochastic optimization." <1412.6980>
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00197}{197}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a2af9cd2488f63f9300258c552acf0cd2}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a2af9cd2488f63f9300258c552acf0cd2} 
sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{params}{, }\item[{}]{learning\+\_\+rate\+\_\+init}{ = {\ttfamily 0.001}, }\item[{}]{beta\+\_\+1}{ = {\ttfamily 0.9}, }\item[{}]{beta\+\_\+2}{ = {\ttfamily 0.999}, }\item[{}]{epsilon}{ = {\ttfamily 1e-\/8}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00243}{243}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_acc5a97518444bd40d2446bd2f12c7968}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!\_get\_updates@{\_get\_updates}}
\index{\_get\_updates@{\_get\_updates}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{\_get\_updates()}{\_get\_updates()}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_acc5a97518444bd40d2446bd2f12c7968} 
sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+\_\+get\+\_\+updates (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{grads}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Get the values used to update params with given gradients

Parameters
----------
grads : list, length = len(coefs_) + len(intercepts_)
    Containing gradients with respect to coefs_ and intercepts_ in MLP
    model. So length should be aligned with params

Returns
-------
updates : list, length = len(grads)
    The values to add to params
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{255}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



References \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00145}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+beta\+\_\+1}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00248}{beta\+\_\+1}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00146}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+beta\+\_\+2}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00249}{beta\+\_\+2}}, \mbox{\hyperlink{__machar_8py_source_l00324}{numpy.\+core.\+\_\+machar.\+Mach\+Ar.\+epsilon}}, \mbox{\hyperlink{getlimits_8py_source_l00045}{numpy.\+core.\+getlimits.\+Mach\+Ar\+Like.\+epsilon}}, \mbox{\hyperlink{__rbf_8py_source_l00242}{scipy.\+interpolate.\+\_\+rbf.\+Rbf.\+epsilon}}, \mbox{\hyperlink{__rbfinterp_8py_source_l00392}{scipy.\+interpolate.\+\_\+rbfinterp.\+RBFInterpolator.\+epsilon}}, \mbox{\hyperlink{__huber_8py_source_l00270}{sklearn.\+linear\+\_\+model.\+\_\+huber.\+Huber\+Regressor.\+epsilon}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l00120}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+Base\+SGD.\+epsilon}}, \mbox{\hyperlink{test__passive__aggressive_8py_source_l00034}{sklearn.\+linear\+\_\+model.\+tests.\+test\+\_\+passive\+\_\+aggressive.\+My\+Passive\+Aggressive.\+epsilon}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00147}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+epsilon}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00250}{epsilon}}, \mbox{\hyperlink{__gb_8py_source_l00398}{sklearn.\+ensemble.\+\_\+gb.\+Base\+Gradient\+Boosting.\+learning\+\_\+rate}}, \mbox{\hyperlink{gradient__boosting_8py_source_l00202}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Base\+Hist\+Gradient\+Boosting.\+learning\+\_\+rate}}, \mbox{\hyperlink{__weight__boosting_8py_source_l00089}{sklearn.\+ensemble.\+\_\+weight\+\_\+boosting.\+Base\+Weight\+Boosting.\+learning\+\_\+rate}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l00119}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+Base\+SGD.\+learning\+\_\+rate}}, \mbox{\hyperlink{__t__sne_8py_source_l00832}{sklearn.\+manifold.\+\_\+t\+\_\+sne.\+TSNE.\+learning\+\_\+rate}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00130}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+learning\+\_\+rate}}, \mbox{\hyperlink{__rbm_8py_source_l00151}{sklearn.\+neural\+\_\+network.\+\_\+rbm.\+Bernoulli\+RBM.\+learning\+\_\+rate}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00026}{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Base\+Optimizer.\+learning\+\_\+rate}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00131}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+learning\+\_\+rate\+\_\+init}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00025}{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Base\+Optimizer.\+learning\+\_\+rate\+\_\+init}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00252}{ms}}, \mbox{\hyperlink{numpy_2core_2tests_2test__numeric_8py_source_l00354}{numpy.\+core.\+tests.\+test\+\_\+numeric.\+Test\+Bool\+Array.\+t}}, \mbox{\hyperlink{scipy_2integrate_2__ivp_2base_8py_source_l00247}{scipy.\+integrate.\+\_\+ivp.\+base.\+Dense\+Output.\+t}}, \mbox{\hyperlink{scipy_2integrate_2__ivp_2base_8py_source_l00134}{scipy.\+integrate.\+\_\+ivp.\+base.\+Ode\+Solver.\+t}}, \mbox{\hyperlink{bdf_8py_source_l00405}{scipy.\+integrate.\+\_\+ivp.\+bdf.\+BDF.\+t}}, \mbox{\hyperlink{__ivp_2lsoda_8py_source_l00167}{scipy.\+integrate.\+\_\+ivp.\+lsoda.\+LSODA.\+t}}, \mbox{\hyperlink{radau_8py_source_l00527}{scipy.\+integrate.\+\_\+ivp.\+radau.\+Radau.\+t}}, \mbox{\hyperlink{rk_8py_source_l00547}{scipy.\+integrate.\+\_\+ivp.\+rk.\+DOP853.\+t}}, \mbox{\hyperlink{rk_8py_source_l00170}{scipy.\+integrate.\+\_\+ivp.\+rk.\+Runge\+Kutta.\+t}}, \mbox{\hyperlink{__ode_8py_source_l00368}{scipy.\+integrate.\+\_\+ode.\+ode.\+t}}, \mbox{\hyperlink{interpolate_2__bsplines_8py_source_l00212}{scipy.\+interpolate.\+\_\+bsplines.\+BSpline.\+t}}, \mbox{\hyperlink{interpolate_2tests_2test__bsplines_8py_source_l01449}{scipy.\+interpolate.\+tests.\+test\+\_\+bsplines.\+Test\+LSQ.\+t}}, \mbox{\hyperlink{test__blas_8py_source_l00830}{scipy.\+linalg.\+tests.\+test\+\_\+blas.\+Test\+BLAS3\+Symm.\+t}}, \mbox{\hyperlink{test__blas_8py_source_l00912}{scipy.\+linalg.\+tests.\+test\+\_\+blas.\+Test\+BLAS3\+Syr2k.\+t}}, \mbox{\hyperlink{test__blas_8py_source_l00872}{scipy.\+linalg.\+tests.\+test\+\_\+blas.\+Test\+BLAS3\+Syrk.\+t}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00251}{t}}, \mbox{\hyperlink{test__distributions_8py_source_l06935}{scipy.\+stats.\+tests.\+test\+\_\+distributions.\+Test\+Studentized\+Range.\+vs}}, and \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00253}{vs}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00028}{sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Base\+Optimizer.\+update\+\_\+params()}}.



\doxysubsection{Member Data Documentation}
\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!beta\_1@{beta\_1}}
\index{beta\_1@{beta\_1}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{beta\_1}{beta\_1}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab} 
sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+beta\+\_\+1 = beta\+\_\+1}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00248}{248}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}.

\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!beta\_2@{beta\_2}}
\index{beta\_2@{beta\_2}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{beta\_2}{beta\_2}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e} 
sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+beta\+\_\+2 = beta\+\_\+2}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00249}{249}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}.

\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af38071d35a96bdf6d266c3095c74c5da}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!epsilon@{epsilon}}
\index{epsilon@{epsilon}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{epsilon}{epsilon}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af38071d35a96bdf6d266c3095c74c5da} 
sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+epsilon = epsilon}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00250}{250}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00521}{sklearn.\+svm.\+\_\+base.\+Base\+Lib\+SVM.\+\_\+decision\+\_\+function()}}, \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}, \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00295}{sklearn.\+svm.\+\_\+base.\+Base\+Lib\+SVM.\+\_\+validate\+\_\+targets()}}, \mbox{\hyperlink{svm_2__classes_8py_source_l00545}{sklearn.\+svm.\+\_\+classes.\+Linear\+SVR.\+fit()}}, \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00420}{sklearn.\+svm.\+\_\+base.\+Base\+Lib\+SVM.\+predict()}}, and \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00876}{sklearn.\+svm.\+\_\+base.\+Base\+SVC.\+predict\+\_\+log\+\_\+proba()}}.

\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!ms@{ms}}
\index{ms@{ms}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{ms}{ms}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57} 
list sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+ms = \mbox{[}np.\+zeros\+\_\+like(param) for param in params\mbox{]}}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00252}{252}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}.

\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!t@{t}}
\index{t@{t}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{t}{t}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6} 
int sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+t = 0}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00251}{251}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}.

\Hypertarget{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}\index{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}!vs@{vs}}
\index{vs@{vs}!sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer@{sklearn.neural\_network.\_stochastic\_optimizers.AdamOptimizer}}
\doxysubsubsection{\texorpdfstring{vs}{vs}}
{\footnotesize\ttfamily \label{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6} 
list sklearn.\+neural\+\_\+network.\+\_\+stochastic\+\_\+optimizers.\+Adam\+Optimizer.\+vs = \mbox{[}np.\+zeros\+\_\+like(param) for param in params\mbox{]}}



Definition at line \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00253}{253}} of file \mbox{\hyperlink{__stochastic__optimizers_8py_source}{\+\_\+stochastic\+\_\+optimizers.\+py}}.



Referenced by \mbox{\hyperlink{__stochastic__optimizers_8py_source_l00255}{\+\_\+get\+\_\+updates()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/neural\+\_\+network/\+\_\+stochastic\+\_\+optimizers.\+py\end{DoxyCompactItemize}
