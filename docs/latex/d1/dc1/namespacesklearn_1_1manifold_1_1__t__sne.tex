\doxysection{sklearn.\+manifold.\+\_\+t\+\_\+sne Namespace Reference}
\hypertarget{namespacesklearn_1_1manifold_1_1__t__sne}{}\label{namespacesklearn_1_1manifold_1_1__t__sne}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1manifold_1_1__t__sne_1_1TSNE}{TSNE}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_ad8b7c25c5600bbaa275abe387cef55d3}{\+\_\+joint\+\_\+probabilities}} (distances, desired\+\_\+perplexity, verbose)
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_ae142c57fc0866dacf6978fe62facd79c}{\+\_\+joint\+\_\+probabilities\+\_\+nn}} (distances, desired\+\_\+perplexity, verbose)
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_a26ce7302d5085e587421770508bd7cad}{\+\_\+kl\+\_\+divergence}} (params, P, degrees\+\_\+of\+\_\+freedom, n\+\_\+samples, n\+\_\+components, skip\+\_\+num\+\_\+points=0, compute\+\_\+error=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_a454a53a34098a51ad06ebc4890b7b961}{\+\_\+kl\+\_\+divergence\+\_\+bh}} (params, P, degrees\+\_\+of\+\_\+freedom, n\+\_\+samples, n\+\_\+components, angle=0.\+5, skip\+\_\+num\+\_\+points=0, verbose=False, compute\+\_\+error=\mbox{\hyperlink{classTrue}{True}}, num\+\_\+threads=1)
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_a926a76e5870dca32aa3988f202c841bd}{\+\_\+gradient\+\_\+descent}} (objective, p0, it, max\+\_\+iter, n\+\_\+iter\+\_\+check=1, n\+\_\+iter\+\_\+without\+\_\+progress=300, momentum=0.\+8, learning\+\_\+rate=200.\+0, min\+\_\+gain=0.\+01, min\+\_\+grad\+\_\+norm=1e-\/7, verbose=0, args=None, kwargs=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_a53b4608cc501ac550d5207ffb9dc43cc}{trustworthiness}} (X, X\+\_\+embedded, \texorpdfstring{$\ast$}{*}, n\+\_\+neighbors=5, metric="{}euclidean"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1manifold_1_1__t__sne_a996e540cc45c8dd2e580bd50e79425c3}{MACHINE\+\_\+\+EPSILON}} = np.\+finfo(np.\+double).eps
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_a926a76e5870dca32aa3988f202c841bd}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!\_gradient\_descent@{\_gradient\_descent}}
\index{\_gradient\_descent@{\_gradient\_descent}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{\_gradient\_descent()}{\_gradient\_descent()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_a926a76e5870dca32aa3988f202c841bd} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+\_\+gradient\+\_\+descent (\begin{DoxyParamCaption}\item[{}]{objective}{, }\item[{}]{p0}{, }\item[{}]{it}{, }\item[{}]{max\+\_\+iter}{, }\item[{}]{n\+\_\+iter\+\_\+check}{ = {\ttfamily 1}, }\item[{}]{n\+\_\+iter\+\_\+without\+\_\+progress}{ = {\ttfamily 300}, }\item[{}]{momentum}{ = {\ttfamily 0.8}, }\item[{}]{learning\+\_\+rate}{ = {\ttfamily 200.0}, }\item[{}]{min\+\_\+gain}{ = {\ttfamily 0.01}, }\item[{}]{min\+\_\+grad\+\_\+norm}{ = {\ttfamily 1e-\/7}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{args}{ = {\ttfamily None}, }\item[{}]{kwargs}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Batch gradient descent with momentum and individual gains.

Parameters
----------
objective : callable
    Should return a tuple of cost and gradient for a given parameter
    vector. When expensive to compute, the cost can optionally
    be None and can be computed every n_iter_check steps using
    the objective_error function.

p0 : array-like of shape (n_params,)
    Initial parameter vector.

it : int
    Current number of iterations (this function will be called more than
    once during the optimization).

max_iter : int
    Maximum number of gradient descent iterations.

n_iter_check : int, default=1
    Number of iterations before evaluating the global error. If the error
    is sufficiently low, we abort the optimization.

n_iter_without_progress : int, default=300
    Maximum number of iterations without progress before we abort the
    optimization.

momentum : float within (0.0, 1.0), default=0.8
    The momentum generates a weight for previous gradients that decays
    exponentially.

learning_rate : float, default=200.0
    The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If
    the learning rate is too high, the data may look like a 'ball' with any
    point approximately equidistant from its nearest neighbours. If the
    learning rate is too low, most points may look compressed in a dense
    cloud with few outliers.

min_gain : float, default=0.01
    Minimum individual gain for each parameter.

min_grad_norm : float, default=1e-7
    If the gradient norm is below this threshold, the optimization will
    be aborted.

verbose : int, default=0
    Verbosity level.

args : sequence, default=None
    Arguments to pass to objective function.

kwargs : dict, default=None
    Keyword arguments to pass to objective function.

Returns
-------
p : ndarray of shape (n_params,)
    Optimum parameters.

error : float
    Optimum.

i : int
    Last iteration.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00301}{301}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.



Referenced by \mbox{\hyperlink{__t__sne_8py_source_l01051}{sklearn.\+manifold.\+\_\+t\+\_\+sne.\+TSNE.\+\_\+tsne()}}.

\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_ad8b7c25c5600bbaa275abe387cef55d3}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!\_joint\_probabilities@{\_joint\_probabilities}}
\index{\_joint\_probabilities@{\_joint\_probabilities}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{\_joint\_probabilities()}{\_joint\_probabilities()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_ad8b7c25c5600bbaa275abe387cef55d3} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+\_\+joint\+\_\+probabilities (\begin{DoxyParamCaption}\item[{}]{distances}{, }\item[{}]{desired\+\_\+perplexity}{, }\item[{}]{verbose}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute joint probabilities p_ij from distances.

Parameters
----------
distances : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Distances of samples are stored as condensed matrices, i.e.
    we omit the diagonal and duplicate entries and store everything
    in a one-dimensional array.

desired_perplexity : float
    Desired perplexity of the joint probability distributions.

verbose : int
    Verbosity level.

Returns
-------
P : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Condensed joint probability matrix.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00038}{38}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.



Referenced by \mbox{\hyperlink{__t__sne_8py_source_l00852}{sklearn.\+manifold.\+\_\+t\+\_\+sne.\+TSNE.\+\_\+fit()}}.

\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_ae142c57fc0866dacf6978fe62facd79c}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!\_joint\_probabilities\_nn@{\_joint\_probabilities\_nn}}
\index{\_joint\_probabilities\_nn@{\_joint\_probabilities\_nn}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{\_joint\_probabilities\_nn()}{\_joint\_probabilities\_nn()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_ae142c57fc0866dacf6978fe62facd79c} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+\_\+joint\+\_\+probabilities\+\_\+nn (\begin{DoxyParamCaption}\item[{}]{distances}{, }\item[{}]{desired\+\_\+perplexity}{, }\item[{}]{verbose}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute joint probabilities p_ij from distances using just nearest
neighbors.

This method is approximately equal to _joint_probabilities. The latter
is O(N), but limiting the joint probability to nearest neighbors improves
this substantially to O(uN).

Parameters
----------
distances : sparse matrix of shape (n_samples, n_samples)
    Distances of samples to its n_neighbors nearest neighbors. All other
    distances are left to zero (and are not materialized in memory).
    Matrix should be of CSR format.

desired_perplexity : float
    Desired perplexity of the joint probability distributions.

verbose : int
    Verbosity level.

Returns
-------
P : sparse matrix of shape (n_samples, n_samples)
    Condensed joint probability matrix with only nearest neighbors. Matrix
    will be of CSR format.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00071}{71}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.



Referenced by \mbox{\hyperlink{__t__sne_8py_source_l00852}{sklearn.\+manifold.\+\_\+t\+\_\+sne.\+TSNE.\+\_\+fit()}}.

\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_a26ce7302d5085e587421770508bd7cad}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!\_kl\_divergence@{\_kl\_divergence}}
\index{\_kl\_divergence@{\_kl\_divergence}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{\_kl\_divergence()}{\_kl\_divergence()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_a26ce7302d5085e587421770508bd7cad} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+\_\+kl\+\_\+divergence (\begin{DoxyParamCaption}\item[{}]{params}{, }\item[{}]{P}{, }\item[{}]{degrees\+\_\+of\+\_\+freedom}{, }\item[{}]{n\+\_\+samples}{, }\item[{}]{n\+\_\+components}{, }\item[{}]{skip\+\_\+num\+\_\+points}{ = {\ttfamily 0}, }\item[{}]{compute\+\_\+error}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}t-SNE objective function: gradient of the KL divergence
of p_ijs and q_ijs and the absolute error.

Parameters
----------
params : ndarray of shape (n_params,)
    Unraveled embedding.

P : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Condensed joint probability matrix.

degrees_of_freedom : int
    Degrees of freedom of the Student's-t distribution.

n_samples : int
    Number of samples.

n_components : int
    Dimension of the embedded space.

skip_num_points : int, default=0
    This does not compute the gradient for points with indices below
    `skip_num_points`. This is useful when computing transforms of new
    data where you'd like to keep the old data fixed.

compute_error: bool, default=True
    If False, the kl_divergence is not computed and returns NaN.

Returns
-------
kl_divergence : float
    Kullback-Leibler divergence of p_ij and q_ij.

grad : ndarray of shape (n_params,)
    Unraveled gradient of the Kullback-Leibler divergence with respect to
    the embedding.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00128}{128}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.

\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_a454a53a34098a51ad06ebc4890b7b961}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!\_kl\_divergence\_bh@{\_kl\_divergence\_bh}}
\index{\_kl\_divergence\_bh@{\_kl\_divergence\_bh}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{\_kl\_divergence\_bh()}{\_kl\_divergence\_bh()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_a454a53a34098a51ad06ebc4890b7b961} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+\_\+kl\+\_\+divergence\+\_\+bh (\begin{DoxyParamCaption}\item[{}]{params}{, }\item[{}]{P}{, }\item[{}]{degrees\+\_\+of\+\_\+freedom}{, }\item[{}]{n\+\_\+samples}{, }\item[{}]{n\+\_\+components}{, }\item[{}]{angle}{ = {\ttfamily 0.5}, }\item[{}]{skip\+\_\+num\+\_\+points}{ = {\ttfamily 0}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{compute\+\_\+error}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{num\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}t-SNE objective function: KL divergence of p_ijs and q_ijs.

Uses Barnes-Hut tree methods to calculate the gradient that
runs in O(NlogN) instead of O(N^2).

Parameters
----------
params : ndarray of shape (n_params,)
    Unraveled embedding.

P : sparse matrix of shape (n_samples, n_sample)
    Sparse approximate joint probability matrix, computed only for the
    k nearest-neighbors and symmetrized. Matrix should be of CSR format.

degrees_of_freedom : int
    Degrees of freedom of the Student's-t distribution.

n_samples : int
    Number of samples.

n_components : int
    Dimension of the embedded space.

angle : float, default=0.5
    This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.
    'angle' is the angular size (referred to as theta in [3]) of a distant
    node as measured from a point. If this size is below 'angle' then it is
    used as a summary node of all points contained within it.
    This method is not very sensitive to changes in this parameter
    in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing
    computation time and angle greater 0.8 has quickly increasing error.

skip_num_points : int, default=0
    This does not compute the gradient for points with indices below
    `skip_num_points`. This is useful when computing transforms of new
    data where you'd like to keep the old data fixed.

verbose : int, default=False
    Verbosity level.

compute_error: bool, default=True
    If False, the kl_divergence is not computed and returns NaN.

num_threads : int, default=1
    Number of threads used to compute the gradient. This is set here to
    avoid calling _openmp_effective_n_threads for each gradient step.

Returns
-------
kl_divergence : float
    Kullback-Leibler divergence of p_ij and q_ij.

grad : ndarray of shape (n_params,)
    Unraveled gradient of the Kullback-Leibler divergence with respect to
    the embedding.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00205}{205}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.

\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_a53b4608cc501ac550d5207ffb9dc43cc}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!trustworthiness@{trustworthiness}}
\index{trustworthiness@{trustworthiness}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{trustworthiness()}{trustworthiness()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_a53b4608cc501ac550d5207ffb9dc43cc} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+trustworthiness (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{X\+\_\+embedded}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{n\+\_\+neighbors}{ = {\ttfamily 5}, }\item[{}]{metric}{ = {\ttfamily "{}euclidean"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Indicate to what extent the local structure is retained.

The trustworthiness is within [0, 1]. It is defined as

.. math::

    T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1}
        \sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k))

where for each sample i, :math:`\mathcal{N}_{i}^{k}` are its k nearest
neighbors in the output space, and every sample j is its :math:`r(i, j)`-th
nearest neighbor in the input space. In other words, any unexpected nearest
neighbors in the output space are penalised in proportion to their rank in
the input space.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features) or \
    (n_samples, n_samples)
    If the metric is 'precomputed' X must be a square distance
    matrix. Otherwise it contains a sample per row.

X_embedded : {array-like, sparse matrix} of shape (n_samples, n_components)
    Embedding of the training data in low-dimensional space.

n_neighbors : int, default=5
    The number of neighbors that will be considered. Should be fewer than
    `n_samples / 2` to ensure the trustworthiness to lies within [0, 1], as
    mentioned in [1]_. An error will be raised otherwise.

metric : str or callable, default='euclidean'
    Which metric to use for computing pairwise distances between samples
    from the original input space. If metric is 'precomputed', X must be a
    matrix of pairwise distances or squared distances. Otherwise, for a list
    of available metrics, see the documentation of argument metric in
    `sklearn.pairwise.pairwise_distances` and metrics listed in
    `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the
    "cosine" metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.

    .. versionadded:: 0.20

Returns
-------
trustworthiness : float
    Trustworthiness of the low-dimensional embedding.

References
----------
.. [1] Jarkko Venna and Samuel Kaski. 2001. Neighborhood
       Preservation in Nonlinear Projection Methods: An Experimental Study.
       In Proceedings of the International Conference on Artificial Neural Networks
       (ICANN '01). Springer-Verlag, Berlin, Heidelberg, 485-491.

.. [2] Laurens van der Maaten. Learning a Parametric Embedding by Preserving
       Local Structure. Proceedings of the Twelfth International Conference on
       Artificial Intelligence and Statistics, PMLR 5:384-391, 2009.

Examples
--------
>>> from sklearn.datasets import make_blobs
>>> from sklearn.decomposition import PCA
>>> from sklearn.manifold import trustworthiness
>>> X, _ = make_blobs(n_samples=100, n_features=10, centers=3, random_state=42)
>>> X_embedded = PCA(n_components=2).fit_transform(X)
>>> print(f"{trustworthiness(X, X_embedded, n_neighbors=5):.2f}")
0.92
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00456}{456}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1manifold_1_1__t__sne_a996e540cc45c8dd2e580bd50e79425c3}\index{sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}!MACHINE\_EPSILON@{MACHINE\_EPSILON}}
\index{MACHINE\_EPSILON@{MACHINE\_EPSILON}!sklearn.manifold.\_t\_sne@{sklearn.manifold.\_t\_sne}}
\doxysubsubsection{\texorpdfstring{MACHINE\_EPSILON}{MACHINE\_EPSILON}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1manifold_1_1__t__sne_a996e540cc45c8dd2e580bd50e79425c3} 
sklearn.\+manifold.\+\_\+t\+\_\+sne.\+MACHINE\+\_\+\+EPSILON = np.\+finfo(np.\+double).eps}



Definition at line \mbox{\hyperlink{__t__sne_8py_source_l00035}{35}} of file \mbox{\hyperlink{__t__sne_8py_source}{\+\_\+t\+\_\+sne.\+py}}.

