\doxysection{sklearn.\+linear\+\_\+model.\+\_\+ridge Namespace Reference}
\hypertarget{namespacesklearn_1_1linear__model_1_1__ridge}{}\label{namespacesklearn_1_1linear__model_1_1__ridge}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__BaseRidge}{\+\_\+\+Base\+Ridge}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__BaseRidgeCV}{\+\_\+\+Base\+Ridge\+CV}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__IdentityClassifier}{\+\_\+\+Identity\+Classifier}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__IdentityRegressor}{\+\_\+\+Identity\+Regressor}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__RidgeClassifierMixin}{\+\_\+\+Ridge\+Classifier\+Mixin}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__RidgeGCV}{\+\_\+\+Ridge\+GCV}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__X__CenterStackOp}{\+\_\+\+X\+\_\+\+Center\+Stack\+Op}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1__XT__CenterStackOp}{\+\_\+\+XT\+\_\+\+Center\+Stack\+Op}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1Ridge}{Ridge}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1RidgeClassifier}{Ridge\+Classifier}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1RidgeClassifierCV}{Ridge\+Classifier\+CV}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1RidgeCV}{Ridge\+CV}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a795b22ea17397cfd97b4d7ef98508b37}{\+\_\+get\+\_\+rescaled\+\_\+operator}} (X, X\+\_\+offset, sample\+\_\+weight\+\_\+sqrt)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a56d787b3475471d14facd04607a0e15a}{\+\_\+solve\+\_\+sparse\+\_\+cg}} (X, y, alpha, max\+\_\+iter=None, tol=1e-\/4, verbose=0, X\+\_\+offset=None, X\+\_\+scale=None, sample\+\_\+weight\+\_\+sqrt=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_ab80be8bf17e021a8de74e71f9648d077}{\+\_\+solve\+\_\+lsqr}} (X, y, \texorpdfstring{$\ast$}{*}, alpha, fit\+\_\+intercept=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+iter=None, tol=1e-\/4, X\+\_\+offset=None, X\+\_\+scale=None, sample\+\_\+weight\+\_\+sqrt=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_aa184b154e12613beead709efe18e1473}{\+\_\+solve\+\_\+cholesky}} (X, y, alpha)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a91b5180ca5403e673bb49a1cc983c7fb}{\+\_\+solve\+\_\+cholesky\+\_\+kernel}} (\mbox{\hyperlink{classK}{K}}, y, alpha, sample\+\_\+weight=None, copy=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a4a643a4b40516c605d51a8c439eed070}{\+\_\+solve\+\_\+svd}} (X, y, alpha, xp=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a91df6db9c40254448075d9b8920e0111}{\+\_\+solve\+\_\+lbfgs}} (X, y, alpha, positive=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+iter=None, tol=1e-\/4, X\+\_\+offset=None, X\+\_\+scale=None, sample\+\_\+weight\+\_\+sqrt=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_ae1f2e3f483b38f77b5eaa5d6ca708ab0}{\+\_\+get\+\_\+valid\+\_\+accept\+\_\+sparse}} (is\+\_\+\+X\+\_\+sparse, solver)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_af3aaa5807b87a9626d68d99fc82a6394}{ridge\+\_\+regression}} (X, y, alpha, \texorpdfstring{$\ast$}{*}, sample\+\_\+weight=None, solver="{}auto"{}, max\+\_\+iter=None, tol=1e-\/4, verbose=0, positive=False, random\+\_\+state=None, return\+\_\+n\+\_\+iter=False, return\+\_\+intercept=False, check\+\_\+input=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a9394458a04f9be072e6c981308d78bd2}{\+\_\+ridge\+\_\+regression}} (X, y, alpha, sample\+\_\+weight=None, solver="{}auto"{}, max\+\_\+iter=None, tol=1e-\/4, verbose=0, positive=False, random\+\_\+state=None, return\+\_\+n\+\_\+iter=False, return\+\_\+intercept=False, return\+\_\+solver=False, X\+\_\+scale=None, X\+\_\+offset=None, check\+\_\+input=\mbox{\hyperlink{classTrue}{True}}, fit\+\_\+intercept=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a47b0624cd491f30cb6f9a49b6e6a3b33}{resolve\+\_\+solver}} (solver, positive, return\+\_\+intercept, is\+\_\+sparse, xp)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_acd87f44a7572f69cdb12fccd8443a00d}{resolve\+\_\+solver\+\_\+for\+\_\+numpy}} (positive, return\+\_\+intercept, is\+\_\+sparse)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_a348f25c593d0291db80e203699ca8061}{\+\_\+check\+\_\+gcv\+\_\+mode}} (X, gcv\+\_\+mode)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__ridge_af3808616fb4e997b0aae9d88c8c8e0a8}{\+\_\+find\+\_\+smallest\+\_\+angle}} (query, vectors)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Ridge regression
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a348f25c593d0291db80e203699ca8061}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_check\_gcv\_mode@{\_check\_gcv\_mode}}
\index{\_check\_gcv\_mode@{\_check\_gcv\_mode}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_check\_gcv\_mode()}{\_check\_gcv\_mode()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a348f25c593d0291db80e203699ca8061} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+check\+\_\+gcv\+\_\+mode (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{gcv\+\_\+mode}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l01581}{1581}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_af3808616fb4e997b0aae9d88c8c8e0a8}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_find\_smallest\_angle@{\_find\_smallest\_angle}}
\index{\_find\_smallest\_angle@{\_find\_smallest\_angle}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_find\_smallest\_angle()}{\_find\_smallest\_angle()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_af3808616fb4e997b0aae9d88c8c8e0a8} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+find\+\_\+smallest\+\_\+angle (\begin{DoxyParamCaption}\item[{}]{query}{, }\item[{}]{vectors}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Find the column of vectors that is most aligned with the query.

Both query and the columns of vectors must have their l2 norm equal to 1.

Parameters
----------
query : ndarray of shape (n_samples,)
    Normalized query vector.

vectors : ndarray of shape (n_samples, n_features)
    Vectors to which we compare query, as columns. Must be normalized.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__ridge_8py_source_l01591}{1591}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.



Referenced by \mbox{\hyperlink{__ridge_8py_source_l02016}{sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+\+Ridge\+GCV.\+\_\+solve\+\_\+eigen\+\_\+covariance\+\_\+intercept()}}, \mbox{\hyperlink{__ridge_8py_source_l01948}{sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+\+Ridge\+GCV.\+\_\+solve\+\_\+eigen\+\_\+gram()}}, and \mbox{\hyperlink{__ridge_8py_source_l02073}{sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+\+Ridge\+GCV.\+\_\+solve\+\_\+svd\+\_\+design\+\_\+matrix()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a795b22ea17397cfd97b4d7ef98508b37}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_get\_rescaled\_operator@{\_get\_rescaled\_operator}}
\index{\_get\_rescaled\_operator@{\_get\_rescaled\_operator}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_get\_rescaled\_operator()}{\_get\_rescaled\_operator()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a795b22ea17397cfd97b4d7ef98508b37} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+get\+\_\+rescaled\+\_\+operator (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{X\+\_\+offset}{, }\item[{}]{sample\+\_\+weight\+\_\+sqrt}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Create LinearOperator for matrix products with implicit centering.

Matrix product `LinearOperator @ coef` returns `(X - X_offset) @ coef`.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__ridge_8py_source_l00056}{56}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.



References \mbox{\hyperlink{__ridge_8py_source_l00056}{\+\_\+get\+\_\+rescaled\+\_\+operator()}}.



Referenced by \mbox{\hyperlink{__ridge_8py_source_l00056}{\+\_\+get\+\_\+rescaled\+\_\+operator()}}, and \mbox{\hyperlink{__ridge_8py_source_l00156}{\+\_\+solve\+\_\+lsqr()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_ae1f2e3f483b38f77b5eaa5d6ca708ab0}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_get\_valid\_accept\_sparse@{\_get\_valid\_accept\_sparse}}
\index{\_get\_valid\_accept\_sparse@{\_get\_valid\_accept\_sparse}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_get\_valid\_accept\_sparse()}{\_get\_valid\_accept\_sparse()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_ae1f2e3f483b38f77b5eaa5d6ca708ab0} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+get\+\_\+valid\+\_\+accept\+\_\+sparse (\begin{DoxyParamCaption}\item[{}]{is\+\_\+\+X\+\_\+sparse}{, }\item[{}]{solver}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00368}{368}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a9394458a04f9be072e6c981308d78bd2}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_ridge\_regression@{\_ridge\_regression}}
\index{\_ridge\_regression@{\_ridge\_regression}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_ridge\_regression()}{\_ridge\_regression()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a9394458a04f9be072e6c981308d78bd2} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+ridge\+\_\+regression (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{solver}{ = {\ttfamily "{}auto"{}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{positive}{ = {\ttfamily False}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{return\+\_\+intercept}{ = {\ttfamily False}, }\item[{}]{return\+\_\+solver}{ = {\ttfamily False}, }\item[{}]{X\+\_\+scale}{ = {\ttfamily None}, }\item[{}]{X\+\_\+offset}{ = {\ttfamily None}, }\item[{}]{check\+\_\+input}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{fit\+\_\+intercept}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00597}{597}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_aa184b154e12613beead709efe18e1473}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_cholesky@{\_solve\_cholesky}}
\index{\_solve\_cholesky@{\_solve\_cholesky}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_cholesky()}{\_solve\_cholesky()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_aa184b154e12613beead709efe18e1473} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+cholesky (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00201}{201}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a91b5180ca5403e673bb49a1cc983c7fb}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_cholesky\_kernel@{\_solve\_cholesky\_kernel}}
\index{\_solve\_cholesky\_kernel@{\_solve\_cholesky\_kernel}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_cholesky\_kernel()}{\_solve\_cholesky\_kernel()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a91b5180ca5403e673bb49a1cc983c7fb} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+cholesky\+\_\+kernel (\begin{DoxyParamCaption}\item[{}]{K}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{copy}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00223}{223}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a91df6db9c40254448075d9b8920e0111}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_lbfgs@{\_solve\_lbfgs}}
\index{\_solve\_lbfgs@{\_solve\_lbfgs}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_lbfgs()}{\_solve\_lbfgs()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a91df6db9c40254448075d9b8920e0111} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+lbfgs (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{}]{positive}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{X\+\_\+offset}{ = {\ttfamily None}, }\item[{}]{X\+\_\+scale}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight\+\_\+sqrt}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Solve ridge regression with LBFGS.

The main purpose is fitting with forcing coefficients to be positive.
For unconstrained ridge regression, there are faster dedicated solver methods.
Note that with positive bounds on the coefficients, LBFGS seems faster
than scipy.optimize.lsq_linear.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__ridge_8py_source_l00297}{297}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.



Referenced by \mbox{\hyperlink{__ridge_8py_source_l00416}{ridge\+\_\+regression()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_ab80be8bf17e021a8de74e71f9648d077}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_lsqr@{\_solve\_lsqr}}
\index{\_solve\_lsqr@{\_solve\_lsqr}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_lsqr()}{\_solve\_lsqr()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_ab80be8bf17e021a8de74e71f9648d077} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+lsqr (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{alpha}{, }\item[{}]{fit\+\_\+intercept}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{X\+\_\+offset}{ = {\ttfamily None}, }\item[{}]{X\+\_\+scale}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight\+\_\+sqrt}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Solve Ridge regression via LSQR.

We expect that y is always mean centered.
If X is dense, we expect it to be mean centered such that we can solve
    ||y - Xw||_2^2 + alpha * ||w||_2^2

If X is sparse, we expect X_offset to be given such that we can solve
    ||y - (X - X_offset)w||_2^2 + alpha * ||w||_2^2

With sample weights S=diag(sample_weight), this becomes
    ||sqrt(S) (y - (X - X_offset) w)||_2^2 + alpha * ||w||_2^2
and we expect y and X to already be rescaled, i.e. sqrt(S) @ y, sqrt(S) @ X. In
this case, X_offset is the sample_weight weighted mean of X before scaling by
sqrt(S). The objective then reads
   ||y - (X - sqrt(S) X_offset) w)||_2^2 + alpha * ||w||_2^2
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__ridge_8py_source_l00145}{145}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.



References \mbox{\hyperlink{__ridge_8py_source_l00056}{\+\_\+get\+\_\+rescaled\+\_\+operator()}}.



Referenced by \mbox{\hyperlink{__ridge_8py_source_l00416}{ridge\+\_\+regression()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a56d787b3475471d14facd04607a0e15a}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_sparse\_cg@{\_solve\_sparse\_cg}}
\index{\_solve\_sparse\_cg@{\_solve\_sparse\_cg}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_sparse\_cg()}{\_solve\_sparse\_cg()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a56d787b3475471d14facd04607a0e15a} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+sparse\+\_\+cg (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{X\+\_\+offset}{ = {\ttfamily None}, }\item[{}]{X\+\_\+scale}{ = {\ttfamily None}, }\item[{}]{sample\+\_\+weight\+\_\+sqrt}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00072}{72}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a4a643a4b40516c605d51a8c439eed070}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!\_solve\_svd@{\_solve\_svd}}
\index{\_solve\_svd@{\_solve\_svd}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{\_solve\_svd()}{\_solve\_svd()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a4a643a4b40516c605d51a8c439eed070} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+\_\+solve\+\_\+svd (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{}]{xp}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00285}{285}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_a47b0624cd491f30cb6f9a49b6e6a3b33}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!resolve\_solver@{resolve\_solver}}
\index{resolve\_solver@{resolve\_solver}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{resolve\_solver()}{resolve\_solver()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_a47b0624cd491f30cb6f9a49b6e6a3b33} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+resolve\+\_\+solver (\begin{DoxyParamCaption}\item[{}]{solver}{, }\item[{}]{positive}{, }\item[{}]{return\+\_\+intercept}{, }\item[{}]{is\+\_\+sparse}{, }\item[{}]{xp}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00826}{826}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_acd87f44a7572f69cdb12fccd8443a00d}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!resolve\_solver\_for\_numpy@{resolve\_solver\_for\_numpy}}
\index{resolve\_solver\_for\_numpy@{resolve\_solver\_for\_numpy}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{resolve\_solver\_for\_numpy()}{resolve\_solver\_for\_numpy()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_acd87f44a7572f69cdb12fccd8443a00d} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+resolve\+\_\+solver\+\_\+for\+\_\+numpy (\begin{DoxyParamCaption}\item[{}]{positive}{, }\item[{}]{return\+\_\+intercept}{, }\item[{}]{is\+\_\+sparse}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{__ridge_8py_source_l00858}{858}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__ridge_af3aaa5807b87a9626d68d99fc82a6394}\index{sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}!ridge\_regression@{ridge\_regression}}
\index{ridge\_regression@{ridge\_regression}!sklearn.linear\_model.\_ridge@{sklearn.linear\_model.\_ridge}}
\doxysubsubsection{\texorpdfstring{ridge\_regression()}{ridge\_regression()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__ridge_af3aaa5807b87a9626d68d99fc82a6394} 
sklearn.\+linear\+\_\+model.\+\_\+ridge.\+ridge\+\_\+regression (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{alpha}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{solver}{ = {\ttfamily "{}auto"{}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily None}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{positive}{ = {\ttfamily False}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{return\+\_\+intercept}{ = {\ttfamily False}, }\item[{}]{check\+\_\+input}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Solve the ridge equation by the method of normal equations.

Read more in the :ref:`User Guide <ridge_regression>`.

Parameters
----------
X : {array-like, sparse matrix, LinearOperator} of shape \
    (n_samples, n_features)
    Training data.

y : array-like of shape (n_samples,) or (n_samples, n_targets)
    Target values.

alpha : float or array-like of shape (n_targets,)
    Constant that multiplies the L2 term, controlling regularization
    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.

    When `alpha = 0`, the objective is equivalent to ordinary least
    squares, solved by the :class:`LinearRegression` object. For numerical
    reasons, using `alpha = 0` with the `Ridge` object is not advised.
    Instead, you should use the :class:`LinearRegression` object.

    If an array is passed, penalties are assumed to be specific to the
    targets. Hence they must correspond in number.

sample_weight : float or array-like of shape (n_samples,), default=None
    Individual weights for each sample. If given a float, every sample
    will have the same weight. If sample_weight is not None and
    solver='auto', the solver will be set to 'cholesky'.

    .. versionadded:: 0.17

solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', \
        'sag', 'saga', 'lbfgs'}, default='auto'
    Solver to use in the computational routines:

    - 'auto' chooses the solver automatically based on the type of data.

    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge
      coefficients. It is the most stable solver, in particular more stable
      for singular matrices than 'cholesky' at the cost of being slower.

    - 'cholesky' uses the standard scipy.linalg.solve function to
      obtain a closed-form solution via a Cholesky decomposition of
      dot(X.T, X)

    - 'sparse_cg' uses the conjugate gradient solver as found in
      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
      more appropriate than 'cholesky' for large-scale data
      (possibility to set `tol` and `max_iter`).

    - 'lsqr' uses the dedicated regularized least-squares routine
      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
      procedure.

    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
      its improved, unbiased version named SAGA. Both methods also use an
      iterative procedure, and are often faster than other solvers when
      both n_samples and n_features are large. Note that 'sag' and
      'saga' fast convergence is only guaranteed on features with
      approximately the same scale. You can preprocess the data with a
      scaler from sklearn.preprocessing.

    - 'lbfgs' uses L-BFGS-B algorithm implemented in
      `scipy.optimize.minimize`. It can be used only when `positive`
      is True.

    All solvers except 'svd' support both dense and sparse data. However, only
    'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when
    `fit_intercept` is True.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.

max_iter : int, default=None
    Maximum number of iterations for conjugate gradient solver.
    For the 'sparse_cg' and 'lsqr' solvers, the default value is determined
    by scipy.sparse.linalg. For 'sag' and saga solver, the default value is
    1000. For 'lbfgs' solver, the default value is 15000.

tol : float, default=1e-4
    Precision of the solution. Note that `tol` has no effect for solvers 'svd' and
    'cholesky'.

    .. versionchanged:: 1.2
       Default value changed from 1e-3 to 1e-4 for consistency with other linear
       models.

verbose : int, default=0
    Verbosity level. Setting verbose > 0 will display additional
    information depending on the solver used.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.
    Only 'lbfgs' solver is supported in this case.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.
    See :term:`Glossary <random_state>` for details.

return_n_iter : bool, default=False
    If True, the method also returns `n_iter`, the actual number of
    iteration performed by the solver.

    .. versionadded:: 0.17

return_intercept : bool, default=False
    If True and if X is sparse, the method also returns the intercept,
    and the solver is automatically changed to 'sag'. This is only a
    temporary fix for fitting the intercept with sparse data. For dense
    data, use sklearn.linear_model._preprocess_data before your regression.

    .. versionadded:: 0.17

check_input : bool, default=True
    If False, the input arrays X and y will not be checked.

    .. versionadded:: 0.21

Returns
-------
coef : ndarray of shape (n_features,) or (n_targets, n_features)
    Weight vector(s).

n_iter : int, optional
    The actual number of iteration performed by the solver.
    Only returned if `return_n_iter` is True.

intercept : float or ndarray of shape (n_targets,)
    The intercept of the model. Only returned if `return_intercept`
    is True and if X is a scipy sparse array.

Notes
-----
This function won't compute the intercept.

Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to ``1 / (2C)`` in other linear
models such as :class:`~sklearn.linear_model.LogisticRegression` or
:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_regression
>>> from sklearn.linear_model import ridge_regression
>>> rng = np.random.RandomState(0)
>>> X = rng.randn(100, 4)
>>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)
>>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True,
...                                    random_state=0)
>>> coef
array([ 1.97, -1., -2.69e-3, -9.27e-4 ])
>>> intercept
np.float64(-.0012)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__ridge_8py_source_l00401}{401}} of file \mbox{\hyperlink{__ridge_8py_source}{\+\_\+ridge.\+py}}.



References \mbox{\hyperlink{__ridge_8py_source_l00307}{\+\_\+solve\+\_\+lbfgs()}}, and \mbox{\hyperlink{__ridge_8py_source_l00156}{\+\_\+solve\+\_\+lsqr()}}.

