\doxysection{\+\_\+stochastic\+\_\+optimizers.\+py}
\hypertarget{__stochastic__optimizers_8py_source}{}\label{__stochastic__optimizers_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/neural\_network/\_stochastic\_optimizers.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/neural\_network/\_stochastic\_optimizers.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1neural__network_1_1__stochastic__optimizers}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}Stochastic\ optimization\ methods\ for\ MLP"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00002}00002\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00003}00003\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00004}00004\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00005}00005\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00006}00006\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00008}00008\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00009}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{00009}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{BaseOptimizer}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00010}00010\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Base\ (Stochastic)\ gradient\ descent\ optimizer}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00011}00011\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00012}00012\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00013}00013\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00014}00014\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\_init\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00015}00015\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ used.\ It\ controls\ the\ step-\/size\ in\ updating}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00016}00016\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ weights}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00017}00017\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00018}00018\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00019}00019\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00020}00020\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00021}00021\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ current\ learning\ rate}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00022}00022\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00023}00023\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00024}00024\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ learning\_rate\_init=0.1):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00025}00025\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_ab5689b36eec135b26ea1378a0c1e0ac2}{learning\_rate\_init}}\ =\ learning\_rate\_init}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00026}00026\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ =\ float(learning\_rate\_init)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00027}00027\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00028}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a840bd3275bac4a6db219ca7370977baf}{00028}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a840bd3275bac4a6db219ca7370977baf}{update\_params}}(self,\ params,\ grads):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00029}00029\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Update\ parameters\ with\ given\ gradients}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00030}00030\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00031}00031\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00032}00032\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00033}00033\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ params\ :\ list\ of\ length\ =\ len(coefs\_)\ +\ len(intercepts\_)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00034}00034\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ concatenated\ list\ containing\ coefs\_\ and\ intercepts\_\ in\ MLP}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00035}00035\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ model.\ Used\ for\ initializing\ velocities\ and\ updating\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00036}00036\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00037}00037\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ grads\ :\ list\ of\ length\ =\ len(params)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00038}00038\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Containing\ gradients\ with\ respect\ to\ coefs\_\ and\ intercepts\_\ in\ MLP}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00039}00039\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ model.\ So\ length\ should\ be\ aligned\ with\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00040}00040\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00041}00041\ \ \ \ \ \ \ \ \ updates\ =\ self.\_get\_updates(grads)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00042}00042\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ param,\ update\ \textcolor{keywordflow}{in}\ zip((p\ \textcolor{keywordflow}{for}\ p\ \textcolor{keywordflow}{in}\ params),\ updates):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00043}00043\ \ \ \ \ \ \ \ \ \ \ \ \ param\ +=\ update}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00044}00044\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00045}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a23867e22d6b166756b2cf4faebc12c9d}{00045}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a23867e22d6b166756b2cf4faebc12c9d}{iteration\_ends}}(self,\ time\_step):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00046}00046\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Perform\ update\ to\ learning\ rate\ and\ potentially\ other\ states\ at\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00047}00047\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ end\ of\ an\ iteration}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00048}00048\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00049}00049\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{pass}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00050}00050\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00051}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_aeeb3ca05e9540cba44a543cd9a39cc87}{00051}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_aeeb3ca05e9540cba44a543cd9a39cc87}{trigger\_stopping}}(self,\ msg,\ verbose):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00052}00052\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Decides\ whether\ it\ is\ time\ to\ stop\ training}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00053}00053\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00054}00054\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00055}00055\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00056}00056\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ msg\ :\ str}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00057}00057\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Message\ passed\ in\ for\ verbose\ output}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00058}00058\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00059}00059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ verbose\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00060}00060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Print\ message\ to\ stdin\ if\ True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00061}00061\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00062}00062\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00063}00063\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00064}00064\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ is\_stopping\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00065}00065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ True\ if\ training\ needs\ to\ stop}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00066}00066\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ verbose:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ \ \ \ \ print(msg\ +\ \textcolor{stringliteral}{"{}\ Stopping."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00069}00069\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00070}00070\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00071}00071\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00072}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer}{00072}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer}{SGDOptimizer}}(\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{BaseOptimizer}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00073}00073\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Stochastic\ gradient\ descent\ optimizer\ with\ momentum}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00074}00074\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00075}00075\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00076}00076\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00077}00077\ \textcolor{stringliteral}{\ \ \ \ params\ :\ list,\ length\ =\ len(coefs\_)\ +\ len(intercepts\_)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00078}00078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ concatenated\ list\ containing\ coefs\_\ and\ intercepts\_\ in\ MLP\ model.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00079}00079\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Used\ for\ initializing\ velocities\ and\ updating\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00080}00080\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00081}00081\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\_init\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00082}00082\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ used.\ It\ controls\ the\ step-\/size\ in\ updating}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00083}00083\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ weights}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00084}00084\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00085}00085\ \textcolor{stringliteral}{\ \ \ \ lr\_schedule\ :\ \{'constant',\ 'adaptive',\ 'invscaling'\},\ default='constant'}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00086}00086\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Learning\ rate\ schedule\ for\ weight\ updates.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00087}00087\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00088}00088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'constant',\ is\ a\ constant\ learning\ rate\ given\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00089}00089\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ 'learning\_rate\_init'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00090}00090\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'invscaling'\ gradually\ decreases\ the\ learning\ rate\ 'learning\_rate\_'\ at}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ each\ time\ step\ 't'\ using\ an\ inverse\ scaling\ exponent\ of\ 'power\_t'.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00093}00093\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ learning\_rate\_\ =\ learning\_rate\_init\ /\ pow(t,\ power\_t)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00094}00094\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/'adaptive',\ keeps\ the\ learning\ rate\ constant\ to}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ 'learning\_rate\_init'\ as\ long\ as\ the\ training\ keeps\ decreasing.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ Each\ time\ 2\ consecutive\ epochs\ fail\ to\ decrease\ the\ training\ loss\ by}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ tol,\ or\ fail\ to\ increase\ validation\ score\ by\ tol\ if\ 'early\_stopping'}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00099}00099\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ is\ on,\ the\ current\ learning\ rate\ is\ divided\ by\ 5.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00100}00100\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ momentum\ :\ float,\ default=0.9}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Value\ of\ momentum\ used,\ must\ be\ larger\ than\ or\ equal\ to\ 0}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00103}00103\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ nesterov\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ to\ use\ nesterov's\ momentum\ or\ not.\ Use\ nesterov's\ if\ True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00106}00106\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ power\_t\ :\ float,\ default=0.5}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00108}00108\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Power\ of\ time\ step\ 't'\ in\ inverse\ scaling.\ See\ \`{}lr\_schedule\`{}\ for}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00109}00109\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ more\ details.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00110}00110\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ current\ learning\ rate}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00115}00115\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ velocities\ :\ list,\ length\ =\ len(params)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ velocities\ that\ are\ used\ to\ update\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00118}00118\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00119}00119\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00120}00120\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00121}00121\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00122}00122\ \ \ \ \ \ \ \ \ params,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ learning\_rate\_init=0.1,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ lr\_schedule="{}constant"{},}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ momentum=0.9,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ nesterov=True,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ power\_t=0.5,}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00128}00128\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(learning\_rate\_init)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00130}00130\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a5f01d6fdde478110353a4ce8c9b4a2e4}{lr\_schedule}}\ =\ lr\_schedule}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00132}00132\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a9c7a8564a2c0a0fe5455759a66ed58c4}{momentum}}\ =\ momentum}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a6bef0537230d045f78f54066d39631d5}{nesterov}}\ =\ nesterov}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_ac1a453d8bdb0a114690ca79f3b6d6458}{power\_t}}\ =\ power\_t}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00135}00135\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_af105b8a0251ed901fc672d7dcb552b4d}{velocities}}\ =\ [np.zeros\_like(param)\ \textcolor{keywordflow}{for}\ param\ \textcolor{keywordflow}{in}\ params]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00136}00136\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00137}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a94cef6294a4a466a5954ae16135d2a59}{00137}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a94cef6294a4a466a5954ae16135d2a59}{iteration\_ends}}(self,\ time\_step):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00138}00138\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Perform\ updates\ to\ learning\ rate\ and\ potential\ other\ states\ at\ the}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00139}00139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ end\ of\ an\ iteration}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00140}00140\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00141}00141\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00142}00142\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00143}00143\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ time\_step\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00144}00144\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ number\ of\ training\ samples\ trained\ on\ so\ far,\ used\ to\ update}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00145}00145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ learning\ rate\ for\ 'invscaling'}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00146}00146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00147}00147\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a5f01d6fdde478110353a4ce8c9b4a2e4}{lr\_schedule}}\ ==\ \textcolor{stringliteral}{"{}invscaling"{}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ =\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00149}00149\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ float(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_ab5689b36eec135b26ea1378a0c1e0ac2}{learning\_rate\_init}})\ /\ (time\_step\ +\ 1)\ **\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_ac1a453d8bdb0a114690ca79f3b6d6458}{power\_t}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00150}00150\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00151}00151\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00152}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a89122ecbecd6649d51b412b3e3886f81}{00152}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a89122ecbecd6649d51b412b3e3886f81}{trigger\_stopping}}(self,\ msg,\ verbose):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00153}00153\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a5f01d6fdde478110353a4ce8c9b4a2e4}{lr\_schedule}}\ !=\ \textcolor{stringliteral}{"{}adaptive"{}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00154}00154\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ verbose:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ print(msg\ +\ \textcolor{stringliteral}{"{}\ Stopping."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00157}00157\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ <=\ 1e-\/6:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ verbose:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ print(msg\ +\ \textcolor{stringliteral}{"{}\ Learning\ rate\ too\ small.\ Stopping."{}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00162}00162\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00163}00163\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ /=\ 5.0}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ verbose:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ \ \ \ \ print(msg\ +\ \textcolor{stringliteral}{"{}\ Setting\ learning\ rate\ to\ \%f"{}}\ \%\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00167}00167\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00168}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a8fa46424589d48d987f0e9e3d512bc37}{00168}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a8fa46424589d48d987f0e9e3d512bc37}{\_get\_updates}}(self,\ grads):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Get\ the\ values\ used\ to\ update\ params\ with\ given\ gradients}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00170}00170\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00171}00171\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00172}00172\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00173}00173\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ grads\ :\ list,\ length\ =\ len(coefs\_)\ +\ len(intercepts\_)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00174}00174\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Containing\ gradients\ with\ respect\ to\ coefs\_\ and\ intercepts\_\ in\ MLP}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ model.\ So\ length\ should\ be\ aligned\ with\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00176}00176\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ updates\ :\ list,\ length\ =\ len(grads)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ values\ to\ add\ to\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00182}00182\ \ \ \ \ \ \ \ \ updates\ =\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00183}00183\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a9c7a8564a2c0a0fe5455759a66ed58c4}{momentum}}\ *\ velocity\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ *\ grad}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00184}00184\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ velocity,\ grad\ \textcolor{keywordflow}{in}\ zip(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_af105b8a0251ed901fc672d7dcb552b4d}{velocities}},\ grads)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00185}00185\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00186}00186\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_af105b8a0251ed901fc672d7dcb552b4d}{velocities}}\ =\ updates}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00187}00187\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00188}00188\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a6bef0537230d045f78f54066d39631d5}{nesterov}}:}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00189}00189\ \ \ \ \ \ \ \ \ \ \ \ \ updates\ =\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00190}00190\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_a9c7a8564a2c0a0fe5455759a66ed58c4}{momentum}}\ *\ velocity\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ *\ grad}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00191}00191\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ velocity,\ grad\ \textcolor{keywordflow}{in}\ zip(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1SGDOptimizer_af105b8a0251ed901fc672d7dcb552b4d}{velocities}},\ grads)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00193}00193\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00194}00194\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ updates}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00195}00195\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00196}00196\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00197}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer}{00197}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer}{AdamOptimizer}}(\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer}{BaseOptimizer}}):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00198}00198\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Stochastic\ gradient\ descent\ optimizer\ with\ Adam}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00199}00199\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00200}00200\ \textcolor{stringliteral}{\ \ \ \ Note:\ All\ default\ values\ are\ from\ the\ original\ Adam\ paper}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00201}00201\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00202}00202\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00203}00203\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00204}00204\ \textcolor{stringliteral}{\ \ \ \ params\ :\ list,\ length\ =\ len(coefs\_)\ +\ len(intercepts\_)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00205}00205\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ concatenated\ list\ containing\ coefs\_\ and\ intercepts\_\ in\ MLP\ model.}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00206}00206\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Used\ for\ initializing\ velocities\ and\ updating\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00207}00207\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00208}00208\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\_init\ :\ float,\ default=0.001}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00209}00209\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ learning\ rate\ used.\ It\ controls\ the\ step-\/size\ in\ updating}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00210}00210\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ weights}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00211}00211\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00212}00212\ \textcolor{stringliteral}{\ \ \ \ beta\_1\ :\ float,\ default=0.9}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00213}00213\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Exponential\ decay\ rate\ for\ estimates\ of\ first\ moment\ vector,\ should\ be}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00214}00214\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ in\ [0,\ 1)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00215}00215\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00216}00216\ \textcolor{stringliteral}{\ \ \ \ beta\_2\ :\ float,\ default=0.999}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00217}00217\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Exponential\ decay\ rate\ for\ estimates\ of\ second\ moment\ vector,\ should\ be}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00218}00218\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ in\ [0,\ 1)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00219}00219\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00220}00220\ \textcolor{stringliteral}{\ \ \ \ epsilon\ :\ float,\ default=1e-\/8}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00221}00221\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Value\ for\ numerical\ stability}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00222}00222\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00223}00223\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00224}00224\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00225}00225\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00226}00226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ current\ learning\ rate}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00227}00227\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00228}00228\ \textcolor{stringliteral}{\ \ \ \ t\ :\ int}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00229}00229\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Timestep}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00230}00230\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00231}00231\ \textcolor{stringliteral}{\ \ \ \ ms\ :\ list,\ length\ =\ len(params)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00232}00232\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ First\ moment\ vectors}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00233}00233\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00234}00234\ \textcolor{stringliteral}{\ \ \ \ vs\ :\ list,\ length\ =\ len(params)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00235}00235\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Second\ moment\ vectors}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00236}00236\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00237}00237\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00238}00238\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00239}00239\ \textcolor{stringliteral}{\ \ \ \ :arxiv:\`{}Kingma,\ Diederik,\ and\ Jimmy\ Ba\ (2014)\ "{}Adam:\ A\ method\ for}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00240}00240\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ stochastic\ optimization."{}\ <1412.6980>}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00241}00241\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00242}00242\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00243}00243\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ self,\ params,\ learning\_rate\_init=0.001,\ beta\_1=0.9,\ beta\_2=0.999,\ epsilon=1e-\/8}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00245}00245\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(learning\_rate\_init)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00247}00247\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}{beta\_1}}\ =\ beta\_1}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}{beta\_2}}\ =\ beta\_2}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af38071d35a96bdf6d266c3095c74c5da}{epsilon}}\ =\ epsilon}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}{t}}\ =\ 0}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}{ms}}\ =\ [np.zeros\_like(param)\ \textcolor{keywordflow}{for}\ param\ \textcolor{keywordflow}{in}\ params]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}{vs}}\ =\ [np.zeros\_like(param)\ \textcolor{keywordflow}{for}\ param\ \textcolor{keywordflow}{in}\ params]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00254}00254\ }
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00255}\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_acc5a97518444bd40d2446bd2f12c7968}{00255}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_acc5a97518444bd40d2446bd2f12c7968}{\_get\_updates}}(self,\ grads):}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Get\ the\ values\ used\ to\ update\ params\ with\ given\ gradients}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00257}00257\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00258}00258\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00259}00259\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00260}00260\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ grads\ :\ list,\ length\ =\ len(coefs\_)\ +\ len(intercepts\_)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00261}00261\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Containing\ gradients\ with\ respect\ to\ coefs\_\ and\ intercepts\_\ in\ MLP}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00262}00262\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ model.\ So\ length\ should\ be\ aligned\ with\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00263}00263\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00264}00264\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00265}00265\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00266}00266\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ updates\ :\ list,\ length\ =\ len(grads)}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00267}00267\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ values\ to\ add\ to\ params}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00268}00268\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00269}00269\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}{t}}\ +=\ 1}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00270}00270\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}{ms}}\ =\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00271}00271\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}{beta\_1}}\ *\ m\ +\ (1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}{beta\_1}})\ *\ grad}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00272}00272\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m,\ grad\ \textcolor{keywordflow}{in}\ zip(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}{ms}},\ grads)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}{vs}}\ =\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00275}00275\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}{beta\_2}}\ *\ v\ +\ (1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}{beta\_2}})\ *\ (grad**2)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ v,\ grad\ \textcolor{keywordflow}{in}\ zip(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}{vs}},\ grads)}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00277}00277\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00278}00278\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ =\ (}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_ab5689b36eec135b26ea1378a0c1e0ac2}{learning\_rate\_init}}}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ \ \ \ \ *\ np.sqrt(1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_afade0290e697eb3d1c669f0f4ad5140e}{beta\_2}}**self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}{t}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00281}00281\ \ \ \ \ \ \ \ \ \ \ \ \ /\ (1\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a518a78fcf250601d748652d959ecfdab}{beta\_1}}**self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_ab973099fc9494009205bdcae485782b6}{t}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00282}00282\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00283}00283\ \ \ \ \ \ \ \ \ updates\ =\ [}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ \ \ \ \ -\/self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1BaseOptimizer_a909a1524f14f1c374998e353818c5830}{learning\_rate}}\ *\ m\ /\ (np.sqrt(v)\ +\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af38071d35a96bdf6d266c3095c74c5da}{epsilon}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00285}00285\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m,\ v\ \textcolor{keywordflow}{in}\ zip(self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_a10e13a34eb1688ece5fad08ef8792c57}{ms}},\ self.\mbox{\hyperlink{classsklearn_1_1neural__network_1_1__stochastic__optimizers_1_1AdamOptimizer_af17a51fda6f503623bbfc6f3171eb8c6}{vs}})}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__stochastic__optimizers_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ updates}

\end{DoxyCode}
