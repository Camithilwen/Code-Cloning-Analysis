\doxysection{test\+\_\+glm.\+py}
\hypertarget{test__glm_8py_source}{}\label{test__glm_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_glm/tests/test\_glm.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/linear\_model/\_glm/tests/test\_glm.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm}{00001}}\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00002}00002\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00004}00004\ \textcolor{keyword}{import}\ itertools}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00005}00005\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00006}00006\ \textcolor{keyword}{from}\ functools\ \textcolor{keyword}{import}\ partial}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00008}00008\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00009}00009\ \textcolor{keyword}{import}\ pytest}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00010}00010\ \textcolor{keyword}{import}\ scipy}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00011}00011\ \textcolor{keyword}{from}\ scipy\ \textcolor{keyword}{import}\ linalg}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00012}00012\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacescipy_1_1optimize}{scipy.optimize}}\ \textcolor{keyword}{import}\ minimize,\ root}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00014}00014\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1__loss}{sklearn.\_loss}}\ \textcolor{keyword}{import}\ HalfBinomialLoss,\ HalfPoissonLoss,\ HalfTweedieLoss}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00015}00015\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1link}{sklearn.\_loss.link}}\ \textcolor{keyword}{import}\ IdentityLink,\ LogLink}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00016}00016\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1base}{sklearn.base}}\ \textcolor{keyword}{import}\ clone}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00017}00017\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1datasets}{sklearn.datasets}}\ \textcolor{keyword}{import}\ make\_low\_rank\_matrix,\ make\_regression}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00018}00018\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1exceptions}{sklearn.exceptions}}\ \textcolor{keyword}{import}\ ConvergenceWarning}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00019}00019\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1linear__model}{sklearn.linear\_model}}\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00020}00020\ \ \ \ \ GammaRegressor,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00021}00021\ \ \ \ \ PoissonRegressor,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00022}00022\ \ \ \ \ Ridge,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00023}00023\ \ \ \ \ TweedieRegressor,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00024}00024\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00025}00025\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm}{sklearn.linear\_model.\_glm}}\ \textcolor{keyword}{import}\ \_GeneralizedLinearRegressor}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00026}00026\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1__newton__solver}{sklearn.linear\_model.\_glm.\_newton\_solver}}\ \textcolor{keyword}{import}\ NewtonCholeskySolver}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00027}00027\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__linear__loss}{sklearn.linear\_model.\_linear\_loss}}\ \textcolor{keyword}{import}\ LinearModelLoss}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00028}00028\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1metrics}{sklearn.metrics}}\ \textcolor{keyword}{import}\ d2\_tweedie\_score,\ mean\_poisson\_deviance}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00029}00029\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1model__selection}{sklearn.model\_selection}}\ \textcolor{keyword}{import}\ train\_test\_split}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00030}00030\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacesklearn_1_1utils_1_1__testing}{sklearn.utils.\_testing}}\ \textcolor{keyword}{import}\ assert\_allclose}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00031}00031\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00032}00032\ SOLVERS\ =\ [\textcolor{stringliteral}{"{}lbfgs"{}},\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00033}00033\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00034}00034\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00035}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_1_1BinomialRegressor}{00035}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_1_1BinomialRegressor}{BinomialRegressor}}(\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00036}\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_1_1BinomialRegressor_aa9db0cac93ded4f507706b5fec7b2ad7}{00036}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_1_1BinomialRegressor_aa9db0cac93ded4f507706b5fec7b2ad7}{\_get\_loss}}(self):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00037}00037\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00038}00038\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00039}00039\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00040}00040\ \textcolor{keyword}{def\ }\_special\_minimize(fun,\ grad,\ x,\ tol\_NM,\ tol):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00041}00041\ \ \ \ \ \textcolor{comment}{\#\ Find\ good\ starting\ point\ by\ Nelder-\/Mead}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00042}00042\ \ \ \ \ res\_NM\ =\ minimize(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00043}00043\ \ \ \ \ \ \ \ \ fun,\ x,\ method=\textcolor{stringliteral}{"{}Nelder-\/Mead"{}},\ options=\{\textcolor{stringliteral}{"{}xatol"{}}:\ tol\_NM,\ \textcolor{stringliteral}{"{}fatol"{}}:\ tol\_NM\}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00044}00044\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00045}00045\ \ \ \ \ \textcolor{comment}{\#\ Now\ refine\ via\ root\ finding\ on\ the\ gradient\ of\ the\ function,\ which\ is}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00046}00046\ \ \ \ \ \textcolor{comment}{\#\ more\ precise\ than\ minimizing\ the\ function\ itself.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00047}00047\ \ \ \ \ res\ =\ root(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00048}00048\ \ \ \ \ \ \ \ \ grad,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00049}00049\ \ \ \ \ \ \ \ \ res\_NM.x,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00050}00050\ \ \ \ \ \ \ \ \ method=\textcolor{stringliteral}{"{}lm"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00051}00051\ \ \ \ \ \ \ \ \ options=\{\textcolor{stringliteral}{"{}ftol"{}}:\ tol,\ \textcolor{stringliteral}{"{}xtol"{}}:\ tol,\ \textcolor{stringliteral}{"{}gtol"{}}:\ tol\},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00052}00052\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00053}00053\ \ \ \ \ \textcolor{keywordflow}{return}\ res.x}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00054}00054\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00055}00055\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00056}00056\ \textcolor{preprocessor}{@pytest.fixture(scope="{}module"{})}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00057}00057\ \textcolor{keyword}{def\ }regression\_data():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00058}00058\ \ \ \ \ X,\ y\ =\ make\_regression(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00059}00059\ \ \ \ \ \ \ \ \ n\_samples=107,\ n\_features=10,\ n\_informative=80,\ noise=0.5,\ random\_state=2}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00060}00060\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00061}00061\ \ \ \ \ \textcolor{keywordflow}{return}\ X,\ y}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00062}00062\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00063}00063\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00064}00064\ \textcolor{preprocessor}{@pytest.fixture}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00065}00065\ \ \ \ \ params=itertools.product(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00066}00066\ \ \ \ \ \ \ \ \ [\textcolor{stringliteral}{"{}long"{}},\ \textcolor{stringliteral}{"{}wide"{}}],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_1_1BinomialRegressor}{BinomialRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00069}00069\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00070}00070\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{GammaRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00071}00071\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TweedieRegressor(power=3.0),\ \ \#\ too\ difficult}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00072}00072\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TweedieRegressor(power=0,\ link="{}log"{}),\ \ \#\ too\ difficult}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=1.5),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00075}00075\ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00076}00076\ \ \ \ \ ids=\textcolor{keyword}{lambda}\ param:\ f\textcolor{stringliteral}{"{}\{param[0]\}-\/\{param[1]\}"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00077}00077\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00078}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a3c56c14ef11366c466ad2a1e72913281}{00078}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a3c56c14ef11366c466ad2a1e72913281}{glm\_dataset}}(global\_random\_seed,\ request):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00079}00079\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Dataset\ with\ GLM\ solutions,\ well\ conditioned\ X.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00080}00080\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00081}00081\ \textcolor{stringliteral}{\ \ \ \ This\ is\ inspired\ by\ ols\_ridge\_dataset\ in\ test\_ridge.py.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00082}00082\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00083}00083\ \textcolor{stringliteral}{\ \ \ \ The\ construction\ is\ based\ on\ the\ SVD\ decomposition\ of\ X\ =\ U\ S\ V'.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00084}00084\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00085}00085\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00086}00086\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00087}00087\ \textcolor{stringliteral}{\ \ \ \ type\ :\ \{"{}long"{},\ "{}wide"{}\}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00088}00088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ "{}long"{},\ then\ n\_samples\ >\ n\_features.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00089}00089\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ "{}wide"{},\ then\ n\_features\ >\ n\_samples.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ model\ :\ a\ GLM\ model}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00091}00091\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ For\ "{}wide"{},\ we\ return\ the\ minimum\ norm\ solution:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00093}00093\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00094}00094\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ ||w||\_2\ subject\ to\ w\ =\ argmin\ deviance(X,\ y,\ w)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00095}00095\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ deviance\ is\ always\ minimized\ if\ y\ =\ inverse\_link(X\ w)\ is\ possible\ to}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ achieve,\ which\ it\ is\ in\ the\ wide\ data\ case.\ Therefore,\ we\ can\ construct\ the}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ solution\ with\ minimum\ norm\ like\ (wide)\ OLS:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00099}00099\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ ||w||\_2\ subject\ to\ link(y)\ =\ raw\_prediction\ =\ X\ w}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00101}00101\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00103}00103\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ model\ :\ GLM\ model}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ X\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Last\ column\ of\ 1,\ i.e.\ intercept.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ y\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00108}00108\ \textcolor{stringliteral}{\ \ \ \ coef\_unpenalized\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00109}00109\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Minimum\ norm\ solutions,\ i.e.\ min\ sum(loss(w))\ (with\ minimum\ ||w||\_2\ in}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00110}00110\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ case\ of\ ambiguity)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Last\ coefficient\ is\ intercept.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ coef\_penalized\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ GLM\ solution\ with\ alpha=l2\_reg\_strength=1,\ i.e.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ 1/n\ *\ sum(loss)\ +\ ||w[:-\/1]||\_2\string^2.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Last\ coefficient\ is\ intercept.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ l2\_reg\_strength\ :\ float}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Always\ equal\ 1.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00118}00118\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00119}00119\ \ \ \ \ data\_type,\ model\ =\ request.param}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00120}00120\ \ \ \ \ \textcolor{comment}{\#\ Make\ larger\ dim\ more\ than\ double\ as\ big\ as\ the\ smaller\ one.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00121}00121\ \ \ \ \ \textcolor{comment}{\#\ This\ helps\ when\ constructing\ singular\ matrices\ like\ (X,\ X).}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00122}00122\ \ \ \ \ \textcolor{keywordflow}{if}\ data\_type\ ==\ \textcolor{stringliteral}{"{}long"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ n\_samples,\ n\_features\ =\ 12,\ 4}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00124}00124\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ n\_samples,\ n\_features\ =\ 4,\ 12}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00126}00126\ \ \ \ \ k\ =\ min(n\_samples,\ n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00127}00127\ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00128}00128\ \ \ \ \ X\ =\ make\_low\_rank\_matrix(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ effective\_rank=k,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00132}00132\ \ \ \ \ \ \ \ \ tail\_strength=0.1,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ random\_state=rng,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00134}00134\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00135}00135\ \ \ \ \ X[:,\ -\/1]\ =\ 1\ \ \textcolor{comment}{\#\ last\ columns\ acts\ as\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00136}00136\ \ \ \ \ U,\ s,\ Vt\ =\ linalg.svd(X,\ full\_matrices=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00137}00137\ \ \ \ \ \textcolor{keyword}{assert}\ np.all(s\ >\ 1e-\/3)\ \ \textcolor{comment}{\#\ to\ be\ sure}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00138}00138\ \ \ \ \ \textcolor{keyword}{assert}\ np.max(s)\ /\ np.min(s)\ <\ 100\ \ \textcolor{comment}{\#\ condition\ number\ of\ X}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00139}00139\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00140}00140\ \ \ \ \ \textcolor{keywordflow}{if}\ data\_type\ ==\ \textcolor{stringliteral}{"{}long"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00141}00141\ \ \ \ \ \ \ \ \ coef\_unpenalized\ =\ rng.uniform(low=1,\ high=3,\ size=n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00142}00142\ \ \ \ \ \ \ \ \ coef\_unpenalized\ *=\ rng.choice([-\/1,\ 1],\ size=n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ X\ @\ coef\_unpenalized}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00144}00144\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00145}00145\ \ \ \ \ \ \ \ \ raw\_prediction\ =\ rng.uniform(low=-\/3,\ high=3,\ size=n\_samples)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00146}00146\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ minimum\ norm\ solution\ min\ ||w||\_2\ such\ that\ raw\_prediction\ =\ X\ w:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00147}00147\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ w\ =\ X'(XX')\string^-\/1\ raw\_prediction\ =\ V\ s\string^-\/1\ U'\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ coef\_unpenalized\ =\ Vt.T\ @\ np.diag(1\ /\ s)\ @\ U.T\ @\ raw\_prediction}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00149}00149\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00150}00150\ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=model.\_get\_loss(),\ fit\_intercept=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00151}00151\ \ \ \ \ sw\ =\ np.full(shape=n\_samples,\ fill\_value=1\ /\ n\_samples)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00152}00152\ \ \ \ \ y\ =\ linear\_loss.base\_loss.link.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00153}00153\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00154}00154\ \ \ \ \ \textcolor{comment}{\#\ Add\ penalty\ l2\_reg\_strength\ *\ ||coef||\_2\string^2\ for\ l2\_reg\_strength=1\ and\ solve\ with}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00155}00155\ \ \ \ \ \textcolor{comment}{\#\ optimizer.\ Note\ that\ the\ problem\ is\ well\ conditioned\ such\ that\ we\ get\ accurate}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00156}00156\ \ \ \ \ \textcolor{comment}{\#\ results.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00157}00157\ \ \ \ \ l2\_reg\_strength\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00158}00158\ \ \ \ \ fun\ =\ partial(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ linear\_loss.loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00160}00160\ \ \ \ \ \ \ \ \ X=X[:,\ :-\/1],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00162}00162\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00163}00163\ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00164}00164\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00165}00165\ \ \ \ \ grad\ =\ partial(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ linear\_loss.gradient,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00167}00167\ \ \ \ \ \ \ \ \ X=X[:,\ :-\/1],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00170}00170\ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00171}00171\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00172}00172\ \ \ \ \ coef\_penalized\_with\_intercept\ =\ \_special\_minimize(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ fun,\ grad,\ coef\_unpenalized,\ tol\_NM=1e-\/6,\ tol=1e-\/14}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00174}00174\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00175}00175\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00176}00176\ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=model.\_get\_loss(),\ fit\_intercept=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00177}00177\ \ \ \ \ fun\ =\ partial(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00178}00178\ \ \ \ \ \ \ \ \ linear\_loss.loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00179}00179\ \ \ \ \ \ \ \ \ X=X[:,\ :-\/1],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00180}00180\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00181}00181\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00182}00182\ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00183}00183\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00184}00184\ \ \ \ \ grad\ =\ partial(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00185}00185\ \ \ \ \ \ \ \ \ linear\_loss.gradient,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00186}00186\ \ \ \ \ \ \ \ \ X=X[:,\ :-\/1],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00187}00187\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00188}00188\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00189}00189\ \ \ \ \ \ \ \ \ l2\_reg\_strength=l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00190}00190\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00191}00191\ \ \ \ \ coef\_penalized\_without\_intercept\ =\ \_special\_minimize(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ fun,\ grad,\ coef\_unpenalized[:-\/1],\ tol\_NM=1e-\/6,\ tol=1e-\/14}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00193}00193\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00194}00194\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00195}00195\ \ \ \ \ \textcolor{comment}{\#\ To\ be\ sure}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00196}00196\ \ \ \ \ \textcolor{keyword}{assert}\ np.linalg.norm(coef\_penalized\_with\_intercept)\ <\ np.linalg.norm(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ coef\_unpenalized}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00198}00198\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00199}00199\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00200}00200\ \ \ \ \ \textcolor{keywordflow}{return}\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ model,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00204}00204\ \ \ \ \ \ \ \ \ coef\_unpenalized,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00205}00205\ \ \ \ \ \ \ \ \ coef\_penalized\_with\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00206}00206\ \ \ \ \ \ \ \ \ coef\_penalized\_without\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00207}00207\ \ \ \ \ \ \ \ \ l2\_reg\_strength,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00208}00208\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00209}00209\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00210}00210\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00211}00211\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00212}00212\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00213}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a06274e2990bfbb419c29ddc3ccbcb6fb}{00213}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a06274e2990bfbb419c29ddc3ccbcb6fb}{test\_glm\_regression}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00214}00214\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00215}00215\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00216}00216\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00217}00217\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00218}00218\ \ \ \ \ model,\ X,\ y,\ \_,\ coef\_with\_intercept,\ coef\_without\_intercept,\ alpha\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00219}00219\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00221}00221\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00223}00223\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00225}00225\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00226}00226\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00227}00227\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00228}00228\ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00229}00229\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00230}00230\ \ \ \ \ \ \ \ \ coef\ =\ coef\_with\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00231}00231\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00232}00232\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00233}00233\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00234}00234\ \ \ \ \ \ \ \ \ coef\ =\ coef\_without\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00235}00235\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00236}00236\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00237}00237\ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00238}00238\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00239}00239\ \ \ \ \ rtol\ =\ 5e-\/5\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 1e-\/9}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00240}00240\ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00241}00241\ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00242}00242\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00243}00243\ \ \ \ \ \textcolor{comment}{\#\ Same\ with\ sample\_weight.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00244}00244\ \ \ \ \ model\ =\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ clone(model).set\_params(**params).fit(X,\ y,\ sample\_weight=np.ones(X.shape[0]))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00246}00246\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00247}00247\ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00248}00248\ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00249}00249\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00250}00250\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00251}00251\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00252}00252\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00253}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a3dbf52b26ccff035431f498891df08e0}{00253}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a3dbf52b26ccff035431f498891df08e0}{test\_glm\_regression\_hstacked\_X}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00254}00254\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution\ on\ hstacked\ data.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00255}00255\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00256}00256\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00257}00257\ \textcolor{stringliteral}{\ \ \ \ Fit\ on\ [X]\ with\ alpha\ is\ the\ same\ as\ fit\ on\ [X,\ X]/2\ with\ alpha/2.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00258}00258\ \textcolor{stringliteral}{\ \ \ \ For\ long\ X,\ [X,\ X]\ is\ still\ a\ long\ but\ singular\ matrix.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00259}00259\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00260}00260\ \ \ \ \ model,\ X,\ y,\ \_,\ coef\_with\_intercept,\ coef\_without\_intercept,\ alpha\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00261}00261\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00262}00262\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00263}00263\ \ \ \ \ \ \ \ \ alpha=alpha\ /\ 2,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00265}00265\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00267}00267\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00268}00268\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00269}00269\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00270}00270\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00271}00271\ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00272}00272\ \ \ \ \ X\ =\ 0.5\ *\ np.concatenate((X,\ X),\ axis=1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00273}00273\ \ \ \ \ \textcolor{keyword}{assert}\ np.linalg.matrix\_rank(X)\ <=\ min(n\_samples,\ n\_features\ -\/\ 1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00274}00274\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00275}00275\ \ \ \ \ \ \ \ \ coef\ =\ coef\_with\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00277}00277\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00278}00278\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00279}00279\ \ \ \ \ \ \ \ \ coef\ =\ coef\_without\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00280}00280\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00281}00281\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00282}00282\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00283}00283\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ Investigate\ if\ the\ ConvergenceWarning\ that\ can\ appear\ in\ some}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00284}00284\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cases\ should\ be\ considered\ a\ bug\ or\ not.\ In\ the\ mean\ time\ we\ don't}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00285}00285\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fail\ when\ the\ assertions\ below\ pass\ irrespective\ of\ the\ presence\ of}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ warning.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}ignore"{}},\ ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00289}00289\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00290}00290\ \ \ \ \ rtol\ =\ 2e-\/4\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 5e-\/9}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00291}00291\ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00292}00292\ \ \ \ \ assert\_allclose(model.coef\_,\ np.r\_[coef,\ coef],\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00293}00293\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00294}00294\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00295}00295\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00296}00296\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00297}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a0230a8f385629306c4eefdf1e9609731}{00297}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a0230a8f385629306c4eefdf1e9609731}{test\_glm\_regression\_vstacked\_X}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00298}00298\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution\ on\ vstacked\ data.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00299}00299\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00300}00300\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00301}00301\ \textcolor{stringliteral}{\ \ \ \ Fit\ on\ [X]\ with\ alpha\ is\ the\ same\ as\ fit\ on\ [X],\ [y]}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00302}00302\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [X],\ [y]\ with\ 1\ *\ alpha.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00303}00303\ \textcolor{stringliteral}{\ \ \ \ It\ is\ the\ same\ alpha\ as\ the\ average\ loss\ stays\ the\ same.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00304}00304\ \textcolor{stringliteral}{\ \ \ \ For\ wide\ X,\ [X',\ X']\ is\ a\ singular\ matrix.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00305}00305\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00306}00306\ \ \ \ \ model,\ X,\ y,\ \_,\ coef\_with\_intercept,\ coef\_without\_intercept,\ alpha\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00307}00307\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00308}00308\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00314}00314\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00315}00315\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00316}00316\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00317}00317\ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00318}00318\ \ \ \ \ X\ =\ np.concatenate((X,\ X),\ axis=0)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00319}00319\ \ \ \ \ \textcolor{keyword}{assert}\ np.linalg.matrix\_rank(X)\ <=\ min(n\_samples,\ n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00320}00320\ \ \ \ \ y\ =\ np.r\_[y,\ y]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00321}00321\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ coef\ =\ coef\_with\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00323}00323\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00325}00325\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ coef\ =\ coef\_without\_intercept}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00328}00328\ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00329}00329\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00330}00330\ \ \ \ \ rtol\ =\ 3e-\/5\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 5e-\/9}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00331}00331\ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00332}00332\ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00333}00333\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00334}00334\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00335}00335\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00336}00336\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00337}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a32650e0b3ce52f01457f9f63e58c8a4a}{00337}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a32650e0b3ce52f01457f9f63e58c8a4a}{test\_glm\_regression\_unpenalized}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00338}00338\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ unpenalized\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00339}00339\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00340}00340\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00341}00341\ \textcolor{stringliteral}{\ \ \ \ Note:\ This\ checks\ the\ minimum\ norm\ solution\ for\ wide\ X,\ i.e.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00342}00342\ \textcolor{stringliteral}{\ \ \ \ n\_samples\ <\ n\_features:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00343}00343\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ ||w||\_2\ subject\ to\ w\ =\ argmin\ deviance(X,\ y,\ w)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00344}00344\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00345}00345\ \ \ \ \ model,\ X,\ y,\ coef,\ \_,\ \_,\ \_\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00346}00346\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00347}00347\ \ \ \ \ alpha\ =\ 0\ \ \textcolor{comment}{\#\ unpenalized}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00348}00348\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00351}00351\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00352}00352\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00353}00353\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00354}00354\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00355}00355\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00356}00356\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00357}00357\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00360}00360\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00361}00361\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00362}00362\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00363}00363\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00364}00364\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00365}00365\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver.startswith(\textcolor{stringliteral}{"{}newton"{}})\ \textcolor{keywordflow}{and}\ n\_samples\ <\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ newton\ solvers\ should\ warn\ and\ automatically\ fallback\ to\ LBFGS}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00367}00367\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ in\ this\ case.\ The\ model\ should\ still\ converge.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=\mbox{\hyperlink{classscipy_1_1linalg_1_1__misc_1_1LinAlgWarning}{scipy.linalg.LinAlgWarning}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00369}00369\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ Investigate\ if\ the\ ConvergenceWarning\ that\ can\ appear\ in\ some}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00370}00370\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cases\ should\ be\ considered\ a\ bug\ or\ not.\ In\ the\ mean\ time\ we\ don't}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00371}00371\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fail\ when\ the\ assertions\ below\ pass\ irrespective\ of\ the\ presence\ of}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00372}00372\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ warning.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00373}00373\ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00374}00374\ \ \ \ \ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00375}00375\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00376}00376\ \ \ \ \ \textcolor{comment}{\#\ FIXME:\ \`{}assert\_allclose(model.coef\_,\ coef)`\ should\ work\ for\ all\ cases\ but\ fails}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00377}00377\ \ \ \ \ \textcolor{comment}{\#\ for\ the\ wide/fat\ case\ with\ n\_features\ >\ n\_samples.\ Most\ current\ GLM\ solvers\ do}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00378}00378\ \ \ \ \ \textcolor{comment}{\#\ NOT\ return\ the\ minimum\ norm\ solution\ with\ fit\_intercept=True.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00379}00379\ \ \ \ \ \textcolor{keywordflow}{if}\ n\_samples\ >\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00380}00380\ \ \ \ \ \ \ \ \ rtol\ =\ 5e-\/5\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 1e-\/7}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00381}00381\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00382}00382\ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00383}00383\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00384}00384\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ As\ it\ is\ an\ underdetermined\ problem,\ prediction\ =\ y.\ The\ following\ shows\ that}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00385}00385\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ get\ a\ solution,\ i.e.\ a\ (non-\/unique)\ minimum\ of\ the\ objective\ function\ ...}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00386}00386\ \ \ \ \ \ \ \ \ rtol\ =\ 5e-\/5}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00387}00387\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00388}00388\ \ \ \ \ \ \ \ \ \ \ \ \ rtol\ =\ 5e-\/4}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00389}00389\ \ \ \ \ \ \ \ \ assert\_allclose(model.predict(X),\ y,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00390}00390\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00391}00391\ \ \ \ \ \ \ \ \ norm\_solution\ =\ np.linalg.norm(np.r\_[intercept,\ coef])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00392}00392\ \ \ \ \ \ \ \ \ norm\_model\ =\ np.linalg.norm(np.r\_[model.intercept\_,\ model.coef\_])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00393}00393\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00394}00394\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ This\ solver\ shows\ random\ behaviour.\ Sometimes\ it\ finds\ solutions}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00395}00395\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ with\ norm\_model\ <=\ norm\_solution!\ So\ we\ check\ conditionally.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00396}00396\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ norm\_model\ <\ (1\ +\ 1e-\/12)\ *\ norm\_solution:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00397}00397\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00398}00398\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00399}00399\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{and}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00400}00400\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ But\ it\ is\ not\ the\ minimum\ norm\ solution.\ Otherwise\ the\ norms\ would\ be}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00401}00401\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ equal.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00402}00402\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ norm\_model\ >\ (1\ +\ 1e-\/12)\ *\ norm\_solution}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00403}00403\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00404}00404\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ https://github.com/scikit-\/learn/scikit-\/learn/issues/23670.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00405}00405\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note:\ Even\ adding\ a\ tiny\ penalty\ does\ not\ give\ the\ minimal\ norm\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ We\ could\ have\ naively\ expected\ LBFGS\ to\ find\ the\ minimal\ norm}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ solution\ by\ adding\ a\ very\ small\ penalty.\ Even\ that\ fails\ for\ a\ reason\ we}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ do\ not\ properly\ understand\ at\ this\ point.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ When\ \`{}fit\_intercept=False`,\ LBFGS\ naturally\ converges\ to\ the\ minimum\ norm}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ solution\ on\ this\ problem.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ Do\ we\ have\ any\ theoretical\ guarantees\ why\ this\ should\ be\ the\ case?}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00414}00414\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00415}00415\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00416}00416\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00417}00417\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00418}00418\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00419}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a4b3b2a2da2f67e8f44cd981350127cd8}{00419}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a4b3b2a2da2f67e8f44cd981350127cd8}{test\_glm\_regression\_unpenalized\_hstacked\_X}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00420}00420\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ unpenalized\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00421}00421\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00422}00422\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00423}00423\ \textcolor{stringliteral}{\ \ \ \ GLM\ fit\ on\ [X]\ is\ the\ same\ as\ fit\ on\ [X,\ X]/2.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00424}00424\ \textcolor{stringliteral}{\ \ \ \ For\ long\ X,\ [X,\ X]\ is\ a\ singular\ matrix\ and\ we\ check\ against\ the\ minimum\ norm}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00425}00425\ \textcolor{stringliteral}{\ \ \ \ solution:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00426}00426\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ ||w||\_2\ subject\ to\ w\ =\ argmin\ deviance(X,\ y,\ w)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00427}00427\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00428}00428\ \ \ \ \ model,\ X,\ y,\ coef,\ \_,\ \_,\ \_\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00429}00429\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00430}00430\ \ \ \ \ alpha\ =\ 0\ \ \textcolor{comment}{\#\ unpenalized}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00431}00431\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00435}00435\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00436}00436\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00437}00437\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00438}00438\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00439}00439\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00440}00440\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00441}00441\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00443}00443\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\_samples\ >\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ \ \ \ \ X\ =\ 0.5\ *\ np.concatenate((X,\ X),\ axis=1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00446}00446\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ To\ know\ the\ minimum\ norm\ solution,\ we\ keep\ one\ intercept\ column\ and\ do}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00448}00448\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ not\ divide\ by\ 2.\ Later\ on,\ we\ must\ take\ special\ care.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ \ \ \ \ X\ =\ np.c\_[X[:,\ :-\/1],\ X[:,\ :-\/1],\ X[:,\ -\/1]]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00450}00450\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ X\ =\ 0.5\ *\ np.concatenate((X,\ X),\ axis=1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00453}00453\ \ \ \ \ \textcolor{keyword}{assert}\ np.linalg.matrix\_rank(X)\ <=\ min(n\_samples,\ n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00454}00454\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00455}00455\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00456}00456\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver.startswith(\textcolor{stringliteral}{"{}newton"{}}):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00457}00457\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ newton\ solvers\ should\ warn\ and\ automatically\ fallback\ to\ LBFGS}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00458}00458\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ in\ this\ case.\ The\ model\ should\ still\ converge.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00459}00459\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=\mbox{\hyperlink{classscipy_1_1linalg_1_1__misc_1_1LinAlgWarning}{scipy.linalg.LinAlgWarning}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00460}00460\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ Investigate\ if\ the\ ConvergenceWarning\ that\ can\ appear\ in\ some}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00461}00461\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cases\ should\ be\ considered\ a\ bug\ or\ not.\ In\ the\ mean\ time\ we\ don't}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00462}00462\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fail\ when\ the\ assertions\ below\ pass\ irrespective\ of\ the\ presence\ of}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00463}00463\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ warning.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00464}00464\ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00465}00465\ \ \ \ \ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00466}00466\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00467}00467\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept\ \textcolor{keywordflow}{and}\ n\_samples\ <\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00468}00468\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Here\ we\ take\ special\ care.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00469}00469\ \ \ \ \ \ \ \ \ model\_intercept\ =\ 2\ *\ model.intercept\_}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00470}00470\ \ \ \ \ \ \ \ \ model\_coef\ =\ 2\ *\ model.coef\_[:-\/1]\ \ \textcolor{comment}{\#\ exclude\ the\ other\ intercept\ term.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00471}00471\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ For\ minimum\ norm\ solution,\ we\ would\ have}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00472}00472\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ assert\ model.intercept\_\ ==\ pytest.approx(model.coef\_[-\/1])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00473}00473\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00474}00474\ \ \ \ \ \ \ \ \ model\_intercept\ =\ model.intercept\_}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00475}00475\ \ \ \ \ \ \ \ \ model\_coef\ =\ model.coef\_}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00476}00476\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00477}00477\ \ \ \ \ \textcolor{keywordflow}{if}\ n\_samples\ >\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00478}00478\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model\_intercept\ ==\ pytest.approx(intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00479}00479\ \ \ \ \ \ \ \ \ rtol\ =\ 1e-\/4}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00480}00480\ \ \ \ \ \ \ \ \ assert\_allclose(model\_coef,\ np.r\_[coef,\ coef],\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00481}00481\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00482}00482\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ As\ it\ is\ an\ underdetermined\ problem,\ prediction\ =\ y.\ The\ following\ shows\ that}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00483}00483\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ get\ a\ solution,\ i.e.\ a\ (non-\/unique)\ minimum\ of\ the\ objective\ function\ ...}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ rtol\ =\ 1e-\/6\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 5e-\/6}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ assert\_allclose(model.predict(X),\ y,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00486}00486\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{and}\ fit\_intercept)\ \textcolor{keywordflow}{or}\ solver\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00487}00487\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Same\ as\ in\ test\_glm\_regression\_unpenalized.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ But\ it\ is\ not\ the\ minimum\ norm\ solution.\ Otherwise\ the\ norms\ would\ be}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00489}00489\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ equal.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00490}00490\ \ \ \ \ \ \ \ \ \ \ \ \ norm\_solution\ =\ np.linalg.norm(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00491}00491\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 0.5\ *\ np.r\_[intercept,\ intercept,\ coef,\ coef]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00492}00492\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00493}00493\ \ \ \ \ \ \ \ \ \ \ \ \ norm\_model\ =\ np.linalg.norm(np.r\_[model.intercept\_,\ model.coef\_])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00494}00494\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ norm\_model\ >\ (1\ +\ 1e-\/12)\ *\ norm\_solution}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00495}00495\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ For\ minimum\ norm\ solution,\ we\ would\ have}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00496}00496\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ assert\ model.intercept\_\ ==\ pytest.approx(model.coef\_[-\/1])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model\_intercept\ ==\ pytest.approx(intercept,\ rel=5e-\/6)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(model\_coef,\ np.r\_[coef,\ coef],\ rtol=1e-\/4)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00500}00500\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00501}00501\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00502}00502\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00503}00503\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00504}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a37f27cabeb181f1ec4a44ae8721ce907}{00504}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a37f27cabeb181f1ec4a44ae8721ce907}{test\_glm\_regression\_unpenalized\_vstacked\_X}}(solver,\ fit\_intercept,\ glm\_dataset):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00505}00505\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ unpenalized\ GLM\ converges\ for\ all\ solvers\ to\ correct\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00506}00506\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00507}00507\ \textcolor{stringliteral}{\ \ \ \ We\ work\ with\ a\ simple\ constructed\ data\ set\ with\ known\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00508}00508\ \textcolor{stringliteral}{\ \ \ \ GLM\ fit\ on\ [X]\ is\ the\ same\ as\ fit\ on\ [X],\ [y]}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00509}00509\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [X],\ [y].}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00510}00510\ \textcolor{stringliteral}{\ \ \ \ For\ wide\ X,\ [X',\ X']\ is\ a\ singular\ matrix\ and\ we\ check\ against\ the\ minimum\ norm}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00511}00511\ \textcolor{stringliteral}{\ \ \ \ solution:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00512}00512\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\ ||w||\_2\ subject\ to\ w\ =\ argmin\ deviance(X,\ y,\ w)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00513}00513\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00514}00514\ \ \ \ \ model,\ X,\ y,\ coef,\ \_,\ \_,\ \_\ =\ glm\_dataset}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00515}00515\ \ \ \ \ n\_samples,\ n\_features\ =\ X.shape}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00516}00516\ \ \ \ \ alpha\ =\ 0\ \ \textcolor{comment}{\#\ unpenalized}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00517}00517\ \ \ \ \ params\ =\ dict(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00518}00518\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00519}00519\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00520}00520\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00521}00521\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00522}00522\ \ \ \ \ \ \ \ \ max\_iter=1000,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00523}00523\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00524}00524\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00525}00525\ \ \ \ \ model\ =\ clone(model).set\_params(**params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00526}00526\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00527}00527\ \ \ \ \ \ \ \ \ X\ =\ X[:,\ :-\/1]\ \ \textcolor{comment}{\#\ remove\ intercept}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00528}00528\ \ \ \ \ \ \ \ \ intercept\ =\ coef[-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00529}00529\ \ \ \ \ \ \ \ \ coef\ =\ coef[:-\/1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00530}00530\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00531}00531\ \ \ \ \ \ \ \ \ intercept\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00532}00532\ \ \ \ \ X\ =\ np.concatenate((X,\ X),\ axis=0)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00533}00533\ \ \ \ \ \textcolor{keyword}{assert}\ np.linalg.matrix\_rank(X)\ <=\ min(n\_samples,\ n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00534}00534\ \ \ \ \ y\ =\ np.r\_[y,\ y]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00535}00535\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00536}00536\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00537}00537\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver.startswith(\textcolor{stringliteral}{"{}newton"{}})\ \textcolor{keywordflow}{and}\ n\_samples\ <\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00538}00538\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ newton\ solvers\ should\ warn\ and\ automatically\ fallback\ to\ LBFGS}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ in\ this\ case.\ The\ model\ should\ still\ converge.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00540}00540\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=\mbox{\hyperlink{classscipy_1_1linalg_1_1__misc_1_1LinAlgWarning}{scipy.linalg.LinAlgWarning}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00541}00541\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ Investigate\ if\ the\ ConvergenceWarning\ that\ can\ appear\ in\ some}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00542}00542\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ cases\ should\ be\ considered\ a\ bug\ or\ not.\ In\ the\ mean\ time\ we\ don't}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00543}00543\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fail\ when\ the\ assertions\ below\ pass\ irrespective\ of\ the\ presence\ of}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00544}00544\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ warning.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00545}00545\ \ \ \ \ \ \ \ \ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00546}00546\ \ \ \ \ \ \ \ \ model.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00547}00547\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00548}00548\ \ \ \ \ \textcolor{keywordflow}{if}\ n\_samples\ >\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00549}00549\ \ \ \ \ \ \ \ \ rtol\ =\ 5e-\/5\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 1e-\/6}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00550}00550\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00551}00551\ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00552}00552\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00553}00553\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ As\ it\ is\ an\ underdetermined\ problem,\ prediction\ =\ y.\ The\ following\ shows\ that}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00554}00554\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ get\ a\ solution,\ i.e.\ a\ (non-\/unique)\ minimum\ of\ the\ objective\ function\ ...}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00555}00555\ \ \ \ \ \ \ \ \ rtol\ =\ 1e-\/6\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 5e-\/6}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00556}00556\ \ \ \ \ \ \ \ \ assert\_allclose(model.predict(X),\ y,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00557}00557\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00558}00558\ \ \ \ \ \ \ \ \ norm\_solution\ =\ np.linalg.norm(np.r\_[intercept,\ coef])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00559}00559\ \ \ \ \ \ \ \ \ norm\_model\ =\ np.linalg.norm(np.r\_[model.intercept\_,\ model.coef\_])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00560}00560\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00561}00561\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX:\ This\ solver\ shows\ random\ behaviour.\ Sometimes\ it\ finds\ solutions}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00562}00562\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ with\ norm\_model\ <=\ norm\_solution!\ So\ we\ check\ conditionally.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00563}00563\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ (norm\_model\ >\ (1\ +\ 1e-\/12)\ *\ norm\_solution):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=1e-\/4)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{and}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00567}00567\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Same\ as\ in\ test\_glm\_regression\_unpenalized.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00568}00568\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ But\ it\ is\ not\ the\ minimum\ norm\ solution.\ Otherwise\ the\ norms\ would\ be}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00569}00569\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ equal.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00570}00570\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ norm\_model\ >\ (1\ +\ 1e-\/12)\ *\ norm\_solution}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00571}00571\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00572}00572\ \ \ \ \ \ \ \ \ \ \ \ \ rtol\ =\ 1e-\/5\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}\ \textcolor{keywordflow}{else}\ 1e-\/4}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00573}00573\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ model.intercept\_\ ==\ pytest.approx(intercept,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00574}00574\ \ \ \ \ \ \ \ \ \ \ \ \ assert\_allclose(model.coef\_,\ coef,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00575}00575\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00576}00576\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00577}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a57765c5e677bc4233365549889ec960c}{00577}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a57765c5e677bc4233365549889ec960c}{test\_sample\_weights\_validation}}():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00578}00578\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ the\ raised\ errors\ in\ the\ validation\ of\ sample\_weight."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00579}00579\ \ \ \ \ \textcolor{comment}{\#\ scalar\ value\ but\ not\ positive}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00580}00580\ \ \ \ \ X\ =\ [[1]]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00581}00581\ \ \ \ \ y\ =\ [1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00582}00582\ \ \ \ \ weights\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00583}00583\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00584}00584\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00585}00585\ \ \ \ \ \textcolor{comment}{\#\ Positive\ weights\ are\ accepted}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00586}00586\ \ \ \ \ glm.fit(X,\ y,\ sample\_weight=1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00587}00587\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00588}00588\ \ \ \ \ \textcolor{comment}{\#\ 2d\ array}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00589}00589\ \ \ \ \ weights\ =\ [[0]]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00590}00590\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=\textcolor{stringliteral}{"{}must\ be\ 1D\ array\ or\ scalar"{}}):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00591}00591\ \ \ \ \ \ \ \ \ glm.fit(X,\ y,\ weights)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00592}00592\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00593}00593\ \ \ \ \ \textcolor{comment}{\#\ 1d\ but\ wrong\ length}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00594}00594\ \ \ \ \ weights\ =\ [1,\ 0]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00595}00595\ \ \ \ \ msg\ =\ \textcolor{stringliteral}{r"{}sample\_weight.shape\ ==\ \(\backslash\)(2,\(\backslash\)),\ expected\ \(\backslash\)(1,\(\backslash\))!"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00596}00596\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=msg):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00597}00597\ \ \ \ \ \ \ \ \ glm.fit(X,\ y,\ weights)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00598}00598\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00599}00599\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00600}00600\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00601}00601\ \ \ \ \ \textcolor{stringliteral}{"{}glm"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00602}00602\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00603}00603\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=3),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00604}00604\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00605}00605\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{GammaRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00606}00606\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=1.5),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00607}00607\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00608}00608\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00609}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_aa256fb9d4312ac2dd0f356dbf980897b}{00609}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_aa256fb9d4312ac2dd0f356dbf980897b}{test\_glm\_wrong\_y\_range}}(glm):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00610}00610\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00611}00611\ \textcolor{stringliteral}{\ \ \ \ Test\ that\ fitting\ a\ GLM\ model\ raises\ a\ ValueError\ when\ \`{}y\`{}\ contains}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00612}00612\ \textcolor{stringliteral}{\ \ \ \ values\ outside\ the\ valid\ range\ for\ the\ given\ distribution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00613}00613\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00614}00614\ \textcolor{stringliteral}{\ \ \ \ Generalized\ Linear\ Models\ (GLMs)\ with\ certain\ distributions,\ such\ as}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00615}00615\ \textcolor{stringliteral}{\ \ \ \ Poisson,\ Gamma,\ and\ Tweedie\ (with\ power\ >\ 1),\ require\ \`{}y\`{}\ to\ be}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00616}00616\ \textcolor{stringliteral}{\ \ \ \ non-\/negative.\ This\ test\ ensures\ that\ passing\ a\ \`{}y\`{}\ array\ containing}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00617}00617\ \textcolor{stringliteral}{\ \ \ \ negative\ values\ triggers\ the\ expected\ ValueError\ with\ the\ correct\ message.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00618}00618\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00619}00619\ \ \ \ \ y\ =\ np.array([-\/1,\ 2])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00620}00620\ \ \ \ \ X\ =\ np.array([[1],\ [1]])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00621}00621\ \ \ \ \ msg\ =\ \textcolor{stringliteral}{r"{}Some\ value\(\backslash\)(s\(\backslash\))\ of\ y\ are\ out\ of\ the\ valid\ range\ of\ the\ loss"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00622}00622\ \ \ \ \ \textcolor{keyword}{with}\ pytest.raises(ValueError,\ match=msg):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00623}00623\ \ \ \ \ \ \ \ \ glm.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00624}00624\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00625}00625\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00626}00626\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00627}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a0364e08f009b7216af3645ae0ee47ab6}{00627}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a0364e08f009b7216af3645ae0ee47ab6}{test\_glm\_identity\_regression}}(fit\_intercept):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00628}00628\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ GLM\ regression\ with\ identity\ link\ on\ a\ simple\ dataset."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00629}00629\ \ \ \ \ coef\ =\ [1.0,\ 2.0]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00630}00630\ \ \ \ \ X\ =\ np.array([[1,\ 1,\ 1,\ 1,\ 1],\ [0,\ 1,\ 2,\ 3,\ 4]]).T}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00631}00631\ \ \ \ \ y\ =\ np.dot(X,\ coef)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00632}00632\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00633}00633\ \ \ \ \ \ \ \ \ alpha=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00634}00634\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00635}00635\ \ \ \ \ \ \ \ \ tol=1e-\/12,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00636}00636\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00637}00637\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00638}00638\ \ \ \ \ \ \ \ \ glm.fit(X[:,\ 1:],\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00639}00639\ \ \ \ \ \ \ \ \ assert\_allclose(glm.coef\_,\ coef[1:])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00640}00640\ \ \ \ \ \ \ \ \ assert\_allclose(glm.intercept\_,\ coef[0])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00641}00641\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00642}00642\ \ \ \ \ \ \ \ \ glm.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00643}00643\ \ \ \ \ \ \ \ \ assert\_allclose(glm.coef\_,\ coef)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00644}00644\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00645}00645\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00646}00646\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [False,\ True])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00647}00647\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}alpha"{},\ [0.0,\ 1.0])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00648}00648\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00649}00649\ \ \ \ \ \textcolor{stringliteral}{"{}GLMEstimator"{}},\ [\_GeneralizedLinearRegressor,\ PoissonRegressor,\ GammaRegressor]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00650}00650\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00651}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a697b9b016afe5bc089525fa66931dfb1}{00651}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a697b9b016afe5bc089525fa66931dfb1}{test\_glm\_sample\_weight\_consistency}}(fit\_intercept,\ alpha,\ GLMEstimator):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00652}00652\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ the\ impact\ of\ sample\_weight\ is\ consistent"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00653}00653\ \ \ \ \ rng\ =\ np.random.RandomState(0)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00654}00654\ \ \ \ \ n\_samples,\ n\_features\ =\ 10,\ 5}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00655}00655\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00656}00656\ \ \ \ \ X\ =\ rng.rand(n\_samples,\ n\_features)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00657}00657\ \ \ \ \ y\ =\ rng.rand(n\_samples)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00658}00658\ \ \ \ \ glm\_params\ =\ dict(alpha=alpha,\ fit\_intercept=fit\_intercept)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00659}00659\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00660}00660\ \ \ \ \ glm\ =\ GLMEstimator(**glm\_params).fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00661}00661\ \ \ \ \ coef\ =\ glm.coef\_.copy()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00662}00662\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00663}00663\ \ \ \ \ \textcolor{comment}{\#\ sample\_weight=np.ones(..)\ should\ be\ equivalent\ to\ sample\_weight=None}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00664}00664\ \ \ \ \ sample\_weight\ =\ np.ones(y.shape)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00665}00665\ \ \ \ \ glm.fit(X,\ y,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00666}00666\ \ \ \ \ assert\_allclose(glm.coef\_,\ coef)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00667}00667\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00668}00668\ \ \ \ \ \textcolor{comment}{\#\ sample\_weight\ are\ normalized\ to\ 1\ so,\ scaling\ them\ has\ no\ effect}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00669}00669\ \ \ \ \ sample\_weight\ =\ 2\ *\ np.ones(y.shape)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00670}00670\ \ \ \ \ glm.fit(X,\ y,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00671}00671\ \ \ \ \ assert\_allclose(glm.coef\_,\ coef)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00672}00672\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00673}00673\ \ \ \ \ \textcolor{comment}{\#\ setting\ one\ element\ of\ sample\_weight\ to\ 0\ is\ equivalent\ to\ removing}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00674}00674\ \ \ \ \ \textcolor{comment}{\#\ the\ corresponding\ sample}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00675}00675\ \ \ \ \ sample\_weight\ =\ np.ones(y.shape)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00676}00676\ \ \ \ \ sample\_weight[-\/1]\ =\ 0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00677}00677\ \ \ \ \ glm.fit(X,\ y,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00678}00678\ \ \ \ \ coef1\ =\ glm.coef\_.copy()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00679}00679\ \ \ \ \ glm.fit(X[:-\/1],\ y[:-\/1])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00680}00680\ \ \ \ \ assert\_allclose(glm.coef\_,\ coef1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00681}00681\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00682}00682\ \ \ \ \ \textcolor{comment}{\#\ check\ that\ multiplying\ sample\_weight\ by\ 2\ is\ equivalent}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00683}00683\ \ \ \ \ \textcolor{comment}{\#\ to\ repeating\ corresponding\ samples\ twice}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00684}00684\ \ \ \ \ X2\ =\ np.concatenate([X,\ X[:\ n\_samples\ //\ 2]],\ axis=0)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00685}00685\ \ \ \ \ y2\ =\ np.concatenate([y,\ y[:\ n\_samples\ //\ 2]])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00686}00686\ \ \ \ \ sample\_weight\_1\ =\ np.ones(len(y))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00687}00687\ \ \ \ \ sample\_weight\_1[:\ n\_samples\ //\ 2]\ =\ 2}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00688}00688\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00689}00689\ \ \ \ \ glm1\ =\ GLMEstimator(**glm\_params).fit(X,\ y,\ sample\_weight=sample\_weight\_1)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00690}00690\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00691}00691\ \ \ \ \ glm2\ =\ GLMEstimator(**glm\_params).fit(X2,\ y2,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00692}00692\ \ \ \ \ assert\_allclose(glm1.coef\_,\ glm2.coef\_)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00693}00693\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00694}00694\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00695}00695\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00696}00696\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00697}00697\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00698}00698\ \ \ \ \ \textcolor{stringliteral}{"{}estimator"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00699}00699\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00700}00700\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00701}00701\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{GammaRegressor}}(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00702}00702\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=3.0),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00703}00703\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=0,\ link=\textcolor{stringliteral}{"{}log"{}}),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00704}00704\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=1.5),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00705}00705\ \ \ \ \ \ \ \ \ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=4.5),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00706}00706\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00707}00707\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00708}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a7384ddd24bbc50126dfb8d6329e91688}{00708}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a7384ddd24bbc50126dfb8d6329e91688}{test\_glm\_log\_regression}}(solver,\ fit\_intercept,\ estimator):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00709}00709\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ GLM\ regression\ with\ log\ link\ on\ a\ simple\ dataset."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00710}00710\ \ \ \ \ coef\ =\ [0.2,\ -\/0.1]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00711}00711\ \ \ \ \ X\ =\ np.array([[0,\ 1,\ 2,\ 3,\ 4],\ [1,\ 1,\ 1,\ 1,\ 1]]).T}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00712}00712\ \ \ \ \ y\ =\ np.exp(np.dot(X,\ coef))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00713}00713\ \ \ \ \ glm\ =\ clone(estimator).set\_params(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ alpha=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00717}00717\ \ \ \ \ \ \ \ \ tol=1e-\/8,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00718}00718\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00719}00719\ \ \ \ \ \textcolor{keywordflow}{if}\ fit\_intercept:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00720}00720\ \ \ \ \ \ \ \ \ res\ =\ glm.fit(X[:,\ :-\/1],\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ assert\_allclose(res.coef\_,\ coef[:-\/1],\ rtol=1e-\/6)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ assert\_allclose(res.intercept\_,\ coef[-\/1],\ rtol=1e-\/6)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00723}00723\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00724}00724\ \ \ \ \ \ \ \ \ res\ =\ glm.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ assert\_allclose(res.coef\_,\ coef,\ rtol=2e-\/6)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00726}00726\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00727}00727\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00728}00728\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ SOLVERS)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00729}00729\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00730}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a07094df74368adbd794a9d2a5104afbe}{00730}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a07094df74368adbd794a9d2a5104afbe}{test\_warm\_start}}(solver,\ fit\_intercept,\ global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00731}00731\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00732}00732\ \textcolor{stringliteral}{\ \ \ \ Test\ that\ \`{}warm\_start=True\`{}\ enables\ incremental\ fitting\ in\ PoissonRegressor.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00733}00733\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00734}00734\ \textcolor{stringliteral}{\ \ \ \ This\ test\ verifies\ that\ when\ using\ \`{}warm\_start=True\`{},\ the\ model\ continues}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00735}00735\ \textcolor{stringliteral}{\ \ \ \ optimizing\ from\ previous\ coefficients\ instead\ of\ restarting\ from\ scratch.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00736}00736\ \textcolor{stringliteral}{\ \ \ \ It\ ensures\ that\ after\ an\ initial\ fit\ with\ \`{}max\_iter=1\`{},\ the\ model\ has\ a}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00737}00737\ \textcolor{stringliteral}{\ \ \ \ higher\ objective\ function\ value\ (indicating\ incomplete\ optimization).}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00738}00738\ \textcolor{stringliteral}{\ \ \ \ The\ test\ then\ checks\ whether\ allowing\ additional\ iterations\ enables}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00739}00739\ \textcolor{stringliteral}{\ \ \ \ convergence\ to\ a\ solution\ comparable\ to\ a\ fresh\ training\ run\ (\`{}warm\_start=False\`{}).}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00740}00740\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00741}00741\ \ \ \ \ n\_samples,\ n\_features\ =\ 100,\ 10}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00742}00742\ \ \ \ \ X,\ y\ =\ make\_regression(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00743}00743\ \ \ \ \ \ \ \ \ n\_samples=n\_samples,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00744}00744\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00745}00745\ \ \ \ \ \ \ \ \ n\_informative=n\_features\ -\/\ 2,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00746}00746\ \ \ \ \ \ \ \ \ bias=fit\_intercept\ *\ 1.0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00747}00747\ \ \ \ \ \ \ \ \ noise=1.0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00748}00748\ \ \ \ \ \ \ \ \ random\_state=global\_random\_seed,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00749}00749\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00750}00750\ \ \ \ \ y\ =\ np.abs(y)\ \ \textcolor{comment}{\#\ Poisson\ requires\ non-\/negative\ targets.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00751}00751\ \ \ \ \ alpha\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00752}00752\ \ \ \ \ params\ =\ \{}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00753}00753\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}solver"{}}:\ solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00754}00754\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}fit\_intercept"{}}:\ fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00755}00755\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}tol"{}}:\ 1e-\/10,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00756}00756\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00757}00757\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00758}00758\ \ \ \ \ glm1\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(warm\_start=\textcolor{keyword}{False},\ max\_iter=1000,\ alpha=alpha,\ **params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00759}00759\ \ \ \ \ glm1.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00760}00760\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00761}00761\ \ \ \ \ glm2\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(warm\_start=\textcolor{keyword}{True},\ max\_iter=1,\ alpha=alpha,\ **params)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00762}00762\ \ \ \ \ \textcolor{comment}{\#\ As\ we\ intentionally\ set\ max\_iter=1\ such\ that\ the\ solver\ should\ raise\ a}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00763}00763\ \ \ \ \ \textcolor{comment}{\#\ ConvergenceWarning.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00764}00764\ \ \ \ \ \textcolor{keyword}{with}\ pytest.warns(ConvergenceWarning):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00765}00765\ \ \ \ \ \ \ \ \ glm2.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00766}00766\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00767}00767\ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00768}00768\ \ \ \ \ \ \ \ \ base\_loss=glm1.\_get\_loss(),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00769}00769\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00770}00770\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00771}00771\ \ \ \ \ sw\ =\ np.full\_like(y,\ fill\_value=1\ /\ n\_samples)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00772}00772\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00773}00773\ \ \ \ \ objective\_glm1\ =\ linear\_loss.loss(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00774}00774\ \ \ \ \ \ \ \ \ coef=np.r\_[glm1.coef\_,\ glm1.intercept\_]\ \textcolor{keywordflow}{if}\ fit\_intercept\ \textcolor{keywordflow}{else}\ glm1.coef\_,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00775}00775\ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00776}00776\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00777}00777\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00778}00778\ \ \ \ \ \ \ \ \ l2\_reg\_strength=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00779}00779\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00780}00780\ \ \ \ \ objective\_glm2\ =\ linear\_loss.loss(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00781}00781\ \ \ \ \ \ \ \ \ coef=np.r\_[glm2.coef\_,\ glm2.intercept\_]\ \textcolor{keywordflow}{if}\ fit\_intercept\ \textcolor{keywordflow}{else}\ glm2.coef\_,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00782}00782\ \ \ \ \ \ \ \ \ X=X,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00783}00783\ \ \ \ \ \ \ \ \ y=y,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00784}00784\ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00785}00785\ \ \ \ \ \ \ \ \ l2\_reg\_strength=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00786}00786\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00787}00787\ \ \ \ \ \textcolor{keyword}{assert}\ objective\_glm1\ <\ objective\_glm2}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00788}00788\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00789}00789\ \ \ \ \ glm2.set\_params(max\_iter=1000)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00790}00790\ \ \ \ \ glm2.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00791}00791\ \ \ \ \ \textcolor{comment}{\#\ The\ two\ models\ are\ not\ exactly\ identical\ since\ the\ lbfgs\ solver}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00792}00792\ \ \ \ \ \textcolor{comment}{\#\ computes\ the\ approximate\ hessian\ from\ previous\ iterations,\ which}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00793}00793\ \ \ \ \ \textcolor{comment}{\#\ will\ not\ be\ strictly\ identical\ in\ the\ case\ of\ a\ warm\ start.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00794}00794\ \ \ \ \ rtol\ =\ 2e-\/4\ \textcolor{keywordflow}{if}\ solver\ ==\ \textcolor{stringliteral}{"{}lbfgs"{}}\ \textcolor{keywordflow}{else}\ 1e-\/9}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00795}00795\ \ \ \ \ assert\_allclose(glm1.coef\_,\ glm2.coef\_,\ rtol=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00796}00796\ \ \ \ \ assert\_allclose(glm1.score(X,\ y),\ glm2.score(X,\ y),\ rtol=1e-\/5)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00797}00797\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00798}00798\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00799}00799\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}n\_samples,\ n\_features"{},\ [(100,\ 10)},\ (10,\ 100)])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00800}00800\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}fit\_intercept"{},\ [True,\ False])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00801}00801\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}sample\_weight"{},\ [None,\ True])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00802}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_af5968893264979b2095562d244fbf456}{00802}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_af5968893264979b2095562d244fbf456}{test\_normal\_ridge\_comparison}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00803}00803\ \ \ \ \ n\_samples,\ n\_features,\ fit\_intercept,\ sample\_weight,\ request}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00804}00804\ ):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00805}00805\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compare\ with\ Ridge\ regression\ for\ Normal\ distributions."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00806}00806\ \ \ \ \ test\_size\ =\ 10}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00807}00807\ \ \ \ \ X,\ y\ =\ make\_regression(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00808}00808\ \ \ \ \ \ \ \ \ n\_samples=n\_samples\ +\ test\_size,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00809}00809\ \ \ \ \ \ \ \ \ n\_features=n\_features,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00810}00810\ \ \ \ \ \ \ \ \ n\_informative=n\_features\ -\/\ 2,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00811}00811\ \ \ \ \ \ \ \ \ noise=0.5,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00812}00812\ \ \ \ \ \ \ \ \ random\_state=42,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00813}00813\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00814}00814\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00815}00815\ \ \ \ \ \textcolor{keywordflow}{if}\ n\_samples\ >\ n\_features:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00816}00816\ \ \ \ \ \ \ \ \ ridge\_params\ =\ \{\textcolor{stringliteral}{"{}solver"{}}:\ \textcolor{stringliteral}{"{}svd"{}}\}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00817}00817\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00818}00818\ \ \ \ \ \ \ \ \ ridge\_params\ =\ \{\textcolor{stringliteral}{"{}solver"{}}:\ \textcolor{stringliteral}{"{}saga"{}},\ \textcolor{stringliteral}{"{}max\_iter"{}}:\ 1000000,\ \textcolor{stringliteral}{"{}tol"{}}:\ 1e-\/7\}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00819}00819\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00820}00820\ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00821}00821\ \ \ \ \ \ \ \ \ X\_train,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00822}00822\ \ \ \ \ \ \ \ \ X\_test,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ y\_train,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00824}00824\ \ \ \ \ \ \ \ \ y\_test,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00825}00825\ \ \ \ \ )\ =\ train\_test\_split(X,\ y,\ test\_size=test\_size,\ random\_state=0)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00826}00826\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00827}00827\ \ \ \ \ alpha\ =\ 1.0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00828}00828\ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00829}00829\ \ \ \ \ \ \ \ \ sw\_train\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00830}00830\ \ \ \ \ \ \ \ \ alpha\_ridge\ =\ alpha\ *\ n\_samples}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00831}00831\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00832}00832\ \ \ \ \ \ \ \ \ sw\_train\ =\ np.random.RandomState(0).rand(len(y\_train))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00833}00833\ \ \ \ \ \ \ \ \ alpha\_ridge\ =\ alpha\ *\ sw\_train.sum()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00834}00834\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00835}00835\ \ \ \ \ \textcolor{comment}{\#\ GLM\ has\ 1/(2*n)\ *\ Loss\ +\ 1/2*L2,\ Ridge\ has\ Loss\ +\ L2}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00836}00836\ \ \ \ \ ridge\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__ridge_1_1Ridge}{Ridge}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00837}00837\ \ \ \ \ \ \ \ \ alpha=alpha\_ridge,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00838}00838\ \ \ \ \ \ \ \ \ random\_state=42,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00839}00839\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00840}00840\ \ \ \ \ \ \ \ \ **ridge\_params,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00841}00841\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00842}00842\ \ \ \ \ ridge.fit(X\_train,\ y\_train,\ sample\_weight=sw\_train)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00843}00843\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00844}00844\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00845}00845\ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00846}00846\ \ \ \ \ \ \ \ \ fit\_intercept=fit\_intercept,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00847}00847\ \ \ \ \ \ \ \ \ max\_iter=300,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00848}00848\ \ \ \ \ \ \ \ \ tol=1e-\/5,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00849}00849\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00850}00850\ \ \ \ \ glm.fit(X\_train,\ y\_train,\ sample\_weight=sw\_train)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00851}00851\ \ \ \ \ \textcolor{keyword}{assert}\ glm.coef\_.shape\ ==\ (X.shape[1],)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00852}00852\ \ \ \ \ assert\_allclose(glm.coef\_,\ ridge.coef\_,\ atol=5e-\/5)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00853}00853\ \ \ \ \ assert\_allclose(glm.intercept\_,\ ridge.intercept\_,\ rtol=1e-\/5)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00854}00854\ \ \ \ \ assert\_allclose(glm.predict(X\_train),\ ridge.predict(X\_train),\ rtol=2e-\/4)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00855}00855\ \ \ \ \ assert\_allclose(glm.predict(X\_test),\ ridge.predict(X\_test),\ rtol=2e-\/4)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00856}00856\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00857}00857\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00858}00858\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}solver"{},\ ["{}lbfgs"{},\ "{}newton-\/cholesky"{}])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00859}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ac3cea64ca818d55393a0e25c5242731f}{00859}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ac3cea64ca818d55393a0e25c5242731f}{test\_poisson\_glmnet}}(solver):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00860}00860\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compare\ Poisson\ regression\ with\ L2\ regularization\ and\ LogLink\ to\ glmnet"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00861}00861\ \ \ \ \ \textcolor{comment}{\#\ library("{}glmnet"{})}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00862}00862\ \ \ \ \ \textcolor{comment}{\#\ options(digits=10)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00863}00863\ \ \ \ \ \textcolor{comment}{\#\ df\ <-\/\ data.frame(a=c(-\/2,-\/1,1,2),\ b=c(0,0,1,1),\ y=c(0,1,1,2))}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00864}00864\ \ \ \ \ \textcolor{comment}{\#\ x\ <-\/\ data.matrix(df[,c("{}a"{},\ "{}b"{})])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00865}00865\ \ \ \ \ \textcolor{comment}{\#\ y\ <-\/\ df\$y}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00866}00866\ \ \ \ \ \textcolor{comment}{\#\ fit\ <-\/\ glmnet(x=x,\ y=y,\ alpha=0,\ intercept=T,\ family="{}poisson"{},}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00867}00867\ \ \ \ \ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ standardize=F,\ thresh=1e-\/10,\ nlambda=10000)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00868}00868\ \ \ \ \ \textcolor{comment}{\#\ coef(fit,\ s=1)}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00869}00869\ \ \ \ \ \textcolor{comment}{\#\ (Intercept)\ -\/0.12889386979}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00870}00870\ \ \ \ \ \textcolor{comment}{\#\ a\ \ \ \ \ \ \ \ \ \ \ \ 0.29019207995}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00871}00871\ \ \ \ \ \textcolor{comment}{\#\ b\ \ \ \ \ \ \ \ \ \ \ \ 0.03741173122}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00872}00872\ \ \ \ \ X\ =\ np.array([[-\/2,\ -\/1,\ 1,\ 2],\ [0,\ 0,\ 1,\ 1]]).T}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00873}00873\ \ \ \ \ y\ =\ np.array([0,\ 1,\ 1,\ 2])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00874}00874\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ alpha=1,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ fit\_intercept=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ tol=1e-\/7,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00878}00878\ \ \ \ \ \ \ \ \ max\_iter=300,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00879}00879\ \ \ \ \ \ \ \ \ solver=solver,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00880}00880\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00881}00881\ \ \ \ \ glm.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00882}00882\ \ \ \ \ assert\_allclose(glm.intercept\_,\ -\/0.12889386979,\ rtol=1e-\/5)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00883}00883\ \ \ \ \ assert\_allclose(glm.coef\_,\ [0.29019207995,\ 0.03741173122],\ rtol=1e-\/5)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00884}00884\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00885}00885\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00886}00886\ \textcolor{keyword}{def\ }test\_convergence\_warning(regression\_data):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00887}00887\ \ \ \ \ X,\ y\ =\ regression\_data}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00888}00888\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00889}00889\ \ \ \ \ est\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1__GeneralizedLinearRegressor}{\_GeneralizedLinearRegressor}}(max\_iter=1,\ tol=1e-\/20)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00890}00890\ \ \ \ \ \textcolor{keyword}{with}\ pytest.warns(ConvergenceWarning):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00891}00891\ \ \ \ \ \ \ \ \ est.fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00892}00892\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00893}00893\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00894}00894\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00895}00895\ \ \ \ \ \textcolor{stringliteral}{"{}name,\ link\_class"{}},\ [(\textcolor{stringliteral}{"{}identity"{}},\ IdentityLink),\ (\textcolor{stringliteral}{"{}log"{}},\ LogLink)]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00896}00896\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00897}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a50919cda2d7576cc0ac4cd8379a528a4}{00897}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a50919cda2d7576cc0ac4cd8379a528a4}{test\_tweedie\_link\_argument}}(name,\ link\_class):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00898}00898\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ GLM\ link\ argument\ set\ as\ string."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00899}00899\ \ \ \ \ y\ =\ np.array([0.1,\ 0.5])\ \ \textcolor{comment}{\#\ in\ range\ of\ all\ distributions}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00900}00900\ \ \ \ \ X\ =\ np.array([[1],\ [2]])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00901}00901\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=1,\ link=name).fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00902}00902\ \ \ \ \ \textcolor{keyword}{assert}\ isinstance(glm.\_base\_loss.link,\ link\_class)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00903}00903\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00904}00904\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00905}00905\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00906}00906\ \ \ \ \ \textcolor{stringliteral}{"{}power,\ expected\_link\_class"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00907}00907\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00908}00908\ \ \ \ \ \ \ \ \ (0,\ IdentityLink),\ \ \textcolor{comment}{\#\ normal}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00909}00909\ \ \ \ \ \ \ \ \ (1,\ LogLink),\ \ \textcolor{comment}{\#\ poisson}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00910}00910\ \ \ \ \ \ \ \ \ (2,\ LogLink),\ \ \textcolor{comment}{\#\ gamma}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00911}00911\ \ \ \ \ \ \ \ \ (3,\ LogLink),\ \ \textcolor{comment}{\#\ inverse-\/gaussian}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00912}00912\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00913}00913\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00914}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a6ca035e8ddaa7721d1d5a7b77b74c9c5}{00914}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a6ca035e8ddaa7721d1d5a7b77b74c9c5}{test\_tweedie\_link\_auto}}(power,\ expected\_link\_class):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00915}00915\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ link='auto'\ delivers\ the\ expected\ link\ function"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00916}00916\ \ \ \ \ y\ =\ np.array([0.1,\ 0.5])\ \ \textcolor{comment}{\#\ in\ range\ of\ all\ distributions}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00917}00917\ \ \ \ \ X\ =\ np.array([[1],\ [2]])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00918}00918\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(link=\textcolor{stringliteral}{"{}auto"{}},\ power=power).fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00919}00919\ \ \ \ \ \textcolor{keyword}{assert}\ isinstance(glm.\_base\_loss.link,\ expected\_link\_class)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00920}00920\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00921}00921\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00922}00922\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}power"{},\ [0,\ 1,\ 1.5,\ 2,\ 3])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00923}00923\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}link"{},\ ["{}log"{},\ "{}identity"{}])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00924}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ac819ccb713b021733ada2638a13ce589}{00924}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ac819ccb713b021733ada2638a13ce589}{test\_tweedie\_score}}(regression\_data,\ power,\ link):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00925}00925\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ GLM\ score\ equals\ d2\_tweedie\_score\ for\ Tweedie\ losses."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00926}00926\ \ \ \ \ X,\ y\ =\ regression\_data}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00927}00927\ \ \ \ \ \textcolor{comment}{\#\ make\ y\ positive}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00928}00928\ \ \ \ \ y\ =\ np.abs(y)\ +\ 1.0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00929}00929\ \ \ \ \ glm\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=power,\ link=link).fit(X,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00930}00930\ \ \ \ \ \textcolor{keyword}{assert}\ glm.score(X,\ y)\ ==\ pytest.approx(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00931}00931\ \ \ \ \ \ \ \ \ d2\_tweedie\_score(y,\ glm.predict(X),\ power=power)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00932}00932\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00933}00933\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00934}00934\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00935}00935\ \textcolor{preprocessor}{@pytest.mark.parametrize}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00936}00936\ \ \ \ \ \textcolor{stringliteral}{"{}estimator,\ value"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00937}00937\ \ \ \ \ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00938}00938\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(),\ \textcolor{keyword}{True}),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00939}00939\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1GammaRegressor}{GammaRegressor}}(),\ \textcolor{keyword}{True}),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00940}00940\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=1.5),\ \textcolor{keyword}{True}),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00941}00941\ \ \ \ \ \ \ \ \ (\mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1TweedieRegressor}{TweedieRegressor}}(power=0),\ \textcolor{keyword}{False}),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00942}00942\ \ \ \ \ ],}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00943}00943\ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00944}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_adcc3fe7a911da0aaf3fb36e650ef8d10}{00944}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_adcc3fe7a911da0aaf3fb36e650ef8d10}{test\_tags}}(estimator,\ value):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00945}00945\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ that\ \`{}positive\_only\`{}\ tag\ is\ correctly\ set\ for\ different\ estimators."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00946}00946\ \ \ \ \ \textcolor{keyword}{assert}\ estimator.\_\_sklearn\_tags\_\_().target\_tags.positive\_only\ \textcolor{keywordflow}{is}\ value}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00947}00947\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00948}00948\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00949}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ae426c26f4a0512a32805109d5a16565b}{00949}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_ae426c26f4a0512a32805109d5a16565b}{test\_linalg\_warning\_with\_newton\_solver}}(global\_random\_seed):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00950}00950\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00951}00951\ \textcolor{stringliteral}{\ \ \ \ Test\ that\ the\ Newton\ solver\ raises\ a\ warning\ and\ falls\ back\ to\ LBFGS\ when}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00952}00952\ \textcolor{stringliteral}{\ \ \ \ encountering\ a\ singular\ or\ ill-\/conditioned\ Hessian\ matrix.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00953}00953\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00954}00954\ \textcolor{stringliteral}{\ \ \ \ This\ test\ assess\ the\ behavior\ of\ \`{}PoissonRegressor\`{}\ with\ the\ "{}newton-\/cholesky"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00955}00955\ \textcolor{stringliteral}{\ \ \ \ solver.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00956}00956\ \textcolor{stringliteral}{\ \ \ \ It\ verifies\ the\ following:-\/}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00957}00957\ \textcolor{stringliteral}{\ \ \ \ -\/\ The\ model\ significantly\ improves\ upon\ the\ constant\ baseline\ deviance.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00958}00958\ \textcolor{stringliteral}{\ \ \ \ -\/\ LBFGS\ remains\ robust\ on\ collinear\ data.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00959}00959\ \textcolor{stringliteral}{\ \ \ \ -\/\ The\ Newton\ solver\ raises\ a\ \`{}LinAlgWarning\`{}\ on\ collinear\ data\ and\ falls}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00960}00960\ \textcolor{stringliteral}{\ \ \ \ \ \ back\ to\ LBFGS.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00961}00961\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00962}00962\ \ \ \ \ newton\_solver\ =\ \textcolor{stringliteral}{"{}newton-\/cholesky"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00963}00963\ \ \ \ \ rng\ =\ np.random.RandomState(global\_random\_seed)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00964}00964\ \ \ \ \ \textcolor{comment}{\#\ Use\ at\ least\ 20\ samples\ to\ reduce\ the\ likelihood\ of\ getting\ a\ degenerate}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00965}00965\ \ \ \ \ \textcolor{comment}{\#\ dataset\ for\ any\ global\_random\_seed.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00966}00966\ \ \ \ \ X\_orig\ =\ rng.normal(size=(20,\ 3))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00967}00967\ \ \ \ \ y\ =\ rng.poisson(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00968}00968\ \ \ \ \ \ \ \ \ np.exp(X\_orig\ @\ np.ones(X\_orig.shape[1])),\ size=X\_orig.shape[0]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00969}00969\ \ \ \ \ ).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00970}00970\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00971}00971\ \ \ \ \ \textcolor{comment}{\#\ Collinear\ variation\ of\ the\ same\ input\ features.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00972}00972\ \ \ \ \ X\_collinear\ =\ np.hstack([X\_orig]\ *\ 10)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00973}00973\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00974}00974\ \ \ \ \ \textcolor{comment}{\#\ Let's\ consider\ the\ deviance\ of\ a\ constant\ baseline\ on\ this\ problem.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00975}00975\ \ \ \ \ baseline\_pred\ =\ np.full\_like(y,\ y.mean())}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00976}00976\ \ \ \ \ constant\_model\_deviance\ =\ mean\_poisson\_deviance(y,\ baseline\_pred)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00977}00977\ \ \ \ \ \textcolor{keyword}{assert}\ constant\_model\_deviance\ >\ 1.0}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00978}00978\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00979}00979\ \ \ \ \ \textcolor{comment}{\#\ No\ warning\ raised\ on\ well-\/conditioned\ design,\ even\ without\ regularization.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00980}00980\ \ \ \ \ tol\ =\ 1e-\/10}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00981}00981\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00982}00982\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}error"{}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00983}00983\ \ \ \ \ \ \ \ \ reg\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(solver=newton\_solver,\ alpha=0.0,\ tol=tol).fit(X\_orig,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00984}00984\ \ \ \ \ original\_newton\_deviance\ =\ mean\_poisson\_deviance(y,\ reg.predict(X\_orig))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00985}00985\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00986}00986\ \ \ \ \ \textcolor{comment}{\#\ On\ this\ dataset,\ we\ should\ have\ enough\ data\ points\ to\ not\ make\ it}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00987}00987\ \ \ \ \ \textcolor{comment}{\#\ possible\ to\ get\ a\ near\ zero\ deviance\ (for\ the\ any\ of\ the\ admissible}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00988}00988\ \ \ \ \ \textcolor{comment}{\#\ random\ seeds).\ This\ will\ make\ it\ easier\ to\ interpret\ meaning\ of\ rtol\ in}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00989}00989\ \ \ \ \ \textcolor{comment}{\#\ the\ subsequent\ assertions:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00990}00990\ \ \ \ \ \textcolor{keyword}{assert}\ original\_newton\_deviance\ >\ 0.2}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00991}00991\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00992}00992\ \ \ \ \ \textcolor{comment}{\#\ We\ check\ that\ the\ model\ could\ successfully\ fit\ information\ in\ X\_orig\ to}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00993}00993\ \ \ \ \ \textcolor{comment}{\#\ improve\ upon\ the\ constant\ baseline\ by\ a\ large\ margin\ (when\ evaluated\ on}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00994}00994\ \ \ \ \ \textcolor{comment}{\#\ the\ traing\ set).}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00995}00995\ \ \ \ \ \textcolor{keyword}{assert}\ constant\_model\_deviance\ -\/\ original\_newton\_deviance\ >\ 0.1}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00996}00996\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00997}00997\ \ \ \ \ \textcolor{comment}{\#\ LBFGS\ is\ robust\ to\ a\ collinear\ design\ because\ its\ approximation\ of\ the}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00998}00998\ \ \ \ \ \textcolor{comment}{\#\ Hessian\ is\ Symmeric\ Positive\ Definite\ by\ construction.\ Let's\ record\ its}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l00999}00999\ \ \ \ \ \textcolor{comment}{\#\ solution}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01000}01000\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01001}01001\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}error"{}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01002}01002\ \ \ \ \ \ \ \ \ reg\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(solver=\textcolor{stringliteral}{"{}lbfgs"{}},\ alpha=0.0,\ tol=tol).fit(X\_collinear,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01003}01003\ \ \ \ \ collinear\_lbfgs\_deviance\ =\ mean\_poisson\_deviance(y,\ reg.predict(X\_collinear))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01004}01004\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01005}01005\ \ \ \ \ \textcolor{comment}{\#\ The\ LBFGS\ solution\ on\ the\ collinear\ is\ expected\ to\ reach\ a\ comparable}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01006}01006\ \ \ \ \ \textcolor{comment}{\#\ solution\ to\ the\ Newton\ solution\ on\ the\ original\ data.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01007}01007\ \ \ \ \ rtol\ =\ 1e-\/6}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01008}01008\ \ \ \ \ \textcolor{keyword}{assert}\ collinear\_lbfgs\_deviance\ ==\ pytest.approx(original\_newton\_deviance,\ rel=rtol)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01009}01009\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01010}01010\ \ \ \ \ \textcolor{comment}{\#\ Fitting\ a\ Newton\ solver\ on\ the\ collinear\ version\ of\ the\ training\ data}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01011}01011\ \ \ \ \ \textcolor{comment}{\#\ without\ regularization\ should\ raise\ an\ informative\ warning\ and\ fallback}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01012}01012\ \ \ \ \ \textcolor{comment}{\#\ to\ the\ LBFGS\ solver.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01013}01013\ \ \ \ \ msg\ =\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01014}01014\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ inner\ solver\ of\ .*Newton.*Solver\ stumbled\ upon\ a\ singular\ or\ very\ "{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01015}01015\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}ill-\/conditioned\ Hessian\ matrix"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01016}01016\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01017}01017\ \ \ \ \ \textcolor{keyword}{with}\ pytest.warns(\mbox{\hyperlink{classscipy_1_1linalg_1_1__misc_1_1LinAlgWarning}{scipy.linalg.LinAlgWarning}},\ match=msg):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01018}01018\ \ \ \ \ \ \ \ \ reg\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(solver=newton\_solver,\ alpha=0.0,\ tol=tol).fit(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01019}01019\ \ \ \ \ \ \ \ \ \ \ \ \ X\_collinear,\ y}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01020}01020\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01021}01021\ \ \ \ \ \textcolor{comment}{\#\ As\ a\ result\ we\ should\ still\ automatically\ converge\ to\ a\ good\ solution.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01022}01022\ \ \ \ \ collinear\_newton\_deviance\ =\ mean\_poisson\_deviance(y,\ reg.predict(X\_collinear))}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01023}01023\ \ \ \ \ \textcolor{keyword}{assert}\ collinear\_newton\_deviance\ ==\ pytest.approx(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01024}01024\ \ \ \ \ \ \ \ \ original\_newton\_deviance,\ rel=rtol}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01025}01025\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01026}01026\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01027}01027\ \ \ \ \ \textcolor{comment}{\#\ Increasing\ the\ regularization\ slightly\ should\ make\ the\ problem\ go\ away:}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01028}01028\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01029}01029\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}error"{}},\ \mbox{\hyperlink{classscipy_1_1linalg_1_1__misc_1_1LinAlgWarning}{scipy.linalg.LinAlgWarning}})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01030}01030\ \ \ \ \ \ \ \ \ reg\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1glm_1_1PoissonRegressor}{PoissonRegressor}}(solver=newton\_solver,\ alpha=1e-\/10).fit(X\_collinear,\ y)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01031}01031\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01032}01032\ \ \ \ \ \textcolor{comment}{\#\ The\ slightly\ penalized\ model\ on\ the\ collinear\ data\ should\ be\ close\ enough}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01033}01033\ \ \ \ \ \textcolor{comment}{\#\ to\ the\ unpenalized\ model\ on\ the\ original\ data.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01034}01034\ \ \ \ \ penalized\_collinear\_newton\_deviance\ =\ mean\_poisson\_deviance(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01035}01035\ \ \ \ \ \ \ \ \ y,\ reg.predict(X\_collinear)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01036}01036\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01037}01037\ \ \ \ \ \textcolor{keyword}{assert}\ penalized\_collinear\_newton\_deviance\ ==\ pytest.approx(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01038}01038\ \ \ \ \ \ \ \ \ original\_newton\_deviance,\ rel=rtol}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01039}01039\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01040}01040\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01041}01041\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01042}01042\ \textcolor{preprocessor}{@pytest.mark.parametrize("{}verbose"{},\ [0,\ 1,\ 2])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01043}\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a40f4070af88dd33bc6a47515e426cf73}{01043}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__glm_1_1tests_1_1test__glm_a40f4070af88dd33bc6a47515e426cf73}{test\_newton\_solver\_verbosity}}(capsys,\ verbose):}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01044}01044\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Test\ the\ std\ output\ of\ verbose\ newton\ solvers."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01045}01045\ \ \ \ \ y\ =\ np.array([1,\ 2],\ dtype=float)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01046}01046\ \ \ \ \ X\ =\ np.array([[1.0,\ 0],\ [0,\ 1]],\ dtype=float)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01047}01047\ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(),\ fit\_intercept=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01048}01048\ \ \ \ \ sol\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1__newton__solver_1_1NewtonCholeskySolver}{NewtonCholeskySolver}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01049}01049\ \ \ \ \ \ \ \ \ coef=linear\_loss.init\_zero\_coef(X),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01050}01050\ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01051}01051\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01052}01052\ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01053}01053\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01054}01054\ \ \ \ \ sol.solve(X,\ y,\ \textcolor{keywordtype}{None})\ \ \textcolor{comment}{\#\ returns\ array([0.,\ 0.69314758])}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01055}01055\ \ \ \ \ captured\ =\ capsys.readouterr()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01056}01056\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01057}01057\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01058}01058\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ captured.out\ ==\ \textcolor{stringliteral}{"{}"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01059}01059\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01060}01060\ \ \ \ \ \ \ \ \ msg\ =\ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01061}01061\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Newton\ iter=1"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01062}01062\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Check\ Convergence"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01063}01063\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}1.\ max\ |gradient|"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01064}01064\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}2.\ Newton\ decrement"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01065}01065\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Solver\ did\ converge\ at\ loss\ =\ "{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01066}01066\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01067}01067\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m\ \textcolor{keywordflow}{in}\ msg:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01068}01068\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ m\ \textcolor{keywordflow}{in}\ captured.out}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01069}01069\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01070}01070\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose\ >=\ 2:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01071}01071\ \ \ \ \ \ \ \ \ msg\ =\ [\textcolor{stringliteral}{"{}Backtracking\ Line\ Search"{}},\ \textcolor{stringliteral}{"{}line\ search\ iteration="{}}]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01072}01072\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m\ \textcolor{keywordflow}{in}\ msg:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01073}01073\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ m\ \textcolor{keywordflow}{in}\ captured.out}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01074}01074\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01075}01075\ \ \ \ \ \textcolor{comment}{\#\ Set\ the\ Newton\ solver\ to\ a\ state\ with\ a\ completely\ wrong\ Newton\ step.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01076}01076\ \ \ \ \ sol\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1__newton__solver_1_1NewtonCholeskySolver}{NewtonCholeskySolver}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01077}01077\ \ \ \ \ \ \ \ \ coef=linear\_loss.init\_zero\_coef(X),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01078}01078\ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01079}01079\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01080}01080\ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01081}01081\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01082}01082\ \ \ \ \ sol.setup(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01083}01083\ \ \ \ \ sol.iteration\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01084}01084\ \ \ \ \ sol.update\_gradient\_hessian(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01085}01085\ \ \ \ \ sol.coef\_newton\ =\ np.array([1.0,\ 0])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01086}01086\ \ \ \ \ sol.gradient\_times\_newton\ =\ sol.gradient\ @\ sol.coef\_newton}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01087}01087\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01088}01088\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}ignore"{}},\ ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01089}01089\ \ \ \ \ \ \ \ \ sol.line\_search(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01090}01090\ \ \ \ \ \ \ \ \ captured\ =\ capsys.readouterr()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01091}01091\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose\ >=\ 1:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01092}01092\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01093}01093\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Line\ search\ did\ not\ converge\ and\ resorts\ to\ lbfgs\ instead."{}}\ \textcolor{keywordflow}{in}\ captured.out}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01094}01094\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01095}01095\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01096}01096\ \ \ \ \ \textcolor{comment}{\#\ Set\ the\ Newton\ solver\ to\ a\ state\ with\ bad\ Newton\ step\ such\ that\ the\ loss}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01097}01097\ \ \ \ \ \textcolor{comment}{\#\ improvement\ in\ line\ search\ is\ tiny.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01098}01098\ \ \ \ \ sol\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1__newton__solver_1_1NewtonCholeskySolver}{NewtonCholeskySolver}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01099}01099\ \ \ \ \ \ \ \ \ coef=np.array([1e-\/12,\ 0.69314758]),}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01100}01100\ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01101}01101\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01102}01102\ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01103}01103\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01104}01104\ \ \ \ \ sol.setup(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01105}01105\ \ \ \ \ sol.iteration\ =\ 1}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01106}01106\ \ \ \ \ sol.update\_gradient\_hessian(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01107}01107\ \ \ \ \ sol.coef\_newton\ =\ np.array([1e-\/6,\ 0])}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01108}01108\ \ \ \ \ sol.gradient\_times\_newton\ =\ sol.gradient\ @\ sol.coef\_newton}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01109}01109\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01110}01110\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}ignore"{}},\ ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01111}01111\ \ \ \ \ \ \ \ \ sol.line\_search(X=X,\ y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01112}01112\ \ \ \ \ \ \ \ \ captured\ =\ capsys.readouterr()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01113}01113\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose\ >=\ 2:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01114}01114\ \ \ \ \ \ \ \ \ msg\ =\ [}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01115}01115\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}line\ search\ iteration="{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01116}01116\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}check\ loss\ improvement\ <=\ armijo\ term:"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01117}01117\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}check\ loss\ |improvement|\ <=\ eps\ *\ |loss\_old|:"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01118}01118\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}check\ sum(|gradient|)\ <\ sum(|gradient\_old|):"{}},}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01119}01119\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01120}01120\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m\ \textcolor{keywordflow}{in}\ msg:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01121}01121\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ m\ \textcolor{keywordflow}{in}\ captured.out}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01122}01122\ }
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01123}01123\ \ \ \ \ \textcolor{comment}{\#\ Test\ for\ a\ case\ with\ negative\ hessian.\ We\ badly\ initialize\ coef\ for\ a\ Tweedie}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01124}01124\ \ \ \ \ \textcolor{comment}{\#\ loss\ with\ non-\/canonical\ link,\ e.g.\ Inverse\ Gaussian\ deviance\ with\ a\ log\ link.}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01125}01125\ \ \ \ \ linear\_loss\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__linear__loss_1_1LinearModelLoss}{LinearModelLoss}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01126}01126\ \ \ \ \ \ \ \ \ base\_loss=\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(power=3),\ fit\_intercept=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01127}01127\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01128}01128\ \ \ \ \ sol\ =\ \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__glm_1_1__newton__solver_1_1NewtonCholeskySolver}{NewtonCholeskySolver}}(}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01129}01129\ \ \ \ \ \ \ \ \ coef=linear\_loss.init\_zero\_coef(X)\ +\ 1,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01130}01130\ \ \ \ \ \ \ \ \ linear\_loss=linear\_loss,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01131}01131\ \ \ \ \ \ \ \ \ l2\_reg\_strength=0,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01132}01132\ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01133}01133\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01134}01134\ \ \ \ \ \textcolor{keyword}{with}\ warnings.catch\_warnings():}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01135}01135\ \ \ \ \ \ \ \ \ warnings.simplefilter(\textcolor{stringliteral}{"{}ignore"{}},\ ConvergenceWarning)}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01136}01136\ \ \ \ \ \ \ \ \ sol.solve(X,\ y,\ \textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01137}01137\ \ \ \ \ captured\ =\ capsys.readouterr()}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01138}01138\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose\ >=\ 1:}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01139}01139\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ (}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01140}01140\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ inner\ solver\ detected\ a\ pointwise\ Hessian\ with\ many\ negative\ values"{}}}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01141}01141\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}\ and\ resorts\ to\ lbfgs\ instead."{}}\ \textcolor{keywordflow}{in}\ captured.out}
\DoxyCodeLine{\Hypertarget{test__glm_8py_source_l01142}01142\ \ \ \ \ \ \ \ \ )}

\end{DoxyCode}
