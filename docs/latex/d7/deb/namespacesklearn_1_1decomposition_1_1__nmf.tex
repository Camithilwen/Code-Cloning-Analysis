\doxysection{sklearn.\+decomposition.\+\_\+nmf Namespace Reference}
\hypertarget{namespacesklearn_1_1decomposition_1_1__nmf}{}\label{namespacesklearn_1_1decomposition_1_1__nmf}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__nmf_1_1__BaseNMF}{\+\_\+\+Base\+NMF}}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__nmf_1_1MiniBatchNMF}{Mini\+Batch\+NMF}}
\item 
class \mbox{\hyperlink{classsklearn_1_1decomposition_1_1__nmf_1_1NMF}{NMF}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a6e559cdf3a40c3c84e2f872f8cb79bad}{norm}} (x)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a4b8fde89e737a10205d4e0480f4b678d}{trace\+\_\+dot}} (X, Y)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a7db5413fcde9cbca39cb7bef150ccf83}{\+\_\+check\+\_\+init}} (A, shape, whom)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_af28d297dd2bd8cbfbbec46f183a19f73}{\+\_\+beta\+\_\+divergence}} (X, W, H, beta, square\+\_\+root=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a8e1e7eda092577e8f9515f13646d4dd3}{\+\_\+special\+\_\+sparse\+\_\+dot}} (W, H, X)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a2ecfb21949e642b20aa31334a24022a6}{\+\_\+beta\+\_\+loss\+\_\+to\+\_\+float}} (beta\+\_\+loss)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_ae6b88b757d9a2b51559c6851fcf794fc}{\+\_\+initialize\+\_\+nmf}} (X, n\+\_\+components, init=None, eps=1e-\/6, random\+\_\+state=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a7625681941ef7d3eaa301844b70f6a37}{\+\_\+update\+\_\+coordinate\+\_\+descent}} (X, W, Ht, l1\+\_\+reg, l2\+\_\+reg, shuffle, random\+\_\+state)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a1e419b7547218e3e63acbb67417c729a}{\+\_\+fit\+\_\+coordinate\+\_\+descent}} (X, W, H, tol=1e-\/4, max\+\_\+iter=200, l1\+\_\+reg\+\_\+W=0, l1\+\_\+reg\+\_\+H=0, l2\+\_\+reg\+\_\+W=0, l2\+\_\+reg\+\_\+H=0, update\+\_\+H=\mbox{\hyperlink{classTrue}{True}}, verbose=0, shuffle=False, random\+\_\+state=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_aa96c1b1190107de9198dcf265ec2ae07}{\+\_\+multiplicative\+\_\+update\+\_\+w}} (X, W, H, beta\+\_\+loss, l1\+\_\+reg\+\_\+W, l2\+\_\+reg\+\_\+W, gamma, H\+\_\+sum=None, HHt=None, XHt=None, update\+\_\+H=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a755378ea5231288eb0861db8c8b8018b}{\+\_\+multiplicative\+\_\+update\+\_\+h}} (X, W, H, beta\+\_\+loss, l1\+\_\+reg\+\_\+H, l2\+\_\+reg\+\_\+H, gamma, A=None, B=None, rho=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a2396834d55cd7ee5c334433651161bfd}{\+\_\+fit\+\_\+multiplicative\+\_\+update}} (X, W, H, beta\+\_\+loss="{}frobenius"{}, max\+\_\+iter=200, tol=1e-\/4, l1\+\_\+reg\+\_\+W=0, l1\+\_\+reg\+\_\+H=0, l2\+\_\+reg\+\_\+W=0, l2\+\_\+reg\+\_\+H=0, update\+\_\+H=\mbox{\hyperlink{classTrue}{True}}, verbose=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_afc9f024d386723f2619a8d0d5f29dca5}{non\+\_\+negative\+\_\+factorization}} (X, W=None, H=None, n\+\_\+components="{}auto"{}, \texorpdfstring{$\ast$}{*}, init=None, update\+\_\+H=\mbox{\hyperlink{classTrue}{True}}, solver="{}cd"{}, beta\+\_\+loss="{}frobenius"{}, tol=1e-\/4, max\+\_\+iter=200, alpha\+\_\+W=0.\+0, alpha\+\_\+H="{}same"{}, l1\+\_\+ratio=0.\+0, random\+\_\+state=None, verbose=0, shuffle=False)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1decomposition_1_1__nmf_a9bfb8c23341015a3bcd74d3c3f4a363c}{EPSILON}} = np.\+finfo(np.\+float32).eps
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Non-negative matrix factorization.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_af28d297dd2bd8cbfbbec46f183a19f73}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_beta\_divergence@{\_beta\_divergence}}
\index{\_beta\_divergence@{\_beta\_divergence}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_beta\_divergence()}{\_beta\_divergence()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_af28d297dd2bd8cbfbbec46f183a19f73} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+beta\+\_\+divergence (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{beta}{, }\item[{}]{square\+\_\+root}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the beta-divergence of X and dot(W, H).

Parameters
----------
X : float or array-like of shape (n_samples, n_features)

W : float or array-like of shape (n_samples, n_components)

H : float or array-like of shape (n_components, n_features)

beta : float or {'frobenius', 'kullback-leibler', 'itakura-saito'}
    Parameter of the beta-divergence.
    If beta == 2, this is half the Frobenius *squared* norm.
    If beta == 1, this is the generalized Kullback-Leibler divergence.
    If beta == 0, this is the Itakura-Saito divergence.
    Else, this is the general beta-divergence.

square_root : bool, default=False
    If True, return np.sqrt(2 * res)
    For beta == 2, it corresponds to the Frobenius norm.

Returns
-------
    res : float
        Beta divergence of X and np.dot(X, H).
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00085}{85}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



References \mbox{\hyperlink{__nmf_8py_source_l00213}{\+\_\+beta\+\_\+loss\+\_\+to\+\_\+float()}}, \mbox{\hyperlink{__nmf_8py_source_l00192}{\+\_\+special\+\_\+sparse\+\_\+dot()}}, and \mbox{\hyperlink{__nmf_8py_source_l00055}{trace\+\_\+dot()}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00744}{\+\_\+fit\+\_\+multiplicative\+\_\+update()}}, \mbox{\hyperlink{__nmf_8py_source_l02073}{sklearn.\+decomposition.\+\_\+nmf.\+Mini\+Batch\+NMF.\+\_\+minibatch\+\_\+step()}}, \mbox{\hyperlink{__nmf_8py_source_l02183}{sklearn.\+decomposition.\+\_\+nmf.\+Mini\+Batch\+NMF.\+fit\+\_\+transform()}}, and \mbox{\hyperlink{__nmf_8py_source_l01594}{sklearn.\+decomposition.\+\_\+nmf.\+NMF.\+fit\+\_\+transform()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a2ecfb21949e642b20aa31334a24022a6}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_beta\_loss\_to\_float@{\_beta\_loss\_to\_float}}
\index{\_beta\_loss\_to\_float@{\_beta\_loss\_to\_float}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_beta\_loss\_to\_float()}{\_beta\_loss\_to\_float()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a2ecfb21949e642b20aa31334a24022a6} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+beta\+\_\+loss\+\_\+to\+\_\+float (\begin{DoxyParamCaption}\item[{}]{beta\+\_\+loss}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Convert string beta_loss to float.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00213}{213}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00085}{\+\_\+beta\+\_\+divergence()}}, and \mbox{\hyperlink{__nmf_8py_source_l00744}{\+\_\+fit\+\_\+multiplicative\+\_\+update()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a7db5413fcde9cbca39cb7bef150ccf83}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_check\_init@{\_check\_init}}
\index{\_check\_init@{\_check\_init}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_check\_init()}{\_check\_init()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a7db5413fcde9cbca39cb7bef150ccf83} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+check\+\_\+init (\begin{DoxyParamCaption}\item[{}]{A}{, }\item[{}]{shape}{, }\item[{}]{whom}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__nmf_8py_source_l00068}{68}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a1e419b7547218e3e63acbb67417c729a}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_fit\_coordinate\_descent@{\_fit\_coordinate\_descent}}
\index{\_fit\_coordinate\_descent@{\_fit\_coordinate\_descent}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_fit\_coordinate\_descent()}{\_fit\_coordinate\_descent()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a1e419b7547218e3e63acbb67417c729a} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+fit\+\_\+coordinate\+\_\+descent (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 200}, }\item[{}]{l1\+\_\+reg\+\_\+W}{ = {\ttfamily 0}, }\item[{}]{l1\+\_\+reg\+\_\+H}{ = {\ttfamily 0}, }\item[{}]{l2\+\_\+reg\+\_\+W}{ = {\ttfamily 0}, }\item[{}]{l2\+\_\+reg\+\_\+H}{ = {\ttfamily 0}, }\item[{}]{update\+\_\+H}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{shuffle}{ = {\ttfamily False}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent

The objective function is minimized with an alternating minimization of W
and H. Each minimization is done with a cyclic (up to a permutation of the
features) Coordinate Descent.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Constant matrix.

W : array-like of shape (n_samples, n_components)
    Initial guess for the solution.

H : array-like of shape (n_components, n_features)
    Initial guess for the solution.

tol : float, default=1e-4
    Tolerance of the stopping condition.

max_iter : int, default=200
    Maximum number of iterations before timing out.

l1_reg_W : float, default=0.
    L1 regularization parameter for W.

l1_reg_H : float, default=0.
    L1 regularization parameter for H.

l2_reg_W : float, default=0.
    L2 regularization parameter for W.

l2_reg_H : float, default=0.
    L2 regularization parameter for H.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

verbose : int, default=0
    The verbosity level.

shuffle : bool, default=False
    If true, randomize the order of coordinates in the CD solver.

random_state : int, RandomState instance or None, default=None
    Used to randomize the coordinates in the CD solver, when
    ``shuffle`` is set to ``True``. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    The number of iterations done by the algorithm.

References
----------
.. [1] :doi:`"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations" <10.1587/transfun.E92.A.708>`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00406}{406}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



References \mbox{\hyperlink{__nmf_8py_source_l00376}{\+\_\+update\+\_\+coordinate\+\_\+descent()}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l01638}{sklearn.\+decomposition.\+\_\+nmf.\+NMF.\+\_\+fit\+\_\+transform()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a2396834d55cd7ee5c334433651161bfd}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_fit\_multiplicative\_update@{\_fit\_multiplicative\_update}}
\index{\_fit\_multiplicative\_update@{\_fit\_multiplicative\_update}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_fit\_multiplicative\_update()}{\_fit\_multiplicative\_update()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a2396834d55cd7ee5c334433651161bfd} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+fit\+\_\+multiplicative\+\_\+update (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{beta\+\_\+loss}{ = {\ttfamily "{}frobenius"{}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 200}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{l1\+\_\+reg\+\_\+W}{ = {\ttfamily 0}, }\item[{}]{l1\+\_\+reg\+\_\+H}{ = {\ttfamily 0}, }\item[{}]{l2\+\_\+reg\+\_\+W}{ = {\ttfamily 0}, }\item[{}]{l2\+\_\+reg\+\_\+H}{ = {\ttfamily 0}, }\item[{}]{update\+\_\+H}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute Non-negative Matrix Factorization with Multiplicative Update.

The objective function is _beta_divergence(X, WH) and is minimized with an
alternating minimization of W and H. Each minimization is done with a
Multiplicative Update.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Constant input matrix.

W : array-like of shape (n_samples, n_components)
    Initial guess for the solution.

H : array-like of shape (n_components, n_features)
    Initial guess for the solution.

beta_loss : float or {'frobenius', 'kullback-leibler', \
        'itakura-saito'}, default='frobenius'
    String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.
    Beta divergence to be minimized, measuring the distance between X
    and the dot product WH. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input
    matrix X cannot contain zeros.

max_iter : int, default=200
    Number of iterations.

tol : float, default=1e-4
    Tolerance of the stopping condition.

l1_reg_W : float, default=0.
    L1 regularization parameter for W.

l1_reg_H : float, default=0.
    L1 regularization parameter for H.

l2_reg_W : float, default=0.
    L2 regularization parameter for W.

l2_reg_H : float, default=0.
    L2 regularization parameter for H.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

verbose : int, default=0
    The verbosity level.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    The number of iterations done by the algorithm.

References
----------
Lee, D. D., & Seung, H., S. (2001). Algorithms for Non-negative Matrix
Factorization. Adv. Neural Inform. Process. Syst.. 13.
Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix
factorization with the beta-divergence. Neural Computation, 23(9).
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00731}{731}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



References \mbox{\hyperlink{__nmf_8py_source_l00085}{\+\_\+beta\+\_\+divergence()}}, \mbox{\hyperlink{__nmf_8py_source_l00213}{\+\_\+beta\+\_\+loss\+\_\+to\+\_\+float()}}, \mbox{\hyperlink{__nmf_8py_source_l00636}{\+\_\+multiplicative\+\_\+update\+\_\+h()}}, and \mbox{\hyperlink{__nmf_8py_source_l00538}{\+\_\+multiplicative\+\_\+update\+\_\+w()}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l01638}{sklearn.\+decomposition.\+\_\+nmf.\+NMF.\+\_\+fit\+\_\+transform()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_ae6b88b757d9a2b51559c6851fcf794fc}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_initialize\_nmf@{\_initialize\_nmf}}
\index{\_initialize\_nmf@{\_initialize\_nmf}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_initialize\_nmf()}{\_initialize\_nmf()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_ae6b88b757d9a2b51559c6851fcf794fc} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+initialize\+\_\+nmf (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{n\+\_\+components}{, }\item[{}]{init}{ = {\ttfamily None}, }\item[{}]{eps}{ = {\ttfamily 1e-\/6}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Algorithms for NMF initialization.

Computes an initial guess for the non-negative
rank k matrix approximation for X: X = WH.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    The data matrix to be decomposed.

n_components : int
    The number of components desired in the approximation.

init :  {'random', 'nndsvd', 'nndsvda', 'nndsvdar'}, default=None
    Method used to initialize the procedure.
    Valid options:

    - None: 'nndsvda' if n_components <= min(n_samples, n_features),
        otherwise 'random'.

    - 'random': non-negative random matrices, scaled with:
        sqrt(X.mean() / n_components)

    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
        initialization (better for sparseness)

    - 'nndsvda': NNDSVD with zeros filled with the average of X
        (better when sparsity is not desired)

    - 'nndsvdar': NNDSVD with zeros filled with small random values
        (generally faster, less accurate alternative to NNDSVDa
        for when sparsity is not desired)

    - 'custom': use custom matrices W and H

    .. versionchanged:: 1.1
        When `init=None` and n_components is less than n_samples and n_features
        defaults to `nndsvda` instead of `nndsvd`.

eps : float, default=1e-6
    Truncate all values less then this in output to zero.

random_state : int, RandomState instance or None, default=None
    Used when ``init`` == 'nndsvdar' or 'random'. Pass an int for
    reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
W : array-like of shape (n_samples, n_components)
    Initial guesses for solving X ~= WH.

H : array-like of shape (n_components, n_features)
    Initial guesses for solving X ~= WH.

References
----------
C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for
nonnegative matrix factorization - Pattern Recognition, 2008
http://tinyurl.com/nndsvd
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00221}{221}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l01194}{sklearn.\+decomposition.\+\_\+nmf.\+\_\+\+Base\+NMF.\+\_\+check\+\_\+w\+\_\+h()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a755378ea5231288eb0861db8c8b8018b}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_multiplicative\_update\_h@{\_multiplicative\_update\_h}}
\index{\_multiplicative\_update\_h@{\_multiplicative\_update\_h}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_multiplicative\_update\_h()}{\_multiplicative\_update\_h()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a755378ea5231288eb0861db8c8b8018b} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+multiplicative\+\_\+update\+\_\+h (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{beta\+\_\+loss}{, }\item[{}]{l1\+\_\+reg\+\_\+H}{, }\item[{}]{l2\+\_\+reg\+\_\+H}{, }\item[{}]{gamma}{, }\item[{}]{A}{ = {\ttfamily None}, }\item[{}]{B}{ = {\ttfamily None}, }\item[{}]{rho}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}update H in Multiplicative Update NMF.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00634}{634}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



References \mbox{\hyperlink{__nmf_8py_source_l00192}{\+\_\+special\+\_\+sparse\+\_\+dot()}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00744}{\+\_\+fit\+\_\+multiplicative\+\_\+update()}}, and \mbox{\hyperlink{__nmf_8py_source_l02073}{sklearn.\+decomposition.\+\_\+nmf.\+Mini\+Batch\+NMF.\+\_\+minibatch\+\_\+step()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_aa96c1b1190107de9198dcf265ec2ae07}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_multiplicative\_update\_w@{\_multiplicative\_update\_w}}
\index{\_multiplicative\_update\_w@{\_multiplicative\_update\_w}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_multiplicative\_update\_w()}{\_multiplicative\_update\_w()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_aa96c1b1190107de9198dcf265ec2ae07} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+multiplicative\+\_\+update\+\_\+w (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{beta\+\_\+loss}{, }\item[{}]{l1\+\_\+reg\+\_\+W}{, }\item[{}]{l2\+\_\+reg\+\_\+W}{, }\item[{}]{gamma}{, }\item[{}]{H\+\_\+sum}{ = {\ttfamily None}, }\item[{}]{HHt}{ = {\ttfamily None}, }\item[{}]{XHt}{ = {\ttfamily None}, }\item[{}]{update\+\_\+H}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Update W in Multiplicative Update NMF.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00526}{526}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



References \mbox{\hyperlink{__nmf_8py_source_l00192}{\+\_\+special\+\_\+sparse\+\_\+dot()}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00744}{\+\_\+fit\+\_\+multiplicative\+\_\+update()}}, \mbox{\hyperlink{__nmf_8py_source_l02073}{sklearn.\+decomposition.\+\_\+nmf.\+Mini\+Batch\+NMF.\+\_\+minibatch\+\_\+step()}}, and \mbox{\hyperlink{__nmf_8py_source_l02046}{sklearn.\+decomposition.\+\_\+nmf.\+Mini\+Batch\+NMF.\+\_\+solve\+\_\+\+W()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a8e1e7eda092577e8f9515f13646d4dd3}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_special\_sparse\_dot@{\_special\_sparse\_dot}}
\index{\_special\_sparse\_dot@{\_special\_sparse\_dot}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_special\_sparse\_dot()}{\_special\_sparse\_dot()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a8e1e7eda092577e8f9515f13646d4dd3} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+special\+\_\+sparse\+\_\+dot (\begin{DoxyParamCaption}\item[{}]{W}{, }\item[{}]{H}{, }\item[{}]{X}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes np.dot(W, H), only where X is non zero.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00192}{192}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00085}{\+\_\+beta\+\_\+divergence()}}, \mbox{\hyperlink{__nmf_8py_source_l00636}{\+\_\+multiplicative\+\_\+update\+\_\+h()}}, and \mbox{\hyperlink{__nmf_8py_source_l00538}{\+\_\+multiplicative\+\_\+update\+\_\+w()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a7625681941ef7d3eaa301844b70f6a37}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!\_update\_coordinate\_descent@{\_update\_coordinate\_descent}}
\index{\_update\_coordinate\_descent@{\_update\_coordinate\_descent}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{\_update\_coordinate\_descent()}{\_update\_coordinate\_descent()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a7625681941ef7d3eaa301844b70f6a37} 
sklearn.\+decomposition.\+\_\+nmf.\+\_\+update\+\_\+coordinate\+\_\+descent (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{, }\item[{}]{Ht}{, }\item[{}]{l1\+\_\+reg}{, }\item[{}]{l2\+\_\+reg}{, }\item[{}]{shuffle}{, }\item[{}]{random\+\_\+state}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Helper function for _fit_coordinate_descent.

Update W to minimize the objective function, iterating once over all
coordinates. By symmetry, to update H, one can call
_update_coordinate_descent(X.T, Ht, W, ...).
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00376}{376}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00420}{\+\_\+fit\+\_\+coordinate\+\_\+descent()}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_afc9f024d386723f2619a8d0d5f29dca5}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!non\_negative\_factorization@{non\_negative\_factorization}}
\index{non\_negative\_factorization@{non\_negative\_factorization}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{non\_negative\_factorization()}{non\_negative\_factorization()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_afc9f024d386723f2619a8d0d5f29dca5} 
sklearn.\+decomposition.\+\_\+nmf.\+non\+\_\+negative\+\_\+factorization (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{W}{ = {\ttfamily None}, }\item[{}]{H}{ = {\ttfamily None}, }\item[{}]{n\+\_\+components}{ = {\ttfamily "{}auto"{}}, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{init}{ = {\ttfamily None}, }\item[{}]{update\+\_\+H}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{solver}{ = {\ttfamily "{}cd"{}}, }\item[{}]{beta\+\_\+loss}{ = {\ttfamily "{}frobenius"{}}, }\item[{}]{tol}{ = {\ttfamily 1e-\/4}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 200}, }\item[{}]{alpha\+\_\+W}{ = {\ttfamily 0.0}, }\item[{}]{alpha\+\_\+H}{ = {\ttfamily "{}same"{}}, }\item[{}]{l1\+\_\+ratio}{ = {\ttfamily 0.0}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{shuffle}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute Non-negative Matrix Factorization (NMF).

Find two non-negative matrices (W, H) whose product approximates the non-
negative matrix X. This factorization can be used for example for
dimensionality reduction, source separation or topic extraction.

The objective function is:

.. math::

    L(W, H) &= 0.5 * ||X - WH||_{loss}^2

            &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1

            &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1

            &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2

            &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,

where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and
:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)

The generic norm :math:`||X - WH||_{loss}^2` may represent
the Frobenius norm or another supported beta-divergence loss.
The choice between options is controlled by the `beta_loss` parameter.

The regularization terms are scaled by `n_features` for `W` and by `n_samples` for
`H` to keep their impact balanced with respect to one another and to the data fit
term as independent as possible of the size `n_samples` of the training set.

The objective function is minimized with an alternating minimization of W
and H. If H is given and update_H=False, it solves for W only.

Note that the transformed data is named W and the components matrix is named H. In
the NMF literature, the naming convention is usually the opposite since the data
matrix X is transposed.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Constant matrix.

W : array-like of shape (n_samples, n_components), default=None
    If `init='custom'`, it is used as initial guess for the solution.
    If `update_H=False`, it is initialised as an array of zeros, unless
    `solver='mu'`, then it is filled with values calculated by
    `np.sqrt(X.mean() / self._n_components)`.
    If `None`, uses the initialisation method specified in `init`.

H : array-like of shape (n_components, n_features), default=None
    If `init='custom'`, it is used as initial guess for the solution.
    If `update_H=False`, it is used as a constant, to solve for W only.
    If `None`, uses the initialisation method specified in `init`.

n_components : int or {'auto'} or None, default='auto'
    Number of components. If `None`, all features are kept.
    If `n_components='auto'`, the number of components is automatically inferred
    from `W` or `H` shapes.

    .. versionchanged:: 1.4
        Added `'auto'` value.

    .. versionchanged:: 1.6
        Default value changed from `None` to `'auto'`.

init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
    Method used to initialize the procedure.

    Valid options:

    - None: 'nndsvda' if n_components < n_features, otherwise 'random'.
    - 'random': non-negative random matrices, scaled with:
      `sqrt(X.mean() / n_components)`
    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
      initialization (better for sparseness)
    - 'nndsvda': NNDSVD with zeros filled with the average of X
      (better when sparsity is not desired)
    - 'nndsvdar': NNDSVD with zeros filled with small random values
      (generally faster, less accurate alternative to NNDSVDa
      for when sparsity is not desired)
    - 'custom': If `update_H=True`, use custom matrices W and H which must both
      be provided. If `update_H=False`, then only custom matrix H is used.

    .. versionchanged:: 0.23
        The default value of `init` changed from 'random' to None in 0.23.

    .. versionchanged:: 1.1
        When `init=None` and n_components is less than n_samples and n_features
        defaults to `nndsvda` instead of `nndsvd`.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

solver : {'cd', 'mu'}, default='cd'
    Numerical solver to use:

    - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical
      Alternating Least Squares (Fast HALS).
    - 'mu' is a Multiplicative Update solver.

    .. versionadded:: 0.17
       Coordinate Descent solver.

    .. versionadded:: 0.19
       Multiplicative Update solver.

beta_loss : float or {'frobenius', 'kullback-leibler', \
        'itakura-saito'}, default='frobenius'
    Beta divergence to be minimized, measuring the distance between X
    and the dot product WH. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input
    matrix X cannot contain zeros. Used only in 'mu' solver.

    .. versionadded:: 0.19

tol : float, default=1e-4
    Tolerance of the stopping condition.

max_iter : int, default=200
    Maximum number of iterations before timing out.

alpha_W : float, default=0.0
    Constant that multiplies the regularization terms of `W`. Set it to zero
    (default) to have no regularization on `W`.

    .. versionadded:: 1.0

alpha_H : float or "same", default="same"
    Constant that multiplies the regularization terms of `H`. Set it to zero to
    have no regularization on `H`. If "same" (default), it takes the same value as
    `alpha_W`.

    .. versionadded:: 1.0

l1_ratio : float, default=0.0
    The regularization mixing parameter, with 0 <= l1_ratio <= 1.
    For l1_ratio = 0 the penalty is an elementwise L2 penalty
    (aka Frobenius Norm).
    For l1_ratio = 1 it is an elementwise L1 penalty.
    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.

random_state : int, RandomState instance or None, default=None
    Used for NMF initialisation (when ``init`` == 'nndsvdar' or
    'random'), and in Coordinate Descent. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    The verbosity level.

shuffle : bool, default=False
    If true, randomize the order of coordinates in the CD solver.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    Actual number of iterations.

References
----------
.. [1] :doi:`"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations" <10.1587/transfun.E92.A.708>`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.

.. [2] :doi:`"Algorithms for nonnegative matrix factorization with the
   beta-divergence" <10.1162/NECO_a_00168>`
   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).

Examples
--------
>>> import numpy as np
>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import non_negative_factorization
>>> W, H, n_iter = non_negative_factorization(
...     X, n_components=2, init='random', random_state=0)
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00905}{905}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a6e559cdf3a40c3c84e2f872f8cb79bad}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!norm@{norm}}
\index{norm@{norm}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{norm()}{norm()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a6e559cdf3a40c3c84e2f872f8cb79bad} 
sklearn.\+decomposition.\+\_\+nmf.\+norm (\begin{DoxyParamCaption}\item[{}]{x}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Dot product-based Euclidean norm implementation.

See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/

Parameters
----------
x : array-like
    Vector for which to compute the norm.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00042}{42}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.

\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a4b8fde89e737a10205d4e0480f4b678d}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!trace\_dot@{trace\_dot}}
\index{trace\_dot@{trace\_dot}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{trace\_dot()}{trace\_dot()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a4b8fde89e737a10205d4e0480f4b678d} 
sklearn.\+decomposition.\+\_\+nmf.\+trace\+\_\+dot (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{Y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Trace of np.dot(X, Y.T).

Parameters
----------
X : array-like
    First matrix.
Y : array-like
    Second matrix.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__nmf_8py_source_l00055}{55}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.



Referenced by \mbox{\hyperlink{__nmf_8py_source_l00085}{\+\_\+beta\+\_\+divergence()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1decomposition_1_1__nmf_a9bfb8c23341015a3bcd74d3c3f4a363c}\index{sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}!EPSILON@{EPSILON}}
\index{EPSILON@{EPSILON}!sklearn.decomposition.\_nmf@{sklearn.decomposition.\_nmf}}
\doxysubsubsection{\texorpdfstring{EPSILON}{EPSILON}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1decomposition_1_1__nmf_a9bfb8c23341015a3bcd74d3c3f4a363c} 
sklearn.\+decomposition.\+\_\+nmf.\+EPSILON = np.\+finfo(np.\+float32).eps}



Definition at line \mbox{\hyperlink{__nmf_8py_source_l00039}{39}} of file \mbox{\hyperlink{__nmf_8py_source}{\+\_\+nmf.\+py}}.

