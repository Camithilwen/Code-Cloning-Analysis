\doxysection{sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss Namespace Reference}
\hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss}{}\label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a394f99548ee5e0c18657763c5c1c018d}{loss\+\_\+instance\+\_\+name}} (param)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction}} (loss, n\+\_\+samples, y\+\_\+bound=(-\/100, 100), raw\+\_\+bound=(-\/5, 5), seed=42)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}{numerical\+\_\+derivative}} (func, x, eps)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a5d4a6ec724cf2afd511ada41995b3a37}{test\+\_\+loss\+\_\+boundary}} (loss)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae56255afa73f8bec4bf8a84f72f047a8}{test\+\_\+loss\+\_\+boundary\+\_\+y\+\_\+true}} (loss, y\+\_\+true\+\_\+success, y\+\_\+true\+\_\+fail)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a7a99b30b2769b07502d139824291cd45}{test\+\_\+loss\+\_\+boundary\+\_\+y\+\_\+pred}} (loss, y\+\_\+pred\+\_\+success, y\+\_\+pred\+\_\+fail)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a198aa1c112a68148858fa15453e67345}{test\+\_\+loss\+\_\+on\+\_\+specific\+\_\+values}} (loss, y\+\_\+true, raw\+\_\+prediction, loss\+\_\+true, gradient\+\_\+true, hessian\+\_\+true)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a244ef4411a18107728e1ac4966639003}{test\+\_\+loss\+\_\+dtype}} (loss, readonly\+\_\+memmap, dtype\+\_\+in, dtype\+\_\+out, sample\+\_\+weight, out1, out2, n\+\_\+threads)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a31f0b42e1adf3846ad9d2dd01f347af9}{test\+\_\+loss\+\_\+same\+\_\+as\+\_\+\+C\+\_\+functions}} (loss, sample\+\_\+weight)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a00f38156d3d6c5364582a272001ae958}{test\+\_\+loss\+\_\+gradients\+\_\+are\+\_\+the\+\_\+same}} (loss, sample\+\_\+weight, global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a936efc1d6f952c5be473378a41cbf761}{test\+\_\+sample\+\_\+weight\+\_\+multiplies}} (loss, sample\+\_\+weight, global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ac5868a699406c3aec9683359fd63ebb1}{test\+\_\+graceful\+\_\+squeezing}} (loss)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae10d3bf88c3a64607a239fce5530e35a}{test\+\_\+loss\+\_\+of\+\_\+perfect\+\_\+prediction}} (loss, sample\+\_\+weight)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_af6443b1da2b78215f19dadf136a19716}{test\+\_\+gradients\+\_\+hessians\+\_\+numerically}} (loss, sample\+\_\+weight, global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae2cf5e4c5116a170f359167fcfeb8a54}{test\+\_\+derivatives}} (loss, x0, y\+\_\+true)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ad18e8017ef3e06ab7571a39b5a0a626e}{test\+\_\+loss\+\_\+intercept\+\_\+only}} (loss, sample\+\_\+weight)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a83fe0602eb695611eaee9e0cea74d7aa}{test\+\_\+specific\+\_\+fit\+\_\+intercept\+\_\+only}} (loss, func, random\+\_\+dist, global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2eeea9ba800414d69a0aae766c08d9a2}{test\+\_\+multinomial\+\_\+loss\+\_\+fit\+\_\+intercept\+\_\+only}} ()
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a21382cbbe6025751258b4f95d7ee7bc2}{test\+\_\+multinomial\+\_\+cy\+\_\+gradient}} (global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a399c5812263096939327a59669033bb1}{test\+\_\+binomial\+\_\+and\+\_\+multinomial\+\_\+loss}} (global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a6f4d248074ccac0f2fcda24370abf6e9}{test\+\_\+binomial\+\_\+vs\+\_\+alternative\+\_\+formulation}} (y\+\_\+true, y\+\_\+pred, global\+\_\+dtype)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a64e4f9e9c3d30c779dc1df184f70c62d}{test\+\_\+predict\+\_\+proba}} (loss, global\+\_\+random\+\_\+seed)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aed5e6b902447152bb1f9f5d398830d9d}{test\+\_\+init\+\_\+gradient\+\_\+and\+\_\+hessians}} (loss, sample\+\_\+weight, dtype, order)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a225e895b285451d57c5eb2c0f92642de}{test\+\_\+init\+\_\+gradient\+\_\+and\+\_\+hessian\+\_\+raises}} (loss, params, err\+\_\+msg)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9d437e26e291830a6f7f3b2107f1842}{test\+\_\+loss\+\_\+init\+\_\+parameter\+\_\+validation}} (loss, params, err\+\_\+type, err\+\_\+msg)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_afdc7d3349965971783f33103c7debb69}{test\+\_\+loss\+\_\+pickle}} (loss)
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9bb886015d6009257e9a601b5be95ae}{test\+\_\+tweedie\+\_\+log\+\_\+identity\+\_\+consistency}} (p)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2e547bf309ead91bde7600be03fc01b9}{ALL\+\_\+\+LOSSES}} = list(\+\_\+\+LOSSES.\+values())
\item 
list \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a4f4094434d6c3280473404eba33f0bbb}{LOSS\+\_\+\+INSTANCES}} = \mbox{[}loss() for loss in ALL\+\_\+\+LOSSES\mbox{]}
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a1e75af958ca19de34410a62fbf0ecb9d}{quantile}}
\item 
\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a38f62f770434f1daf34751961f519abd}{power}}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ab59b142e46405d390a88235f892d31d3}{Y\+\_\+\+COMMON\+\_\+\+PARAMS}}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a68afee3ba6c626df0015e63d1e26ed8a}{Y\+\_\+\+TRUE\+\_\+\+PARAMS}}
\item 
list \mbox{\hyperlink{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a9579babf070569c6cd1fd44ae71142dc}{Y\+\_\+\+PRED\+\_\+\+PARAMS}}
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a394f99548ee5e0c18657763c5c1c018d}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!loss\_instance\_name@{loss\_instance\_name}}
\index{loss\_instance\_name@{loss\_instance\_name}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{loss\_instance\_name()}{loss\_instance\_name()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a394f99548ee5e0c18657763c5c1c018d} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+loss\+\_\+instance\+\_\+name (\begin{DoxyParamCaption}\item[{}]{param}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{test__loss_8py_source_l00052}{52}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!numerical\_derivative@{numerical\_derivative}}
\index{numerical\_derivative@{numerical\_derivative}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{numerical\_derivative()}{numerical\_derivative()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a431181b5602a1a7b33b258a69ed3d4a7} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+numerical\+\_\+derivative (\begin{DoxyParamCaption}\item[{}]{func}{, }\item[{}]{x}{, }\item[{}]{eps}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Helper function for numerical (first) derivatives.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00104}{104}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{test__loss_8py_source_l00791}{test\+\_\+gradients\+\_\+hessians\+\_\+numerically()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!random\_y\_true\_raw\_prediction@{random\_y\_true\_raw\_prediction}}
\index{random\_y\_true\_raw\_prediction@{random\_y\_true\_raw\_prediction}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{random\_y\_true\_raw\_prediction()}{random\_y\_true\_raw\_prediction()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa2b1f37b47fcfcbb7332c3669b2ba8a6} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{n\+\_\+samples}{, }\item[{}]{y\+\_\+bound}{ = {\ttfamily (-\/100,~100)}, }\item[{}]{raw\+\_\+bound}{ = {\ttfamily (-\/5,~5)}, }\item[{}]{seed}{ = {\ttfamily 42}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Random generate y_true and raw_prediction in valid range.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00067}{67}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



Referenced by \mbox{\hyperlink{test__loss_8py_source_l00709}{test\+\_\+graceful\+\_\+squeezing()}}, \mbox{\hyperlink{test__loss_8py_source_l00791}{test\+\_\+gradients\+\_\+hessians\+\_\+numerically()}}, \mbox{\hyperlink{test__loss_8py_source_l00386}{test\+\_\+loss\+\_\+dtype()}}, \mbox{\hyperlink{test__loss_8py_source_l00558}{test\+\_\+loss\+\_\+gradients\+\_\+are\+\_\+the\+\_\+same()}}, \mbox{\hyperlink{test__loss_8py_source_l01299}{test\+\_\+loss\+\_\+pickle()}}, \mbox{\hyperlink{test__loss_8py_source_l00478}{test\+\_\+loss\+\_\+same\+\_\+as\+\_\+\+C\+\_\+functions()}}, \mbox{\hyperlink{test__loss_8py_source_l01077}{test\+\_\+multinomial\+\_\+cy\+\_\+gradient()}}, \mbox{\hyperlink{test__loss_8py_source_l01161}{test\+\_\+predict\+\_\+proba()}}, \mbox{\hyperlink{test__loss_8py_source_l00639}{test\+\_\+sample\+\_\+weight\+\_\+multiplies()}}, and \mbox{\hyperlink{test__loss_8py_source_l01317}{test\+\_\+tweedie\+\_\+log\+\_\+identity\+\_\+consistency()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a399c5812263096939327a59669033bb1}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_binomial\_and\_multinomial\_loss@{test\_binomial\_and\_multinomial\_loss}}
\index{test\_binomial\_and\_multinomial\_loss@{test\_binomial\_and\_multinomial\_loss}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_binomial\_and\_multinomial\_loss()}{test\_binomial\_and\_multinomial\_loss()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a399c5812263096939327a59669033bb1} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+binomial\+\_\+and\+\_\+multinomial\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that multinomial loss with n_classes = 2 is the same as binomial loss.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01107}{1107}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a6f4d248074ccac0f2fcda24370abf6e9}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_binomial\_vs\_alternative\_formulation@{test\_binomial\_vs\_alternative\_formulation}}
\index{test\_binomial\_vs\_alternative\_formulation@{test\_binomial\_vs\_alternative\_formulation}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_binomial\_vs\_alternative\_formulation()}{test\_binomial\_vs\_alternative\_formulation()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a6f4d248074ccac0f2fcda24370abf6e9} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+binomial\+\_\+vs\+\_\+alternative\+\_\+formulation (\begin{DoxyParamCaption}\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+pred}{, }\item[{}]{global\+\_\+dtype}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that both formulations of the binomial deviance agree.

Often, the binomial deviance or log loss is written in terms of a variable
z in {-1, +1}, but we use y in {0, 1}, hence z = 2 * y - 1.
ESL II Eq. (10.18):

    -loglike(z, f) = log(1 + exp(-2 * z * f))

Note:
    - ESL 2*f = raw_prediction, hence the factor 2 of ESL disappears.
    - Deviance = -2*loglike + .., but HalfBinomialLoss is half of the
      deviance, hence the factor of 2 cancels in the comparison.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01126}{1126}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae2cf5e4c5116a170f359167fcfeb8a54}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_derivatives@{test\_derivatives}}
\index{test\_derivatives@{test\_derivatives}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_derivatives()}{test\_derivatives()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae2cf5e4c5116a170f359167fcfeb8a54} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+derivatives (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{x0}{, }\item[{}]{y\+\_\+true}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that gradients are zero at the minimum of the loss.

We check this on a single value/sample using Halley's method with the
first and second order derivatives computed by the Loss instance.
Note that methods of Loss instances operate on arrays while the newton
root finder expects a scalar or a one-element array for this purpose.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00898}{898}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ac5868a699406c3aec9683359fd63ebb1}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_graceful\_squeezing@{test\_graceful\_squeezing}}
\index{test\_graceful\_squeezing@{test\_graceful\_squeezing}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_graceful\_squeezing()}{test\_graceful\_squeezing()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ac5868a699406c3aec9683359fd63ebb1} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+graceful\+\_\+squeezing (\begin{DoxyParamCaption}\item[{}]{loss}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that reshaped raw_prediction gives same results.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00709}{709}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_af6443b1da2b78215f19dadf136a19716}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_gradients\_hessians\_numerically@{test\_gradients\_hessians\_numerically}}
\index{test\_gradients\_hessians\_numerically@{test\_gradients\_hessians\_numerically}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_gradients\_hessians\_numerically()}{test\_gradients\_hessians\_numerically()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_af6443b1da2b78215f19dadf136a19716} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+gradients\+\_\+hessians\+\_\+numerically (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test gradients and hessians with numerical derivatives.

Gradient should equal the numerical derivatives of the loss function.
Hessians should equal the numerical derivatives of gradients.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00791}{791}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00104}{numerical\+\_\+derivative()}}, and \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a225e895b285451d57c5eb2c0f92642de}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_init\_gradient\_and\_hessian\_raises@{test\_init\_gradient\_and\_hessian\_raises}}
\index{test\_init\_gradient\_and\_hessian\_raises@{test\_init\_gradient\_and\_hessian\_raises}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_init\_gradient\_and\_hessian\_raises()}{test\_init\_gradient\_and\_hessian\_raises()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a225e895b285451d57c5eb2c0f92642de} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+init\+\_\+gradient\+\_\+and\+\_\+hessian\+\_\+raises (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{params}{, }\item[{}]{err\+\_\+msg}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that init_gradient_and_hessian raises errors for invalid input.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01254}{1254}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aed5e6b902447152bb1f9f5d398830d9d}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_init\_gradient\_and\_hessians@{test\_init\_gradient\_and\_hessians}}
\index{test\_init\_gradient\_and\_hessians@{test\_init\_gradient\_and\_hessians}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_init\_gradient\_and\_hessians()}{test\_init\_gradient\_and\_hessians()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aed5e6b902447152bb1f9f5d398830d9d} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+init\+\_\+gradient\+\_\+and\+\_\+hessians (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{dtype}{, }\item[{}]{order}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that init_gradient_and_hessian works as expected.

passing sample_weight to a loss correctly influences the constant_hessian
attribute, and consequently the shape of the hessian array.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01208}{1208}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a5d4a6ec724cf2afd511ada41995b3a37}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_boundary@{test\_loss\_boundary}}
\index{test\_loss\_boundary@{test\_loss\_boundary}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_boundary()}{test\_loss\_boundary()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a5d4a6ec724cf2afd511ada41995b3a37} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+boundary (\begin{DoxyParamCaption}\item[{}]{loss}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test interval ranges of y_true and y_pred in losses.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00119}{119}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a7a99b30b2769b07502d139824291cd45}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_boundary\_y\_pred@{test\_loss\_boundary\_y\_pred}}
\index{test\_loss\_boundary\_y\_pred@{test\_loss\_boundary\_y\_pred}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_boundary\_y\_pred()}{test\_loss\_boundary\_y\_pred()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a7a99b30b2769b07502d139824291cd45} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+boundary\+\_\+y\+\_\+pred (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{y\+\_\+pred\+\_\+success}{, }\item[{}]{y\+\_\+pred\+\_\+fail}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test boundaries of y_pred for loss functions.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00221}{221}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae56255afa73f8bec4bf8a84f72f047a8}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_boundary\_y\_true@{test\_loss\_boundary\_y\_true}}
\index{test\_loss\_boundary\_y\_true@{test\_loss\_boundary\_y\_true}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_boundary\_y\_true()}{test\_loss\_boundary\_y\_true()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae56255afa73f8bec4bf8a84f72f047a8} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+boundary\+\_\+y\+\_\+true (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{y\+\_\+true\+\_\+success}{, }\item[{}]{y\+\_\+true\+\_\+fail}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test boundaries of y_true for loss functions.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00209}{209}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a244ef4411a18107728e1ac4966639003}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_dtype@{test\_loss\_dtype}}
\index{test\_loss\_dtype@{test\_loss\_dtype}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_dtype()}{test\_loss\_dtype()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a244ef4411a18107728e1ac4966639003} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+dtype (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{readonly\+\_\+memmap}{, }\item[{}]{dtype\+\_\+in}{, }\item[{}]{dtype\+\_\+out}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{out1}{, }\item[{}]{out2}{, }\item[{}]{n\+\_\+threads}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test acceptance of dtypes, readonly and writeable arrays in loss functions.

Check that loss accepts if all input arrays are either all float32 or all
float64, and all output arrays are either all float32 or all float64.

Also check that input arrays can be readonly, e.g. memory mapped.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00384}{384}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a00f38156d3d6c5364582a272001ae958}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_gradients\_are\_the\_same@{test\_loss\_gradients\_are\_the\_same}}
\index{test\_loss\_gradients\_are\_the\_same@{test\_loss\_gradients\_are\_the\_same}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_gradients\_are\_the\_same()}{test\_loss\_gradients\_are\_the\_same()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a00f38156d3d6c5364582a272001ae958} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+gradients\+\_\+are\+\_\+the\+\_\+same (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that loss and gradient are the same across different functions.

Also test that output arguments contain correct results.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00558}{558}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9d437e26e291830a6f7f3b2107f1842}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_init\_parameter\_validation@{test\_loss\_init\_parameter\_validation}}
\index{test\_loss\_init\_parameter\_validation@{test\_loss\_init\_parameter\_validation}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_init\_parameter\_validation()}{test\_loss\_init\_parameter\_validation()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9d437e26e291830a6f7f3b2107f1842} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+init\+\_\+parameter\+\_\+validation (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{params}{, }\item[{}]{err\+\_\+type}{, }\item[{}]{err\+\_\+msg}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that loss raises errors for invalid input.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01292}{1292}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ad18e8017ef3e06ab7571a39b5a0a626e}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_intercept\_only@{test\_loss\_intercept\_only}}
\index{test\_loss\_intercept\_only@{test\_loss\_intercept\_only}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_intercept\_only()}{test\_loss\_intercept\_only()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ad18e8017ef3e06ab7571a39b5a0a626e} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that fit_intercept_only returns the argmin of the loss.

Also test that the gradient is zero at the minimum.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00946}{946}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae10d3bf88c3a64607a239fce5530e35a}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_of\_perfect\_prediction@{test\_loss\_of\_perfect\_prediction}}
\index{test\_loss\_of\_perfect\_prediction@{test\_loss\_of\_perfect\_prediction}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_of\_perfect\_prediction()}{test\_loss\_of\_perfect\_prediction()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ae10d3bf88c3a64607a239fce5530e35a} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+of\+\_\+perfect\+\_\+prediction (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test value of perfect predictions.

Loss of y_pred = y_true plus constant_to_optimal_zero should sums up to
zero.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00741}{741}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a198aa1c112a68148858fa15453e67345}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_on\_specific\_values@{test\_loss\_on\_specific\_values}}
\index{test\_loss\_on\_specific\_values@{test\_loss\_on\_specific\_values}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_on\_specific\_values()}{test\_loss\_on\_specific\_values()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a198aa1c112a68148858fa15453e67345} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+on\+\_\+specific\+\_\+values (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{loss\+\_\+true}{, }\item[{}]{gradient\+\_\+true}{, }\item[{}]{hessian\+\_\+true}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test losses, gradients and hessians at specific values.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00349}{349}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_afdc7d3349965971783f33103c7debb69}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_pickle@{test\_loss\_pickle}}
\index{test\_loss\_pickle@{test\_loss\_pickle}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_pickle()}{test\_loss\_pickle()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_afdc7d3349965971783f33103c7debb69} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+pickle (\begin{DoxyParamCaption}\item[{}]{loss}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that losses can be pickled.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01299}{1299}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a31f0b42e1adf3846ad9d2dd01f347af9}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_loss\_same\_as\_C\_functions@{test\_loss\_same\_as\_C\_functions}}
\index{test\_loss\_same\_as\_C\_functions@{test\_loss\_same\_as\_C\_functions}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_loss\_same\_as\_C\_functions()}{test\_loss\_same\_as\_C\_functions()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a31f0b42e1adf3846ad9d2dd01f347af9} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+loss\+\_\+same\+\_\+as\+\_\+\+C\+\_\+functions (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that Python and Cython functions return same results.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00478}{478}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a21382cbbe6025751258b4f95d7ee7bc2}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_multinomial\_cy\_gradient@{test\_multinomial\_cy\_gradient}}
\index{test\_multinomial\_cy\_gradient@{test\_multinomial\_cy\_gradient}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_multinomial\_cy\_gradient()}{test\_multinomial\_cy\_gradient()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a21382cbbe6025751258b4f95d7ee7bc2} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+multinomial\+\_\+cy\+\_\+gradient (\begin{DoxyParamCaption}\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that Multinomial cy_gradient gives the same result as gradient.

CyHalfMultinomialLoss does not inherit from CyLossFunction and has a different API.
As a consequence, the functions like `loss` and `gradient` do not rely on `cy_loss`
and `cy_gradient`.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01077}{1077}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2eeea9ba800414d69a0aae766c08d9a2}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_multinomial\_loss\_fit\_intercept\_only@{test\_multinomial\_loss\_fit\_intercept\_only}}
\index{test\_multinomial\_loss\_fit\_intercept\_only@{test\_multinomial\_loss\_fit\_intercept\_only}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_multinomial\_loss\_fit\_intercept\_only()}{test\_multinomial\_loss\_fit\_intercept\_only()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2eeea9ba800414d69a0aae766c08d9a2} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+multinomial\+\_\+loss\+\_\+fit\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that fit_intercept_only returns the mean functional for CCE.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01054}{1054}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a64e4f9e9c3d30c779dc1df184f70c62d}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_predict\_proba@{test\_predict\_proba}}
\index{test\_predict\_proba@{test\_predict\_proba}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_predict\_proba()}{test\_predict\_proba()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a64e4f9e9c3d30c779dc1df184f70c62d} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+predict\+\_\+proba (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that predict_proba and gradient_proba work as expected.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01161}{1161}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a936efc1d6f952c5be473378a41cbf761}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_sample\_weight\_multiplies@{test\_sample\_weight\_multiplies}}
\index{test\_sample\_weight\_multiplies@{test\_sample\_weight\_multiplies}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_sample\_weight\_multiplies()}{test\_sample\_weight\_multiplies()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a936efc1d6f952c5be473378a41cbf761} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+sample\+\_\+weight\+\_\+multiplies (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test sample weights in loss, gradients and hessians.

Make sure that passing sample weights to loss, gradient and hessian
computation methods is equivalent to multiplying by the weights.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l00639}{639}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a83fe0602eb695611eaee9e0cea74d7aa}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_specific\_fit\_intercept\_only@{test\_specific\_fit\_intercept\_only}}
\index{test\_specific\_fit\_intercept\_only@{test\_specific\_fit\_intercept\_only}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_specific\_fit\_intercept\_only()}{test\_specific\_fit\_intercept\_only()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a83fe0602eb695611eaee9e0cea74d7aa} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+specific\+\_\+fit\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{func}{, }\item[{}]{random\+\_\+dist}{, }\item[{}]{global\+\_\+random\+\_\+seed}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test that fit_intercept_only returns the correct functional.

We test the functional for specific, meaningful distributions, e.g.
squared error estimates the expectation of a probability distribution.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01023}{1023}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9bb886015d6009257e9a601b5be95ae}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!test\_tweedie\_log\_identity\_consistency@{test\_tweedie\_log\_identity\_consistency}}
\index{test\_tweedie\_log\_identity\_consistency@{test\_tweedie\_log\_identity\_consistency}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{test\_tweedie\_log\_identity\_consistency()}{test\_tweedie\_log\_identity\_consistency()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_aa9bb886015d6009257e9a601b5be95ae} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+test\+\_\+tweedie\+\_\+log\+\_\+identity\+\_\+consistency (\begin{DoxyParamCaption}\item[{}]{p}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Test for identical losses when only the link function is different.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{test__loss_8py_source_l01317}{1317}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.



References \mbox{\hyperlink{test__loss_8py_source_l00069}{random\+\_\+y\+\_\+true\+\_\+raw\+\_\+prediction()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2e547bf309ead91bde7600be03fc01b9}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!ALL\_LOSSES@{ALL\_LOSSES}}
\index{ALL\_LOSSES@{ALL\_LOSSES}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{ALL\_LOSSES}{ALL\_LOSSES}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a2e547bf309ead91bde7600be03fc01b9} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+ALL\+\_\+\+LOSSES = list(\+\_\+\+LOSSES.\+values())}



Definition at line \mbox{\hyperlink{test__loss_8py_source_l00033}{33}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a4f4094434d6c3280473404eba33f0bbb}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!LOSS\_INSTANCES@{LOSS\_INSTANCES}}
\index{LOSS\_INSTANCES@{LOSS\_INSTANCES}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{LOSS\_INSTANCES}{LOSS\_INSTANCES}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a4f4094434d6c3280473404eba33f0bbb} 
list sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+LOSS\+\_\+\+INSTANCES = \mbox{[}loss() for loss in ALL\+\_\+\+LOSSES\mbox{]}}



Definition at line \mbox{\hyperlink{test__loss_8py_source_l00035}{35}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a38f62f770434f1daf34751961f519abd}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!power@{power}}
\index{power@{power}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{power}{power}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a38f62f770434f1daf34751961f519abd} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+power}



Definition at line \mbox{\hyperlink{test__loss_8py_source_l00040}{40}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a1e75af958ca19de34410a62fbf0ecb9d}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!quantile@{quantile}}
\index{quantile@{quantile}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{quantile}{quantile}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a1e75af958ca19de34410a62fbf0ecb9d} 
sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+quantile}



Definition at line \mbox{\hyperlink{test__loss_8py_source_l00038}{38}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ab59b142e46405d390a88235f892d31d3}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!Y\_COMMON\_PARAMS@{Y\_COMMON\_PARAMS}}
\index{Y\_COMMON\_PARAMS@{Y\_COMMON\_PARAMS}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{Y\_COMMON\_PARAMS}{Y\_COMMON\_PARAMS}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_ab59b142e46405d390a88235f892d31d3} 
list sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+Y\+\_\+\+COMMON\+\_\+\+PARAMS}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{00003\ \ \ \ \ (HalfSquaredError(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00004\ \ \ \ \ (AbsoluteError(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00005\ \ \ \ \ (PinballLoss(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00006\ \ \ \ \ (HuberLoss(),\ [-\/100,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00007\ \ \ \ \ (HalfPoissonLoss(),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{00008\ \ \ \ \ (HalfGammaLoss(),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{00009\ \ \ \ \ (HalfTweedieLoss(power=-\/3),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00010\ \ \ \ \ (HalfTweedieLoss(power=0),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00011\ \ \ \ \ (HalfTweedieLoss(power=1.5),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{00012\ \ \ \ \ (HalfTweedieLoss(power=2),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{00013\ \ \ \ \ (HalfTweedieLoss(power=3),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{00014\ \ \ \ \ (HalfTweedieLossIdentity(power=-\/3),\ [0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00015\ \ \ \ \ (HalfTweedieLossIdentity(power=0),\ [-\/3,\ -\/0.1,\ 0,\ 0.1,\ 100],\ [-\/np.inf,\ np.inf]),}
\DoxyCodeLine{00016\ \ \ \ \ (HalfTweedieLossIdentity(power=1.5),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ np.inf]),}
\DoxyCodeLine{00017\ \ \ \ \ (HalfTweedieLossIdentity(power=2),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{00018\ \ \ \ \ (HalfTweedieLossIdentity(power=3),\ [0.1,\ 100],\ [-\/np.inf,\ -\/3,\ -\/0.1,\ 0,\ np.inf]),}
\DoxyCodeLine{00019\ \ \ \ \ (HalfBinomialLoss(),\ [0.1,\ 0.5,\ 0.9],\ [-\/np.inf,\ -\/1,\ 2,\ np.inf]),}
\DoxyCodeLine{00020\ \ \ \ \ (HalfMultinomialLoss(),\ [],\ [-\/np.inf,\ -\/1,\ 1.1,\ np.inf]),}
\DoxyCodeLine{00021\ ]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{test__loss_8py_source_l00155}{155}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a9579babf070569c6cd1fd44ae71142dc}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!Y\_PRED\_PARAMS@{Y\_PRED\_PARAMS}}
\index{Y\_PRED\_PARAMS@{Y\_PRED\_PARAMS}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{Y\_PRED\_PARAMS}{Y\_PRED\_PARAMS}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a9579babf070569c6cd1fd44ae71142dc} 
list sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+Y\+\_\+\+PRED\+\_\+\+PARAMS}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{00003\ \ \ \ \ (HalfPoissonLoss(),\ [],\ [0]),}
\DoxyCodeLine{00004\ \ \ \ \ (HalfTweedieLoss(power=-\/3),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{00005\ \ \ \ \ (HalfTweedieLoss(power=0),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{00006\ \ \ \ \ (HalfTweedieLoss(power=1.5),\ [],\ [0]),}
\DoxyCodeLine{00007\ \ \ \ \ (HalfTweedieLossIdentity(power=-\/3),\ [],\ [-\/3,\ -\/0.1,\ 0]),}
\DoxyCodeLine{00008\ \ \ \ \ (HalfTweedieLossIdentity(power=0),\ [-\/3,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{00009\ \ \ \ \ (HalfTweedieLossIdentity(power=1.5),\ [],\ [0]),}
\DoxyCodeLine{00010\ \ \ \ \ (HalfBinomialLoss(),\ [],\ [0,\ 1]),}
\DoxyCodeLine{00011\ \ \ \ \ (HalfMultinomialLoss(),\ [0.1,\ 0.5],\ [0,\ 1]),}
\DoxyCodeLine{00012\ ]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{test__loss_8py_source_l00191}{191}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

\Hypertarget{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a68afee3ba6c626df0015e63d1e26ed8a}\index{sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}!Y\_TRUE\_PARAMS@{Y\_TRUE\_PARAMS}}
\index{Y\_TRUE\_PARAMS@{Y\_TRUE\_PARAMS}!sklearn.\_loss.tests.test\_loss@{sklearn.\_loss.tests.test\_loss}}
\doxysubsubsection{\texorpdfstring{Y\_TRUE\_PARAMS}{Y\_TRUE\_PARAMS}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1__loss_1_1tests_1_1test__loss_a68afee3ba6c626df0015e63d1e26ed8a} 
list sklearn.\+\_\+loss.\+tests.\+test\+\_\+loss.\+Y\+\_\+\+TRUE\+\_\+\+PARAMS}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ [\ \ \textcolor{comment}{\#\ type:\ ignore[var-\/annotated]}}
\DoxyCodeLine{00002\ \ \ \ \ \textcolor{comment}{\#\ (loss,\ [y\ success],\ [y\ fail])}}
\DoxyCodeLine{00003\ \ \ \ \ (HalfPoissonLoss(),\ [0],\ []),}
\DoxyCodeLine{00004\ \ \ \ \ (HuberLoss(),\ [0],\ []),}
\DoxyCodeLine{00005\ \ \ \ \ (HalfTweedieLoss(power=-\/3),\ [-\/100,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{00006\ \ \ \ \ (HalfTweedieLoss(power=0),\ [-\/100,\ 0],\ []),}
\DoxyCodeLine{00007\ \ \ \ \ (HalfTweedieLoss(power=1.5),\ [0],\ []),}
\DoxyCodeLine{00008\ \ \ \ \ (HalfTweedieLossIdentity(power=-\/3),\ [-\/100,\ -\/0.1,\ 0],\ []),}
\DoxyCodeLine{00009\ \ \ \ \ (HalfTweedieLossIdentity(power=0),\ [-\/100,\ 0],\ []),}
\DoxyCodeLine{00010\ \ \ \ \ (HalfTweedieLossIdentity(power=1.5),\ [0],\ []),}
\DoxyCodeLine{00011\ \ \ \ \ (HalfBinomialLoss(),\ [0,\ 1],\ []),}
\DoxyCodeLine{00012\ \ \ \ \ (HalfMultinomialLoss(),\ [0.0,\ 1.0,\ 2],\ []),}
\DoxyCodeLine{00013\ ]}

\end{DoxyCode}


Definition at line \mbox{\hyperlink{test__loss_8py_source_l00178}{178}} of file \mbox{\hyperlink{test__loss_8py_source}{test\+\_\+loss.\+py}}.

