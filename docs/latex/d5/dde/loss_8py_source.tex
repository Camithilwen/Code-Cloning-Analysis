\doxysection{loss.\+py}
\hypertarget{loss_8py_source}{}\label{loss_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/\_loss/loss.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/\_loss/loss.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1__loss_1_1loss}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00002}00002\ \textcolor{stringliteral}{This\ module\ contains\ loss\ classes\ suitable\ for\ fitting.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00003}00003\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00004}00004\ \textcolor{stringliteral}{It\ is\ not\ part\ of\ the\ public\ API.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00005}00005\ \textcolor{stringliteral}{Specific\ losses\ are\ used\ for\ regression,\ binary\ classification\ or\ multiclass}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00006}00006\ \textcolor{stringliteral}{classification.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00007}00007\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00008}00008\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00009}00009\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00010}00010\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00011}00011\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00012}00012\ \textcolor{comment}{\#\ Goals:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00013}00013\ \textcolor{comment}{\#\ -\/\ Provide\ a\ common\ private\ module\ for\ loss\ functions/classes.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00014}00014\ \textcolor{comment}{\#\ -\/\ To\ be\ used\ in:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00015}00015\ \textcolor{comment}{\#\ \ \ -\/\ LogisticRegression}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00016}00016\ \textcolor{comment}{\#\ \ \ -\/\ PoissonRegressor,\ GammaRegressor,\ TweedieRegressor}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00017}00017\ \textcolor{comment}{\#\ \ \ -\/\ HistGradientBoostingRegressor,\ HistGradientBoostingClassifier}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00018}00018\ \textcolor{comment}{\#\ \ \ -\/\ GradientBoostingRegressor,\ GradientBoostingClassifier}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00019}00019\ \textcolor{comment}{\#\ \ \ -\/\ SGDRegressor,\ SGDClassifier}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00020}00020\ \textcolor{comment}{\#\ -\/\ Replace\ link\ module\ of\ GLMs.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00021}00021\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00022}00022\ \textcolor{keyword}{import}\ numbers}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00023}00023\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00024}00024\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00025}00025\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacescipy_1_1special}{scipy.special}}\ \textcolor{keyword}{import}\ xlogy}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00026}00026\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00027}00027\ \textcolor{keyword}{from}\ ..utils\ \textcolor{keyword}{import}\ check\_scalar}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00028}00028\ \textcolor{keyword}{from}\ ..utils.stats\ \textcolor{keyword}{import}\ \_weighted\_percentile}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00029}00029\ \textcolor{keyword}{from}\ .\_loss\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00030}00030\ \ \ \ \ CyAbsoluteError,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00031}00031\ \ \ \ \ CyExponentialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00032}00032\ \ \ \ \ CyHalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00033}00033\ \ \ \ \ CyHalfGammaLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00034}00034\ \ \ \ \ CyHalfMultinomialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00035}00035\ \ \ \ \ CyHalfPoissonLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00036}00036\ \ \ \ \ CyHalfSquaredError,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00037}00037\ \ \ \ \ CyHalfTweedieLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00038}00038\ \ \ \ \ CyHalfTweedieLossIdentity,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00039}00039\ \ \ \ \ CyHuberLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00040}00040\ \ \ \ \ CyPinballLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00041}00041\ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00042}00042\ \textcolor{keyword}{from}\ .link\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00043}00043\ \ \ \ \ HalfLogitLink,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00044}00044\ \ \ \ \ IdentityLink,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00045}00045\ \ \ \ \ Interval,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00046}00046\ \ \ \ \ LogitLink,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00047}00047\ \ \ \ \ LogLink,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00048}00048\ \ \ \ \ MultinomialLogit,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00049}00049\ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00050}00050\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00051}00051\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00052}00052\ \textcolor{comment}{\#\ Note:\ The\ shape\ of\ raw\_prediction\ for\ multiclass\ classifications\ are}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00053}00053\ \textcolor{comment}{\#\ -\/\ GradientBoostingClassifier:\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00054}00054\ \textcolor{comment}{\#\ -\/\ HistGradientBoostingClassifier:\ (n\_classes,\ n\_samples)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00055}00055\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00056}00056\ \textcolor{comment}{\#\ Note:\ Instead\ of\ inheritance\ like}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00057}00057\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00058}00058\ \textcolor{comment}{\#\ \ \ \ class\ BaseLoss(BaseLink,\ CyLossFunction):}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00059}00059\ \textcolor{comment}{\#\ \ \ \ ...}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00060}00060\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00061}00061\ \textcolor{comment}{\#\ \ \ \ \#\ Note:\ Naturally,\ we\ would\ inherit\ in\ the\ following\ order}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00062}00062\ \textcolor{comment}{\#\ \ \ \ \#\ \ \ \ \ class\ HalfSquaredError(IdentityLink,\ CyHalfSquaredError,\ BaseLoss)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00063}00063\ \textcolor{comment}{\#\ \ \ \ \#\ \ \ But\ because\ of\ https://github.com/cython/cython/issues/4350\ we\ set\ BaseLoss\ as}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00064}00064\ \textcolor{comment}{\#\ \ \ \ \#\ \ \ the\ last\ one.\ This,\ of\ course,\ changes\ the\ MRO.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00065}00065\ \textcolor{comment}{\#\ \ \ \ class\ HalfSquaredError(IdentityLink,\ CyHalfSquaredError,\ BaseLoss):}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00066}00066\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00067}00067\ \textcolor{comment}{\#\ we\ use\ composition.\ This\ way\ we\ improve\ maintainability\ by\ avoiding\ the\ above}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00068}00068\ \textcolor{comment}{\#\ mentioned\ Cython\ edge\ case\ and\ have\ easier\ to\ understand\ code\ (which\ method\ calls}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00069}00069\ \textcolor{comment}{\#\ which\ code).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00070}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{00070}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00071}00071\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Base\ class\ for\ a\ loss\ function\ of\ 1-\/dimensional\ targets.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00072}00072\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00073}00073\ \textcolor{stringliteral}{\ \ \ \ Conventions:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00074}00074\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00075}00075\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ y\_true.shape\ =\ sample\_weight.shape\ =\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00076}00076\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ y\_pred.shape\ =\ raw\_prediction.shape\ =\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00077}00077\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ is\_multiclass\ is\ true\ (multiclass\ classification),\ then}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00078}00078\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ y\_pred.shape\ =\ raw\_prediction.shape\ =\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00079}00079\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ Note\ that\ this\ corresponds\ to\ the\ return\ value\ of\ decision\_function.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00080}00080\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00081}00081\ \textcolor{stringliteral}{\ \ \ \ y\_true,\ y\_pred,\ sample\_weight\ and\ raw\_prediction\ must\ either\ be\ all\ float64}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00082}00082\ \textcolor{stringliteral}{\ \ \ \ or\ all\ float32.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00083}00083\ \textcolor{stringliteral}{\ \ \ \ gradient\ and\ hessian\ must\ be\ either\ both\ float64\ or\ both\ float32.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00084}00084\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00085}00085\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ y\_pred\ =\ link.inverse(raw\_prediction).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00086}00086\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00087}00087\ \textcolor{stringliteral}{\ \ \ \ Specific\ loss\ classes\ can\ inherit\ specific\ link\ classes\ to\ satisfy}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00088}00088\ \textcolor{stringliteral}{\ \ \ \ BaseLink's\ abstractmethods.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00089}00089\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ sample\_weight\ :\ \{None,\ ndarray\}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00093}00093\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ sample\_weight\ is\ None,\ the\ hessian\ might\ be\ constant.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00094}00094\ \textcolor{stringliteral}{\ \ \ \ n\_classes\ :\ \{None,\ int\}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ classes\ for\ classification,\ else\ None.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00096}00096\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00099}00099\ \textcolor{stringliteral}{\ \ \ \ closs:\ CyLossFunction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ link\ :\ BaseLink}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ interval\_y\_true\ :\ Interval}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Valid\ interval\ for\ y\_true}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00103}00103\ \textcolor{stringliteral}{\ \ \ \ interval\_y\_pred\ :\ Interval}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Valid\ Interval\ for\ y\_pred}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ differentiable\ :\ bool}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Indicates\ whether\ or\ not\ loss\ function\ is\ differentiable\ in}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ everywhere.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00108}00108\ \textcolor{stringliteral}{\ \ \ \ need\_update\_leaves\_values\ :\ bool}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00109}00109\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Indicates\ whether\ decision\ trees\ in\ gradient\ boosting\ need\ to\ uptade}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00110}00110\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ leave\ values\ after\ having\ been\ fit\ to\ the\ (negative)\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00111}00111\ \textcolor{stringliteral}{\ \ \ \ approx\_hessian\ :\ bool}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00112}00112\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Indicates\ whether\ the\ hessian\ is\ approximated\ or\ exact.\ If,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00113}00113\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ approximated,\ it\ should\ be\ larger\ or\ equal\ to\ the\ exact\ one.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00114}00114\ \textcolor{stringliteral}{\ \ \ \ constant\_hessian\ :\ bool}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00115}00115\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Indicates\ whether\ the\ hessian\ is\ one\ for\ this\ loss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00116}00116\ \textcolor{stringliteral}{\ \ \ \ is\_multiclass\ :\ bool}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00117}00117\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Indicates\ whether\ n\_classes\ >\ 2\ is\ allowed.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00118}00118\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00119}00119\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00120}00120\ \ \ \ \ \textcolor{comment}{\#\ For\ gradient\ boosted\ decision\ trees:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00121}00121\ \ \ \ \ \textcolor{comment}{\#\ This\ variable\ indicates\ whether\ the\ loss\ requires\ the\ leaves\ values\ to}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00122}00122\ \ \ \ \ \textcolor{comment}{\#\ be\ updated\ once\ the\ tree\ has\ been\ trained.\ The\ trees\ are\ trained\ to}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00123}00123\ \ \ \ \ \textcolor{comment}{\#\ predict\ a\ Newton-\/Raphson\ step\ (see\ grower.\_finalize\_leaf()).\ But\ for}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00124}00124\ \ \ \ \ \textcolor{comment}{\#\ some\ losses\ (e.g.\ least\ absolute\ deviation)\ we\ need\ to\ adjust\ the\ tree}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00125}00125\ \ \ \ \ \textcolor{comment}{\#\ values\ to\ account\ for\ the\ "{}line\ search"{}\ of\ the\ gradient\ descent}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00126}00126\ \ \ \ \ \textcolor{comment}{\#\ procedure.\ See\ the\ original\ paper\ Greedy\ Function\ Approximation:\ A}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00127}00127\ \ \ \ \ \textcolor{comment}{\#\ Gradient\ Boosting\ Machine\ by\ Friedman}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00128}00128\ \ \ \ \ \textcolor{comment}{\#\ (https://statweb.stanford.edu/\string~jhf/ftp/trebst.pdf)\ for\ the\ theory.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00129}00129\ \ \ \ \ differentiable\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00130}00130\ \ \ \ \ need\_update\_leaves\_values\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00131}00131\ \ \ \ \ is\_multiclass\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00132}00132\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00133}00133\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ closs,\ link,\ n\_classes=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}\ =\ closs}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00135}00135\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}\ =\ link}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00136}00136\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\_hessian}}\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00138}00138\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\_classes}}\ =\ n\_classes}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00139}00139\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.inf,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.interval\_y\_pred}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00141}00141\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00142}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{00142}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{in\_y\_true\_range}}(self,\ y):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ True\ if\ y\ is\ in\ the\ valid\ range\ of\ y\_true.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00144}00144\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00145}00145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00146}00146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00147}00147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00148}00148\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00149}00149\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}.includes(y)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00150}00150\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00151}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{00151}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\_y\_pred\_range}}(self,\ y):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00152}00152\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ True\ if\ y\ is\ in\ the\ valid\ range\ of\ y\_pred.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00153}00153\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00154}00154\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00155}00155\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00156}00156\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00157}00157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.includes(y)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00159}00159\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00160}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{00160}}\ \ \ \ \ \textcolor{keyword}{def\ }loss(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00161}00161\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00162}00162\ \ \ \ \ \ \ \ \ y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00163}00163\ \ \ \ \ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ loss\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00167}00167\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ pointwise\ loss\ value\ for\ each\ input.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00169}00169\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00170}00170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00171}00171\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00172}00172\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00173}00173\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00174}00174\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00176}00176\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ result\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00182}00182\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00183}00183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00184}00184\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00185}00185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00186}00186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00187}00187\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ :\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00188}00188\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ loss\ function.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00189}00189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00190}00190\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00191}00191\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00193}00193\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00194}00194\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00195}00195\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00196}00196\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.loss(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00198}00198\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out=loss\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss\_out}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00204}00204\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00205}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{00205}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\_gradient}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00206}00206\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00207}00207\ \ \ \ \ \ \ \ \ y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00208}00208\ \ \ \ \ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00209}00209\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00210}00210\ \ \ \ \ \ \ \ \ loss\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00211}00211\ \ \ \ \ \ \ \ \ gradient\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00212}00212\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00213}00213\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ loss\ and\ gradient\ w.r.t.\ raw\_prediction\ for\ each\ input.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00215}00215\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00216}00216\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00217}00217\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00218}00218\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00219}00219\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00220}00220\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00221}00221\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00222}00222\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00223}00223\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00224}00224\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00225}00225\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00226}00226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ loss\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00227}00227\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00228}00228\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00229}00229\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00230}00230\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ gradient\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00231}00231\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00232}00232\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00233}00233\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00234}00234\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00235}00235\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00236}00236\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00237}00237\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ :\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00238}00238\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ loss\ function.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00239}00239\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00240}00240\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ array\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00241}00241\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00242}00242\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00243}00243\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ loss\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out\ =\ np.empty\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00247}00247\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out\ =\ np.empty\_like(y\_true,\ dtype=gradient\_out.dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(raw\_prediction,\ dtype=loss\_out.dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00251}00251\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ gradient\_out.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ gradient\_out.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00257}00257\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\_gradient}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00260}00260\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out=loss\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00263}00263\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00265}00265\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss\_out,\ gradient\_out}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00267}00267\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00268}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{00268}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00269}00269\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00270}00270\ \ \ \ \ \ \ \ \ y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00271}00271\ \ \ \ \ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00272}00272\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00273}00273\ \ \ \ \ \ \ \ \ gradient\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00274}00274\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00275}00275\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00276}00276\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ gradient\ of\ loss\ w.r.t\ raw\_prediction\ for\ each\ input.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00277}00277\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00278}00278\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00279}00279\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00280}00280\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00281}00281\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00282}00282\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00283}00283\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00284}00284\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00285}00285\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00286}00286\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00287}00287\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00288}00288\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00289}00289\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ result\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00290}00290\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00291}00291\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00292}00292\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00293}00293\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00294}00294\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00295}00295\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00296}00296\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ array\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00297}00297\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00298}00298\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00301}00301\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ gradient\_out.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00306}00306\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ gradient\_out.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00307}00307\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00308}00308\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00311}00311\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00315}00315\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ gradient\_out}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00316}00316\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00317}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{00317}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\_hessian}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00318}00318\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00320}00320\ \ \ \ \ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00321}00321\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00322}00322\ \ \ \ \ \ \ \ \ gradient\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00323}00323\ \ \ \ \ \ \ \ \ hessian\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00325}00325\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ gradient\ and\ hessian\ of\ loss\ w.r.t\ raw\_prediction.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00327}00327\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00328}00328\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00329}00329\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00330}00330\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00331}00331\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00332}00332\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00333}00333\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00334}00334\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00335}00335\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00336}00336\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00337}00337\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00338}00338\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00339}00339\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ gradient\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00340}00340\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00341}00341\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\_out\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00342}00342\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00343}00343\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ hessian\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00344}00344\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00345}00345\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00346}00346\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00347}00347\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00348}00348\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00349}00349\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00350}00350\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ arrays\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00351}00351\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00352}00352\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00353}00353\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\ :\ arrays\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00354}00354\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ hessians.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00355}00355\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00356}00356\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00357}00357\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hessian\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00360}00360\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00361}00361\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(hessian\_out)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00362}00362\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ hessian\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00363}00363\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out\ =\ np.empty\_like(gradient\_out)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00364}00364\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00365}00365\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00367}00367\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ gradient\_out.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00369}00369\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ gradient\_out.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00370}00370\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hessian\_out.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ hessian\_out.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00371}00371\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out\ =\ hessian\_out.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00372}00372\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00373}00373\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\_hessian}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00374}00374\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00375}00375\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00376}00376\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00377}00377\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00378}00378\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\_out=hessian\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00379}00379\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00380}00380\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00381}00381\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ gradient\_out,\ hessian\_out}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00382}00382\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00383}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{00383}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\_\_call\_\_}}(self,\ y\_true,\ raw\_prediction,\ sample\_weight=None,\ n\_threads=1):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00384}00384\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ weighted\ average\ loss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00385}00385\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00386}00386\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00387}00387\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00388}00388\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00389}00389\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00390}00390\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00391}00391\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00392}00392\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00393}00393\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00394}00394\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00395}00395\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00396}00396\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00397}00397\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00398}00398\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00399}00399\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00400}00400\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ :\ float}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00401}00401\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Mean\ or\ averaged\ loss\ function.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00402}00402\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00403}00403\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.average(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00404}00404\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00405}00405\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss\_out=\textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ \ \ \ \ weights=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00413}00413\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00414}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{00414}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{fit\_intercept\_only}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\_prediction\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00416}00416\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00417}00417\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ can\ be\ used\ as\ initial\ estimates\ of\ predictions,\ i.e.\ before\ the}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00418}00418\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ first\ iteration\ in\ fit.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00419}00419\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00420}00420\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00421}00421\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00422}00422\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ array-\/like\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00423}00423\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00424}00424\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00425}00425\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00426}00426\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00427}00427\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00428}00428\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00429}00429\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ numpy\ scalar\ or\ array\ of\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00430}00430\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ predictions\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00431}00431\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ As\ default,\ take\ weighted\ average\ of\ the\ target\ over\ the\ samples}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ axis=0\ and\ then\ transform\ into\ link-\/scale\ (raw\_prediction).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ y\_pred\ =\ np.average(y\_true,\ weights=sample\_weight,\ axis=0)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00435}00435\ \ \ \ \ \ \ \ \ eps\ =\ 10\ *\ np.finfo(y\_pred.dtype).eps}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00436}00436\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.low\ ==\ -\/np.inf:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00438}00438\ \ \ \ \ \ \ \ \ \ \ \ \ a\_min\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.low\_inclusive:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00440}00440\ \ \ \ \ \ \ \ \ \ \ \ \ a\_min\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.low}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00441}00441\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ \ \ \ \ a\_min\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.low\ +\ eps}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00443}00443\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.high\ ==\ np.inf:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ \ \ \ \ a\_max\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00446}00446\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.high\_inclusive:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ \ \ \ \ a\_max\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.high}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00448}00448\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ \ \ \ \ a\_max\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}.high\ -\/\ eps}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00450}00450\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ a\_min\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{and}\ a\_max\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.link(y\_pred)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00453}00453\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00454}00454\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.link(np.clip(y\_pred,\ a\_min,\ a\_max))}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00455}00455\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00456}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{00456}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00457}00457\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Calculate\ term\ dropped\ in\ loss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00458}00458\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00459}00459\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ With\ this\ term\ added,\ the\ loss\ of\ perfect\ predictions\ is\ zero.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00460}00460\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00461}00461\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.zeros\_like(y\_true)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00462}00462\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00463}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{00463}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\_gradient\_and\_hessian}}(self,\ n\_samples,\ dtype=np.float64,\ order="{}F"{}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00464}00464\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Initialize\ arrays\ for\ gradients\ and\ hessians.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00465}00465\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00466}00466\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Unless\ hessians\ are\ constant,\ arrays\ are\ initialized\ with\ undefined\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00467}00467\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00468}00468\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00469}00469\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00470}00470\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_samples\ :\ int}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00471}00471\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ number\ of\ samples,\ usually\ passed\ to\ \`{}fit()\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00472}00472\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ dtype\ :\ \{np.float64,\ np.float32\},\ default=np.float64}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00473}00473\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ dtype\ of\ the\ arrays\ gradient\ and\ hessian.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00474}00474\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ order\ :\ \{'C',\ 'F'\},\ default='F'}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00475}00475\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Order\ of\ the\ arrays\ gradient\ and\ hessian.\ The\ default\ 'F'\ makes\ the\ arrays}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00476}00476\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ contiguous\ along\ samples.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00477}00477\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00478}00478\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00479}00479\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00480}00480\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)\ or\ array\ of\ shape\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00481}00481\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00482}00482\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Empty\ array\ (allocated\ but\ not\ initialized)\ to\ be\ used\ as\ argument}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00483}00483\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ gradient\_out.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00484}00484\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ hessian\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,),\ array\ of\ shape}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00485}00485\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ (n\_samples,\ n\_classes)\ or\ shape\ (1,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00486}00486\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Empty\ (allocated\ but\ not\ initialized)\ array\ to\ be\ used\ as\ argument}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00487}00487\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ hessian\_out.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00488}00488\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ constant\_hessian\ is\ True\ (e.g.\ \`{}HalfSquaredError\`{}),\ the\ array\ is}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00489}00489\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ initialized\ to\ \`{}\`{}1\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00490}00490\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00491}00491\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ dtype\ \textcolor{keywordflow}{not}\ \textcolor{keywordflow}{in}\ (np.float32,\ np.float64):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00492}00492\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00493}00493\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Valid\ options\ for\ 'dtype'\ are\ np.float32\ and\ np.float64.\ "{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00494}00494\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}Got\ dtype=\{dtype\}\ instead."{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00495}00495\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00496}00496\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\_multiclass}}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \ \ \ \ shape\ =\ (n\_samples,\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\_classes}})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ \ \ \ \ shape\ =\ (n\_samples,)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00501}00501\ \ \ \ \ \ \ \ \ gradient\ =\ np.empty(shape=shape,\ dtype=dtype,\ order=order)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00502}00502\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00503}00503\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00504}00504\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ If\ the\ hessians\ are\ constant,\ we\ consider\ them\ equal\ to\ 1.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00505}00505\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ -\/\ This\ is\ correct\ for\ HalfSquaredError}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00506}00506\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ -\/\ For\ AbsoluteError,\ hessians\ are\ actually\ 0,\ but\ they\ are}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00507}00507\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ always\ ignored\ anyway.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00508}00508\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\ =\ np.ones(shape=(1,),\ dtype=dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00509}00509\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00510}00510\ \ \ \ \ \ \ \ \ \ \ \ \ hessian\ =\ np.empty(shape=shape,\ dtype=dtype,\ order=order)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00511}00511\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00512}00512\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ gradient,\ hessian}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00513}00513\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00514}00514\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00515}00515\ \textcolor{comment}{\#\ Note:\ Naturally,\ we\ would\ inherit\ in\ the\ following\ order}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00516}00516\ \textcolor{comment}{\#\ \ \ \ \ \ \ \ \ class\ HalfSquaredError(IdentityLink,\ CyHalfSquaredError,\ BaseLoss)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00517}00517\ \textcolor{comment}{\#\ \ \ \ \ \ \ But\ because\ of\ https://github.com/cython/cython/issues/4350\ we}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00518}00518\ \textcolor{comment}{\#\ \ \ \ \ \ \ set\ BaseLoss\ as\ the\ last\ one.\ This,\ of\ course,\ changes\ the\ MRO.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00519}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{00519}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00520}00520\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ squared\ error\ with\ identity\ link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00521}00521\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00522}00522\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00523}00523\ \textcolor{stringliteral}{\ \ \ \ y\_true\ and\ y\_pred\ all\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00524}00524\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00525}00525\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00526}00526\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00527}00527\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00528}00528\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ squared\ error\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00529}00529\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00530}00530\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ 0.5\ *\ (y\_true\_i\ -\/\ raw\_prediction\_i)**2}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00531}00531\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00532}00532\ \textcolor{stringliteral}{\ \ \ \ The\ factor\ of\ 0.5\ simplifies\ the\ computation\ of\ gradients\ and\ results\ in\ a}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00533}00533\ \textcolor{stringliteral}{\ \ \ \ unit\ hessian\ (and\ is\ consistent\ with\ what\ is\ done\ in\ LightGBM).\ It\ is\ also}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00534}00534\ \textcolor{stringliteral}{\ \ \ \ half\ the\ Normal\ distribution\ deviance.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00535}00535\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00536}00536\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00537}00537\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00538}00538\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(closs=CyHalfSquaredError(),\ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1IdentityLink}{IdentityLink}}())}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}\ =\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00540}00540\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00541}00541\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00542}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError}{00542}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError}{AbsoluteError}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00543}00543\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Absolute\ error\ with\ identity\ link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00544}00544\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00545}00545\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00546}00546\ \textcolor{stringliteral}{\ \ \ \ y\_true\ and\ y\_pred\ all\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00547}00547\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00548}00548\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00549}00549\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00550}00550\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00551}00551\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ the\ absolute\ error\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00552}00552\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00553}00553\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ |y\_true\_i\ -\/\ raw\_prediction\_i|}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00554}00554\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00555}00555\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ exact\ hessian\ =\ 0\ almost\ everywhere\ (except\ at\ one\ point,\ therefore}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00556}00556\ \textcolor{stringliteral}{\ \ \ \ differentiable\ =\ False).\ Optimization\ routines\ like\ in\ HGBT,\ however,\ need\ a}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00557}00557\ \textcolor{stringliteral}{\ \ \ \ hessian\ >\ 0.\ Therefore,\ we\ assign\ 1.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00558}00558\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00559}00559\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00560}00560\ \ \ \ \ differentiable\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00561}00561\ \ \ \ \ need\_update\_leaves\_values\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00562}00562\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00563}00563\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(closs=CyAbsoluteError(),\ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1IdentityLink}{IdentityLink}}())}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\_hessian}}\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}\ =\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00567}00567\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00568}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError_a781e2c4db12449c57511d7a8612fc765}{00568}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError_a781e2c4db12449c57511d7a8612fc765}{fit\_intercept\_only}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00569}00569\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\_prediction\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00570}00570\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00571}00571\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ is\ the\ weighted\ median\ of\ the\ target,\ i.e.\ over\ the\ samples}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00572}00572\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ axis=0.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00573}00573\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00574}00574\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00575}00575\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.median(y\_true,\ axis=0)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00576}00576\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00577}00577\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \_weighted\_percentile(y\_true,\ sample\_weight,\ 50)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00578}00578\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00579}00579\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00580}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{00580}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss}{PinballLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00581}00581\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Quantile\ loss\ aka\ pinball\ loss,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00582}00582\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00583}00583\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00584}00584\ \textcolor{stringliteral}{\ \ \ \ y\_true\ and\ y\_pred\ all\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00585}00585\ \textcolor{stringliteral}{\ \ \ \ quantile\ in\ (0,\ 1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00586}00586\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00587}00587\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00588}00588\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00589}00589\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00590}00590\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ the\ pinball\ loss\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00591}00591\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00592}00592\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ rho\_\{quantile\}(y\_true\_i\ -\/\ raw\_prediction\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00593}00593\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00594}00594\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ rho\_\{quantile\}(u)\ =\ u\ *\ (quantile\ -\/\ 1\_\{u<0\})}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00595}00595\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\ -\/u\ *(1\ -\/\ quantile)\ \ if\ u\ <\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00596}00596\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ u\ *\ quantile\ \ \ \ \ \ \ if\ u\ >=\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00597}00597\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00598}00598\ \textcolor{stringliteral}{\ \ \ \ Note:\ 2\ *\ PinballLoss(quantile=0.5)\ equals\ AbsoluteError().}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00599}00599\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00600}00600\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ exact\ hessian\ =\ 0\ almost\ everywhere\ (except\ at\ one\ point,\ therefore}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00601}00601\ \textcolor{stringliteral}{\ \ \ \ differentiable\ =\ False).\ Optimization\ routines\ like\ in\ HGBT,\ however,\ need\ a}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00602}00602\ \textcolor{stringliteral}{\ \ \ \ hessian\ >\ 0.\ Therefore,\ we\ assign\ 1.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00603}00603\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00604}00604\ \textcolor{stringliteral}{\ \ \ \ Additional\ Attributes}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00605}00605\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00606}00606\ \textcolor{stringliteral}{\ \ \ \ quantile\ :\ float}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00607}00607\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ quantile\ level\ of\ the\ quantile\ to\ be\ estimated.\ Must\ be\ in\ range\ (0,\ 1).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00608}00608\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00609}00609\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00610}00610\ \ \ \ \ differentiable\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00611}00611\ \ \ \ \ need\_update\_leaves\_values\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00612}00612\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00613}00613\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None,\ quantile=0.5):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00614}00614\ \ \ \ \ \ \ \ \ check\_scalar(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00615}00615\ \ \ \ \ \ \ \ \ \ \ \ \ quantile,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00616}00616\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile"{}},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00617}00617\ \ \ \ \ \ \ \ \ \ \ \ \ target\_type=numbers.Real,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00618}00618\ \ \ \ \ \ \ \ \ \ \ \ \ min\_val=0,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00619}00619\ \ \ \ \ \ \ \ \ \ \ \ \ max\_val=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00620}00620\ \ \ \ \ \ \ \ \ \ \ \ \ include\_boundaries=\textcolor{stringliteral}{"{}neither"{}},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00621}00621\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00622}00622\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00623}00623\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyPinballLoss(quantile=float(quantile)),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00624}00624\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1IdentityLink}{IdentityLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00625}00625\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00626}00626\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\_hessian}}\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00627}00627\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}\ =\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00628}00628\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00629}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss_a58d11f3baec2fb7e61e84f040840247f}{00629}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss_a58d11f3baec2fb7e61e84f040840247f}{fit\_intercept\_only}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00630}00630\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\_prediction\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00631}00631\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00632}00632\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ is\ the\ weighted\ median\ of\ the\ target,\ i.e.\ over\ the\ samples}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00633}00633\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ axis=0.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00634}00634\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00635}00635\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00636}00636\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.percentile(y\_true,\ 100\ *\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.quantile,\ axis=0)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00637}00637\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00638}00638\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \_weighted\_percentile(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00639}00639\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true,\ sample\_weight,\ 100\ *\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.quantile}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00640}00640\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00641}00641\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00642}00642\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00643}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{00643}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss}{HuberLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00644}00644\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Huber\ loss,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00645}00645\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00646}00646\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00647}00647\ \textcolor{stringliteral}{\ \ \ \ y\_true\ and\ y\_pred\ all\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00648}00648\ \textcolor{stringliteral}{\ \ \ \ quantile\ in\ (0,\ 1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00649}00649\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00650}00650\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00651}00651\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00652}00652\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00653}00653\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ the\ Huber\ loss\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00654}00654\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00655}00655\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ 1/2\ *\ abserr**2\ \ \ \ \ \ \ \ \ \ \ \ if\ abserr\ <=\ delta}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00656}00656\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ delta\ *\ (abserr\ -\/\ delta/2)\ if\ abserr\ >\ delta}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00657}00657\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00658}00658\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ abserr\ =\ |y\_true\_i\ -\/\ raw\_prediction\_i|}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00659}00659\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ delta\ =\ quantile(abserr,\ self.quantile)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00660}00660\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00661}00661\ \textcolor{stringliteral}{\ \ \ \ Note:\ HuberLoss(quantile=1)\ equals\ HalfSquaredError\ and\ HuberLoss(quantile=0)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00662}00662\ \textcolor{stringliteral}{\ \ \ \ equals\ delta\ *\ (AbsoluteError()\ -\/\ delta/2).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00663}00663\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00664}00664\ \textcolor{stringliteral}{\ \ \ \ Additional\ Attributes}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00665}00665\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00666}00666\ \textcolor{stringliteral}{\ \ \ \ quantile\ :\ float}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00667}00667\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ quantile\ level\ which\ defines\ the\ breaking\ point\ \`{}delta\`{}\ to\ distinguish}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00668}00668\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ between\ absolute\ error\ and\ squared\ error.\ Must\ be\ in\ range\ (0,\ 1).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00669}00669\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00670}00670\ \textcolor{stringliteral}{\ \ \ \ \ Reference}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00671}00671\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00672}00672\ \textcolor{stringliteral}{\ \ \ \ ..\ [1]\ Friedman,\ J.H.\ (2001).\ :doi:\`{}Greedy\ function\ approximation:\ A\ gradient}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00673}00673\ \textcolor{stringliteral}{\ \ \ \ \ \ boosting\ machine\ <10.1214/aos/1013203451>\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00674}00674\ \textcolor{stringliteral}{\ \ \ \ \ \ Annals\ of\ Statistics,\ 29,\ 1189-\/1232.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00675}00675\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00676}00676\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00677}00677\ \ \ \ \ differentiable\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00678}00678\ \ \ \ \ need\_update\_leaves\_values\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00679}00679\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00680}00680\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None,\ quantile=0.9,\ delta=0.5):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00681}00681\ \ \ \ \ \ \ \ \ check\_scalar(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00682}00682\ \ \ \ \ \ \ \ \ \ \ \ \ quantile,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00683}00683\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile"{}},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00684}00684\ \ \ \ \ \ \ \ \ \ \ \ \ target\_type=numbers.Real,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00685}00685\ \ \ \ \ \ \ \ \ \ \ \ \ min\_val=0,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00686}00686\ \ \ \ \ \ \ \ \ \ \ \ \ max\_val=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00687}00687\ \ \ \ \ \ \ \ \ \ \ \ \ include\_boundaries=\textcolor{stringliteral}{"{}neither"{}},}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00688}00688\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00689}00689\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a23d23387c19fe941d5bdd690fa3a4c61}{quantile}}\ =\ quantile\ \ \textcolor{comment}{\#\ This\ is\ better\ stored\ outside\ of\ Cython.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00690}00690\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00691}00691\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyHuberLoss(delta=float(delta)),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00692}00692\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1IdentityLink}{IdentityLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00693}00693\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00694}00694\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\_hessian}}\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00695}00695\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\_hessian}}\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00696}00696\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00697}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01}{00697}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01}{fit\_intercept\_only}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00698}00698\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\_prediction\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00699}00699\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00700}00700\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ is\ the\ weighted\ median\ of\ the\ target,\ i.e.\ over\ the\ samples}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00701}00701\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ axis=0.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00702}00702\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00703}00703\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ See\ formula\ before\ algo\ 4\ in\ Friedman\ (2001),\ but\ we\ apply\ it\ to\ y\_true,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00704}00704\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ not\ to\ the\ residual\ y\_true\ -\/\ raw\_prediction.\ An\ estimator\ like}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00705}00705\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ HistGradientBoostingRegressor\ might\ then\ call\ it\ on\ the\ residual,\ e.g.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00706}00706\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fit\_intercept\_only(y\_true\ -\/\ raw\_prediction).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00707}00707\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00708}00708\ \ \ \ \ \ \ \ \ \ \ \ \ median\ =\ np.percentile(y\_true,\ 50,\ axis=0)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00709}00709\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00710}00710\ \ \ \ \ \ \ \ \ \ \ \ \ median\ =\ \_weighted\_percentile(y\_true,\ sample\_weight,\ 50)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00711}00711\ \ \ \ \ \ \ \ \ diff\ =\ y\_true\ -\/\ median}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00712}00712\ \ \ \ \ \ \ \ \ term\ =\ np.sign(diff)\ *\ np.minimum(self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.delta,\ np.abs(diff))}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ median\ +\ np.average(term,\ weights=sample\_weight)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00714}00714\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00715}00715\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00716}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{00716}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00717}00717\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ Poisson\ deviance\ loss\ with\ log-\/link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00718}00718\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00719}00719\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00720}00720\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ non-\/negative\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00721}00721\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ positive\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00722}00722\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00723}00723\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00724}00724\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ exp(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00725}00725\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00726}00726\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ the\ Poisson\ deviance\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00727}00727\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00728}00728\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ y\_true\_i\ *\ log(y\_true\_i/exp(raw\_prediction\_i))}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00729}00729\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ y\_true\_i\ +\ exp(raw\_prediction\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00730}00730\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00731}00731\ \textcolor{stringliteral}{\ \ \ \ Half\ the\ Poisson\ deviance\ is\ actually\ the\ negative\ log-\/likelihood\ up\ to}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00732}00732\ \textcolor{stringliteral}{\ \ \ \ constant\ terms\ (not\ involving\ raw\_prediction)\ and\ simplifies\ the}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00733}00733\ \textcolor{stringliteral}{\ \ \ \ computation\ of\ the\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00734}00734\ \textcolor{stringliteral}{\ \ \ \ We\ also\ skip\ the\ constant\ term\ \`{}y\_true\_i\ *\ log(y\_true\_i)\ -\/\ y\_true\_i\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00735}00735\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00736}00736\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00737}00737\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00738}00738\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(closs=CyHalfPoissonLoss(),\ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1LogLink}{LogLink}}())}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00739}00739\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{True},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00740}00740\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00741}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss_a0452b3e4776ba0622ff016553c9f0606}{00741}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss_a0452b3e4776ba0622ff016553c9f0606}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00742}00742\ \ \ \ \ \ \ \ \ term\ =\ xlogy(y\_true,\ y\_true)\ -\/\ y\_true}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00743}00743\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00744}00744\ \ \ \ \ \ \ \ \ \ \ \ \ term\ *=\ sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00745}00745\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ term}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00746}00746\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00747}00747\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00748}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{00748}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00749}00749\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ Gamma\ deviance\ loss\ with\ log-\/link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00750}00750\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00751}00751\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00752}00752\ \textcolor{stringliteral}{\ \ \ \ y\_true\ and\ y\_pred\ in\ positive\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00753}00753\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00754}00754\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00755}00755\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ exp(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00756}00756\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00757}00757\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ Gamma\ deviance\ loss\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00758}00758\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00759}00759\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ log(exp(raw\_prediction\_i)/y\_true\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00760}00760\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ y\_true/exp(raw\_prediction\_i)\ -\/\ 1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00761}00761\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00762}00762\ \textcolor{stringliteral}{\ \ \ \ Half\ the\ Gamma\ deviance\ is\ actually\ proportional\ to\ the\ negative\ log-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00763}00763\ \textcolor{stringliteral}{\ \ \ \ likelihood\ up\ to\ constant\ terms\ (not\ involving\ raw\_prediction)\ and}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00764}00764\ \textcolor{stringliteral}{\ \ \ \ simplifies\ the\ computation\ of\ the\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00765}00765\ \textcolor{stringliteral}{\ \ \ \ We\ also\ skip\ the\ constant\ term\ \`{}-\/log(y\_true\_i)\ -\/\ 1\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00766}00766\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00767}00767\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00768}00768\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00769}00769\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(closs=CyHalfGammaLoss(),\ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1LogLink}{LogLink}}())}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00770}00770\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00771}00771\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00772}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss_af4daf777b84688710a0ed01f94954a46}{00772}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss_af4daf777b84688710a0ed01f94954a46}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00773}00773\ \ \ \ \ \ \ \ \ term\ =\ -\/np.log(y\_true)\ -\/\ 1}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00774}00774\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00775}00775\ \ \ \ \ \ \ \ \ \ \ \ \ term\ *=\ sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00776}00776\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ term}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00777}00777\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00778}00778\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00779}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{00779}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss}{HalfTweedieLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00780}00780\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ Tweedie\ deviance\ loss\ with\ log-\/link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00781}00781\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00782}00782\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00783}00783\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ real\ numbers\ for\ power\ <=\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00784}00784\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ non-\/negative\ real\ numbers\ for\ 0\ <\ power\ <\ 2}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00785}00785\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ positive\ real\ numbers\ for\ 2\ <=\ power}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00786}00786\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ positive\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00787}00787\ \textcolor{stringliteral}{\ \ \ \ power\ in\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00788}00788\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00789}00789\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00790}00790\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ exp(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00791}00791\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00792}00792\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ Tweedie\ deviance\ loss\ with\ p=power\ is\ defined}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00793}00793\ \textcolor{stringliteral}{\ \ \ \ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00794}00794\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00795}00795\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ max(y\_true\_i,\ 0)**(2-\/p)\ /\ (1-\/p)\ /\ (2-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00796}00796\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ y\_true\_i\ *\ exp(raw\_prediction\_i)**(1-\/p)\ /\ (1-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00797}00797\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ exp(raw\_prediction\_i)**(2-\/p)\ /\ (2-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00798}00798\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00799}00799\ \textcolor{stringliteral}{\ \ \ \ Taking\ the\ limits\ for\ p=0,\ 1,\ 2\ gives\ HalfSquaredError\ with\ a\ log\ link,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00800}00800\ \textcolor{stringliteral}{\ \ \ \ HalfPoissonLoss\ and\ HalfGammaLoss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00801}00801\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00802}00802\ \textcolor{stringliteral}{\ \ \ \ We\ also\ skip\ constant\ terms,\ but\ those\ are\ different\ for\ p=0,\ 1,\ 2.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00803}00803\ \textcolor{stringliteral}{\ \ \ \ Therefore,\ the\ loss\ is\ not\ continuous\ in\ \`{}power\`{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00804}00804\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00805}00805\ \textcolor{stringliteral}{\ \ \ \ Note\ furthermore\ that\ although\ no\ Tweedie\ distribution\ exists\ for}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00806}00806\ \textcolor{stringliteral}{\ \ \ \ 0\ <\ power\ <\ 1,\ it\ still\ gives\ a\ strictly\ consistent\ scoring\ function\ for}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00807}00807\ \textcolor{stringliteral}{\ \ \ \ the\ expectation.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00808}00808\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00809}00809\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00810}00810\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None,\ power=1.5):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00811}00811\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00812}00812\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyHalfTweedieLoss(power=float(power)),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00813}00813\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1LogLink}{LogLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00814}00814\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00815}00815\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ <=\ 0:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00816}00816\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.inf,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00817}00817\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ <\ 2:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00818}00818\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{True},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00819}00819\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00820}00820\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00821}00821\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00822}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{00822}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00824}00824\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfSquaredError}{HalfSquaredError}}().\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{constant\_to\_optimal\_zero}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00825}00825\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,\ sample\_weight=sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00826}00826\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00827}00827\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00828}00828\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss}{HalfPoissonLoss}}().\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{constant\_to\_optimal\_zero}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00829}00829\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,\ sample\_weight=sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00830}00830\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00831}00831\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ ==\ 2:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00832}00832\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss}{HalfGammaLoss}}().\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{constant\_to\_optimal\_zero}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00833}00833\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,\ sample\_weight=sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00834}00834\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00835}00835\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00836}00836\ \ \ \ \ \ \ \ \ \ \ \ \ p\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00837}00837\ \ \ \ \ \ \ \ \ \ \ \ \ term\ =\ np.power(np.maximum(y\_true,\ 0),\ 2\ -\/\ p)\ /\ (1\ -\/\ p)\ /\ (2\ -\/\ p)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00838}00838\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00839}00839\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ term\ *=\ sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00840}00840\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ term}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00841}00841\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00842}00842\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00843}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{00843}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLossIdentity}{HalfTweedieLossIdentity}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00844}00844\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ Tweedie\ deviance\ loss\ with\ identity\ link,\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00845}00845\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00846}00846\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00847}00847\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ real\ numbers\ for\ power\ <=\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00848}00848\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ non-\/negative\ real\ numbers\ for\ 0\ <\ power\ <\ 2}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00849}00849\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ positive\ real\ numbers\ for\ 2\ <=\ power}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00850}00850\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ positive\ real\ numbers\ for\ power\ !=\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00851}00851\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ real\ numbers\ for\ power\ =\ 0}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00852}00852\ \textcolor{stringliteral}{\ \ \ \ power\ in\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00853}00853\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00854}00854\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00855}00855\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ raw\_prediction}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00856}00856\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00857}00857\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ Tweedie\ deviance\ loss\ with\ p=power\ is\ defined}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00858}00858\ \textcolor{stringliteral}{\ \ \ \ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00859}00859\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00860}00860\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ max(y\_true\_i,\ 0)**(2-\/p)\ /\ (1-\/p)\ /\ (2-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00861}00861\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ y\_true\_i\ *\ raw\_prediction\_i**(1-\/p)\ /\ (1-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00862}00862\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +\ raw\_prediction\_i**(2-\/p)\ /\ (2-\/p)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00863}00863\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00864}00864\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ minimum\ value\ of\ this\ loss\ is\ 0.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00865}00865\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00866}00866\ \textcolor{stringliteral}{\ \ \ \ Note\ furthermore\ that\ although\ no\ Tweedie\ distribution\ exists\ for}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00867}00867\ \textcolor{stringliteral}{\ \ \ \ 0\ <\ power\ <\ 1,\ it\ still\ gives\ a\ strictly\ consistent\ scoring\ function\ for}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00868}00868\ \textcolor{stringliteral}{\ \ \ \ the\ expectation.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00869}00869\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00870}00870\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00871}00871\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None,\ power=1.5):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00872}00872\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00873}00873\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyHalfTweedieLossIdentity(power=float(power)),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00874}00874\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1IdentityLink}{IdentityLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ <=\ 0:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.inf,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00878}00878\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ <\ 2:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00879}00879\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{True},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00880}00880\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00881}00881\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00882}00882\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00883}00883\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.power\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00884}00884\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.inf,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00885}00885\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00886}00886\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00887}00887\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00888}00888\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00889}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{00889}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00890}00890\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Half\ Binomial\ deviance\ loss\ with\ logit\ link,\ for\ binary\ classification.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00891}00891\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00892}00892\ \textcolor{stringliteral}{\ \ \ \ This\ is\ also\ know\ as\ binary\ cross\ entropy,\ log-\/loss\ and\ logistic\ loss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00893}00893\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00894}00894\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00895}00895\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ [0,\ 1],\ i.e.\ regression\ on\ the\ unit\ interval}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00896}00896\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ (0,\ 1),\ i.e.\ boundaries\ excluded}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00897}00897\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00898}00898\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00899}00899\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ expit(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00900}00900\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00901}00901\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ half\ Binomial\ deviance\ is\ defined\ as\ the\ negative}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00902}00902\ \textcolor{stringliteral}{\ \ \ \ log-\/likelihood\ of\ the\ Binomial/Bernoulli\ distribution\ and\ can\ be\ expressed}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00903}00903\ \textcolor{stringliteral}{\ \ \ \ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00904}00904\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00905}00905\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ log(1\ +\ exp(raw\_pred\_i))\ -\/\ y\_true\_i\ *\ raw\_pred\_i}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00906}00906\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00907}00907\ \textcolor{stringliteral}{\ \ \ \ See\ The\ Elements\ of\ Statistical\ Learning,\ by\ Hastie,\ Tibshirani,\ Friedman,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00908}00908\ \textcolor{stringliteral}{\ \ \ \ section\ 4.4.1\ (about\ logistic\ regression).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00909}00909\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00910}00910\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ formulation\ works\ for\ classification,\ y\ =\ \{0,\ 1\},\ as\ well\ as}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00911}00911\ \textcolor{stringliteral}{\ \ \ \ logistic\ regression,\ y\ =\ [0,\ 1].}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00912}00912\ \textcolor{stringliteral}{\ \ \ \ If\ you\ add\ \`{}constant\_to\_optimal\_zero\`{}\ to\ the\ loss,\ you\ get\ half\ the}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00913}00913\ \textcolor{stringliteral}{\ \ \ \ Bernoulli/binomial\ deviance.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00914}00914\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00915}00915\ \textcolor{stringliteral}{\ \ \ \ More\ details:\ Inserting\ the\ predicted\ probability\ y\_pred\ =\ expit(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00916}00916\ \textcolor{stringliteral}{\ \ \ \ in\ the\ loss\ gives\ the\ well\ known::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00917}00917\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00918}00918\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ -\/\ y\_true\_i\ *\ log(y\_pred\_i)\ -\/\ (1\ -\/\ y\_true\_i)\ *\ log(1\ -\/\ y\_pred\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00919}00919\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00920}00920\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00921}00921\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00922}00922\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00923}00923\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyHalfBinomialLoss(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00924}00924\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1LogitLink}{LogitLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00925}00925\ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=2,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00926}00926\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00927}00927\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ 1,\ \textcolor{keyword}{True},\ \textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00928}00928\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00929}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss_aded07705d714690528b90dce6a64c05d}{00929}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss_aded07705d714690528b90dce6a64c05d}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00930}00930\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ This\ is\ non-\/zero\ only\ if\ y\_true\ is\ neither\ 0\ nor\ 1.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00931}00931\ \ \ \ \ \ \ \ \ term\ =\ xlogy(y\_true,\ y\_true)\ +\ xlogy(1\ -\/\ y\_true,\ 1\ -\/\ y\_true)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00932}00932\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00933}00933\ \ \ \ \ \ \ \ \ \ \ \ \ term\ *=\ sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00934}00934\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ term}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00935}00935\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00936}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss_a1d70d91e275994317a261b80adc5e5cb}{00936}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss_a1d70d91e275994317a261b80adc5e5cb}{predict\_proba}}(self,\ raw\_prediction):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00937}00937\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00938}00938\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00939}00939\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00940}00940\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00941}00941\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ array\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ 1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00942}00942\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00943}00943\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00944}00944\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00945}00945\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00946}00946\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proba\ :\ array\ of\ shape\ (n\_samples,\ 2)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00947}00947\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ class\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00948}00948\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00949}00949\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00950}00950\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00951}00951\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00952}00952\ \ \ \ \ \ \ \ \ proba\ =\ np.empty((raw\_prediction.shape[0],\ 2),\ dtype=raw\_prediction.dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00953}00953\ \ \ \ \ \ \ \ \ proba[:,\ 1]\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00954}00954\ \ \ \ \ \ \ \ \ proba[:,\ 0]\ =\ 1\ -\/\ proba[:,\ 1]}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00955}00955\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ proba}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00956}00956\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00957}00957\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00958}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{00958}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00959}00959\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Categorical\ cross-\/entropy\ loss,\ for\ multiclass\ classification.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00960}00960\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00961}00961\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00962}00962\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ \{0,\ 1,\ 2,\ 3,\ ..,\ n\_classes\ -\/\ 1\}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00963}00963\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ has\ n\_classes\ elements,\ each\ element\ in\ (0,\ 1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00964}00964\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00965}00965\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00966}00966\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ softmax(raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00967}00967\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00968}00968\ \textcolor{stringliteral}{\ \ \ \ Note:\ We\ assume\ y\_true\ to\ be\ already\ label\ encoded.\ The\ inverse\ link\ is}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00969}00969\ \textcolor{stringliteral}{\ \ \ \ softmax.\ But\ the\ full\ link\ function\ is\ the\ symmetric\ multinomial\ logit}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00970}00970\ \textcolor{stringliteral}{\ \ \ \ function.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00971}00971\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00972}00972\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ the\ categorical\ cross-\/entropy\ loss\ is\ defined\ as}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00973}00973\ \textcolor{stringliteral}{\ \ \ \ the\ negative\ log-\/likelihood\ of\ the\ multinomial\ distribution,\ it}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00974}00974\ \textcolor{stringliteral}{\ \ \ \ generalizes\ the\ binary\ cross-\/entropy\ to\ more\ than\ 2\ classes::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00975}00975\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00976}00976\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\_i\ =\ log(sum(exp(raw\_pred\_\{i,\ k\}),\ k=0..n\_classes-\/1))}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00977}00977\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ sum(y\_true\_\{i,\ k\}\ *\ raw\_pred\_\{i,\ k\},\ k=0..n\_classes-\/1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00978}00978\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00979}00979\ \textcolor{stringliteral}{\ \ \ \ See\ [1].}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00980}00980\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00981}00981\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ for\ the\ hessian,\ we\ calculate\ only\ the\ diagonal\ part\ in\ the}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00982}00982\ \textcolor{stringliteral}{\ \ \ \ classes:\ If\ the\ full\ hessian\ for\ classes\ k\ and\ l\ and\ sample\ i\ is\ H\_i\_k\_l,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00983}00983\ \textcolor{stringliteral}{\ \ \ \ we\ calculate\ H\_i\_k\_k,\ i.e.\ k=l.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00984}00984\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00985}00985\ \textcolor{stringliteral}{\ \ \ \ Reference}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00986}00986\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00987}00987\ \textcolor{stringliteral}{\ \ \ \ ..\ [1]\ :arxiv:\`{}Simon,\ Noah,\ J.\ Friedman\ and\ T.\ Hastie.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00988}00988\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}A\ Blockwise\ Descent\ Algorithm\ for\ Group-\/penalized\ Multiresponse\ and}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00989}00989\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Multinomial\ Regression"{}.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00990}00990\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ <1311.6529>\`{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00991}00991\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00992}00992\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00993}00993\ \ \ \ \ is\_multiclass\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00994}00994\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00995}00995\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None,\ n\_classes=3):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00996}00996\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00997}00997\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyHalfMultinomialLoss(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00998}00998\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1MultinomialLogit}{MultinomialLogit}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l00999}00999\ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=n\_classes,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01000}01000\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01001}01001\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ np.inf,\ \textcolor{keyword}{True},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01002}01002\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\_y\_pred}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ 1,\ \textcolor{keyword}{False},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01003}01003\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01004}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71}{01004}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71}{in\_y\_true\_range}}(self,\ y):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01005}01005\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ True\ if\ y\ is\ in\ the\ valid\ range\ of\ y\_true.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01006}01006\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01007}01007\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01008}01008\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01009}01009\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01010}01010\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01011}01011\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}.includes(y)\ \textcolor{keywordflow}{and}\ np.all(y.astype(int)\ ==\ y)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01012}01012\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01013}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f}{01013}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f}{fit\_intercept\_only}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01014}01014\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\_prediction\ of\ an\ intercept-\/only\ model.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01015}01015\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01016}01016\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ is\ the\ softmax\ of\ the\ weighted\ average\ of\ the\ target,\ i.e.\ over}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01017}01017\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ samples\ axis=0.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01018}01018\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01019}01019\ \ \ \ \ \ \ \ \ out\ =\ np.zeros(self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\_classes}},\ dtype=y\_true.dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01020}01020\ \ \ \ \ \ \ \ \ eps\ =\ np.finfo(y\_true.dtype).eps}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01021}01021\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\_classes}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01022}01022\ \ \ \ \ \ \ \ \ \ \ \ \ out[k]\ =\ np.average(y\_true\ ==\ k,\ weights=sample\_weight,\ axis=0)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01023}01023\ \ \ \ \ \ \ \ \ \ \ \ \ out[k]\ =\ np.clip(out[k],\ eps,\ 1\ -\/\ eps)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01024}01024\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.link(out[\textcolor{keywordtype}{None},\ :]).reshape(-\/1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01025}01025\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01026}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a1149745c8d4fe3472654d79c60d5f045}{01026}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a1149745c8d4fe3472654d79c60d5f045}{predict\_proba}}(self,\ raw\_prediction):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01027}01027\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01028}01028\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01029}01029\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01030}01030\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01031}01031\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01032}01032\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01033}01033\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01034}01034\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01035}01035\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01036}01036\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proba\ :\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01037}01037\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ class\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01038}01038\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01039}01039\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01040}01040\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01041}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60}{01041}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60}{gradient\_proba}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01042}01042\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01043}01043\ \ \ \ \ \ \ \ \ y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01044}01044\ \ \ \ \ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01045}01045\ \ \ \ \ \ \ \ \ sample\_weight=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01046}01046\ \ \ \ \ \ \ \ \ gradient\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01047}01047\ \ \ \ \ \ \ \ \ proba\_out=None,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01048}01048\ \ \ \ \ \ \ \ \ n\_threads=1,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01049}01049\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01050}01050\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ gradient\ and\ class\ probabilities\ fow\ raw\_prediction.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01051}01051\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01052}01052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01053}01053\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01054}01054\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\_true\ :\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01055}01055\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Observed,\ true\ target\ values.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01056}01056\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01057}01057\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01058}01058\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ None\ or\ C-\/contiguous\ array\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01059}01059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01060}01060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\_out\ :\ None\ or\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01061}01061\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ gradient\ is\ stored.\ If\ None,\ a\ new\ array}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01062}01062\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01063}01063\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proba\_out\ :\ None\ or\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01064}01064\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ A\ location\ into\ which\ the\ class\ probabilities\ are\ stored.\ If\ None,}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01065}01065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ a\ new\ array\ might\ be\ created.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01066}01066\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ n\_threads\ :\ int,\ default=1}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01067}01067\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Might\ use\ openmp\ thread\ parallelism.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01068}01068\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01069}01069\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01070}01070\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01071}01071\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ gradient\ :\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01072}01072\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ gradients.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01073}01073\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01074}01074\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proba\ :\ array\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01075}01075\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ class\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01076}01076\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01077}01077\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ gradient\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01078}01078\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ proba\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01079}01079\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01080}01080\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out\ =\ np.empty\_like(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01081}01081\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01082}01082\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out\ =\ np.empty\_like(proba\_out)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01083}01083\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ proba\_out\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01084}01084\ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out\ =\ np.empty\_like(gradient\_out)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01085}01085\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01086}01086\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}}.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60}{gradient\_proba}}(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01087}01087\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_true,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01088}01088\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_prediction,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01089}01089\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01090}01090\ \ \ \ \ \ \ \ \ \ \ \ \ gradient\_out=gradient\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01091}01091\ \ \ \ \ \ \ \ \ \ \ \ \ proba\_out=proba\_out,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01092}01092\ \ \ \ \ \ \ \ \ \ \ \ \ n\_threads=n\_threads,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01093}01093\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01094}01094\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ gradient\_out,\ proba\_out}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01095}01095\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01096}01096\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01097}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss}{01097}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss}{ExponentialLoss}}(\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{BaseLoss}}):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01098}01098\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Exponential\ loss\ with\ (half)\ logit\ link,\ for\ binary\ classification.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01099}01099\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01100}01100\ \textcolor{stringliteral}{\ \ \ \ This\ is\ also\ know\ as\ boosting\ loss.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01101}01101\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01102}01102\ \textcolor{stringliteral}{\ \ \ \ Domain:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01103}01103\ \textcolor{stringliteral}{\ \ \ \ y\_true\ in\ [0,\ 1],\ i.e.\ regression\ on\ the\ unit\ interval}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01104}01104\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ in\ (0,\ 1),\ i.e.\ boundaries\ excluded}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01105}01105\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01106}01106\ \textcolor{stringliteral}{\ \ \ \ Link:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01107}01107\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ expit(2\ *\ raw\_prediction)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01108}01108\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01109}01109\ \textcolor{stringliteral}{\ \ \ \ For\ a\ given\ sample\ x\_i,\ the\ exponential\ loss\ is\ defined\ as::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01110}01110\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01111}01111\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ y\_true\_i\ *\ exp(-\/raw\_pred\_i))\ +\ (1\ -\/\ y\_true\_i)\ *\ exp(raw\_pred\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01112}01112\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01113}01113\ \textcolor{stringliteral}{\ \ \ \ See:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01114}01114\ \textcolor{stringliteral}{\ \ \ \ -\/\ J.\ Friedman,\ T.\ Hastie,\ R.\ Tibshirani.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01115}01115\ \textcolor{stringliteral}{\ \ \ \ \ \ "{}Additive\ logistic\ regression:\ a\ statistical\ view\ of\ boosting\ (With\ discussion}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01116}01116\ \textcolor{stringliteral}{\ \ \ \ \ \ and\ a\ rejoinder\ by\ the\ authors)."{}\ Ann.\ Statist.\ 28\ (2)\ 337\ -\/\ 407,\ April\ 2000.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01117}01117\ \textcolor{stringliteral}{\ \ \ \ \ \ https://doi.org/10.1214/aos/1016218223}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01118}01118\ \textcolor{stringliteral}{\ \ \ \ -\/\ A.\ Buja,\ W.\ Stuetzle,\ Y.\ Shen.\ (2005).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01119}01119\ \textcolor{stringliteral}{\ \ \ \ \ \ "{}Loss\ Functions\ for\ Binary\ Class\ Probability\ Estimation\ and\ Classification:}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01120}01120\ \textcolor{stringliteral}{\ \ \ \ \ \ Structure\ and\ Applications."{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01121}01121\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01122}01122\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ the\ formulation\ works\ for\ classification,\ y\ =\ \{0,\ 1\},\ as\ well\ as}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01123}01123\ \textcolor{stringliteral}{\ \ \ \ "{}exponential\ logistic"{}\ regression,\ y\ =\ [0,\ 1].}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01124}01124\ \textcolor{stringliteral}{\ \ \ \ Note\ that\ this\ is\ a\ proper\ scoring\ rule,\ but\ without\ it's\ canonical\ link.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01125}01125\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01126}01126\ \textcolor{stringliteral}{\ \ \ \ More\ details:\ Inserting\ the\ predicted\ probability}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01127}01127\ \textcolor{stringliteral}{\ \ \ \ y\_pred\ =\ expit(2\ *\ raw\_prediction)\ in\ the\ loss\ gives::}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01128}01128\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01129}01129\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss(x\_i)\ =\ y\_true\_i\ *\ sqrt((1\ -\/\ y\_pred\_i)\ /\ y\_pred\_i)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01130}01130\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ +\ (1\ -\/\ y\_true\_i)\ *\ sqrt(y\_pred\_i\ /\ (1\ -\/\ y\_pred\_i))}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01131}01131\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01132}01132\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01133}01133\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01134}01134\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01135}01135\ \ \ \ \ \ \ \ \ \ \ \ \ closs=CyExponentialLoss(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01136}01136\ \ \ \ \ \ \ \ \ \ \ \ \ link=\mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1HalfLogitLink}{HalfLogitLink}}(),}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01137}01137\ \ \ \ \ \ \ \ \ \ \ \ \ n\_classes=2,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01138}01138\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01139}01139\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\_y\_true}}\ =\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(0,\ 1,\ \textcolor{keyword}{True},\ \textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01140}01140\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01141}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7}{01141}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7}{constant\_to\_optimal\_zero}}(self,\ y\_true,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01142}01142\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ This\ is\ non-\/zero\ only\ if\ y\_true\ is\ neither\ 0\ nor\ 1.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01143}01143\ \ \ \ \ \ \ \ \ term\ =\ -\/2\ *\ np.sqrt(y\_true\ *\ (1\ -\/\ y\_true))}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01144}01144\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01145}01145\ \ \ \ \ \ \ \ \ \ \ \ \ term\ *=\ sample\_weight}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01146}01146\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ term}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01147}01147\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01148}\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a20f1e01994f889bd868a012a180acc04}{01148}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a20f1e01994f889bd868a012a180acc04}{predict\_proba}}(self,\ raw\_prediction):}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01149}01149\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01150}01150\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01151}01151\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01152}01152\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01153}01153\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_prediction\ :\ array\ of\ shape\ (n\_samples,)\ or\ (n\_samples,\ 1)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01154}01154\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Raw\ prediction\ values\ (in\ link\ space).}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01155}01155\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01156}01156\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01157}01157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01158}01158\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ proba\ :\ array\ of\ shape\ (n\_samples,\ 2)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01159}01159\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Element-\/wise\ class\ probabilities.}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01160}01160\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01161}01161\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Be\ graceful\ to\ shape\ (n\_samples,\ 1)\ -\/>\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01162}01162\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_prediction.ndim\ ==\ 2\ \textcolor{keywordflow}{and}\ raw\_prediction.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01163}01163\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction\ =\ raw\_prediction.squeeze(1)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01164}01164\ \ \ \ \ \ \ \ \ proba\ =\ np.empty((raw\_prediction.shape[0],\ 2),\ dtype=raw\_prediction.dtype)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01165}01165\ \ \ \ \ \ \ \ \ proba[:,\ 1]\ =\ self.\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}}.inverse(raw\_prediction)}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01166}01166\ \ \ \ \ \ \ \ \ proba[:,\ 0]\ =\ 1\ -\/\ proba[:,\ 1]}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01167}01167\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ proba}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01168}01168\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01169}01169\ }
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01170}01170\ \_LOSSES\ =\ \{}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01171}01171\ \ \ \ \ \textcolor{stringliteral}{"{}squared\_error"{}}:\ HalfSquaredError,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01172}01172\ \ \ \ \ \textcolor{stringliteral}{"{}absolute\_error"{}}:\ AbsoluteError,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01173}01173\ \ \ \ \ \textcolor{stringliteral}{"{}pinball\_loss"{}}:\ PinballLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01174}01174\ \ \ \ \ \textcolor{stringliteral}{"{}huber\_loss"{}}:\ HuberLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01175}01175\ \ \ \ \ \textcolor{stringliteral}{"{}poisson\_loss"{}}:\ HalfPoissonLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01176}01176\ \ \ \ \ \textcolor{stringliteral}{"{}gamma\_loss"{}}:\ HalfGammaLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01177}01177\ \ \ \ \ \textcolor{stringliteral}{"{}tweedie\_loss"{}}:\ HalfTweedieLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01178}01178\ \ \ \ \ \textcolor{stringliteral}{"{}binomial\_loss"{}}:\ HalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01179}01179\ \ \ \ \ \textcolor{stringliteral}{"{}multinomial\_loss"{}}:\ HalfMultinomialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01180}01180\ \ \ \ \ \textcolor{stringliteral}{"{}exponential\_loss"{}}:\ ExponentialLoss,}
\DoxyCodeLine{\Hypertarget{loss_8py_source_l01181}01181\ \}}

\end{DoxyCode}
