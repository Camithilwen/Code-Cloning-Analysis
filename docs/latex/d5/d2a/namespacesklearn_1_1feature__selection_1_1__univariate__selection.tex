\doxysection{sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection Namespace Reference}
\hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection}{}\label{namespacesklearn_1_1feature__selection_1_1__univariate__selection}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1__BaseFilter}{\+\_\+\+Base\+Filter}}
\begin{DoxyCompactList}\small\item\em Base classes. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1GenericUnivariateSelect}{Generic\+Univariate\+Select}}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1SelectFdr}{Select\+Fdr}}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1SelectFpr}{Select\+Fpr}}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1SelectFwe}{Select\+Fwe}}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1SelectKBest}{Select\+KBest}}
\item 
class \mbox{\hyperlink{classsklearn_1_1feature__selection_1_1__univariate__selection_1_1SelectPercentile}{Select\+Percentile}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a70745a2c61d07bb19180a2110979be3f}{\+\_\+clean\+\_\+nans}} (scores)
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ace981915c526d804a95d186a927e1a04}{f\+\_\+oneway}} (\texorpdfstring{$\ast$}{*}args)
\begin{DoxyCompactList}\small\item\em Scoring functions. \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a490bd5eefc2e1330c503dacd2ff0cce7}{f\+\_\+classif}} (X, y)
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a252449a3735ac3062f5036b514e26c33}{\+\_\+chisquare}} (f\+\_\+obs, f\+\_\+exp)
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a144d057f54c10af88f150c23a7f98333}{chi2}} (X, y)
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ada752f1b65f1dec32c6ab4a76e7038fc}{r\+\_\+regression}} (X, y, \texorpdfstring{$\ast$}{*}, center=\mbox{\hyperlink{classTrue}{True}}, force\+\_\+finite=\mbox{\hyperlink{classTrue}{True}})
\item 
\mbox{\hyperlink{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ab2c791c69d27eb70231981f8814b1286}{f\+\_\+regression}} (X, y, \texorpdfstring{$\ast$}{*}, center=\mbox{\hyperlink{classTrue}{True}}, force\+\_\+finite=\mbox{\hyperlink{classTrue}{True}})
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Univariate features selection.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a252449a3735ac3062f5036b514e26c33}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!\_chisquare@{\_chisquare}}
\index{\_chisquare@{\_chisquare}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{\_chisquare()}{\_chisquare()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a252449a3735ac3062f5036b514e26c33} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+\_\+chisquare (\begin{DoxyParamCaption}\item[{}]{f\+\_\+obs}{, }\item[{}]{f\+\_\+exp}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Fast replacement for scipy.stats.chisquare.

Version from https://github.com/scipy/scipy/pull/2525 with additional
optimizations.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00174}{174}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



Referenced by \mbox{\hyperlink{__univariate__selection_8py_source_l00200}{chi2()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a70745a2c61d07bb19180a2110979be3f}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!\_clean\_nans@{\_clean\_nans}}
\index{\_clean\_nans@{\_clean\_nans}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{\_clean\_nans()}{\_clean\_nans()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a70745a2c61d07bb19180a2110979be3f} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+\_\+clean\+\_\+nans (\begin{DoxyParamCaption}\item[{}]{scores}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Fixes Issue #1240: NaNs can't be properly compared, so change them to the
smallest value of scores's dtype. -inf seems to be unreliable.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00022}{22}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



Referenced by \mbox{\hyperlink{__univariate__selection_8py_source_l00787}{sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+Select\+KBest.\+\_\+get\+\_\+support\+\_\+mask()}}, and \mbox{\hyperlink{__univariate__selection_8py_source_l00673}{sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+Select\+Percentile.\+\_\+get\+\_\+support\+\_\+mask()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a144d057f54c10af88f150c23a7f98333}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!chi2@{chi2}}
\index{chi2@{chi2}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{chi2()}{chi2()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a144d057f54c10af88f150c23a7f98333} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+chi2 (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute chi-squared stats between each non-negative feature and class.

This score can be used to select the `n_features` features with the
highest values for the test chi-squared statistic from X, which must
contain only **non-negative integer feature values** such as booleans or frequencies
(e.g., term counts in document classification), relative to the classes.

If some of your features are continuous, you need to bin them, for
example by using :class:`~sklearn.preprocessing.KBinsDiscretizer`.

Recall that the chi-square test measures dependence between stochastic
variables, so using this function "weeds out" the features that are the
most likely to be independent of class and therefore irrelevant for
classification.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Sample vectors.

y : array-like of shape (n_samples,)
    Target vector (class labels).

Returns
-------
chi2 : ndarray of shape (n_features,)
    Chi2 statistics for each feature.

p_values : ndarray of shape (n_features,)
    P-values for each feature.

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
f_regression : F-value between label/feature for regression tasks.

Notes
-----
Complexity of this algorithm is O(n_classes * n_features).

Examples
--------
>>> import numpy as np
>>> from sklearn.feature_selection import chi2
>>> X = np.array([[1, 1, 3],
...               [0, 1, 5],
...               [5, 4, 1],
...               [6, 6, 2],
...               [1, 4, 0],
...               [0, 0, 0]])
>>> y = np.array([1, 1, 0, 0, 2, 2])
>>> chi2_stats, p_values = chi2(X, y)
>>> chi2_stats
array([15.3,  6.5       ,  8.9])
>>> p_values
array([0.000456, 0.0387, 0.0116 ])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00200}{200}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



References \mbox{\hyperlink{__univariate__selection_8py_source_l00174}{\+\_\+chisquare()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a490bd5eefc2e1330c503dacd2ff0cce7}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!f\_classif@{f\_classif}}
\index{f\_classif@{f\_classif}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{f\_classif()}{f\_classif()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_a490bd5eefc2e1330c503dacd2ff0cce7} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+f\+\_\+classif (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the ANOVA F-value for the provided sample.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The set of regressors that will be tested sequentially.

y : array-like of shape (n_samples,)
    The target vector.

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.

Examples
--------
>>> from sklearn.datasets import make_classification
>>> from sklearn.feature_selection import f_classif
>>> X, y = make_classification(
...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,
...     shuffle=False, random_state=42
... )
>>> f_statistic, p_values = f_classif(X, y)
>>> f_statistic
array([2.21e+02, 7.02e-01, 1.70e+00, 9.31e-01,
       5.41e+00, 3.25e-01, 4.71e-02, 5.72e-01,
       7.54e-01, 8.90e-02])
>>> p_values
array([7.14e-27, 4.04e-01, 1.96e-01, 3.37e-01,
       2.21e-02, 5.70e-01, 8.29e-01, 4.51e-01,
       3.87e-01, 7.66e-01])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00125}{125}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



References \mbox{\hyperlink{__univariate__selection_8py_source_l00041}{f\+\_\+oneway()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ace981915c526d804a95d186a927e1a04}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!f\_oneway@{f\_oneway}}
\index{f\_oneway@{f\_oneway}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{f\_oneway()}{f\_oneway()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ace981915c526d804a95d186a927e1a04} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+f\+\_\+oneway (\begin{DoxyParamCaption}\item[{\texorpdfstring{$\ast$}{*}}]{args}{}\end{DoxyParamCaption})}



Scoring functions. 

\begin{DoxyVerb}Perform a 1-way ANOVA.

The one-way ANOVA tests the null hypothesis that 2 or more groups have
the same population mean. The test is applied to samples from two or
more groups, possibly with differing sizes.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
*args : {array-like, sparse matrix}
    Sample1, sample2... The sample measurements should be given as
    arguments.

Returns
-------
f_statistic : float
    The computed F-value of the test.
p_value : float
    The associated p-value from the F-distribution.

Notes
-----
The ANOVA test has important assumptions that must be satisfied in order
for the associated p-value to be valid.

1. The samples are independent
2. Each sample is from a normally distributed population
3. The population standard deviations of the groups are all equal. This
   property is known as homoscedasticity.

If these assumptions are not true for a given set of data, it may still be
possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although
with some loss of power.

The algorithm is from Heiman[2], pp.394-7.

See ``scipy.stats.f_oneway`` that should give the same results while
being less efficient.

References
----------
.. [1] Lowry, Richard.  "Concepts and Applications of Inferential
       Statistics". Chapter 14.
       http://vassarstats.net/textbook

.. [2] Heiman, G.W.  Research Methods in Statistics. 2002.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00041}{41}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



Referenced by \mbox{\hyperlink{__univariate__selection_8py_source_l00125}{f\+\_\+classif()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ab2c791c69d27eb70231981f8814b1286}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!f\_regression@{f\_regression}}
\index{f\_regression@{f\_regression}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{f\_regression()}{f\_regression()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ab2c791c69d27eb70231981f8814b1286} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+f\+\_\+regression (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{center}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{force\+\_\+finite}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Univariate linear regression tests returning F-statistic and p-values.

Quick linear model for testing the effect of a single regressor,
sequentially for many regressors.

This is done in 2 steps:

1. The cross correlation between each regressor and the target is computed
   using :func:`r_regression` as::

       E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))

2. It is converted to an F score and then to a p-value.

:func:`f_regression` is derived from :func:`r_regression` and will rank
features in the same order if all the features are positively correlated
with the target.

Note however that contrary to :func:`f_regression`, :func:`r_regression`
values lie in [-1, 1] and can thus be negative. :func:`f_regression` is
therefore recommended as a feature selection criterion to identify
potentially predictive feature for a downstream classifier, irrespective of
the sign of the association with the target variable.

Furthermore :func:`f_regression` returns p-values while
:func:`r_regression` does not.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data matrix.

y : array-like of shape (n_samples,)
    The target vector.

center : bool, default=True
    Whether or not to center the data matrix `X` and the target vector `y`.
    By default, `X` and `y` will be centered.

force_finite : bool, default=True
    Whether or not to force the F-statistics and associated p-values to
    be finite. There are two cases where the F-statistic is expected to not
    be finite:

    - when the target `y` or some features in `X` are constant. In this
      case, the Pearson's R correlation is not defined leading to obtain
      `np.nan` values in the F-statistic and p-value. When
      `force_finite=True`, the F-statistic is set to `0.0` and the
      associated p-value is set to `1.0`.
    - when a feature in `X` is perfectly correlated (or
      anti-correlated) with the target `y`. In this case, the F-statistic
      is expected to be `np.inf`. When `force_finite=True`, the F-statistic
      is set to `np.finfo(dtype).max` and the associated p-value is set to
      `0.0`.

    .. versionadded:: 1.1

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
r_regression: Pearson's R between label/feature for regression tasks.
f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
SelectPercentile: Select features based on percentile of the highest
    scores.

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.feature_selection import f_regression
>>> X, y = make_regression(
...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
... )
>>> f_statistic, p_values = f_regression(X, y)
>>> f_statistic
array([1.21, 2.67e13, 2.66])
>>> p_values
array([0.276, 1.54e-283, 0.11])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00406}{406}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



References \mbox{\hyperlink{__univariate__selection_8py_source_l00301}{r\+\_\+regression()}}.

\Hypertarget{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ada752f1b65f1dec32c6ab4a76e7038fc}\index{sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}!r\_regression@{r\_regression}}
\index{r\_regression@{r\_regression}!sklearn.feature\_selection.\_univariate\_selection@{sklearn.feature\_selection.\_univariate\_selection}}
\doxysubsubsection{\texorpdfstring{r\_regression()}{r\_regression()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1feature__selection_1_1__univariate__selection_ada752f1b65f1dec32c6ab4a76e7038fc} 
sklearn.\+feature\+\_\+selection.\+\_\+univariate\+\_\+selection.\+r\+\_\+regression (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{center}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{force\+\_\+finite}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute Pearson's r for each features and the target.

Pearson's r is also known as the Pearson correlation coefficient.

Linear model for testing the individual effect of each of many regressors.
This is a scoring function to be used in a feature selection procedure, not
a free standing feature selection procedure.

The cross correlation between each regressor and the target is computed
as::

    E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))

For more on usage see the :ref:`User Guide <univariate_feature_selection>`.

.. versionadded:: 1.0

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data matrix.

y : array-like of shape (n_samples,)
    The target vector.

center : bool, default=True
    Whether or not to center the data matrix `X` and the target vector `y`.
    By default, `X` and `y` will be centered.

force_finite : bool, default=True
    Whether or not to force the Pearson's R correlation to be finite.
    In the particular case where some features in `X` or the target `y`
    are constant, the Pearson's R correlation is not defined. When
    `force_finite=False`, a correlation of `np.nan` is returned to
    acknowledge this case. When `force_finite=True`, this value will be
    forced to a minimal correlation of `0.0`.

    .. versionadded:: 1.1

Returns
-------
correlation_coefficient : ndarray of shape (n_features,)
    Pearson's R correlation coefficients of features.

See Also
--------
f_regression: Univariate linear regression tests returning f-statistic
    and p-values.
mutual_info_regression: Mutual information for a continuous target.
f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.feature_selection import r_regression
>>> X, y = make_regression(
...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
... )
>>> r_regression(X, y)
array([-0.157,  1.        , -0.229])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__univariate__selection_8py_source_l00301}{301}} of file \mbox{\hyperlink{__univariate__selection_8py_source}{\+\_\+univariate\+\_\+selection.\+py}}.



Referenced by \mbox{\hyperlink{__univariate__selection_8py_source_l00406}{f\+\_\+regression()}}.

