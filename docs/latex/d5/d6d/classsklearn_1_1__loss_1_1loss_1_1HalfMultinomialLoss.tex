\doxysection{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss Class Reference}
\hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{}\label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}


Inheritance diagram for sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss\+:
% FIG 0


Collaboration diagram for sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a112b861f422bd859e3d4d06930cb3254}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, sample\+\_\+weight=None, n\+\_\+classes=3)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71}{in\+\_\+y\+\_\+true\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f}{fit\+\_\+intercept\+\_\+only}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a1149745c8d4fe3472654d79c60d5f045}{predict\+\_\+proba}} (self, raw\+\_\+prediction)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60}{gradient\+\_\+proba}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, proba\+\_\+out=None, n\+\_\+threads=1)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, closs, link, n\+\_\+classes=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\+\_\+y\+\_\+pred\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\+\_\+gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\+\_\+hessian}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, hessian\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{constant\+\_\+to\+\_\+optimal\+\_\+zero}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\+\_\+gradient\+\_\+and\+\_\+hessian}} (self, n\+\_\+samples, dtype=np.\+float64, order="{}F"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}} = closs
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}} = link
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\+\_\+hessian}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\+\_\+hessian}} = False
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\+\_\+classes}} = n\+\_\+classes
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\+\_\+y\+\_\+true}} = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\+\_\+y\+\_\+pred}} = self.\+link.\+interval\+\_\+y\+\_\+pred
\end{DoxyCompactItemize}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}{differentiable}} = \mbox{\hyperlink{classTrue}{True}}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}{need\+\_\+update\+\_\+leaves\+\_\+values}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\+\_\+multiclass}} = False
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Categorical cross-entropy loss, for multiclass classification.

Domain:
y_true in {0, 1, 2, 3, .., n_classes - 1}
y_pred has n_classes elements, each element in (0, 1)

Link:
y_pred = softmax(raw_prediction)

Note: We assume y_true to be already label encoded. The inverse link is
softmax. But the full link function is the symmetric multinomial logit
function.

For a given sample x_i, the categorical cross-entropy loss is defined as
the negative log-likelihood of the multinomial distribution, it
generalizes the binary cross-entropy to more than 2 classes::

    loss_i = log(sum(exp(raw_pred_{i, k}), k=0..n_classes-1))
            - sum(y_true_{i, k} * raw_pred_{i, k}, k=0..n_classes-1)

See [1].

Note that for the hessian, we calculate only the diagonal part in the
classes: If the full hessian for classes k and l and sample i is H_i_k_l,
we calculate H_i_k_k, i.e. k=l.

Reference
---------
.. [1] :arxiv:`Simon, Noah, J. Friedman and T. Hastie.
    "A Blockwise Descent Algorithm for Group-penalized Multiresponse and
    Multinomial Regression".
    <1311.6529>`
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00958}{958}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a112b861f422bd859e3d4d06930cb3254}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a112b861f422bd859e3d4d06930cb3254} 
sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{n\+\_\+classes}{ = {\ttfamily 3}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{loss_8py_source_l00995}{995}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}!fit\_intercept\_only@{fit\_intercept\_only}}
\index{fit\_intercept\_only@{fit\_intercept\_only}!sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}
\doxysubsubsection{\texorpdfstring{fit\_intercept\_only()}{fit\_intercept\_only()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f} 
sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+fit\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute raw_prediction of an intercept-only model.

This is the softmax of the weighted average of the target, i.e. over
the samples axis=0.
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l01013}{1013}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{__internal_2cache_8py_source_l00190}{pip.\+\_\+internal.\+cache.\+Cache\+Entry.\+link}}, \mbox{\hyperlink{pip_2__internal_2exceptions_8py_source_l00100}{pip.\+\_\+internal.\+exceptions.\+Diagnostic\+Pip\+Error.\+link}}, \mbox{\hyperlink{candidate_8py_source_l00015}{pip.\+\_\+internal.\+models.\+candidate.\+Installation\+Candidate.\+link}}, \mbox{\hyperlink{pip_2__internal_2req_2constructors_8py_source_l00203}{pip.\+\_\+internal.\+req.\+constructors.\+Requirement\+Parts.\+link}}, \mbox{\hyperlink{req__install_8py_source_l00113}{pip.\+\_\+internal.\+req.\+req\+\_\+install.\+Install\+Requirement.\+link}}, \mbox{\hyperlink{pip_2__vendor_2rich_2style_8py_source_l00418}{pip.\+\_\+vendor.\+rich.\+style.\+Style.\+link}}, \mbox{\hyperlink{loss_8py_source_l00135}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+link}}, and \mbox{\hyperlink{loss_8py_source_l00138}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+n\+\_\+classes}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}!gradient\_proba@{gradient\_proba}}
\index{gradient\_proba@{gradient\_proba}!sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}
\doxysubsubsection{\texorpdfstring{gradient\_proba()}{gradient\_proba()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a0b21efc47c262709b7809dd6ec3f8b60} 
sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+gradient\+\_\+proba (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{gradient\+\_\+out}{ = {\ttfamily None}, }\item[{}]{proba\+\_\+out}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute gradient and class probabilities fow raw_prediction.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : array of shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
gradient_out : None or array of shape (n_samples, n_classes)
    A location into which the gradient is stored. If None, a new array
    might be created.
proba_out : None or array of shape (n_samples, n_classes)
    A location into which the class probabilities are stored. If None,
    a new array might be created.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
gradient : array of shape (n_samples, n_classes)
    Element-wise gradients.

proba : array of shape (n_samples, n_classes)
    Element-wise class probabilities.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l01041}{1041}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+closs}}, and \mbox{\hyperlink{loss_8py_source_l01049}{gradient\+\_\+proba()}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l01049}{gradient\+\_\+proba()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}!in\_y\_true\_range@{in\_y\_true\_range}}
\index{in\_y\_true\_range@{in\_y\_true\_range}!sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}
\doxysubsubsection{\texorpdfstring{in\_y\_true\_range()}{in\_y\_true\_range()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71} 
sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+in\+\_\+y\+\_\+true\+\_\+range (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Return True if y is in the valid range of y_true.

Parameters
----------
y : ndarray
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l01004}{1004}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00139}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+interval\+\_\+y\+\_\+true}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a1149745c8d4fe3472654d79c60d5f045}\index{sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}!predict\_proba@{predict\_proba}}
\index{predict\_proba@{predict\_proba}!sklearn.\_loss.loss.HalfMultinomialLoss@{sklearn.\_loss.loss.HalfMultinomialLoss}}
\doxysubsubsection{\texorpdfstring{predict\_proba()}{predict\_proba()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a1149745c8d4fe3472654d79c60d5f045} 
sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+predict\+\_\+proba (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{raw\+\_\+prediction}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Predict probabilities.

Parameters
----------
raw_prediction : array of shape (n_samples, n_classes)
    Raw prediction values (in link space).

Returns
-------
proba : array of shape (n_samples, n_classes)
    Element-wise class probabilities.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l01026}{1026}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{__internal_2cache_8py_source_l00190}{pip.\+\_\+internal.\+cache.\+Cache\+Entry.\+link}}, \mbox{\hyperlink{pip_2__internal_2exceptions_8py_source_l00100}{pip.\+\_\+internal.\+exceptions.\+Diagnostic\+Pip\+Error.\+link}}, \mbox{\hyperlink{candidate_8py_source_l00015}{pip.\+\_\+internal.\+models.\+candidate.\+Installation\+Candidate.\+link}}, \mbox{\hyperlink{pip_2__internal_2req_2constructors_8py_source_l00203}{pip.\+\_\+internal.\+req.\+constructors.\+Requirement\+Parts.\+link}}, \mbox{\hyperlink{req__install_8py_source_l00113}{pip.\+\_\+internal.\+req.\+req\+\_\+install.\+Install\+Requirement.\+link}}, \mbox{\hyperlink{pip_2__vendor_2rich_2style_8py_source_l00418}{pip.\+\_\+vendor.\+rich.\+style.\+Style.\+link}}, and \mbox{\hyperlink{loss_8py_source_l00135}{sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+link}}.



Referenced by \mbox{\hyperlink{calibration_8py_source_l00520}{sklearn.\+calibration.\+Calibrated\+Classifier\+CV.\+predict()}}, \mbox{\hyperlink{dummy_8py_source_l00252}{sklearn.\+dummy.\+Dummy\+Classifier.\+predict()}}, \mbox{\hyperlink{__bagging_8py_source_l00937}{sklearn.\+ensemble.\+\_\+bagging.\+Bagging\+Classifier.\+predict()}}, \mbox{\hyperlink{__forest_8py_source_l00882}{sklearn.\+ensemble.\+\_\+forest.\+Forest\+Classifier.\+predict()}}, \mbox{\hyperlink{__voting_8py_source_l00407}{sklearn.\+ensemble.\+\_\+voting.\+Voting\+Classifier.\+predict()}}, \mbox{\hyperlink{neighbors_2__classification_8py_source_l00241}{sklearn.\+neighbors.\+\_\+classification.\+KNeighbors\+Classifier.\+predict()}}, \mbox{\hyperlink{neighbors_2__classification_8py_source_l00719}{sklearn.\+neighbors.\+\_\+classification.\+Radius\+Neighbors\+Classifier.\+predict()}}, \mbox{\hyperlink{__label__propagation_8py_source_l00173}{sklearn.\+semi\+\_\+supervised.\+\_\+label\+\_\+propagation.\+Base\+Label\+Propagation.\+predict()}}, \mbox{\hyperlink{discriminant__analysis_8py_source_l00784}{sklearn.\+discriminant\+\_\+analysis.\+Linear\+Discriminant\+Analysis.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{dummy_8py_source_l00402}{sklearn.\+dummy.\+Dummy\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__bagging_8py_source_l01045}{sklearn.\+ensemble.\+\_\+bagging.\+Bagging\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__forest_8py_source_l00969}{sklearn.\+ensemble.\+\_\+forest.\+Forest\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__gb_8py_source_l01685}{sklearn.\+ensemble.\+\_\+gb.\+Gradient\+Boosting\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__weight__boosting_8py_source_l00845}{sklearn.\+ensemble.\+\_\+weight\+\_\+boosting.\+Ada\+Boost\+Classifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__logistic_8py_source_l01473}{sklearn.\+linear\+\_\+model.\+\_\+logistic.\+Logistic\+Regression.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l01358}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+SGDClassifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{multioutput_8py_source_l01109}{sklearn.\+multioutput.\+Classifier\+Chain.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l01333}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+MLPClassifier.\+predict\+\_\+log\+\_\+proba()}}, \mbox{\hyperlink{sklearn_2svm_2__base_8py_source_l00876}{sklearn.\+svm.\+\_\+base.\+Base\+SVC.\+predict\+\_\+log\+\_\+proba()}}, and \mbox{\hyperlink{tree_2__classes_8py_source_l01069}{sklearn.\+tree.\+\_\+classes.\+Decision\+Tree\+Classifier.\+predict\+\_\+log\+\_\+proba()}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/\+\_\+loss/loss.\+py\end{DoxyCompactItemize}
