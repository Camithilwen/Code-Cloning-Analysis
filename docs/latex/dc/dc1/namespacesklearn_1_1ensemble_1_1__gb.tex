\doxysection{sklearn.\+ensemble.\+\_\+gb Namespace Reference}
\hypertarget{namespacesklearn_1_1ensemble_1_1__gb}{}\label{namespacesklearn_1_1ensemble_1_1__gb}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting}{Base\+Gradient\+Boosting}}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier}{Gradient\+Boosting\+Classifier}}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor}{Gradient\+Boosting\+Regressor}}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter}{Verbose\+Reporter}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{\+\_\+safe\+\_\+divide}} (numerator, denominator)
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}{\+\_\+init\+\_\+raw\+\_\+predictions}} (X, estimator, loss, use\+\_\+predict\+\_\+proba)
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476}{\+\_\+update\+\_\+terminal\+\_\+regions}} (loss, tree, X, y, neg\+\_\+gradient, raw\+\_\+prediction, sample\+\_\+weight, sample\+\_\+mask, learning\+\_\+rate=0.\+1, k=0)
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6}{set\+\_\+huber\+\_\+delta}} (loss, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a9297c6c6b7e0a53f66a9d9c1ea711da6}{\+\_\+\+LOSSES}} = \+\_\+\+LOSSES.\+copy()
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Gradient Boosted Regression Trees.

This module contains methods for fitting gradient boosted regression trees for
both classification and regression.

The module structure is the following:

- The ``BaseGradientBoosting`` base class implements a common ``fit`` method
  for all the estimators in the module. Regression and classification
  only differ in the concrete ``LossFunction`` used.

- ``GradientBoostingClassifier`` implements gradient boosting for
  classification problems.

- ``GradientBoostingRegressor`` implements gradient boosting for
  regression problems.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}!\_init\_raw\_predictions@{\_init\_raw\_predictions}}
\index{\_init\_raw\_predictions@{\_init\_raw\_predictions}!sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection{\texorpdfstring{\_init\_raw\_predictions()}{\_init\_raw\_predictions()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5} 
sklearn.\+ensemble.\+\_\+gb.\+\_\+init\+\_\+raw\+\_\+predictions (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{estimator}{, }\item[{}]{loss}{, }\item[{}]{use\+\_\+predict\+\_\+proba}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Return the initial raw predictions.

Parameters
----------
X : ndarray of shape (n_samples, n_features)
    The data array.
estimator : object
    The estimator to use to compute the predictions.
loss : BaseLoss
    An instance of a loss function class.
use_predict_proba : bool
    Whether estimator.predict_proba is used instead of estimator.predict.

Returns
-------
raw_predictions : ndarray of shape (n_samples, K)
    The initial raw predictions. K is equal to 1 for binary
    classification and regression, and equal to the number of classes
    for multiclass classification. ``raw_predictions`` is casted
    into float64.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__gb_8py_source_l00086}{86}} of file \mbox{\hyperlink{__gb_8py_source}{\+\_\+gb.\+py}}.



Referenced by \mbox{\hyperlink{__gb_8py_source_l00948}{sklearn.\+ensemble.\+\_\+gb.\+Base\+Gradient\+Boosting.\+\_\+raw\+\_\+predict\+\_\+init()}}, and \mbox{\hyperlink{__gb_8py_source_l00615}{sklearn.\+ensemble.\+\_\+gb.\+Base\+Gradient\+Boosting.\+fit()}}.

\Hypertarget{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}!\_safe\_divide@{\_safe\_divide}}
\index{\_safe\_divide@{\_safe\_divide}!sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection{\texorpdfstring{\_safe\_divide()}{\_safe\_divide()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1} 
sklearn.\+ensemble.\+\_\+gb.\+\_\+safe\+\_\+divide (\begin{DoxyParamCaption}\item[{}]{numerator}{, }\item[{}]{denominator}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Prevents overflow and division by zero.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__gb_8py_source_l00065}{65}} of file \mbox{\hyperlink{__gb_8py_source}{\+\_\+gb.\+py}}.



Referenced by \mbox{\hyperlink{__gb_8py_source_l00139}{\+\_\+update\+\_\+terminal\+\_\+regions()}}.

\Hypertarget{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}!\_update\_terminal\_regions@{\_update\_terminal\_regions}}
\index{\_update\_terminal\_regions@{\_update\_terminal\_regions}!sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection{\texorpdfstring{\_update\_terminal\_regions()}{\_update\_terminal\_regions()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476} 
sklearn.\+ensemble.\+\_\+gb.\+\_\+update\+\_\+terminal\+\_\+regions (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{tree}{, }\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{neg\+\_\+gradient}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{, }\item[{}]{sample\+\_\+mask}{, }\item[{}]{learning\+\_\+rate}{ = {\ttfamily 0.1}, }\item[{}]{k}{ = {\ttfamily 0}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Update the leaf values to be predicted by the tree and raw_prediction.

The current raw predictions of the model (of this stage) are updated.

Additionally, the terminal regions (=leaves) of the given tree are updated as well.
This corresponds to the line search step in "Greedy Function Approximation" by
Friedman, Algorithm 1 step 5.

Update equals:
    argmin_{x} loss(y_true, raw_prediction_old + x * tree.value)

For non-trivial cases like the Binomial loss, the update has no closed formula and
is an approximation, again, see the Friedman paper.

Also note that the update formula for the SquaredError is the identity. Therefore,
in this case, the leaf values don't need an update and only the raw_predictions are
updated (with the learning rate included).

Parameters
----------
loss : BaseLoss
tree : tree.Tree
    The tree object.
X : ndarray of shape (n_samples, n_features)
    The data array.
y : ndarray of shape (n_samples,)
    The target labels.
neg_gradient : ndarray of shape (n_samples,)
    The negative gradient.
raw_prediction : ndarray of shape (n_samples, n_trees_per_iteration)
    The raw predictions (i.e. values from the tree leaves) of the
    tree ensemble at iteration ``i - 1``.
sample_weight : ndarray of shape (n_samples,)
    The weight of each sample.
sample_mask : ndarray of shape (n_samples,)
    The sample mask to be used.
learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by
     ``learning_rate``.
k : int, default=0
    The index of the estimator being updated.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__gb_8py_source_l00128}{128}} of file \mbox{\hyperlink{__gb_8py_source}{\+\_\+gb.\+py}}.



References \mbox{\hyperlink{__gb_8py_source_l00065}{\+\_\+safe\+\_\+divide()}}.

\Hypertarget{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}!set\_huber\_delta@{set\_huber\_delta}}
\index{set\_huber\_delta@{set\_huber\_delta}!sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection{\texorpdfstring{set\_huber\_delta()}{set\_huber\_delta()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6} 
sklearn.\+ensemble.\+\_\+gb.\+set\+\_\+huber\+\_\+delta (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculate and set self.closs.delta based on self.quantile.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__gb_8py_source_l00266}{266}} of file \mbox{\hyperlink{__gb_8py_source}{\+\_\+gb.\+py}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1ensemble_1_1__gb_a9297c6c6b7e0a53f66a9d9c1ea711da6}\index{sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}!\_LOSSES@{\_LOSSES}}
\index{\_LOSSES@{\_LOSSES}!sklearn.ensemble.\_gb@{sklearn.ensemble.\_gb}}
\doxysubsubsection{\texorpdfstring{\_LOSSES}{\_LOSSES}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__gb_a9297c6c6b7e0a53f66a9d9c1ea711da6} 
sklearn.\+ensemble.\+\_\+gb.\+\_\+\+LOSSES = \+\_\+\+LOSSES.\+copy()\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__gb_8py_source_l00056}{56}} of file \mbox{\hyperlink{__gb_8py_source}{\+\_\+gb.\+py}}.

