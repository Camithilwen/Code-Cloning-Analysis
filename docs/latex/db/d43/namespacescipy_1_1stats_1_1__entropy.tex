\doxysection{scipy.\+stats.\+\_\+entropy Namespace Reference}
\hypertarget{namespacescipy_1_1stats_1_1__entropy}{}\label{namespacescipy_1_1stats_1_1__entropy}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
np.\+number\texorpdfstring{$\vert$}{|}np.\+ndarray \mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}{entropy}} (np.\+typing.\+Array\+Like pk, np.\+typing.\+Array\+Like\texorpdfstring{$\vert$}{|}None qk=None, float\texorpdfstring{$\vert$}{|}None base=None, int axis=0)
\item 
np.\+number\texorpdfstring{$\vert$}{|}np.\+ndarray \mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}{differential\+\_\+entropy}} (np.\+typing.\+Array\+Like values, \texorpdfstring{$\ast$}{*}, int\texorpdfstring{$\vert$}{|}None window\+\_\+length=None, float\texorpdfstring{$\vert$}{|}None base=None, int axis=0, str method="{}auto"{})
\item 
\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a8da8253d95c4150183c20f4d9f9d95ea}{\+\_\+pad\+\_\+along\+\_\+last\+\_\+axis}} (X, m)
\item 
\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a2a01b018776843abfb9261305cd30c68}{\+\_\+vasicek\+\_\+entropy}} (X, m)
\item 
\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ad777cd154d4b45a745d1ccd812dd3af2}{\+\_\+van\+\_\+es\+\_\+entropy}} (X, m)
\item 
\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ac9b0d2377dc302465e2da02678b76846}{\+\_\+ebrahimi\+\_\+entropy}} (X, m)
\item 
\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_ae2d17de2a6387d3fa0b41294c50f91be}{\+\_\+correa\+\_\+entropy}} (X, m)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a5e66c878132652ec57633b93a8fc0903}{\+\_\+\+\_\+all\+\_\+\+\_\+}} = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}{entropy}}\textquotesingle{}, \textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}{differential\+\_\+entropy}}\textquotesingle{}\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Created on Fri Apr  2 09:06:05 2021

@author: matth
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacescipy_1_1stats_1_1__entropy_ae2d17de2a6387d3fa0b41294c50f91be}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_correa\_entropy@{\_correa\_entropy}}
\index{\_correa\_entropy@{\_correa\_entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_correa\_entropy()}{\_correa\_entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_ae2d17de2a6387d3fa0b41294c50f91be} 
scipy.\+stats.\+\_\+entropy.\+\_\+correa\+\_\+entropy (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{m}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the Correa estimator as described in [6].\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00382}{382}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_ac9b0d2377dc302465e2da02678b76846}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_ebrahimi\_entropy@{\_ebrahimi\_entropy}}
\index{\_ebrahimi\_entropy@{\_ebrahimi\_entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_ebrahimi\_entropy()}{\_ebrahimi\_entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_ac9b0d2377dc302465e2da02678b76846} 
scipy.\+stats.\+\_\+entropy.\+\_\+ebrahimi\+\_\+entropy (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{m}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the Ebrahimi estimator as described in [6].\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00365}{365}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_a8da8253d95c4150183c20f4d9f9d95ea}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_pad\_along\_last\_axis@{\_pad\_along\_last\_axis}}
\index{\_pad\_along\_last\_axis@{\_pad\_along\_last\_axis}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_pad\_along\_last\_axis()}{\_pad\_along\_last\_axis()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_a8da8253d95c4150183c20f4d9f9d95ea} 
scipy.\+stats.\+\_\+entropy.\+\_\+pad\+\_\+along\+\_\+last\+\_\+axis (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{m}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Pad the data for computing the rolling window difference.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00335}{335}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_ad777cd154d4b45a745d1ccd812dd3af2}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_van\_es\_entropy@{\_van\_es\_entropy}}
\index{\_van\_es\_entropy@{\_van\_es\_entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_van\_es\_entropy()}{\_van\_es\_entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_ad777cd154d4b45a745d1ccd812dd3af2} 
scipy.\+stats.\+\_\+entropy.\+\_\+van\+\_\+es\+\_\+entropy (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{m}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the van Es estimator as described in [6].\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00354}{354}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_a2a01b018776843abfb9261305cd30c68}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_vasicek\_entropy@{\_vasicek\_entropy}}
\index{\_vasicek\_entropy@{\_vasicek\_entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_vasicek\_entropy()}{\_vasicek\_entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_a2a01b018776843abfb9261305cd30c68} 
scipy.\+stats.\+\_\+entropy.\+\_\+vasicek\+\_\+entropy (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{m}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the Vasicek estimator as described in [6] Eq. 1.3.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00345}{345}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!differential\_entropy@{differential\_entropy}}
\index{differential\_entropy@{differential\_entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{differential\_entropy()}{differential\_entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6} 
 np.\+number \texorpdfstring{$\vert$}{|} np.\+ndarray scipy.\+stats.\+\_\+entropy.\+differential\+\_\+entropy (\begin{DoxyParamCaption}\item[{np.\+typing.\+Array\+Like}]{values}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{int \texorpdfstring{$\vert$}{|} None }]{window\+\_\+length}{ = {\ttfamily None}, }\item[{float \texorpdfstring{$\vert$}{|} None }]{base}{ = {\ttfamily None}, }\item[{int }]{axis}{ = {\ttfamily 0}, }\item[{str }]{method}{ = {\ttfamily "{}auto"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Given a sample of a distribution, estimate the differential entropy.

Several estimation methods are available using the `method` parameter. By
default, a method is selected based the size of the sample.

Parameters
----------
values : sequence
    Sample from a continuous distribution.
window_length : int, optional
    Window length for computing Vasicek estimate. Must be an integer
    between 1 and half of the sample size. If ``None`` (the default), it
    uses the heuristic value

    .. math::
        \left \lfloor \sqrt{n} + 0.5 \right \rfloor

    where :math:`n` is the sample size. This heuristic was originally
    proposed in [2]_ and has become common in the literature.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the differential entropy is calculated.
    Default is 0.
method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional
    The method used to estimate the differential entropy from the sample.
    Default is ``'auto'``.  See Notes for more information.

Returns
-------
entropy : float
    The calculated differential entropy.

Notes
-----
This function will converge to the true differential entropy in the limit

.. math::
    n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0

The optimal choice of ``window_length`` for a given sample size depends on
the (unknown) distribution. Typically, the smoother the density of the
distribution, the larger the optimal value of ``window_length`` [1]_.

The following options are available for the `method` parameter.

* ``'vasicek'`` uses the estimator presented in [1]_. This is
  one of the first and most influential estimators of differential entropy.
* ``'van es'`` uses the bias-corrected estimator presented in [3]_, which
  is not only consistent but, under some conditions, asymptotically normal.
* ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown
  in simulation to have smaller bias and mean squared error than
  the Vasicek estimator.
* ``'correa'`` uses the estimator presented in [5]_ based on local linear
  regression. In a simulation study, it had consistently smaller mean
  square error than the Vasiceck estimator, but it is more expensive to
  compute.
* ``'auto'`` selects the method automatically (default). Currently,
  this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``
  for moderate sample sizes (11-1000), and ``'vasicek'`` for larger
  samples, but this behavior is subject to change in future versions.

All estimators are implemented as described in [6]_.

References
----------
.. [1] Vasicek, O. (1976). A test for normality based on sample entropy.
       Journal of the Royal Statistical Society:
       Series B (Methodological), 38(1), 54-59.
.. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based
       goodness-of-fit test for exponentiality. Communications in
       Statistics-Theory and Methods, 28(5), 1183-1202.
.. [3] Van Es, B. (1992). Estimating functionals related to a density by a
       class of statistics based on spacings. Scandinavian Journal of
       Statistics, 61-72.
.. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures
       of sample entropy. Statistics & Probability Letters, 20(3), 225-234.
.. [5] Correa, J. C. (1995). A new estimator of entropy. Communications
       in Statistics-Theory and Methods, 24(10), 2439-2449.
.. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.
       Annals of Data Science, 2(2), 231-241.
       https://link.springer.com/article/10.1007/s40745-015-0045-9

Examples
--------
>>> import numpy as np
>>> from scipy.stats import differential_entropy, norm

Entropy of a standard normal distribution:

>>> rng = np.random.default_rng()
>>> values = rng.standard_normal(100)
>>> differential_entropy(values)
1.3407817436640392

Compare with the true entropy:

>>> float(norm.entropy())
1.4189385332046727

For several sample sizes between 5 and 1000, compare the accuracy of
the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,
compare the root mean squared error (over 1000 trials) between the estimate
and the true differential entropy of the distribution.

>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>>
>>>
>>> def rmse(res, expected):
...     '''Root mean squared error'''
...     return np.sqrt(np.mean((res - expected)**2))
>>>
>>>
>>> a, b = np.log10(5), np.log10(1000)
>>> ns = np.round(np.logspace(a, b, 10)).astype(int)
>>> reps = 1000  # number of repetitions for each sample size
>>> expected = stats.expon.entropy()
>>>
>>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}
>>> for method in method_errors:
...     for n in ns:
...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)
...        res = stats.differential_entropy(rvs, method=method, axis=-1)
...        error = rmse(res, expected)
...        method_errors[method].append(error)
>>>
>>> for method, errors in method_errors.items():
...     plt.loglog(ns, errors, label=method)
>>>
>>> plt.legend()
>>> plt.xlabel('sample size')
>>> plt.ylabel('RMSE (1000 trials)')
>>> plt.title('Entropy Estimator Error (Exponential Distribution)')
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00147}{147}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

\Hypertarget{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!entropy@{entropy}}
\index{entropy@{entropy}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{entropy()}{entropy()}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3} 
 np.\+number \texorpdfstring{$\vert$}{|} np.\+ndarray scipy.\+stats.\+\_\+entropy.\+entropy (\begin{DoxyParamCaption}\item[{np.\+typing.\+Array\+Like}]{pk}{, }\item[{np.\+typing.\+Array\+Like \texorpdfstring{$\vert$}{|} None }]{qk}{ = {\ttfamily None}, }\item[{float \texorpdfstring{$\vert$}{|} None }]{base}{ = {\ttfamily None}, }\item[{int }]{axis}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculate the Shannon entropy/relative entropy of given distribution(s).

If only probabilities `pk` are given, the Shannon entropy is calculated as
``H = -sum(pk * log(pk))``.

If `qk` is not None, then compute the relative entropy
``D = sum(pk * log(pk / qk))``. This quantity is also known
as the Kullback-Leibler divergence.

This routine will normalize `pk` and `qk` if they don't sum to 1.

Parameters
----------
pk : array_like
    Defines the (discrete) distribution. Along each axis-slice of ``pk``,
    element ``i`` is the  (possibly unnormalized) probability of event
    ``i``.
qk : array_like, optional
    Sequence against which the relative entropy is computed. Should be in
    the same format as `pk`.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the entropy is calculated. Default is 0.

Returns
-------
S : {float, array_like}
    The calculated entropy.

Notes
-----
Informally, the Shannon entropy quantifies the expected uncertainty
inherent in the possible outcomes of a discrete random variable.
For example,
if messages consisting of sequences of symbols from a set are to be
encoded and transmitted over a noiseless channel, then the Shannon entropy
``H(pk)`` gives a tight lower bound for the average number of units of
information needed per symbol if the symbols occur with frequencies
governed by the discrete distribution `pk` [1]_. The choice of base
determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.

The relative entropy, ``D(pk|qk)``, quantifies the increase in the average
number of units of information needed per symbol if the encoding is
optimized for the probability distribution `qk` instead of the true
distribution `pk`. Informally, the relative entropy quantifies the expected
excess in surprise experienced if one believes the true distribution is
`qk` when it is actually `pk`.

A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the
equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with
the formula ``CE = -sum(pk * log(qk))``. It gives the average
number of units of information needed per symbol if an encoding is
optimized for the probability distribution `qk` when the true distribution
is `pk`. It is not computed directly by `entropy`, but it can be computed
using two calls to the function (see Examples).

See [2]_ for more information.

References
----------
.. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.
       Bell System Technical Journal, 27: 379-423.
       https://doi.org/10.1002/j.1538-7305.1948.tb01338.x
.. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information
       Theory (Wiley Series in Telecommunications and Signal Processing).
       Wiley-Interscience, USA.


Examples
--------
The outcome of a fair coin is the most uncertain:

>>> import numpy as np
>>> from scipy.stats import entropy
>>> base = 2  # work in units of bits
>>> pk = np.array([1/2, 1/2])  # fair coin
>>> H = entropy(pk, base=base)
>>> H
1.0
>>> H == -np.sum(pk * np.log(pk)) / np.log(base)
True

The outcome of a biased coin is less uncertain:

>>> qk = np.array([9/10, 1/10])  # biased coin
>>> entropy(qk, base=base)
0.46899559358928117

The relative entropy between the fair coin and biased coin is calculated
as:

>>> D = entropy(pk, qk, base=base)
>>> D
0.7369655941662062
>>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)
True

The cross entropy can be calculated as the sum of the entropy and
relative entropy`:

>>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)
>>> CE
1.736965594166206
>>> CE == -np.sum(pk * np.log(qk)) / np.log(base)
True
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__entropy_8py_source_l00015}{15}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacescipy_1_1stats_1_1__entropy_a5e66c878132652ec57633b93a8fc0903}\index{scipy.stats.\_entropy@{scipy.stats.\_entropy}!\_\_all\_\_@{\_\_all\_\_}}
\index{\_\_all\_\_@{\_\_all\_\_}!scipy.stats.\_entropy@{scipy.stats.\_entropy}}
\doxysubsubsection{\texorpdfstring{\_\_all\_\_}{\_\_all\_\_}}
{\footnotesize\ttfamily \label{namespacescipy_1_1stats_1_1__entropy_a5e66c878132652ec57633b93a8fc0903} 
list scipy.\+stats.\+\_\+entropy.\+\_\+\+\_\+all\+\_\+\+\_\+ = \mbox{[}\textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_a380fa277399a04d09cc26299819e3aa3}{entropy}}\textquotesingle{}, \textquotesingle{}\mbox{\hyperlink{namespacescipy_1_1stats_1_1__entropy_aed93073865f48caca12491e0d550b6e6}{differential\+\_\+entropy}}\textquotesingle{}\mbox{]}\hspace{0.3cm}{\ttfamily [private]}}



Definition at line \mbox{\hyperlink{__entropy_8py_source_l00012}{12}} of file \mbox{\hyperlink{__entropy_8py_source}{\+\_\+entropy.\+py}}.

