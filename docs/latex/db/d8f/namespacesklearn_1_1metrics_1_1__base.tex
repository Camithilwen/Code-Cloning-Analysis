\doxysection{sklearn.\+metrics.\+\_\+base Namespace Reference}
\hypertarget{namespacesklearn_1_1metrics_1_1__base}{}\label{namespacesklearn_1_1metrics_1_1__base}\index{sklearn.metrics.\_base@{sklearn.metrics.\_base}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__base_ad4126eac16e7eed62f1bf73028248723}{\+\_\+average\+\_\+binary\+\_\+score}} (binary\+\_\+metric, y\+\_\+true, y\+\_\+score, average, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1metrics_1_1__base_a36e6b0ba4e0a3997df5b02e1a6fe41b9}{\+\_\+average\+\_\+multiclass\+\_\+ovo\+\_\+score}} (binary\+\_\+metric, y\+\_\+true, y\+\_\+score, average="{}macro"{})
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Common code for all metrics.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1metrics_1_1__base_ad4126eac16e7eed62f1bf73028248723}\index{sklearn.metrics.\_base@{sklearn.metrics.\_base}!\_average\_binary\_score@{\_average\_binary\_score}}
\index{\_average\_binary\_score@{\_average\_binary\_score}!sklearn.metrics.\_base@{sklearn.metrics.\_base}}
\doxysubsubsection{\texorpdfstring{\_average\_binary\_score()}{\_average\_binary\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__base_ad4126eac16e7eed62f1bf73028248723} 
sklearn.\+metrics.\+\_\+base.\+\_\+average\+\_\+binary\+\_\+score (\begin{DoxyParamCaption}\item[{}]{binary\+\_\+metric}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+score}{, }\item[{}]{average}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Average a binary metric for multilabel classification.

Parameters
----------
y_true : array, shape = [n_samples] or [n_samples, n_classes]
    True binary labels in binary label indicators.

y_score : array, shape = [n_samples] or [n_samples, n_classes]
    Target scores, can either be probability estimates of the positive
    class, confidence values, or binary decisions.

average : {None, 'micro', 'macro', 'samples', 'weighted'}, default='macro'
    If ``None``, the scores for each class are returned. Otherwise,
    this determines the type of averaging performed on the data:

    ``'micro'``:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    ``'samples'``:
        Calculate metrics for each instance, and find their average.

    Will be ignored when ``y_true`` is binary.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

binary_metric : callable, returns shape [n_classes]
    The binary metric function to use.

Returns
-------
score : float or array of shape [n_classes]
    If not ``None``, average the score, else return the score for each
    classes.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2metrics_2__base_8py_source_l00017}{17}} of file \mbox{\hyperlink{sklearn_2metrics_2__base_8py_source}{\+\_\+base.\+py}}.

\Hypertarget{namespacesklearn_1_1metrics_1_1__base_a36e6b0ba4e0a3997df5b02e1a6fe41b9}\index{sklearn.metrics.\_base@{sklearn.metrics.\_base}!\_average\_multiclass\_ovo\_score@{\_average\_multiclass\_ovo\_score}}
\index{\_average\_multiclass\_ovo\_score@{\_average\_multiclass\_ovo\_score}!sklearn.metrics.\_base@{sklearn.metrics.\_base}}
\doxysubsubsection{\texorpdfstring{\_average\_multiclass\_ovo\_score()}{\_average\_multiclass\_ovo\_score()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1metrics_1_1__base_a36e6b0ba4e0a3997df5b02e1a6fe41b9} 
sklearn.\+metrics.\+\_\+base.\+\_\+average\+\_\+multiclass\+\_\+ovo\+\_\+score (\begin{DoxyParamCaption}\item[{}]{binary\+\_\+metric}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{y\+\_\+score}{, }\item[{}]{average}{ = {\ttfamily "{}macro"{}}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Average one-versus-one scores for multiclass classification.

Uses the binary metric for one-vs-one multiclass classification,
where the score is computed according to the Hand & Till (2001) algorithm.

Parameters
----------
binary_metric : callable
    The binary metric function to use that accepts the following as input:
        y_true_target : array, shape = [n_samples_target]
            Some sub-array of y_true for a pair of classes designated
            positive and negative in the one-vs-one scheme.
        y_score_target : array, shape = [n_samples_target]
            Scores corresponding to the probability estimates
            of a sample belonging to the designated positive class label

y_true : array-like of shape (n_samples,)
    True multiclass labels.

y_score : array-like of shape (n_samples, n_classes)
    Target scores corresponding to probability estimates of a sample
    belonging to a particular class.

average : {'macro', 'weighted'}, default='macro'
    Determines the type of averaging performed on the pairwise binary
    metric scores:
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean. This does not take label imbalance into account. Classes
        are assumed to be uniformly distributed.
    ``'weighted'``:
        Calculate metrics for each label, taking into account the
        prevalence of the classes.

Returns
-------
score : float
    Average of the pairwise binary metric scores.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2metrics_2__base_8py_source_l00126}{126}} of file \mbox{\hyperlink{sklearn_2metrics_2__base_8py_source}{\+\_\+base.\+py}}.

