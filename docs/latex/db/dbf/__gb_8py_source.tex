\doxysection{\+\_\+gb.\+py}
\hypertarget{__gb_8py_source}{}\label{__gb_8py_source}\index{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/ensemble/\_gb.py@{/home/jam/Research/IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.12/site-\/packages/sklearn/ensemble/\_gb.py}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00001}\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb}{00001}}\ \textcolor{stringliteral}{"{}"{}"{}Gradient\ Boosted\ Regression\ Trees.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00002}00002\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00003}00003\ \textcolor{stringliteral}{This\ module\ contains\ methods\ for\ fitting\ gradient\ boosted\ regression\ trees\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00004}00004\ \textcolor{stringliteral}{both\ classification\ and\ regression.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00005}00005\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00006}00006\ \textcolor{stringliteral}{The\ module\ structure\ is\ the\ following:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00007}00007\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00008}00008\ \textcolor{stringliteral}{-\/\ The\ \`{}\`{}BaseGradientBoosting\`{}\`{}\ base\ class\ implements\ a\ common\ \`{}\`{}fit\`{}\`{}\ method}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00009}00009\ \textcolor{stringliteral}{\ \ for\ all\ the\ estimators\ in\ the\ module.\ Regression\ and\ classification}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00010}00010\ \textcolor{stringliteral}{\ \ only\ differ\ in\ the\ concrete\ \`{}\`{}LossFunction\`{}\`{}\ used.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00011}00011\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00012}00012\ \textcolor{stringliteral}{-\/\ \`{}\`{}GradientBoostingClassifier\`{}\`{}\ implements\ gradient\ boosting\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00013}00013\ \textcolor{stringliteral}{\ \ classification\ problems.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00014}00014\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00015}00015\ \textcolor{stringliteral}{-\/\ \`{}\`{}GradientBoostingRegressor\`{}\`{}\ implements\ gradient\ boosting\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00016}00016\ \textcolor{stringliteral}{\ \ regression\ problems.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00017}00017\ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00018}00018\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00019}00019\ \textcolor{comment}{\#\ Authors:\ The\ scikit-\/learn\ developers}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00020}00020\ \textcolor{comment}{\#\ SPDX-\/License-\/Identifier:\ BSD-\/3-\/Clause}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00021}00021\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00022}00022\ \textcolor{keyword}{import}\ math}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00023}00023\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00024}00024\ \textcolor{keyword}{from}\ abc\ \textcolor{keyword}{import}\ ABCMeta,\ abstractmethod}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00025}00025\ \textcolor{keyword}{from}\ numbers\ \textcolor{keyword}{import}\ Integral,\ Real}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00026}00026\ \textcolor{keyword}{from}\ time\ \textcolor{keyword}{import}\ time}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00027}00027\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00028}00028\ \textcolor{keyword}{import}\ numpy\ \textcolor{keyword}{as}\ np}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00029}00029\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacescipy_1_1sparse}{scipy.sparse}}\ \textcolor{keyword}{import}\ csc\_matrix,\ csr\_matrix,\ issparse}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00030}00030\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00031}00031\ \textcolor{keyword}{from}\ ..\_loss.loss\ \textcolor{keyword}{import}\ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00032}00032\ \ \ \ \ \_LOSSES,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00033}00033\ \ \ \ \ AbsoluteError,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00034}00034\ \ \ \ \ ExponentialLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00035}00035\ \ \ \ \ HalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00036}00036\ \ \ \ \ HalfMultinomialLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00037}00037\ \ \ \ \ HalfSquaredError,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00038}00038\ \ \ \ \ HuberLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00039}00039\ \ \ \ \ PinballLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00040}00040\ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00041}00041\ \textcolor{keyword}{from}\ ..base\ \textcolor{keyword}{import}\ ClassifierMixin,\ RegressorMixin,\ \_fit\_context,\ is\_classifier}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00042}00042\ \textcolor{keyword}{from}\ ..dummy\ \textcolor{keyword}{import}\ DummyClassifier,\ DummyRegressor}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00043}00043\ \textcolor{keyword}{from}\ ..exceptions\ \textcolor{keyword}{import}\ NotFittedError}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00044}00044\ \textcolor{keyword}{from}\ ..model\_selection\ \textcolor{keyword}{import}\ train\_test\_split}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00045}00045\ \textcolor{keyword}{from}\ ..preprocessing\ \textcolor{keyword}{import}\ LabelEncoder}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00046}00046\ \textcolor{keyword}{from}\ ..tree\ \textcolor{keyword}{import}\ DecisionTreeRegressor}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00047}00047\ \textcolor{keyword}{from}\ ..tree.\_tree\ \textcolor{keyword}{import}\ DOUBLE,\ DTYPE,\ TREE\_LEAF}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00048}00048\ \textcolor{keyword}{from}\ ..utils\ \textcolor{keyword}{import}\ check\_array,\ check\_random\_state,\ column\_or\_1d}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00049}00049\ \textcolor{keyword}{from}\ ..utils.\_param\_validation\ \textcolor{keyword}{import}\ HasMethods,\ Interval,\ StrOptions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00050}00050\ \textcolor{keyword}{from}\ ..utils.multiclass\ \textcolor{keyword}{import}\ check\_classification\_targets}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00051}00051\ \textcolor{keyword}{from}\ ..utils.stats\ \textcolor{keyword}{import}\ \_weighted\_percentile}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00052}00052\ \textcolor{keyword}{from}\ ..utils.validation\ \textcolor{keyword}{import}\ \_check\_sample\_weight,\ check\_is\_fitted,\ validate\_data}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00053}00053\ \textcolor{keyword}{from}\ .\_base\ \textcolor{keyword}{import}\ BaseEnsemble}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00054}00054\ \textcolor{keyword}{from}\ .\_gradient\_boosting\ \textcolor{keyword}{import}\ \_random\_sample\_mask,\ predict\_stage,\ predict\_stages}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00055}00055\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00056}00056\ \_LOSSES\ =\ \_LOSSES.copy()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00057}00057\ \_LOSSES.update(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00058}00058\ \ \ \ \ \{}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00059}00059\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}quantile"{}}:\ PinballLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}huber"{}}:\ HuberLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00061}00061\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00062}00062\ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00063}00063\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00064}00064\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00065}\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{00065}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{\_safe\_divide}}(numerator,\ denominator):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00066}00066\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Prevents\ overflow\ and\ division\ by\ zero."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00067}00067\ \ \ \ \ \textcolor{comment}{\#\ This\ is\ used\ for\ classifiers\ where\ the\ denominator\ might\ become\ zero\ exactly.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00068}00068\ \ \ \ \ \textcolor{comment}{\#\ For\ instance\ for\ log\ loss,\ HalfBinomialLoss,\ if\ proba=0\ or\ proba=1\ exactly,\ then}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00069}00069\ \ \ \ \ \textcolor{comment}{\#\ denominator\ =\ hessian\ =\ 0,\ and\ we\ should\ set\ the\ node\ value\ in\ the\ line\ search\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00070}00070\ \ \ \ \ \textcolor{comment}{\#\ zero\ as\ there\ is\ no\ improvement\ of\ the\ loss\ possible.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00071}00071\ \ \ \ \ \textcolor{comment}{\#\ For\ numerical\ safety,\ we\ do\ this\ already\ for\ extremely\ tiny\ values.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00072}00072\ \ \ \ \ \textcolor{keywordflow}{if}\ abs(denominator)\ <\ 1e-\/150:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ 0.0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00074}00074\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Cast\ to\ Python\ float\ to\ trigger\ Python\ errors,\ e.g.\ ZeroDivisionError,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ without\ relying\ on\ \`{}np.errstate`\ that\ is\ not\ supported\ by\ Pyodide.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ result\ =\ float(numerator)\ /\ float(denominator)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Cast\ to\ Python\ float\ to\ trigger\ a\ ZeroDivisionError\ without\ relying}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00079}00079\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ on\ \`{}np.errstate`\ that\ is\ not\ supported\ by\ Pyodide.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00080}00080\ \ \ \ \ \ \ \ \ result\ =\ float(numerator)\ /\ float(denominator)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ math.isinf(result):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(\textcolor{stringliteral}{"{}overflow\ encountered\ in\ \_safe\_divide"{}},\ RuntimeWarning)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ result}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00084}00084\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00085}00085\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00086}\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}{00086}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}{\_init\_raw\_predictions}}(X,\ estimator,\ loss,\ use\_predict\_proba):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00087}00087\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ the\ initial\ raw\ predictions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00088}00088\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00089}00089\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00090}00090\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00091}00091\ \textcolor{stringliteral}{\ \ \ \ X\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00092}00092\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ data\ array.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00093}00093\ \textcolor{stringliteral}{\ \ \ \ estimator\ :\ object}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00094}00094\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ estimator\ to\ use\ to\ compute\ the\ predictions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00095}00095\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ BaseLoss}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00096}00096\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ An\ instance\ of\ a\ loss\ function\ class.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00097}00097\ \textcolor{stringliteral}{\ \ \ \ use\_predict\_proba\ :\ bool}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00098}00098\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Whether\ estimator.predict\_proba\ is\ used\ instead\ of\ estimator.predict.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00099}00099\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00100}00100\ \textcolor{stringliteral}{\ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00101}00101\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00102}00102\ \textcolor{stringliteral}{\ \ \ \ raw\_predictions\ :\ ndarray\ of\ shape\ (n\_samples,\ K)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00103}00103\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ initial\ raw\ predictions.\ K\ is\ equal\ to\ 1\ for\ binary}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00104}00104\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ classification\ and\ regression,\ and\ equal\ to\ the\ number\ of\ classes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00105}00105\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ multiclass\ classification.\ \`{}\`{}raw\_predictions\`{}\`{}\ is\ casted}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00106}00106\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ into\ float64.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00107}00107\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00108}00108\ \ \ \ \ \textcolor{comment}{\#\ TODO:\ Use\ loss.fit\_intercept\_only\ where\ appropriate\ instead\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00109}00109\ \ \ \ \ \textcolor{comment}{\#\ DummyRegressor\ which\ is\ the\ default\ given\ by\ the\ \`{}init`\ parameter,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00110}00110\ \ \ \ \ \textcolor{comment}{\#\ see\ also\ \_init\_state.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00111}00111\ \ \ \ \ \textcolor{keywordflow}{if}\ use\_predict\_proba:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00112}00112\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Our\ parameter\ validation,\ set\ via\ \_fit\_context\ and\ \_parameter\_constraints}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00113}00113\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ already\ guarantees\ that\ estimator\ has\ a\ predict\_proba\ method.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00114}00114\ \ \ \ \ \ \ \ \ predictions\ =\ estimator.predict\_proba(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00115}00115\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00116}00116\ \ \ \ \ \ \ \ \ \ \ \ \ predictions\ =\ predictions[:,\ 1]\ \ \textcolor{comment}{\#\ probability\ of\ positive\ class}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00117}00117\ \ \ \ \ \ \ \ \ eps\ =\ np.finfo(np.float32).eps\ \ \textcolor{comment}{\#\ FIXME:\ This\ is\ quite\ large!}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00118}00118\ \ \ \ \ \ \ \ \ predictions\ =\ np.clip(predictions,\ eps,\ 1\ -\/\ eps,\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00119}00119\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00120}00120\ \ \ \ \ \ \ \ \ predictions\ =\ estimator.predict(X).astype(np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00121}00121\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00122}00122\ \ \ \ \ \textcolor{keywordflow}{if}\ predictions.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.link.link(predictions).reshape(-\/1,\ 1)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00124}00124\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.link.link(predictions)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00126}00126\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00127}00127\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00128}\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476}{00128}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476}{\_update\_terminal\_regions}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00129}00129\ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00130}00130\ \ \ \ \ tree,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00131}00131\ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00132}00132\ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00133}00133\ \ \ \ \ neg\_gradient,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00134}00134\ \ \ \ \ raw\_prediction,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00135}00135\ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00136}00136\ \ \ \ \ sample\_mask,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00137}00137\ \ \ \ \ learning\_rate=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00138}00138\ \ \ \ \ k=0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00139}00139\ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00140}00140\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Update\ the\ leaf\ values\ to\ be\ predicted\ by\ the\ tree\ and\ raw\_prediction.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00141}00141\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00142}00142\ \textcolor{stringliteral}{\ \ \ \ The\ current\ raw\ predictions\ of\ the\ model\ (of\ this\ stage)\ are\ updated.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00143}00143\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00144}00144\ \textcolor{stringliteral}{\ \ \ \ Additionally,\ the\ terminal\ regions\ (=leaves)\ of\ the\ given\ tree\ are\ updated\ as\ well.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00145}00145\ \textcolor{stringliteral}{\ \ \ \ This\ corresponds\ to\ the\ line\ search\ step\ in\ "{}Greedy\ Function\ Approximation"{}\ by}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00146}00146\ \textcolor{stringliteral}{\ \ \ \ Friedman,\ Algorithm\ 1\ step\ 5.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00147}00147\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00148}00148\ \textcolor{stringliteral}{\ \ \ \ Update\ equals:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00149}00149\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ argmin\_\{x\}\ loss(y\_true,\ raw\_prediction\_old\ +\ x\ *\ tree.value)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00150}00150\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00151}00151\ \textcolor{stringliteral}{\ \ \ \ For\ non-\/trivial\ cases\ like\ the\ Binomial\ loss,\ the\ update\ has\ no\ closed\ formula\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00152}00152\ \textcolor{stringliteral}{\ \ \ \ is\ an\ approximation,\ again,\ see\ the\ Friedman\ paper.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00153}00153\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00154}00154\ \textcolor{stringliteral}{\ \ \ \ Also\ note\ that\ the\ update\ formula\ for\ the\ SquaredError\ is\ the\ identity.\ Therefore,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00155}00155\ \textcolor{stringliteral}{\ \ \ \ in\ this\ case,\ the\ leaf\ values\ don't\ need\ an\ update\ and\ only\ the\ raw\_predictions\ are}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00156}00156\ \textcolor{stringliteral}{\ \ \ \ updated\ (with\ the\ learning\ rate\ included).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00157}00157\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00158}00158\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00159}00159\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00160}00160\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ BaseLoss}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00161}00161\ \textcolor{stringliteral}{\ \ \ \ tree\ :\ tree.Tree}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00162}00162\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ tree\ object.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00163}00163\ \textcolor{stringliteral}{\ \ \ \ X\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00164}00164\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ data\ array.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00165}00165\ \textcolor{stringliteral}{\ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00166}00166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ target\ labels.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00167}00167\ \textcolor{stringliteral}{\ \ \ \ neg\_gradient\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00168}00168\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ negative\ gradient.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00169}00169\ \textcolor{stringliteral}{\ \ \ \ raw\_prediction\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_trees\_per\_iteration)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00170}00170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ raw\ predictions\ (i.e.\ values\ from\ the\ tree\ leaves)\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00171}00171\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ tree\ ensemble\ at\ iteration\ \`{}\`{}i\ -\/\ 1\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00172}00172\ \textcolor{stringliteral}{\ \ \ \ sample\_weight\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00173}00173\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weight\ of\ each\ sample.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00174}00174\ \textcolor{stringliteral}{\ \ \ \ sample\_mask\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00175}00175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ sample\ mask\ to\ be\ used.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00176}00176\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00177}00177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Learning\ rate\ shrinks\ the\ contribution\ of\ each\ tree\ by}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00178}00178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \`{}\`{}learning\_rate\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00179}00179\ \textcolor{stringliteral}{\ \ \ \ k\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00180}00180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ index\ of\ the\ estimator\ being\ updated.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00181}00181\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00182}00182\ \ \ \ \ \textcolor{comment}{\#\ compute\ leaf\ for\ each\ sample\ in\ \`{}\`{}X``.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00183}00183\ \ \ \ \ terminal\_regions\ =\ tree.apply(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00184}00184\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00185}00185\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ isinstance(loss,\ HalfSquaredError):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00186}00186\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ mask\ all\ which\ are\ not\ in\ sample\ mask.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00187}00187\ \ \ \ \ \ \ \ \ masked\_terminal\_regions\ =\ terminal\_regions.copy()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00188}00188\ \ \ \ \ \ \ \ \ masked\_terminal\_regions[\string~sample\_mask]\ =\ -\/1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00189}00189\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00190}00190\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(loss,\ HalfBinomialLoss):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00191}00191\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }compute\_update(y\_,\ indices,\ neg\_gradient,\ raw\_prediction,\ k):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00193}00193\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Make\ a\ single\ Newton-\/Raphson\ step,\ see\ "{}Additive\ Logistic\ Regression:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00194}00194\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ A\ Statistical\ View\ of\ Boosting"{}\ FHT00\ and\ note\ that\ we\ use\ a\ slightly}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00195}00195\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ different\ version\ (factor\ 2)\ of\ "{}F"{}\ with\ proba=expit(raw\_prediction).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00196}00196\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Our\ node\ estimate\ is\ given\ by:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ sum(w\ *\ (y\ -\/\ prob))\ /\ sum(w\ *\ prob\ *\ (1\ -\/\ prob))}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00198}00198\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ take\ advantage\ that:\ y\ -\/\ prob\ =\ neg\_gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\ =\ neg\_gradient.take(indices,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ =\ y\_\ -\/\ neg\_g}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ numerator\ =\ negative\ gradient\ =\ y\ -\/\ prob}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00202}00202\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ numerator\ =\ np.average(neg\_g,\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00203}00203\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ denominator\ =\ hessian\ =\ prob\ *\ (1\ -\/\ prob)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00204}00204\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ denominator\ =\ np.average(prob\ *\ (1\ -\/\ prob),\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00205}00205\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{\_safe\_divide}}(numerator,\ denominator)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00206}00206\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00207}00207\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(loss,\ HalfMultinomialLoss):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00208}00208\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00209}00209\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }compute\_update(y\_,\ indices,\ neg\_gradient,\ raw\_prediction,\ k):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00210}00210\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ take\ advantage\ that:\ y\ -\/\ prob\ =\ neg\_gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00211}00211\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\ =\ neg\_gradient.take(indices,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00212}00212\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\ =\ y\_\ -\/\ neg\_g}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00213}00213\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ K\ =\ loss.n\_classes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00214}00214\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ numerator\ =\ negative\ gradient\ *\ (k\ -\/\ 1)\ /\ k}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00215}00215\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note:\ The\ factor\ (k\ -\/\ 1)/k\ appears\ in\ the\ original\ papers\ "{}Greedy}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00216}00216\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Function\ Approximation"{}\ by\ Friedman\ and\ "{}Additive\ Logistic}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Regression"{}\ by\ Friedman,\ Hastie,\ Tibshirani.\ This\ factor\ is,\ however,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00218}00218\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ wrong\ or\ at\ least\ arbitrary\ as\ it\ directly\ multiplies\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00219}00219\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ learning\_rate.\ We\ keep\ it\ for\ backward\ compatibility.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ numerator\ =\ np.average(neg\_g,\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00221}00221\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ numerator\ *=\ (K\ -\/\ 1)\ /\ K}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ denominator\ =\ (diagonal)\ hessian\ =\ prob\ *\ (1\ -\/\ prob)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00223}00223\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ denominator\ =\ np.average(prob\ *\ (1\ -\/\ prob),\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{\_safe\_divide}}(numerator,\ denominator)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00225}00225\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00226}00226\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(loss,\ ExponentialLoss):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00227}00227\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00228}00228\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }compute\_update(y\_,\ indices,\ neg\_gradient,\ raw\_prediction,\ k):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00229}00229\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\ =\ neg\_gradient.take(indices,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00230}00230\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ numerator\ =\ negative\ gradient\ =\ y\ *\ exp(-\/raw)\ -\/\ (1-\/y)\ *\ exp(raw)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00231}00231\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ numerator\ =\ np.average(neg\_g,\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00232}00232\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ denominator\ =\ hessian\ =\ y\ *\ exp(-\/raw)\ +\ (1-\/y)\ *\ exp(raw)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00233}00233\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ if\ y=0:\ hessian\ =\ exp(raw)\ =\ -\/neg\_g}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00234}00234\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ \ \ \ y=1:\ hessian\ =\ exp(-\/raw)\ =\ neg\_g}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00235}00235\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hessian\ =\ neg\_g.copy()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ hessian[y\_\ ==\ 0]\ *=\ -\/1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00237}00237\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ denominator\ =\ np.average(hessian,\ weights=sw)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00238}00238\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_aaddcf085e3be0d55be96622d588fdff1}{\_safe\_divide}}(numerator,\ denominator)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00239}00239\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00240}00240\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00241}00241\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00242}00242\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }compute\_update(y\_,\ indices,\ neg\_gradient,\ raw\_prediction,\ k):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00243}00243\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ loss.fit\_intercept\_only(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00244}00244\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_\ -\/\ raw\_prediction[indices,\ k],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00245}00245\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sw,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00247}00247\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ update\ each\ leaf\ (=\ perform\ line\ search)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00249}00249\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ leaf\ \textcolor{keywordflow}{in}\ np.nonzero(tree.children\_left\ ==\ TREE\_LEAF)[0]:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00250}00250\ \ \ \ \ \ \ \ \ \ \ \ \ indices\ =\ np.nonzero(masked\_terminal\_regions\ ==\ leaf)[}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00251}00251\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00252}00252\ \ \ \ \ \ \ \ \ \ \ \ \ ]\ \ \textcolor{comment}{\#\ of\ terminal\ regions}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00253}00253\ \ \ \ \ \ \ \ \ \ \ \ \ y\_\ =\ y.take(indices,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00254}00254\ \ \ \ \ \ \ \ \ \ \ \ \ sw\ =\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ sample\_weight[indices]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ \ \ \ \ update\ =\ compute\_update(y\_,\ indices,\ neg\_gradient,\ raw\_prediction,\ k)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00256}00256\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ Multiply\ here\ by\ learning\ rate\ instead\ of\ everywhere\ else.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ \ \ \ \ tree.value[leaf,\ 0,\ 0]\ =\ update}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00259}00259\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00260}00260\ \ \ \ \ \textcolor{comment}{\#\ update\ predictions\ (both\ in-\/bag\ and\ out-\/of-\/bag)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00261}00261\ \ \ \ \ raw\_prediction[:,\ k]\ +=\ learning\_rate\ *\ tree.value[:,\ 0,\ 0].take(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ terminal\_regions,\ axis=0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00263}00263\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00264}00264\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00265}00265\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00266}\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6}{00266}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6}{set\_huber\_delta}}(loss,\ y\_true,\ raw\_prediction,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00267}00267\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Calculate\ and\ set\ self.closs.delta\ based\ on\ self.quantile."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00268}00268\ \ \ \ \ abserr\ =\ np.abs(y\_true\ -\/\ raw\_prediction.squeeze())}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00269}00269\ \ \ \ \ \textcolor{comment}{\#\ sample\_weight\ is\ always\ a\ ndarray,\ never\ None.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00270}00270\ \ \ \ \ delta\ =\ \_weighted\_percentile(abserr,\ sample\_weight,\ 100\ *\ loss.quantile)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00271}00271\ \ \ \ \ loss.closs.delta\ =\ float(delta)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00272}00272\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00273}00273\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00274}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter}{00274}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter}{VerboseReporter}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00275}00275\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Reports\ verbose\ output\ to\ stdout.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00276}00276\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00277}00277\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00278}00278\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00279}00279\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00280}00280\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Verbosity\ level.\ If\ \`{}\`{}verbose==1\`{}\`{}\ output\ is\ printed\ once\ in\ a\ while}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00281}00281\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (when\ iteration\ mod\ verbose\_mod\ is\ zero).;\ if\ larger\ than\ 1\ then\ output}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00282}00282\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ is\ printed\ for\ each\ update.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00283}00283\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00284}00284\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00285}00285\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ verbose):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00286}00286\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a0ea8bc5920879256a50a372376ef6a2a}{verbose}}\ =\ verbose}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00287}00287\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00288}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_acbbe95459abeb8409c51d601f9a604fe}{00288}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_acbbe95459abeb8409c51d601f9a604fe}{init}}(self,\ est,\ begin\_at\_stage=0):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00289}00289\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Initialize\ reporter}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00290}00290\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00291}00291\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00292}00292\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00293}00293\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ est\ :\ Estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00294}00294\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00295}00295\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00296}00296\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ begin\_at\_stage\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00297}00297\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ stage\ at\ which\ to\ begin\ reporting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00298}00298\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ header\ fields\ and\ line\ format\ str}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ header\_fields\ =\ [\textcolor{stringliteral}{"{}Iter"{}},\ \textcolor{stringliteral}{"{}Train\ Loss"{}}]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ verbose\_fmt\ =\ [\textcolor{stringliteral}{"{}\{iter:>10d\}"{}},\ \textcolor{stringliteral}{"{}\{train\_score:>16.4f\}"{}}]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00302}00302\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ do\ oob?}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ est.subsample\ <\ 1:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00304}00304\ \ \ \ \ \ \ \ \ \ \ \ \ header\_fields.append(\textcolor{stringliteral}{"{}OOB\ Improve"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00305}00305\ \ \ \ \ \ \ \ \ \ \ \ \ verbose\_fmt.append(\textcolor{stringliteral}{"{}\{oob\_impr:>16.4f\}"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00306}00306\ \ \ \ \ \ \ \ \ header\_fields.append(\textcolor{stringliteral}{"{}Remaining\ Time"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00307}00307\ \ \ \ \ \ \ \ \ verbose\_fmt.append(\textcolor{stringliteral}{"{}\{remaining\_time:>16s\}"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00308}00308\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00309}00309\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ print\ the\ header\ line}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00310}00310\ \ \ \ \ \ \ \ \ print((\textcolor{stringliteral}{"{}\%10s\ "{}}\ +\ \textcolor{stringliteral}{"{}\%16s\ "{}}\ *\ (len(header\_fields)\ -\/\ 1))\ \%\ tuple(header\_fields))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00311}00311\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af459ef2f1259c1990d75c54c33689c28}{verbose\_fmt}}\ =\ \textcolor{stringliteral}{"{}\ "{}}.join(verbose\_fmt)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00313}00313\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ plot\ verbose\ info\ each\ time\ i\ \%\ verbose\_mod\ ==\ 0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00314}00314\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af759939c83f9759c36c210c0d883c245}{verbose\_mod}}\ =\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00315}00315\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a9ea04931e8e0135d94188f52e92f7abc}{start\_time}}\ =\ time()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00316}00316\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a82e95e8c77c39fd347f291f71fcbcd79}{begin\_at\_stage}}\ =\ begin\_at\_stage}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00317}00317\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00318}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a57fbbf8da05dd5902c9a3f479b69d725}{00318}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a57fbbf8da05dd5902c9a3f479b69d725}{update}}(self,\ j,\ est):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00319}00319\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Update\ reporter\ with\ new\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00320}00320\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00321}00321\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00322}00322\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00323}00323\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ j\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00324}00324\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ new\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00325}00325\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ est\ :\ Estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00326}00326\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00327}00327\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00328}00328\ \ \ \ \ \ \ \ \ do\_oob\ =\ est.subsample\ <\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00329}00329\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ need\ to\ take\ into\ account\ if\ we\ fit\ additional\ estimators.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00330}00330\ \ \ \ \ \ \ \ \ i\ =\ j\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a82e95e8c77c39fd347f291f71fcbcd79}{begin\_at\_stage}}\ \ \textcolor{comment}{\#\ iteration\ relative\ to\ the\ start\ iter}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00331}00331\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (i\ +\ 1)\ \%\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af759939c83f9759c36c210c0d883c245}{verbose\_mod}}\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00332}00332\ \ \ \ \ \ \ \ \ \ \ \ \ oob\_impr\ =\ est.oob\_improvement\_[j]\ \textcolor{keywordflow}{if}\ do\_oob\ \textcolor{keywordflow}{else}\ 0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00333}00333\ \ \ \ \ \ \ \ \ \ \ \ \ remaining\_time\ =\ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00334}00334\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (est.n\_estimators\ -\/\ (j\ +\ 1))\ *\ (time()\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a9ea04931e8e0135d94188f52e92f7abc}{start\_time}})\ /\ float(i\ +\ 1)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ remaining\_time\ >\ 60:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ remaining\_time\ =\ \textcolor{stringliteral}{"{}\{0:.2f\}m"{}}.format(remaining\_time\ /\ 60.0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ remaining\_time\ =\ \textcolor{stringliteral}{"{}\{0:.2f\}s"{}}.format(remaining\_time)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \ \ \ \ print(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af459ef2f1259c1990d75c54c33689c28}{verbose\_fmt}}.format(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00342}00342\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ iter=j\ +\ 1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00343}00343\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ train\_score=est.train\_score\_[j],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ oob\_impr=oob\_impr,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00345}00345\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ remaining\_time=remaining\_time,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00346}00346\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00347}00347\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00348}00348\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_a0ea8bc5920879256a50a372376ef6a2a}{verbose}}\ ==\ 1\ \textcolor{keywordflow}{and}\ ((i\ +\ 1)\ //\ (self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af759939c83f9759c36c210c0d883c245}{verbose\_mod}}\ *\ 10)\ >\ 0):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00349}00349\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ adjust\ verbose\ frequency\ (powers\ of\ 10)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter_af759939c83f9759c36c210c0d883c245}{verbose\_mod}}\ *=\ 10}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00351}00351\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00352}00352\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00353}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting}{00353}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting}{BaseGradientBoosting}}(\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble}{BaseEnsemble}},\ \mbox{\hyperlink{classmetaclass}{metaclass}}=ABCMeta):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00354}00354\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Abstract\ base\ class\ for\ Gradient\ Boosting."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00355}00355\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00356}00356\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00357}00357\ \ \ \ \ \ \ \ \ **DecisionTreeRegressor.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}learning\_rate"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00359}00359\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_estimators"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00360}00360\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}criterion"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}friedman\_mse"{}},\ \textcolor{stringliteral}{"{}squared\_error"{}}\})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00361}00361\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}subsample"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ 1.0,\ closed=\textcolor{stringliteral}{"{}right"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00362}00362\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}verbose"{}}:\ [\textcolor{stringliteral}{"{}verbose"{}}],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00363}00363\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}warm\_start"{}}:\ [\textcolor{stringliteral}{"{}boolean"{}}],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00364}00364\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}validation\_fraction"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ 1.0,\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00365}00365\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_iter\_no\_change"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Integral,\ 1,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}}),\ \textcolor{keywordtype}{None}],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}tol"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ \textcolor{keywordtype}{None},\ closed=\textcolor{stringliteral}{"{}left"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00367}00367\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00368}00368\ \ \ \ \ \_parameter\_constraints.pop(\textcolor{stringliteral}{"{}splitter"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00369}00369\ \ \ \ \ \_parameter\_constraints.pop(\textcolor{stringliteral}{"{}monotonic\_cst"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00370}00370\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00371}00371\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00372}00372\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00373}00373\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00374}00374\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00375}00375\ \ \ \ \ \ \ \ \ loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00376}00376\ \ \ \ \ \ \ \ \ learning\_rate,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00377}00377\ \ \ \ \ \ \ \ \ n\_estimators,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00378}00378\ \ \ \ \ \ \ \ \ criterion,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00379}00379\ \ \ \ \ \ \ \ \ min\_samples\_split,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00380}00380\ \ \ \ \ \ \ \ \ min\_samples\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00381}00381\ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00382}00382\ \ \ \ \ \ \ \ \ max\_depth,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00383}00383\ \ \ \ \ \ \ \ \ min\_impurity\_decrease,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00384}00384\ \ \ \ \ \ \ \ \ init,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00385}00385\ \ \ \ \ \ \ \ \ subsample,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00386}00386\ \ \ \ \ \ \ \ \ max\_features,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00387}00387\ \ \ \ \ \ \ \ \ ccp\_alpha,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00388}00388\ \ \ \ \ \ \ \ \ random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00389}00389\ \ \ \ \ \ \ \ \ alpha=0.9,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00390}00390\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00391}00391\ \ \ \ \ \ \ \ \ max\_leaf\_nodes=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00392}00392\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00393}00393\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00394}00394\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00395}00395\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00396}00396\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00397}00397\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}\ =\ n\_estimators}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00398}00398\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa39daa68b8c51561233d0dae6fc56102}{learning\_rate}}\ =\ learning\_rate}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00399}00399\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}\ =\ loss}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00400}00400\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8ac08755398dc87b7e0180c6b0702d6d}{criterion}}\ =\ criterion}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00401}00401\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a767255b5516d7152bace70cc6f2d1138}{min\_samples\_split}}\ =\ min\_samples\_split}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00402}00402\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a80442da334eb522da4c76cbc4848b730}{min\_samples\_leaf}}\ =\ min\_samples\_leaf}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00403}00403\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa64550c09eab34e9832bf96a1fc59522}{min\_weight\_fraction\_leaf}}\ =\ min\_weight\_fraction\_leaf}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00404}00404\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aee4f6519e504131df8c3a6437b9cf638}{subsample}}\ =\ subsample}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00405}00405\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}\ =\ max\_features}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad86eadb26e5ca2287227e1fb88100e18}{max\_depth}}\ =\ max\_depth}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a05e18adb849e64c4503a9e938cb44317}{min\_impurity\_decrease}}\ =\ min\_impurity\_decrease}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abd2d03f6befc6471495ee6f1252ecbff}{ccp\_alpha}}\ =\ ccp\_alpha}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a93146cd020942c879c20928fe5b189b8}{init}}\ =\ init}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad1c716f9af251ba9f32b00fd644c1afa}{random\_state}}\ =\ random\_state}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ae7e325fb34b11e66f5411034da243259}{alpha}}\ =\ alpha}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5ada7328475c366b38184dea0607633b}{verbose}}\ =\ verbose}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3c86af0638ba492fea25f84ad4b851bb}{max\_leaf\_nodes}}\ =\ max\_leaf\_nodes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00414}00414\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1ae8820802e0de5857a681e20526d02d}{warm\_start}}\ =\ warm\_start}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7d11a5d3f0f12d31ae1b21055b87d732}{validation\_fraction}}\ =\ validation\_fraction}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00416}00416\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3e0c24fe0f00a676ed4c638287fa2c2b}{n\_iter\_no\_change}}\ =\ n\_iter\_no\_change}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00417}00417\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad186c728c5385a367911957d8e8de89e}{tol}}\ =\ tol}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00418}00418\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00419}00419\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00420}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a073521f79a79cf03c458ae5589dfc03c}{00420}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a073521f79a79cf03c458ae5589dfc03c}{\_encode\_y}}(self,\ y=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00421}00421\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Called\ by\ fit\ to\ validate\ and\ encode\ y."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00422}00422\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00423}00423\ \ \ \ \ \textcolor{preprocessor}{@abstractmethod}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00424}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a281e9bbf3cf598fa886f5decff2e877f}{00424}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a281e9bbf3cf598fa886f5decff2e877f}{\_get\_loss}}(self,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00425}00425\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Get\ loss\ object\ from\ sklearn.\_loss.loss."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00426}00426\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00427}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a58d22e9f2f5a7da25f25a4bde9644660}{00427}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a58d22e9f2f5a7da25f25a4bde9644660}{\_fit\_stage}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00428}00428\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00429}00429\ \ \ \ \ \ \ \ \ i,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00430}00430\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00431}00431\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00432}00432\ \ \ \ \ \ \ \ \ raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ sample\_mask,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00435}00435\ \ \ \ \ \ \ \ \ random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00436}00436\ \ \ \ \ \ \ \ \ X\_csc=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ X\_csr=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00438}00438\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ another\ stage\ of\ \`{}\`{}n\_trees\_per\_iteration\_\`{}\`{}\ trees."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00440}00440\ \ \ \ \ \ \ \ \ original\_y\ =\ y}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00441}00441\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(self.\_loss,\ HuberLoss):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00443}00443\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_ada771f8246e218ba36a92c7ac54a7ed6}{set\_huber\_delta}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss=self.\_loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00446}00446\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00447}00447\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00448}00448\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ TODO:\ Without\ oob,\ i.e.\ with\ self.subsample\ =\ 1.0,\ we\ could\ call}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00450}00450\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ self.\_loss.loss\_gradient\ and\ use\ it\ to\ set\ train\_score\_.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00451}00451\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ But\ note\ that\ train\_score\_[i]\ is\ the\ score\ AFTER\ fitting\ the\ i-\/th\ tree.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00452}00452\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Note:\ We\ need\ the\ negative\ gradient!}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00453}00453\ \ \ \ \ \ \ \ \ neg\_gradient\ =\ -\/self.\_loss.gradient(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00454}00454\ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00455}00455\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00456}00456\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=\textcolor{keywordtype}{None},\ \ \textcolor{comment}{\#\ We\ pass\ sample\_weights\ to\ the\ tree\ directly.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00457}00457\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00458}00458\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ 2-\/d\ views\ of\ shape\ (n\_samples,\ n\_trees\_per\_iteration\_)\ or\ (n\_samples,\ 1)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00459}00459\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ on\ neg\_gradient\ to\ simplify\ the\ loop\ over\ n\_trees\_per\_iteration\_.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00460}00460\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ neg\_gradient.ndim\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00461}00461\ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\_view\ =\ neg\_gradient.reshape((-\/1,\ 1))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00462}00462\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00463}00463\ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\_view\ =\ neg\_gradient}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00464}00464\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00465}00465\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(self.n\_trees\_per\_iteration\_):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00466}00466\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\_loss.is\_multiclass:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00467}00467\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\ =\ np.array(original\_y\ ==\ k,\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00468}00468\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00469}00469\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ induce\ regression\ tree\ on\ the\ negative\ gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00470}00470\ \ \ \ \ \ \ \ \ \ \ \ \ tree\ =\ DecisionTreeRegressor(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00471}00471\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ criterion=self.criterion,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00472}00472\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ splitter=\textcolor{stringliteral}{"{}best"{}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00473}00473\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_depth=self.max\_depth,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00474}00474\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_split=self.min\_samples\_split,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00475}00475\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_leaf=self.min\_samples\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00476}00476\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf=self.min\_weight\_fraction\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00477}00477\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ min\_impurity\_decrease=self.min\_impurity\_decrease,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00478}00478\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_features=self.max\_features,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00479}00479\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_leaf\_nodes=self.max\_leaf\_nodes,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00480}00480\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00481}00481\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ccp\_alpha=self.ccp\_alpha,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00482}00482\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00483}00483\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00484}00484\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.subsample\ <\ 1.0:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00485}00485\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ no\ inplace\ multiplication!}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00486}00486\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\ =\ sample\_weight\ *\ sample\_mask.astype(np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00487}00487\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00488}00488\ \ \ \ \ \ \ \ \ \ \ \ \ X\ =\ X\_csc\ \textcolor{keywordflow}{if}\ X\_csc\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ X}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00489}00489\ \ \ \ \ \ \ \ \ \ \ \ \ tree.fit(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00490}00490\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,\ neg\_g\_view[:,\ k],\ sample\_weight=sample\_weight,\ check\_input=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00491}00491\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00492}00492\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00493}00493\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ update\ tree\ leaves}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00494}00494\ \ \ \ \ \ \ \ \ \ \ \ \ X\_for\_tree\_update\ =\ X\_csr\ \textcolor{keywordflow}{if}\ X\_csr\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ X}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00495}00495\ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a7476fcc2faf13ac07b6975fdd3d02476}{\_update\_terminal\_regions}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00496}00496\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\_loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00497}00497\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tree.tree\_,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00498}00498\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_for\_tree\_update,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00499}00499\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00500}00500\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ neg\_g\_view[:,\ k],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00501}00501\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00502}00502\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00503}00503\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_mask,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00504}00504\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=self.learning\_rate,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00505}00505\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ k=k,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00506}00506\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00507}00507\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00508}00508\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ add\ tree\ to\ ensemble}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00509}00509\ \ \ \ \ \ \ \ \ \ \ \ \ self.estimators\_[i,\ k]\ =\ tree}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00510}00510\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00511}00511\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ raw\_predictions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00512}00512\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00513}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ac1061a325c9d0b814121cdd43af8033a}{00513}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ac1061a325c9d0b814121cdd43af8033a}{\_set\_max\_features}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00514}00514\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Set\ self.max\_features\_."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00515}00515\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}},\ str):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00516}00516\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}\ ==\ \textcolor{stringliteral}{"{}auto"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00517}00517\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ is\_classifier(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00518}00518\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ max(1,\ int(np.sqrt(self.n\_features\_in\_)))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00519}00519\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00520}00520\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ self.n\_features\_in\_}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00521}00521\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}\ ==\ \textcolor{stringliteral}{"{}sqrt"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00522}00522\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ max(1,\ int(np.sqrt(self.n\_features\_in\_)))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00523}00523\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:\ \ \textcolor{comment}{\#\ self.max\_features\ ==\ "{}log2"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00524}00524\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ max(1,\ int(np.log2(self.n\_features\_in\_)))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00525}00525\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00526}00526\ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ self.n\_features\_in\_}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00527}00527\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}},\ Integral):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00528}00528\ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00529}00529\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:\ \ \textcolor{comment}{\#\ float}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00530}00530\ \ \ \ \ \ \ \ \ \ \ \ \ max\_features\ =\ max(1,\ int(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5d1cfca0e8a0fcd57a082bc826151719}{max\_features}}\ *\ self.n\_features\_in\_))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00531}00531\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00532}00532\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_adfe689d2640fe7fc7cada8a6db135c69}{max\_features\_}}\ =\ max\_features}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00533}00533\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00534}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a4bc2e618edbdcf28f8e799c1bb51ee07}{00534}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a4bc2e618edbdcf28f8e799c1bb51ee07}{\_init\_state}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00535}00535\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Initialize\ model\ state\ and\ allocate\ model\ state\ data\ structures."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00536}00536\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00537}00537\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a93146cd020942c879c20928fe5b189b8}{init}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00538}00538\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00539}00539\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ is\_classifier(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00540}00540\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ =\ \mbox{\hyperlink{classsklearn_1_1dummy_1_1DummyClassifier}{DummyClassifier}}(strategy=\textcolor{stringliteral}{"{}prior"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00541}00541\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}},\ (AbsoluteError,\ HuberLoss)):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00542}00542\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ =\ \mbox{\hyperlink{classsklearn_1_1dummy_1_1DummyRegressor}{DummyRegressor}}(strategy=\textcolor{stringliteral}{"{}quantile"{}},\ quantile=0.5)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00543}00543\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}},\ PinballLoss):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00544}00544\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ =\ \mbox{\hyperlink{classsklearn_1_1dummy_1_1DummyRegressor}{DummyRegressor}}(strategy=\textcolor{stringliteral}{"{}quantile"{}},\ quantile=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ae7e325fb34b11e66f5411034da243259}{alpha}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00545}00545\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00546}00546\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ =\ \mbox{\hyperlink{classsklearn_1_1dummy_1_1DummyRegressor}{DummyRegressor}}(strategy=\textcolor{stringliteral}{"{}mean"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00547}00547\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00548}00548\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}\ =\ np.empty(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00549}00549\ \ \ \ \ \ \ \ \ \ \ \ \ (self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}},\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}}),\ dtype=object}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00550}00550\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00551}00551\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}\ =\ np.zeros((self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}},),\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00552}00552\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ do\ oob?}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00553}00553\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aee4f6519e504131df8c3a6437b9cf638}{subsample}}\ <\ 1.0:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00554}00554\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}\ =\ np.zeros((self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}),\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00555}00555\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}\ =\ np.zeros((self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}),\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00556}00556\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}\ =\ np.nan}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00557}00557\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00558}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a405d2b3b46d874f6fffcbc83370f187c}{00558}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a405d2b3b46d874f6fffcbc83370f187c}{\_clear\_state}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00559}00559\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Clear\ the\ state\ of\ the\ gradient\ boosting\ model."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00560}00560\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}estimators\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00561}00561\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}\ =\ np.empty((0,\ 0),\ dtype=object)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00562}00562\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}train\_score\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00563}00563\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00564}00564\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_improvement\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00565}00565\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00566}00566\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_scores\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00567}00567\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00568}00568\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_score\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00569}00569\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00570}00570\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}init\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00571}00571\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00572}00572\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}\_rng"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00573}00573\ \ \ \ \ \ \ \ \ \ \ \ \ del\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa47a9a76d70a3ed7f66141a79f0b0b27}{\_rng}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00574}00574\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00575}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3a730a69a39594e1fff891c05bf50c0a}{00575}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3a730a69a39594e1fff891c05bf50c0a}{\_resize\_state}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00576}00576\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Add\ additional\ \`{}\`{}n\_estimators\`{}\`{}\ entries\ to\ all\ attributes."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00577}00577\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ self.n\_estimators\ is\ the\ number\ of\ additional\ est\ to\ fit}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00578}00578\ \ \ \ \ \ \ \ \ total\_n\_estimators\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00579}00579\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ total\_n\_estimators\ <\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0]:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00580}00580\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00581}00581\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}resize\ with\ smaller\ n\_estimators\ \%d\ <\ \%d"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00582}00582\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ (total\_n\_estimators,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[0])}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00583}00583\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00584}00584\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00585}00585\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}\ =\ np.resize(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00586}00586\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}},\ (total\_n\_estimators,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00587}00587\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00588}00588\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}\ =\ np.resize(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}},\ total\_n\_estimators)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00589}00589\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aee4f6519e504131df8c3a6437b9cf638}{subsample}}\ <\ 1\ \textcolor{keywordflow}{or}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_improvement\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00590}00590\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ if\ do\ oob\ resize\ arrays\ or\ create\ new\ if\ not\ available}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00591}00591\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_improvement\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00592}00592\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}\ =\ np.resize(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00593}00593\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}},\ total\_n\_estimators}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00594}00594\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00595}00595\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}\ =\ np.resize(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}},\ total\_n\_estimators)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00596}00596\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}\ =\ np.nan}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00597}00597\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00598}00598\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00599}00599\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (total\_n\_estimators,),\ dtype=np.float64}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00600}00600\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00601}00601\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}\ =\ np.zeros((total\_n\_estimators,),\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00602}00602\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}\ =\ np.nan}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00603}00603\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00604}00604\ \ \ \ \ \textcolor{keyword}{def\ }\_is\_fitted(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00605}00605\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ len(getattr(self,\ \textcolor{stringliteral}{"{}estimators\_"{}},\ []))\ >\ 0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00606}00606\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00607}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8dde52984929a182e5348660db9b6baa}{00607}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8dde52984929a182e5348660db9b6baa}{\_check\_initialized}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00608}00608\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Check\ that\ the\ estimator\ is\ initialized,\ raising\ an\ error\ if\ not."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00609}00609\ \ \ \ \ \ \ \ \ check\_is\_fitted(self)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00610}00610\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00611}00611\ \ \ \ \ \textcolor{preprocessor}{@\_fit\_context}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00612}00612\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ GradientBoosting*.init\ is\ not\ validated\ yet}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00613}00613\ \ \ \ \ \ \ \ \ prefer\_skip\_nested\_validation=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00614}00614\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00615}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad120e255ed97bd84b8ec3e537af9a67a}{00615}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad120e255ed97bd84b8ec3e537af9a67a}{fit}}(self,\ X,\ y,\ sample\_weight=None,\ monitor=None):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00616}00616\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fit\ the\ gradient\ boosting\ model.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00617}00617\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00618}00618\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00619}00619\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00620}00620\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00621}00621\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00622}00622\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00623}00623\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00624}00624\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00625}00625\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ array-\/like\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00626}00626\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Target\ values\ (strings\ or\ integers\ in\ classification,\ real\ numbers}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00627}00627\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ in\ regression)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00628}00628\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ For\ classification,\ labels\ must\ correspond\ to\ classes.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00629}00629\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00630}00630\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ sample\_weight\ :\ array-\/like\ of\ shape\ (n\_samples,),\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00631}00631\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Sample\ weights.\ If\ None,\ then\ samples\ are\ equally\ weighted.\ Splits}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00632}00632\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ that\ would\ create\ child\ nodes\ with\ net\ zero\ or\ negative\ weight\ are}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00633}00633\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ ignored\ while\ searching\ for\ a\ split\ in\ each\ node.\ In\ the\ case\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00634}00634\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ classification,\ splits\ are\ also\ ignored\ if\ they\ would\ result\ in\ any}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00635}00635\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ single\ class\ carrying\ a\ negative\ weight\ in\ either\ child\ node.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00636}00636\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00637}00637\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ monitor\ :\ callable,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00638}00638\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ monitor\ is\ called\ after\ each\ iteration\ with\ the\ current}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00639}00639\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ iteration,\ a\ reference\ to\ the\ estimator\ and\ the\ local\ variables\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00640}00640\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}\_fit\_stages\`{}\`{}\ as\ keyword\ arguments\ \`{}\`{}callable(i,\ self,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00641}00641\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ locals())\`{}\`{}.\ If\ the\ callable\ returns\ \`{}\`{}True\`{}\`{}\ the\ fitting\ procedure}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00642}00642\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ is\ stopped.\ The\ monitor\ can\ be\ used\ for\ various\ things\ such\ as}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00643}00643\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ computing\ held-\/out\ estimates,\ early\ stopping,\ model\ introspect,\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00644}00644\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ snapshotting.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00645}00645\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00646}00646\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00647}00647\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00648}00648\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ self\ :\ object}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00649}00649\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Fitted\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00650}00650\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00651}00651\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1ae8820802e0de5857a681e20526d02d}{warm\_start}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00652}00652\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a405d2b3b46d874f6fffcbc83370f187c}{\_clear\_state}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00653}00653\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00654}00654\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Check\ input}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00655}00655\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Since\ check\_array\ converts\ both\ X\ and\ y\ to\ the\ same\ dtype,\ but\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00656}00656\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ trees\ use\ different\ types\ for\ X\ and\ y,\ checking\ them\ separately.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00657}00657\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00658}00658\ \ \ \ \ \ \ \ \ X,\ y\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00659}00659\ \ \ \ \ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00660}00660\ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00661}00661\ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00662}00662\ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=[\textcolor{stringliteral}{"{}csr"{}},\ \textcolor{stringliteral}{"{}csc"{}},\ \textcolor{stringliteral}{"{}coo"{}}],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00663}00663\ \ \ \ \ \ \ \ \ \ \ \ \ dtype=DTYPE,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00664}00664\ \ \ \ \ \ \ \ \ \ \ \ \ multi\_output=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00665}00665\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00666}00666\ \ \ \ \ \ \ \ \ sample\_weight\_is\_none\ =\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00667}00667\ \ \ \ \ \ \ \ \ sample\_weight\ =\ \_check\_sample\_weight(sample\_weight,\ X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00668}00668\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\_is\_none:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00669}00669\ \ \ \ \ \ \ \ \ \ \ \ \ y\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a073521f79a79cf03c458ae5589dfc03c}{\_encode\_y}}(y=y,\ sample\_weight=\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00670}00670\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00671}00671\ \ \ \ \ \ \ \ \ \ \ \ \ y\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a073521f79a79cf03c458ae5589dfc03c}{\_encode\_y}}(y=y,\ sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00672}00672\ \ \ \ \ \ \ \ \ y\ =\ column\_or\_1d(y,\ warn=\textcolor{keyword}{True})\ \ \textcolor{comment}{\#\ TODO:\ Is\ this\ still\ required?}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00673}00673\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00674}00674\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ac1061a325c9d0b814121cdd43af8033a}{\_set\_max\_features}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00675}00675\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00676}00676\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ self.loss\ is\ guaranteed\ to\ be\ a\ string}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00677}00677\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a281e9bbf3cf598fa886f5decff2e877f}{\_get\_loss}}(sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00678}00678\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00679}00679\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3e0c24fe0f00a676ed4c638287fa2c2b}{n\_iter\_no\_change}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00680}00680\ \ \ \ \ \ \ \ \ \ \ \ \ stratify\ =\ y\ \textcolor{keywordflow}{if}\ is\_classifier(self)\ \textcolor{keywordflow}{else}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00681}00681\ \ \ \ \ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00682}00682\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00683}00683\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00684}00684\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00685}00685\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00686}00686\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00687}00687\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00688}00688\ \ \ \ \ \ \ \ \ \ \ \ \ )\ =\ train\_test\_split(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00689}00689\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00690}00690\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00691}00691\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00692}00692\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad1c716f9af251ba9f32b00fd644c1afa}{random\_state}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00693}00693\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ test\_size=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7d11a5d3f0f12d31ae1b21055b87d732}{validation\_fraction}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00694}00694\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ stratify=stratify,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00695}00695\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00696}00696\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ is\_classifier(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00697}00697\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.n\_classes\_\ !=\ np.unique(y\_train).shape[0]:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00698}00698\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ We\ choose\ to\ error\ here.\ The\ problem\ is\ that\ the\ init}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00699}00699\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ estimator\ would\ be\ trained\ on\ y,\ which\ has\ some\ missing}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00700}00700\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ classes\ now,\ so\ its\ predictions\ would\ not\ have\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00701}00701\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ correct\ shape.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00702}00702\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00703}00703\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ training\ data\ after\ the\ early\ stopping\ split\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00704}00704\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}is\ missing\ some\ classes.\ Try\ using\ another\ random\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00705}00705\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}seed."{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00706}00706\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00707}00707\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00708}00708\ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,\ y\_train,\ sample\_weight\_train\ =\ X,\ y,\ sample\_weight}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00709}00709\ \ \ \ \ \ \ \ \ \ \ \ \ X\_val\ =\ y\_val\ =\ sample\_weight\_val\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00710}00710\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00711}00711\ \ \ \ \ \ \ \ \ n\_samples\ =\ X\_train.shape[0]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00712}00712\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00713}00713\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ First\ time\ calling\ fit.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00714}00714\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad4200ac493f1b0888f44fa39f4e57e92}{\_is\_fitted}}():}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00715}00715\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ init\ state}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00716}00716\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a4bc2e618edbdcf28f8e799c1bb51ee07}{\_init\_state}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00717}00717\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00718}00718\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fit\ initial\ model\ and\ initialize\ raw\ predictions}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00719}00719\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ ==\ \textcolor{stringliteral}{"{}zero"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00720}00720\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00721}00721\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ shape=(n\_samples,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}}),}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00722}00722\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ dtype=np.float64,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00723}00723\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00724}00724\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00725}00725\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ XXX\ clean\ this\ once\ we\ have\ a\ support\_sample\_weight\ tag}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00726}00726\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\_is\_none:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00727}00727\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad120e255ed97bd84b8ec3e537af9a67a}{fit}}(X\_train,\ y\_train)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00728}00728\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00729}00729\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ msg\ =\ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00730}00730\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}The\ initial\ estimator\ \{\}\ does\ not\ support\ sample\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00731}00731\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}weights."{}}.format(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}.\_\_class\_\_.\_\_name\_\_)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00732}00732\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00733}00733\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00734}00734\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad120e255ed97bd84b8ec3e537af9a67a}{fit}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00735}00735\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,\ y\_train,\ sample\_weight=sample\_weight\_train}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00736}00736\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00737}00737\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ TypeError\ \textcolor{keyword}{as}\ e:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00738}00738\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}unexpected\ keyword\ argument\ 'sample\_weight'"{}}\ \textcolor{keywordflow}{in}\ str(e):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00739}00739\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ regular\ estimator\ without\ SW\ support}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00740}00740\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(msg)\ \textcolor{keyword}{from}\ e}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00741}00741\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:\ \ \textcolor{comment}{\#\ regular\ estimator\ whose\ input\ checking\ failed}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00742}00742\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00743}00743\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ ValueError\ \textcolor{keyword}{as}\ e:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00744}00744\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00745}00745\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}pass\ parameters\ to\ specific\ steps\ of\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00746}00746\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}your\ pipeline\ using\ the\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00747}00747\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}stepname\_\_parameter"{}}\ \textcolor{keywordflow}{in}\ str(e)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00748}00748\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ):\ \ \textcolor{comment}{\#\ pipeline}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00749}00749\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(msg)\ \textcolor{keyword}{from}\ e}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00750}00750\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:\ \ \textcolor{comment}{\#\ regular\ estimator\ whose\ input\ checking\ failed}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00751}00751\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00752}00752\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00753}00753\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}{\_init\_raw\_predictions}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00754}00754\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}},\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}},\ is\_classifier(self)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00755}00755\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00756}00756\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00757}00757\ \ \ \ \ \ \ \ \ \ \ \ \ begin\_at\_stage\ =\ 0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00758}00758\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00759}00759\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ rng\ state\ must\ be\ preserved\ if\ warm\_start\ is\ True}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00760}00760\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa47a9a76d70a3ed7f66141a79f0b0b27}{\_rng}}\ =\ check\_random\_state(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad1c716f9af251ba9f32b00fd644c1afa}{random\_state}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00761}00761\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00762}00762\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ warm\ start:\ this\ is\ not\ the\ first\ time\ fit\ was\ called}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00763}00763\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00764}00764\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ add\ more\ estimators\ to\ fitted\ model}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00765}00765\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ invariant:\ warm\_start\ =\ True}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00766}00766\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}\ <\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0]:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00767}00767\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00768}00768\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}n\_estimators=\%d\ must\ be\ larger\ or\ equal\ to\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00769}00769\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}estimators\_.shape[0]=\%d\ when\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00770}00770\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}warm\_start==True"{}}\ \%\ (self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}},\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0])}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00771}00771\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00772}00772\ \ \ \ \ \ \ \ \ \ \ \ \ begin\_at\_stage\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00773}00773\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ The\ requirements\ of\ \_raw\_predict}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00774}00774\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ are\ more\ constrained\ than\ fit.\ It\ accepts\ only\ CSR}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00775}00775\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ matrices.\ Finite\ values\ have\ already\ been\ checked\ in\ \_validate\_data.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00776}00776\ \ \ \ \ \ \ \ \ \ \ \ \ X\_train\ =\ check\_array(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00777}00777\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00778}00778\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ dtype=DTYPE,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00779}00779\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ order=\textcolor{stringliteral}{"{}C"{}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00780}00780\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00781}00781\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ensure\_all\_finite=\textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00782}00782\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00783}00783\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8b9afe035abd471e6af90a6ac0866c23}{\_raw\_predict}}(X\_train)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00784}00784\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3a730a69a39594e1fff891c05bf50c0a}{\_resize\_state}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00785}00785\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00786}00786\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fit\ the\ boosting\ stages}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00787}00787\ \ \ \ \ \ \ \ \ n\_stages\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7ff9d230e95e10bbf2d48a91ab92bdeb}{\_fit\_stages}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00788}00788\ \ \ \ \ \ \ \ \ \ \ \ \ X\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00789}00789\ \ \ \ \ \ \ \ \ \ \ \ \ y\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00790}00790\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00791}00791\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\_train,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00792}00792\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa47a9a76d70a3ed7f66141a79f0b0b27}{\_rng}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00793}00793\ \ \ \ \ \ \ \ \ \ \ \ \ X\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00794}00794\ \ \ \ \ \ \ \ \ \ \ \ \ y\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00795}00795\ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00796}00796\ \ \ \ \ \ \ \ \ \ \ \ \ begin\_at\_stage,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00797}00797\ \ \ \ \ \ \ \ \ \ \ \ \ monitor,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00798}00798\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00799}00799\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00800}00800\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ change\ shape\ of\ arrays\ after\ fit\ (early-\/stopping\ or\ additional\ ests)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00801}00801\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\_stages\ !=\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0]:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00802}00802\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[:n\_stages]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00803}00803\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}[:n\_stages]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00804}00804\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(self,\ \textcolor{stringliteral}{"{}oob\_improvement\_"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00805}00805\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ OOB\ scores\ were\ computed}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00806}00806\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}[:n\_stages]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00807}00807\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[:n\_stages]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00808}00808\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[-\/1]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00809}00809\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_af6c3a43f6267905523ac2d8cb8fe922b}{n\_estimators\_}}\ =\ n\_stages}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00810}00810\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00811}00811\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00812}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7ff9d230e95e10bbf2d48a91ab92bdeb}{00812}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7ff9d230e95e10bbf2d48a91ab92bdeb}{\_fit\_stages}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00813}00813\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00814}00814\ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00815}00815\ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00816}00816\ \ \ \ \ \ \ \ \ raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00817}00817\ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00818}00818\ \ \ \ \ \ \ \ \ random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00819}00819\ \ \ \ \ \ \ \ \ X\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00820}00820\ \ \ \ \ \ \ \ \ y\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00821}00821\ \ \ \ \ \ \ \ \ sample\_weight\_val,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00822}00822\ \ \ \ \ \ \ \ \ begin\_at\_stage=0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00823}00823\ \ \ \ \ \ \ \ \ monitor=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00824}00824\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00825}00825\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Iteratively\ fits\ the\ stages.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00826}00826\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00827}00827\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ each\ stage\ it\ computes\ the\ progress\ (OOB,\ train\ score)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00828}00828\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ delegates\ to\ \`{}\`{}\_fit\_stage\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00829}00829\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns\ the\ number\ of\ stages\ fit;\ might\ differ\ from\ \`{}\`{}n\_estimators\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00830}00830\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ due\ to\ early\ stopping.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00831}00831\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00832}00832\ \ \ \ \ \ \ \ \ n\_samples\ =\ X.shape[0]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00833}00833\ \ \ \ \ \ \ \ \ do\_oob\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aee4f6519e504131df8c3a6437b9cf638}{subsample}}\ <\ 1.0}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00834}00834\ \ \ \ \ \ \ \ \ sample\_mask\ =\ np.ones((n\_samples,),\ dtype=bool)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00835}00835\ \ \ \ \ \ \ \ \ n\_inbag\ =\ max(1,\ int(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aee4f6519e504131df8c3a6437b9cf638}{subsample}}\ *\ n\_samples))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00836}00836\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00837}00837\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5ada7328475c366b38184dea0607633b}{verbose}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00838}00838\ \ \ \ \ \ \ \ \ \ \ \ \ verbose\_reporter\ =\ \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1VerboseReporter}{VerboseReporter}}(verbose=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5ada7328475c366b38184dea0607633b}{verbose}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00839}00839\ \ \ \ \ \ \ \ \ \ \ \ \ verbose\_reporter.init(self,\ begin\_at\_stage)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00840}00840\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00841}00841\ \ \ \ \ \ \ \ \ X\_csc\ =\ \mbox{\hyperlink{classscipy_1_1sparse_1_1__csc_1_1csc__matrix}{csc\_matrix}}(X)\ \textcolor{keywordflow}{if}\ issparse(X)\ \textcolor{keywordflow}{else}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00842}00842\ \ \ \ \ \ \ \ \ X\_csr\ =\ \mbox{\hyperlink{classscipy_1_1sparse_1_1__csr_1_1csr__matrix}{csr\_matrix}}(X)\ \textcolor{keywordflow}{if}\ issparse(X)\ \textcolor{keywordflow}{else}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00843}00843\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00844}00844\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3e0c24fe0f00a676ed4c638287fa2c2b}{n\_iter\_no\_change}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00845}00845\ \ \ \ \ \ \ \ \ \ \ \ \ loss\_history\ =\ np.full(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3e0c24fe0f00a676ed4c638287fa2c2b}{n\_iter\_no\_change}},\ np.inf)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00846}00846\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ We\ create\ a\ generator\ to\ get\ the\ predictions\ for\ X\_val\ after}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00847}00847\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ the\ addition\ of\ each\ successive\ stage}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00848}00848\ \ \ \ \ \ \ \ \ \ \ \ \ y\_val\_pred\_iter\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X\_val,\ check\_input=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00849}00849\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00850}00850\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Older\ versions\ of\ GBT\ had\ its\ own\ loss\ functions.\ With\ the\ new\ common}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00851}00851\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ private\ loss\ function\ submodule\ \_loss,\ we\ often\ are\ a\ factor\ of\ 2}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00852}00852\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ away\ from\ the\ old\ version.\ Here\ we\ keep\ backward\ compatibility\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00853}00853\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ oob\_scores\_\ and\ oob\_improvement\_,\ even\ if\ the\ old\ way\ is\ quite}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00854}00854\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ inconsistent\ (sometimes\ the\ gradient\ is\ half\ the\ gradient,\ sometimes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00855}00855\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ not).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00856}00856\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00857}00857\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00858}00858\ \ \ \ \ \ \ \ \ \ \ \ \ (}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00859}00859\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ HalfSquaredError,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00860}00860\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ HalfBinomialLoss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00861}00861\ \ \ \ \ \ \ \ \ \ \ \ \ ),}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00862}00862\ \ \ \ \ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00863}00863\ \ \ \ \ \ \ \ \ \ \ \ \ factor\ =\ 2}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00864}00864\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00865}00865\ \ \ \ \ \ \ \ \ \ \ \ \ factor\ =\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00866}00866\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00867}00867\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ perform\ boosting\ iterations}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00868}00868\ \ \ \ \ \ \ \ \ i\ =\ begin\_at\_stage}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00869}00869\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i\ \textcolor{keywordflow}{in}\ range(begin\_at\_stage,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a86e55d7edcb9814e8cdb4dfdc522ba94}{n\_estimators}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00870}00870\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ subsampling}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00871}00871\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ do\_oob:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00872}00872\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_mask\ =\ \_random\_sample\_mask(n\_samples,\ n\_inbag,\ random\_state)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00873}00873\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_oob\_masked\ =\ y[\string~sample\_mask]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00874}00874\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight\_oob\_masked\ =\ sample\_weight[\string~sample\_mask]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00875}00875\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ i\ ==\ 0:\ \ \textcolor{comment}{\#\ store\ the\ initial\ loss\ to\ compute\ the\ OOB\ score}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00876}00876\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ initial\_loss\ =\ factor\ *\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00877}00877\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_oob\_masked,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00878}00878\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions[\string~sample\_mask],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00879}00879\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight\_oob\_masked,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00880}00880\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00881}00881\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00882}00882\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ fit\ next\ stage\ of\ trees}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00883}00883\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a58d22e9f2f5a7da25f25a4bde9644660}{\_fit\_stage}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00884}00884\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ i,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00885}00885\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00886}00886\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00887}00887\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00888}00888\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00889}00889\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_mask,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00890}00890\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00891}00891\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_csc=X\_csc,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00892}00892\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X\_csr=X\_csr,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00893}00893\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00894}00894\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00895}00895\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ track\ loss}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00896}00896\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ do\_oob:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00897}00897\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}[i]\ =\ factor\ *\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00898}00898\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y[sample\_mask],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00899}00899\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions[sample\_mask],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00900}00900\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight[sample\_mask],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00901}00901\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00902}00902\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[i]\ =\ factor\ *\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00903}00903\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y\_oob\_masked,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00904}00904\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions[\string~sample\_mask],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00905}00905\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight\_oob\_masked,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00906}00906\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00907}00907\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ previous\_loss\ =\ initial\_loss\ \textcolor{keywordflow}{if}\ i\ ==\ 0\ \textcolor{keywordflow}{else}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[i\ -\/\ 1]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00908}00908\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3ea20b018b7980bc37910acca98a910d}{oob\_improvement\_}}[i]\ =\ previous\_loss\ -\/\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[i]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00909}00909\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad9c539109b901f618f6507f069aa48de}{oob\_score\_}}\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a496a4703ad0fbdcc53036b7335055a56}{oob\_scores\_}}[-\/1]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00910}00910\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00911}00911\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ no\ need\ to\ fancy\ index\ w/\ no\ subsampling}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00912}00912\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abca4f6147265004a8ead8d07e5af1f64}{train\_score\_}}[i]\ =\ factor\ *\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00913}00913\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_true=y,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00914}00914\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ raw\_prediction=raw\_predictions,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00915}00915\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00916}00916\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00917}00917\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00918}00918\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a5ada7328475c366b38184dea0607633b}{verbose}}\ >\ 0:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00919}00919\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ verbose\_reporter.update(i,\ self)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00920}00920\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00921}00921\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ monitor\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00922}00922\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ early\_stopping\ =\ monitor(i,\ self,\ locals())}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00923}00923\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ early\_stopping:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00924}00924\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{break}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00925}00925\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00926}00926\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ We\ also\ provide\ an\ early\ stopping\ based\ on\ the\ score\ from}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00927}00927\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ validation\ set\ (X\_val,\ y\_val),\ if\ n\_iter\_no\_change\ is\ set}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00928}00928\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a3e0c24fe0f00a676ed4c638287fa2c2b}{n\_iter\_no\_change}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00929}00929\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ By\ calling\ next(y\_val\_pred\_iter),\ we\ get\ the\ predictions}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00930}00930\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ for\ X\_val\ after\ the\ addition\ of\ the\ current\ stage}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00931}00931\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ validation\_loss\ =\ factor\ *\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00932}00932\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\_val,\ next(y\_val\_pred\_iter),\ sample\_weight\_val}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00933}00933\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00934}00934\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00935}00935\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Require\ validation\_score\ to\ be\ better\ (less)\ than\ at\ least}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00936}00936\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ one\ of\ the\ last\ n\_iter\_no\_change\ evaluations}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00937}00937\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ np.any(validation\_loss\ +\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ad186c728c5385a367911957d8e8de89e}{tol}}\ <\ loss\_history):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00938}00938\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ loss\_history[i\ \%\ len(loss\_history)]\ =\ validation\_loss}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00939}00939\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00940}00940\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{break}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00941}00941\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00942}00942\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ i\ +\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00943}00943\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00944}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a2ba595e9882c09cf6c3aa83f3482495d}{00944}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a2ba595e9882c09cf6c3aa83f3482495d}{\_make\_estimator}}(self,\ append=True):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00945}00945\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ don't\ need\ \_make\_estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00946}00946\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classNotImplementedError}{NotImplementedError}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00947}00947\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00948}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ab0324bc11588acde6506db2ebc9996d1}{00948}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ab0324bc11588acde6506db2ebc9996d1}{\_raw\_predict\_init}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00949}00949\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Check\ input\ and\ compute\ raw\ predictions\ of\ the\ init\ estimator."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00950}00950\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8dde52984929a182e5348660db9b6baa}{\_check\_initialized}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00951}00951\ \ \ \ \ \ \ \ \ X\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[0,\ 0].\_validate\_X\_predict(X,\ check\_input=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00952}00952\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}}\ ==\ \textcolor{stringliteral}{"{}zero"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00953}00953\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00954}00954\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ shape=(X.shape[0],\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}}),\ dtype=np.float64}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00955}00955\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00956}00956\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00957}00957\ \ \ \ \ \ \ \ \ \ \ \ \ raw\_predictions\ =\ \mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__gb_a11e9a9652fe23b28198c5c62f8b8d8f5}{\_init\_raw\_predictions}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00958}00958\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a7be000b92c76c8899e083ff6b574f295}{init\_}},\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}},\ is\_classifier(self)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00959}00959\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00960}00960\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ raw\_predictions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00961}00961\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00962}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8b9afe035abd471e6af90a6ac0866c23}{00962}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8b9afe035abd471e6af90a6ac0866c23}{\_raw\_predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00963}00963\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Return\ the\ sum\ of\ the\ trees\ raw\ predictions\ (+\ init\ estimator)."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00964}00964\ \ \ \ \ \ \ \ \ check\_is\_fitted(self)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00965}00965\ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ab0324bc11588acde6506db2ebc9996d1}{\_raw\_predict\_init}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00966}00966\ \ \ \ \ \ \ \ \ predict\_stages(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}},\ X,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa39daa68b8c51561233d0dae6fc56102}{learning\_rate}},\ raw\_predictions)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00967}00967\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ raw\_predictions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00968}00968\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00969}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{00969}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(self,\ X,\ check\_input=True):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00970}00970\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ raw\ predictions\ of\ \`{}\`{}X\`{}\`{}\ for\ each\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00971}00971\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00972}00972\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ allows\ monitoring\ (i.e.\ determine\ error\ on\ testing\ set)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00973}00973\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ after\ each\ stage.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00974}00974\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00975}00975\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00976}00976\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00977}00977\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00978}00978\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00979}00979\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00980}00980\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00981}00981\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00982}00982\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ check\_input\ :\ bool,\ default=True}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00983}00983\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ False,\ the\ input\ arrays\ X\ will\ not\ be\ checked.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00984}00984\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00985}00985\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00986}00986\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00987}00987\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ raw\_predictions\ :\ generator\ of\ ndarray\ of\ shape\ (n\_samples,\ k)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00988}00988\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ raw\ predictions\ of\ the\ input\ samples.\ The\ order\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00989}00989\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ classes\ corresponds\ to\ that\ in\ the\ attribute\ :term:\`{}classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00990}00990\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Regression\ and\ binary\ classification\ are\ special\ cases\ with}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00991}00991\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}k\ ==\ 1\`{}\`{},\ otherwise\ \`{}\`{}k==n\_classes\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00992}00992\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00993}00993\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ check\_input:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00994}00994\ \ \ \ \ \ \ \ \ \ \ \ \ X\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00995}00995\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self,\ X,\ dtype=DTYPE,\ order=\textcolor{stringliteral}{"{}C"{}},\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ reset=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00996}00996\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00997}00997\ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ab0324bc11588acde6506db2ebc9996d1}{\_raw\_predict\_init}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00998}00998\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i\ \textcolor{keywordflow}{in}\ range(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0]):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l00999}00999\ \ \ \ \ \ \ \ \ \ \ \ \ predict\_stage(self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}},\ i,\ X,\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa39daa68b8c51561233d0dae6fc56102}{learning\_rate}},\ raw\_predictions)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01000}01000\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ raw\_predictions.copy()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01001}01001\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01002}01002\ \ \ \ \ \textcolor{preprocessor}{@property}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01003}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a6bdb912f2ed55075cdcd7e0ef95999b1}{01003}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a6bdb912f2ed55075cdcd7e0ef95999b1}{feature\_importances\_}}(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01004}01004\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}The\ impurity-\/based\ feature\ importances.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01005}01005\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01006}01006\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ higher,\ the\ more\ important\ the\ feature.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01007}01007\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ importance\ of\ a\ feature\ is\ computed\ as\ the\ (normalized)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01008}01008\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ total\ reduction\ of\ the\ criterion\ brought\ by\ that\ feature.\ \ It\ is\ also}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01009}01009\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ known\ as\ the\ Gini\ importance.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01010}01010\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01011}01011\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Warning:\ impurity-\/based\ feature\ importances\ can\ be\ misleading\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01012}01012\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ high\ cardinality\ features\ (many\ unique\ values).\ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01013}01013\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :func:\`{}sklearn.inspection.permutation\_importance\`{}\ as\ an\ alternative.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01014}01014\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01015}01015\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01016}01016\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01017}01017\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ feature\_importances\_\ :\ ndarray\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01018}01018\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ values\ of\ this\ array\ sum\ to\ 1,\ unless\ all\ trees\ are\ single\ node}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01019}01019\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ trees\ consisting\ of\ only\ the\ root\ node,\ in\ which\ case\ it\ will\ be\ an}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01020}01020\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ array\ of\ zeros.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01021}01021\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01022}01022\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8dde52984929a182e5348660db9b6baa}{\_check\_initialized}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01023}01023\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01024}01024\ \ \ \ \ \ \ \ \ relevant\_trees\ =\ [}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01025}01025\ \ \ \ \ \ \ \ \ \ \ \ \ tree}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01026}01026\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ stage\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01027}01027\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ tree\ \textcolor{keywordflow}{in}\ stage}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01028}01028\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ tree.tree\_.node\_count\ >\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01029}01029\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01030}01030\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ relevant\_trees:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01031}01031\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ degenerate\ case\ where\ all\ trees\ have\ only\ one\ node}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01032}01032\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.zeros(shape=self.n\_features\_in\_,\ dtype=np.float64)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01033}01033\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01034}01034\ \ \ \ \ \ \ \ \ relevant\_feature\_importances\ =\ [}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01035}01035\ \ \ \ \ \ \ \ \ \ \ \ \ tree.tree\_.compute\_feature\_importances(normalize=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01036}01036\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ tree\ \textcolor{keywordflow}{in}\ relevant\_trees}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01037}01037\ \ \ \ \ \ \ \ \ ]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01038}01038\ \ \ \ \ \ \ \ \ avg\_feature\_importances\ =\ np.mean(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01039}01039\ \ \ \ \ \ \ \ \ \ \ \ \ relevant\_feature\_importances,\ axis=0,\ dtype=np.float64}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01040}01040\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01041}01041\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ avg\_feature\_importances\ /\ np.sum(avg\_feature\_importances)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01042}01042\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01043}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aaf075b0f9f925142b8eb7721915534cb}{01043}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aaf075b0f9f925142b8eb7721915534cb}{\_compute\_partial\_dependence\_recursion}}(self,\ grid,\ target\_features):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01044}01044\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Fast\ partial\ dependence\ computation.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01045}01045\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01046}01046\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01047}01047\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01048}01048\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ grid\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_target\_features),\ dtype=np.float32}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01049}01049\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ grid\ points\ on\ which\ the\ partial\ dependence\ should\ be}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01050}01050\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ evaluated.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01051}01051\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ target\_features\ :\ ndarray\ of\ shape\ (n\_target\_features,),\ dtype=np.intp}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01052}01052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ set\ of\ target\ features\ for\ which\ the\ partial\ dependence}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01053}01053\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ should\ be\ evaluated.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01054}01054\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01055}01055\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01056}01056\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01057}01057\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ averaged\_predictions\ :\ ndarray\ of\ shape\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01058}01058\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (n\_trees\_per\_iteration\_,\ n\_samples)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01059}01059\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ value\ of\ the\ partial\ dependence\ function\ on\ each\ grid\ point.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01060}01060\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01061}01061\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a93146cd020942c879c20928fe5b189b8}{init}}\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01062}01062\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01063}01063\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Using\ recursion\ method\ with\ a\ non-\/constant\ init\ predictor\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01064}01064\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}will\ lead\ to\ incorrect\ partial\ dependence\ values.\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01065}01065\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Got\ init=\%s."{}}\ \%\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a93146cd020942c879c20928fe5b189b8}{init}},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01066}01066\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ UserWarning,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01067}01067\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01068}01068\ \ \ \ \ \ \ \ \ grid\ =\ np.asarray(grid,\ dtype=DTYPE,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01069}01069\ \ \ \ \ \ \ \ \ n\_estimators,\ n\_trees\_per\_stage\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01070}01070\ \ \ \ \ \ \ \ \ averaged\_predictions\ =\ np.zeros(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01071}01071\ \ \ \ \ \ \ \ \ \ \ \ \ (n\_trees\_per\_stage,\ grid.shape[0]),\ dtype=np.float64,\ order=\textcolor{stringliteral}{"{}C"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01072}01072\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01073}01073\ \ \ \ \ \ \ \ \ target\_features\ =\ np.asarray(target\_features,\ dtype=np.intp,\ order=\textcolor{stringliteral}{"{}C"{}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01074}01074\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01075}01075\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ stage\ \textcolor{keywordflow}{in}\ range(n\_estimators):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01076}01076\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(n\_trees\_per\_stage):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01077}01077\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tree\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[stage,\ k].tree\_}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01078}01078\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tree.compute\_partial\_dependence(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01079}01079\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ grid,\ target\_features,\ averaged\_predictions[k]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01080}01080\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01081}01081\ \ \ \ \ \ \ \ \ averaged\_predictions\ *=\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_aa39daa68b8c51561233d0dae6fc56102}{learning\_rate}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01082}01082\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01083}01083\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ averaged\_predictions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01084}01084\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01085}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a4da0b5c905a85c8f38430f6679e26ce3}{01085}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a4da0b5c905a85c8f38430f6679e26ce3}{apply}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01086}01086\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Apply\ trees\ in\ the\ ensemble\ to\ X,\ return\ leaf\ indices.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01087}01087\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01088}01088\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.17}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01089}01089\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01090}01090\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01091}01091\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01092}01092\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01093}01093\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ its\ dtype\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01094}01094\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}.\ If\ a\ sparse\ matrix\ is\ provided,\ it\ will}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01095}01095\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ be\ converted\ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01096}01096\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01097}01097\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01098}01098\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01099}01099\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\_leaves\ :\ array-\/like\ of\ shape\ (n\_samples,\ n\_estimators,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01100}01100\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ For\ each\ datapoint\ x\ in\ X\ and\ for\ each\ tree\ in\ the\ ensemble,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01101}01101\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ return\ the\ index\ of\ the\ leaf\ x\ ends\ up\ in\ each\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01102}01102\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ In\ the\ case\ of\ binary\ classification\ n\_classes\ is\ 1.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01103}01103\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01104}01104\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01105}01105\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8dde52984929a182e5348660db9b6baa}{\_check\_initialized}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01106}01106\ \ \ \ \ \ \ \ \ X\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[0,\ 0].\_validate\_X\_predict(X,\ check\_input=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01107}01107\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01108}01108\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ n\_classes\ will\ be\ equal\ to\ 1\ in\ the\ binary\ classification\ or\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01109}01109\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ regression\ case.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01110}01110\ \ \ \ \ \ \ \ \ n\_estimators,\ n\_classes\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01111}01111\ \ \ \ \ \ \ \ \ leaves\ =\ np.zeros((X.shape[0],\ n\_estimators,\ n\_classes))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01112}01112\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01113}01113\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i\ \textcolor{keywordflow}{in}\ range(n\_estimators):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01114}01114\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ j\ \textcolor{keywordflow}{in}\ range(n\_classes):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01115}01115\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ estimator\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}[i,\ j]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01116}01116\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ leaves[:,\ i,\ j]\ =\ estimator.apply(X,\ check\_input=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01117}01117\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01118}01118\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ leaves}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01119}01119\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01120}01120\ \ \ \ \ \textcolor{keyword}{def\ }\_\_sklearn\_tags\_\_(self):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01121}01121\ \ \ \ \ \ \ \ \ tags\ =\ super().\_\_sklearn\_tags\_\_()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01122}01122\ \ \ \ \ \ \ \ \ tags.input\_tags.sparse\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01123}01123\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ tags}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01124}01124\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01125}01125\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01126}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier}{01126}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier}{GradientBoostingClassifier}}(\mbox{\hyperlink{classsklearn_1_1base_1_1ClassifierMixin}{ClassifierMixin}},\ \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting}{BaseGradientBoosting}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01127}01127\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Gradient\ Boosting\ for\ classification.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01128}01128\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01129}01129\ \textcolor{stringliteral}{\ \ \ \ This\ algorithm\ builds\ an\ additive\ model\ in\ a\ forward\ stage-\/wise\ fashion;\ it}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01130}01130\ \textcolor{stringliteral}{\ \ \ \ allows\ for\ the\ optimization\ of\ arbitrary\ differentiable\ loss\ functions.\ In}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01131}01131\ \textcolor{stringliteral}{\ \ \ \ each\ stage\ \`{}\`{}n\_classes\_\`{}\`{}\ regression\ trees\ are\ fit\ on\ the\ negative\ gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01132}01132\ \textcolor{stringliteral}{\ \ \ \ of\ the\ loss\ function,\ e.g.\ binary\ or\ multiclass\ log\ loss.\ Binary}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01133}01133\ \textcolor{stringliteral}{\ \ \ \ classification\ is\ a\ special\ case\ where\ only\ a\ single\ regression\ tree\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01134}01134\ \textcolor{stringliteral}{\ \ \ \ induced.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01135}01135\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01136}01136\ \textcolor{stringliteral}{\ \ \ \ :class:\`{}\string~sklearn.ensemble.HistGradientBoostingClassifier\`{}\ is\ a\ much\ faster\ variant}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01137}01137\ \textcolor{stringliteral}{\ \ \ \ of\ this\ algorithm\ for\ intermediate\ and\ large\ datasets\ (\`{}n\_samples\ >=\ 10\_000\`{})\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01138}01138\ \textcolor{stringliteral}{\ \ \ \ supports\ monotonic\ constraints.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01139}01139\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01140}01140\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <gradient\_boosting>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01141}01141\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01142}01142\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01143}01143\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01144}01144\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ \{'log\_loss',\ 'exponential'\},\ default='log\_loss'}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01145}01145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ loss\ function\ to\ be\ optimized.\ 'log\_loss'\ refers\ to\ binomial\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01146}01146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ multinomial\ deviance,\ the\ same\ as\ used\ in\ logistic\ regression.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01147}01147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ is\ a\ good\ choice\ for\ classification\ with\ probabilistic\ outputs.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01148}01148\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ loss\ 'exponential',\ gradient\ boosting\ recovers\ the\ AdaBoost\ algorithm.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01149}01149\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01150}01150\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01151}01151\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Learning\ rate\ shrinks\ the\ contribution\ of\ each\ tree\ by\ \`{}learning\_rate\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01152}01152\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ There\ is\ a\ trade-\/off\ between\ learning\_rate\ and\ n\_estimators.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01153}01153\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01154}01154\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01155}01155\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ For\ an\ example\ of\ the\ effects\ of\ this\ parameter\ and\ its\ interaction\ with}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01156}01156\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}subsample\`{}\`{},\ see}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01157}01157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regularization.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01158}01158\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01159}01159\ \textcolor{stringliteral}{\ \ \ \ n\_estimators\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01160}01160\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ boosting\ stages\ to\ perform.\ Gradient\ boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01161}01161\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ is\ fairly\ robust\ to\ over-\/fitting\ so\ a\ large\ number\ usually}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01162}01162\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ results\ in\ better\ performance.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01163}01163\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01164}01164\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01165}01165\ \textcolor{stringliteral}{\ \ \ \ subsample\ :\ float,\ default=1.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01166}01166\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ fraction\ of\ samples\ to\ be\ used\ for\ fitting\ the\ individual\ base}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01167}01167\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learners.\ If\ smaller\ than\ 1.0\ this\ results\ in\ Stochastic\ Gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01168}01168\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Boosting.\ \`{}subsample\`{}\ interacts\ with\ the\ parameter\ \`{}n\_estimators\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01169}01169\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Choosing\ \`{}subsample\ <\ 1.0\`{}\ leads\ to\ a\ reduction\ of\ variance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01170}01170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ an\ increase\ in\ bias.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01171}01171\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01172}01172\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01173}01173\ \textcolor{stringliteral}{\ \ \ \ criterion\ :\ \{'friedman\_mse',\ 'squared\_error'\},\ default='friedman\_mse'}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01174}01174\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ function\ to\ measure\ the\ quality\ of\ a\ split.\ Supported\ criteria\ are}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01175}01175\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'friedman\_mse'\ for\ the\ mean\ squared\ error\ with\ improvement\ score\ by}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01176}01176\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Friedman,\ 'squared\_error'\ for\ mean\ squared\ error.\ The\ default\ value\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01177}01177\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'friedman\_mse'\ is\ generally\ the\ best\ as\ it\ can\ provide\ a\ better}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01178}01178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ approximation\ in\ some\ cases.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01179}01179\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01180}01180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01181}01181\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01182}01182\ \textcolor{stringliteral}{\ \ \ \ min\_samples\_split\ :\ int\ or\ float,\ default=2}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01183}01183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ number\ of\ samples\ required\ to\ split\ an\ internal\ node:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01184}01184\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01185}01185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[2,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01186}01186\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}\ and\ \`{}min\_samples\_split\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01187}01187\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ will\ be\ \`{}ceil(min\_samples\_split\ *\ n\_samples)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01188}01188\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01189}01189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01190}01190\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Added\ float\ values\ for\ fractions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01191}01191\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01192}01192\ \textcolor{stringliteral}{\ \ \ \ min\_samples\_leaf\ :\ int\ or\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01193}01193\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ number\ of\ samples\ required\ to\ be\ at\ a\ leaf\ node.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01194}01194\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ A\ split\ point\ at\ any\ depth\ will\ only\ be\ considered\ if\ it\ leaves\ at}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01195}01195\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ least\ \`{}\`{}min\_samples\_leaf\`{}\`{}\ training\ samples\ in\ each\ of\ the\ left\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01196}01196\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ right\ branches.\ \ This\ may\ have\ the\ effect\ of\ smoothing\ the\ model,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01197}01197\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ especially\ in\ regression.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01198}01198\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01199}01199\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01200}01200\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}\ and\ \`{}min\_samples\_leaf\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01201}01201\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ will\ be\ \`{}ceil(min\_samples\_leaf\ *\ n\_samples)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01202}01202\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01203}01203\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01204}01204\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Added\ float\ values\ for\ fractions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01205}01205\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01206}01206\ \textcolor{stringliteral}{\ \ \ \ min\_weight\_fraction\_leaf\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01207}01207\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ weighted\ fraction\ of\ the\ sum\ total\ of\ weights\ (of\ all}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01208}01208\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ input\ samples)\ required\ to\ be\ at\ a\ leaf\ node.\ Samples\ have}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01209}01209\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ equal\ weight\ when\ sample\_weight\ is\ not\ provided.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01210}01210\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ 0.5]\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01211}01211\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01212}01212\ \textcolor{stringliteral}{\ \ \ \ max\_depth\ :\ int\ or\ None,\ default=3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01213}01213\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Maximum\ depth\ of\ the\ individual\ regression\ estimators.\ The\ maximum}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01214}01214\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ depth\ limits\ the\ number\ of\ nodes\ in\ the\ tree.\ Tune\ this\ parameter}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01215}01215\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ best\ performance;\ the\ best\ value\ depends\ on\ the\ interaction}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01216}01216\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ of\ the\ input\ variables.\ If\ None,\ then\ nodes\ are\ expanded\ until}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01217}01217\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ all\ leaves\ are\ pure\ or\ until\ all\ leaves\ contain\ less\ than}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01218}01218\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\_samples\_split\ samples.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01219}01219\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01220}01220\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01221}01221\ \textcolor{stringliteral}{\ \ \ \ min\_impurity\_decrease\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01222}01222\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ A\ node\ will\ be\ split\ if\ this\ split\ induces\ a\ decrease\ of\ the\ impurity}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01223}01223\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ greater\ than\ or\ equal\ to\ this\ value.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01224}01224\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01225}01225\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01226}01226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weighted\ impurity\ decrease\ equation\ is\ the\ following::}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01227}01227\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01228}01228\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ N\_t\ /\ N\ *\ (impurity\ -\/\ N\_t\_R\ /\ N\_t\ *\ right\_impurity}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01229}01229\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ N\_t\_L\ /\ N\_t\ *\ left\_impurity)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01230}01230\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01231}01231\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}N\`{}\`{}\ is\ the\ total\ number\ of\ samples,\ \`{}\`{}N\_t\`{}\`{}\ is\ the\ number\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01232}01232\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples\ at\ the\ current\ node,\ \`{}\`{}N\_t\_L\`{}\`{}\ is\ the\ number\ of\ samples\ in\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01233}01233\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ left\ child,\ and\ \`{}\`{}N\_t\_R\`{}\`{}\ is\ the\ number\ of\ samples\ in\ the\ right\ child.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01234}01234\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01235}01235\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}N\`{}\`{},\ \`{}\`{}N\_t\`{}\`{},\ \`{}\`{}N\_t\_R\`{}\`{}\ and\ \`{}\`{}N\_t\_L\`{}\`{}\ all\ refer\ to\ the\ weighted\ sum,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01236}01236\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ \`{}\`{}sample\_weight\`{}\`{}\ is\ passed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01237}01237\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01238}01238\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01239}01239\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01240}01240\ \textcolor{stringliteral}{\ \ \ \ init\ :\ estimator\ or\ 'zero',\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01241}01241\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ An\ estimator\ object\ that\ is\ used\ to\ compute\ the\ initial\ predictions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01242}01242\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}init\`{}\`{}\ has\ to\ provide\ :term:\`{}fit\`{}\ and\ :term:\`{}predict\_proba\`{}.\ If}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01243}01243\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ 'zero',\ the\ initial\ raw\ predictions\ are\ set\ to\ zero.\ By\ default,\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01244}01244\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}DummyEstimator\`{}\`{}\ predicting\ the\ classes\ priors\ is\ used.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01245}01245\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01246}01246\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance\ or\ None,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01247}01247\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Controls\ the\ random\ seed\ given\ to\ each\ Tree\ estimator\ at\ each}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01248}01248\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ boosting\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01249}01249\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ In\ addition,\ it\ controls\ the\ random\ permutation\ of\ the\ features\ at}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01250}01250\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ each\ split\ (see\ Notes\ for\ more\ details).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01251}01251\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ also\ controls\ the\ random\ splitting\ of\ the\ training\ data\ to\ obtain\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01252}01252\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ set\ if\ \`{}n\_iter\_no\_change\`{}\ is\ not\ None.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01253}01253\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Pass\ an\ int\ for\ reproducible\ output\ across\ multiple\ function\ calls.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01254}01254\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}Glossary\ <random\_state>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01255}01255\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01256}01256\ \textcolor{stringliteral}{\ \ \ \ max\_features\ :\ \{'sqrt',\ 'log2'\},\ int\ or\ float,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01257}01257\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ features\ to\ consider\ when\ looking\ for\ the\ best\ split:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01258}01258\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01259}01259\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01260}01260\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}\ and\ the\ features}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01261}01261\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ considered\ at\ each\ split\ will\ be\ \`{}max(1,\ int(max\_features\ *\ n\_features\_in\_))\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01262}01262\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ 'sqrt',\ then\ \`{}max\_features=sqrt(n\_features)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01263}01263\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ 'log2',\ then\ \`{}max\_features=log2(n\_features)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01264}01264\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ None,\ then\ \`{}max\_features=n\_features\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01265}01265\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01266}01266\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Choosing\ \`{}max\_features\ <\ n\_features\`{}\ leads\ to\ a\ reduction\ of\ variance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01267}01267\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ an\ increase\ in\ bias.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01268}01268\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01269}01269\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Note:\ the\ search\ for\ a\ split\ does\ not\ stop\ until\ at\ least\ one}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01270}01270\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ valid\ partition\ of\ the\ node\ samples\ is\ found,\ even\ if\ it\ requires\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01271}01271\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ effectively\ inspect\ more\ than\ \`{}\`{}max\_features\`{}\`{}\ features.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01272}01272\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01273}01273\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01274}01274\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Enable\ verbose\ output.\ If\ 1\ then\ it\ prints\ progress\ and\ performance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01275}01275\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ once\ in\ a\ while\ (the\ more\ trees\ the\ lower\ the\ frequency).\ If\ greater}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01276}01276\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ than\ 1\ then\ it\ prints\ progress\ and\ performance\ for\ every\ tree.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01277}01277\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01278}01278\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01279}01279\ \textcolor{stringliteral}{\ \ \ \ max\_leaf\_nodes\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01280}01280\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Grow\ trees\ with\ \`{}\`{}max\_leaf\_nodes\`{}\`{}\ in\ best-\/first\ fashion.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01281}01281\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Best\ nodes\ are\ defined\ as\ relative\ reduction\ in\ impurity.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01282}01282\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[2,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01283}01283\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ \`{}None\`{},\ then\ unlimited\ number\ of\ leaf\ nodes.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01284}01284\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01285}01285\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01286}01286\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ fit}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01287}01287\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ add\ more\ estimators\ to\ the\ ensemble,\ otherwise,\ just\ erase\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01288}01288\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ previous\ solution.\ See\ :term:\`{}the\ Glossary\ <warm\_start>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01289}01289\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01290}01290\ \textcolor{stringliteral}{\ \ \ \ validation\_fraction\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01291}01291\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ proportion\ of\ training\ data\ to\ set\ aside\ as\ validation\ set\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01292}01292\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ early\ stopping.\ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01293}01293\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ set\ to\ an\ integer.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01294}01294\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01295}01295\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01296}01296\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01297}01297\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_no\_change\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01298}01298\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ used\ to\ decide\ if\ early\ stopping\ will\ be\ used}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01299}01299\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ to\ terminate\ training\ when\ validation\ score\ is\ not\ improving.\ By}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01300}01300\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ default\ it\ is\ set\ to\ None\ to\ disable\ early\ stopping.\ If\ set\ to\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01301}01301\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ number,\ it\ will\ set\ aside\ \`{}\`{}validation\_fraction\`{}\`{}\ size\ of\ the\ training}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01302}01302\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ data\ as\ validation\ and\ terminate\ training\ when\ validation\ score\ is\ not}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01303}01303\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ improving\ in\ all\ of\ the\ previous\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ numbers\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01304}01304\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ iterations.\ The\ split\ is\ stratified.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01305}01305\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01306}01306\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01307}01307\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_early\_stopping.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01308}01308\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01309}01309\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01310}01310\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01311}01311\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01312}01312\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Tolerance\ for\ the\ early\ stopping.\ When\ the\ loss\ is\ not\ improving}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01313}01313\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ by\ at\ least\ tol\ for\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ iterations\ (if\ set\ to\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01314}01314\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ number),\ the\ training\ stops.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01315}01315\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01316}01316\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01317}01317\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01318}01318\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01319}01319\ \textcolor{stringliteral}{\ \ \ \ ccp\_alpha\ :\ non-\/negative\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01320}01320\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Complexity\ parameter\ used\ for\ Minimal\ Cost-\/Complexity\ Pruning.\ The}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01321}01321\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ subtree\ with\ the\ largest\ cost\ complexity\ that\ is\ smaller\ than}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01322}01322\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}ccp\_alpha\`{}\`{}\ will\ be\ chosen.\ By\ default,\ no\ pruning\ is\ performed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01323}01323\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01324}01324\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :ref:\`{}minimal\_cost\_complexity\_pruning\`{}\ for\ details.\ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01325}01325\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_tree\_plot\_cost\_complexity\_pruning.py\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01326}01326\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ an\ example\ of\ such\ pruning.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01327}01327\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01328}01328\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.22}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01329}01329\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01330}01330\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01331}01331\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01332}01332\ \textcolor{stringliteral}{\ \ \ \ n\_estimators\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01333}01333\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ estimators\ as\ selected\ by\ early\ stopping\ (if}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01334}01334\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ specified).\ Otherwise\ it\ is\ set\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01335}01335\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_estimators\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01336}01336\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01337}01337\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01338}01338\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01339}01339\ \textcolor{stringliteral}{\ \ \ \ n\_trees\_per\_iteration\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01340}01340\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ trees\ that\ are\ built\ at\ each\ iteration.\ For\ binary\ classifiers,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01341}01341\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ this\ is\ always\ 1.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01342}01342\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01343}01343\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.4.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01344}01344\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01345}01345\ \textcolor{stringliteral}{\ \ \ \ feature\_importances\_\ :\ ndarray\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01346}01346\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ impurity-\/based\ feature\ importances.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01347}01347\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ higher,\ the\ more\ important\ the\ feature.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01348}01348\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ importance\ of\ a\ feature\ is\ computed\ as\ the\ (normalized)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01349}01349\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ total\ reduction\ of\ the\ criterion\ brought\ by\ that\ feature.\ \ It\ is\ also}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01350}01350\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ known\ as\ the\ Gini\ importance.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01351}01351\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01352}01352\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Warning:\ impurity-\/based\ feature\ importances\ can\ be\ misleading\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01353}01353\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ high\ cardinality\ features\ (many\ unique\ values).\ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01354}01354\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :func:\`{}sklearn.inspection.permutation\_importance\`{}\ as\ an\ alternative.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01355}01355\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01356}01356\ \textcolor{stringliteral}{\ \ \ \ oob\_improvement\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01357}01357\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ improvement\ in\ loss\ on\ the\ out-\/of-\/bag\ samples}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01358}01358\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ relative\ to\ the\ previous\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01359}01359\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}oob\_improvement\_[0]\`{}\`{}\ is\ the\ improvement\ in}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01360}01360\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ of\ the\ first\ stage\ over\ the\ \`{}\`{}init\`{}\`{}\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01361}01361\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ available\ if\ \`{}\`{}subsample\ <\ 1.0\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01362}01362\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01363}01363\ \textcolor{stringliteral}{\ \ \ \ oob\_scores\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01364}01364\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ full\ history\ of\ the\ loss\ values\ on\ the\ out-\/of-\/bag}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01365}01365\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples.\ Only\ available\ if\ \`{}subsample\ <\ 1.0\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01366}01366\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01367}01367\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01368}01368\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01369}01369\ \textcolor{stringliteral}{\ \ \ \ oob\_score\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01370}01370\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ last\ value\ of\ the\ loss\ on\ the\ out-\/of-\/bag\ samples.\ It\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01371}01371\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ same\ as\ \`{}oob\_scores\_[-\/1]\`{}.\ Only\ available\ if\ \`{}subsample\ <\ 1.0\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01372}01372\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01373}01373\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01374}01374\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01375}01375\ \textcolor{stringliteral}{\ \ \ \ train\_score\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01376}01376\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ i-\/th\ score\ \`{}\`{}train\_score\_[i]\`{}\`{}\ is\ the\ loss\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01377}01377\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ model\ at\ iteration\ \`{}\`{}i\`{}\`{}\ on\ the\ in-\/bag\ sample.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01378}01378\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ \`{}\`{}subsample\ ==\ 1\`{}\`{}\ this\ is\ the\ loss\ on\ the\ training\ data.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01379}01379\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01380}01380\ \textcolor{stringliteral}{\ \ \ \ init\_\ :\ estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01381}01381\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ estimator\ that\ provides\ the\ initial\ predictions.\ Set\ via\ the\ \`{}\`{}init\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01382}01382\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ argument.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01383}01383\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01384}01384\ \textcolor{stringliteral}{\ \ \ \ estimators\_\ :\ ndarray\ of\ DecisionTreeRegressor\ of\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01385}01385\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ shape\ (n\_estimators,\ \`{}\`{}n\_trees\_per\_iteration\_\`{}\`{})}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01386}01386\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ collection\ of\ fitted\ sub-\/estimators.\ \`{}\`{}n\_trees\_per\_iteration\_\`{}\`{}\ is\ 1\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01387}01387\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ binary\ classification,\ otherwise\ \`{}\`{}n\_classes\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01388}01388\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01389}01389\ \textcolor{stringliteral}{\ \ \ \ classes\_\ :\ ndarray\ of\ shape\ (n\_classes,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01390}01390\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ classes\ labels.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01391}01391\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01392}01392\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01393}01393\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01394}01394\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01395}01395\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01396}01396\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01397}01397\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01398}01398\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01399}01399\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01400}01400\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01401}01401\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01402}01402\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01403}01403\ \textcolor{stringliteral}{\ \ \ \ n\_classes\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01404}01404\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ classes.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01405}01405\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01406}01406\ \textcolor{stringliteral}{\ \ \ \ max\_features\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01407}01407\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ inferred\ value\ of\ max\_features.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01408}01408\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01409}01409\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01410}01410\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01411}01411\ \textcolor{stringliteral}{\ \ \ \ HistGradientBoostingClassifier\ :\ Histogram-\/based\ Gradient\ Boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01412}01412\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Classification\ Tree.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01413}01413\ \textcolor{stringliteral}{\ \ \ \ sklearn.tree.DecisionTreeClassifier\ :\ A\ decision\ tree\ classifier.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01414}01414\ \textcolor{stringliteral}{\ \ \ \ RandomForestClassifier\ :\ A\ meta-\/estimator\ that\ fits\ a\ number\ of\ decision}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01415}01415\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ tree\ classifiers\ on\ various\ sub-\/samples\ of\ the\ dataset\ and\ uses}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01416}01416\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ averaging\ to\ improve\ the\ predictive\ accuracy\ and\ control\ over-\/fitting.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01417}01417\ \textcolor{stringliteral}{\ \ \ \ AdaBoostClassifier\ :\ A\ meta-\/estimator\ that\ begins\ by\ fitting\ a\ classifier}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01418}01418\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ on\ the\ original\ dataset\ and\ then\ fits\ additional\ copies\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01419}01419\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ classifier\ on\ the\ same\ dataset\ where\ the\ weights\ of\ incorrectly}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01420}01420\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ classified\ instances\ are\ adjusted\ such\ that\ subsequent\ classifiers}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01421}01421\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ focus\ more\ on\ difficult\ cases.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01422}01422\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01423}01423\ \textcolor{stringliteral}{\ \ \ \ Notes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01424}01424\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01425}01425\ \textcolor{stringliteral}{\ \ \ \ The\ features\ are\ always\ randomly\ permuted\ at\ each\ split.\ Therefore,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01426}01426\ \textcolor{stringliteral}{\ \ \ \ the\ best\ found\ split\ may\ vary,\ even\ with\ the\ same\ training\ data\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01427}01427\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}max\_features=n\_features\`{}\`{},\ if\ the\ improvement\ of\ the\ criterion\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01428}01428\ \textcolor{stringliteral}{\ \ \ \ identical\ for\ several\ splits\ enumerated\ during\ the\ search\ of\ the\ best}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01429}01429\ \textcolor{stringliteral}{\ \ \ \ split.\ To\ obtain\ a\ deterministic\ behaviour\ during\ fitting,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01430}01430\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}random\_state\`{}\`{}\ has\ to\ be\ fixed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01431}01431\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01432}01432\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01433}01433\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01434}01434\ \textcolor{stringliteral}{\ \ \ \ J.\ Friedman,\ Greedy\ Function\ Approximation:\ A\ Gradient\ Boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01435}01435\ \textcolor{stringliteral}{\ \ \ \ Machine,\ The\ Annals\ of\ Statistics,\ Vol.\ 29,\ No.\ 5,\ 2001.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01436}01436\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01437}01437\ \textcolor{stringliteral}{\ \ \ \ J.\ Friedman,\ Stochastic\ Gradient\ Boosting,\ 1999}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01438}01438\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01439}01439\ \textcolor{stringliteral}{\ \ \ \ T.\ Hastie,\ R.\ Tibshirani\ and\ J.\ Friedman.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01440}01440\ \textcolor{stringliteral}{\ \ \ \ Elements\ of\ Statistical\ Learning\ Ed.\ 2,\ Springer,\ 2009.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01441}01441\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01442}01442\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01443}01443\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01444}01444\ \textcolor{stringliteral}{\ \ \ \ The\ following\ example\ shows\ how\ to\ fit\ a\ gradient\ boosting\ classifier\ with}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01445}01445\ \textcolor{stringliteral}{\ \ \ \ 100\ decision\ stumps\ as\ weak\ learners.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01446}01446\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01447}01447\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.datasets\ import\ make\_hastie\_10\_2}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01448}01448\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.ensemble\ import\ GradientBoostingClassifier}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01449}01449\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01450}01450\ \textcolor{stringliteral}{\ \ \ \ >>>\ X,\ y\ =\ make\_hastie\_10\_2(random\_state=0)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01451}01451\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\_train,\ X\_test\ =\ X[:2000],\ X[2000:]}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01452}01452\ \textcolor{stringliteral}{\ \ \ \ >>>\ y\_train,\ y\_test\ =\ y[:2000],\ y[2000:]}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01453}01453\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01454}01454\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf\ =\ GradientBoostingClassifier(n\_estimators=100,\ learning\_rate=1.0,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01455}01455\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ max\_depth=1,\ random\_state=0).fit(X\_train,\ y\_train)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01456}01456\ \textcolor{stringliteral}{\ \ \ \ >>>\ clf.score(X\_test,\ y\_test)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01457}01457\ \textcolor{stringliteral}{\ \ \ \ 0.913}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01458}01458\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01459}01459\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01460}01460\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01461}01461\ \ \ \ \ \ \ \ \ **BaseGradientBoosting.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01462}01462\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}loss"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}log\_loss"{}},\ \textcolor{stringliteral}{"{}exponential"{}}\})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01463}01463\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}init"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}zero"{}}\}),\ \textcolor{keywordtype}{None},\ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1HasMethods}{HasMethods}}([\textcolor{stringliteral}{"{}fit"{}},\ \textcolor{stringliteral}{"{}predict\_proba"{}}])],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01464}01464\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01465}01465\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01466}01466\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01467}01467\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01468}01468\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01469}01469\ \ \ \ \ \ \ \ \ loss="{}log\_loss"{},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01470}01470\ \ \ \ \ \ \ \ \ learning\_rate=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01471}01471\ \ \ \ \ \ \ \ \ n\_estimators=100,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01472}01472\ \ \ \ \ \ \ \ \ subsample=1.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01473}01473\ \ \ \ \ \ \ \ \ criterion="{}friedman\_mse"{},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01474}01474\ \ \ \ \ \ \ \ \ min\_samples\_split=2,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01475}01475\ \ \ \ \ \ \ \ \ min\_samples\_leaf=1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01476}01476\ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01477}01477\ \ \ \ \ \ \ \ \ max\_depth=3,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01478}01478\ \ \ \ \ \ \ \ \ min\_impurity\_decrease=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01479}01479\ \ \ \ \ \ \ \ \ init=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01480}01480\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01481}01481\ \ \ \ \ \ \ \ \ max\_features=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01482}01482\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01483}01483\ \ \ \ \ \ \ \ \ max\_leaf\_nodes=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01484}01484\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01485}01485\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01486}01486\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01487}01487\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01488}01488\ \ \ \ \ \ \ \ \ ccp\_alpha=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01489}01489\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01490}01490\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01491}01491\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01492}01492\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01493}01493\ \ \ \ \ \ \ \ \ \ \ \ \ n\_estimators=n\_estimators,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01494}01494\ \ \ \ \ \ \ \ \ \ \ \ \ criterion=criterion,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01495}01495\ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_split=min\_samples\_split,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01496}01496\ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_leaf=min\_samples\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01497}01497\ \ \ \ \ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf=min\_weight\_fraction\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01498}01498\ \ \ \ \ \ \ \ \ \ \ \ \ max\_depth=max\_depth,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01499}01499\ \ \ \ \ \ \ \ \ \ \ \ \ init=init,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01500}01500\ \ \ \ \ \ \ \ \ \ \ \ \ subsample=subsample,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01501}01501\ \ \ \ \ \ \ \ \ \ \ \ \ max\_features=max\_features,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01502}01502\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01503}01503\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01504}01504\ \ \ \ \ \ \ \ \ \ \ \ \ max\_leaf\_nodes=max\_leaf\_nodes,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01505}01505\ \ \ \ \ \ \ \ \ \ \ \ \ min\_impurity\_decrease=min\_impurity\_decrease,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01506}01506\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01507}01507\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01508}01508\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01509}01509\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01510}01510\ \ \ \ \ \ \ \ \ \ \ \ \ ccp\_alpha=ccp\_alpha,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01511}01511\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01512}01512\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01513}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_ae3d9e05401cbee0eee09311eaee6bbac}{01513}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_ae3d9e05401cbee0eee09311eaee6bbac}{\_encode\_y}}(self,\ y,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01514}01514\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ encode\ classes\ into\ 0\ ...\ n\_classes\ -\/\ 1\ and\ sets\ attributes\ classes\_}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01515}01515\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ and\ n\_trees\_per\_iteration\_}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01516}01516\ \ \ \ \ \ \ \ \ check\_classification\_targets(y)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01517}01517\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01518}01518\ \ \ \ \ \ \ \ \ label\_encoder\ =\ \mbox{\hyperlink{classsklearn_1_1preprocessing_1_1__label_1_1LabelEncoder}{LabelEncoder}}()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01519}01519\ \ \ \ \ \ \ \ \ encoded\_y\_int\ =\ label\_encoder.fit\_transform(y)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01520}01520\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefde291bb6841815e2bd20131196434b}{classes\_}}\ =\ label\_encoder.classes\_}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01521}01521\ \ \ \ \ \ \ \ \ n\_classes\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefde291bb6841815e2bd20131196434b}{classes\_}}.shape[0]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01522}01522\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ only\ 1\ tree\ for\ binary\ classification.\ For\ multiclass\ classification,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01523}01523\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ we\ build\ 1\ tree\ per\ class.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01524}01524\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}}\ =\ 1\ \textcolor{keywordflow}{if}\ n\_classes\ <=\ 2\ \textcolor{keywordflow}{else}\ n\_classes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01525}01525\ \ \ \ \ \ \ \ \ encoded\_y\ =\ encoded\_y\_int.astype(float,\ copy=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01526}01526\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01527}01527\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ From\ here\ on,\ it\ is\ additional\ to\ the\ HGBT\ case.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01528}01528\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ expose\ n\_classes\_\ attribute}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01529}01529\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2524f6aaa0e0c95b386866a1df7829da}{n\_classes\_}}\ =\ n\_classes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01530}01530\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ sample\_weight\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01531}01531\ \ \ \ \ \ \ \ \ \ \ \ \ n\_trim\_classes\ =\ n\_classes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01532}01532\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01533}01533\ \ \ \ \ \ \ \ \ \ \ \ \ n\_trim\_classes\ =\ np.count\_nonzero(np.bincount(encoded\_y\_int,\ sample\_weight))}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01534}01534\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01535}01535\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\_trim\_classes\ <\ 2:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01536}01536\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01537}01537\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}y\ contains\ \%d\ class\ after\ sample\_weight\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01538}01538\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}trimmed\ classes\ with\ zero\ weights,\ while\ a\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01539}01539\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}minimum\ of\ 2\ classes\ are\ required."{}}\ \%\ n\_trim\_classes}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01540}01540\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01541}01541\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ encoded\_y}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01542}01542\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01543}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a8936012022fcb46ab47f22cb5ce36678}{01543}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a8936012022fcb46ab47f22cb5ce36678}{\_get\_loss}}(self,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01544}01544\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}\ ==\ \textcolor{stringliteral}{"{}log\_loss"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01545}01545\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2524f6aaa0e0c95b386866a1df7829da}{n\_classes\_}}\ ==\ 2:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01546}01546\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss}{HalfBinomialLoss}}(sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01547}01547\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01548}01548\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss}{HalfMultinomialLoss}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01549}01549\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ sample\_weight=sample\_weight,\ n\_classes=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2524f6aaa0e0c95b386866a1df7829da}{n\_classes\_}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01550}01550\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01551}01551\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}\ ==\ \textcolor{stringliteral}{"{}exponential"{}}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01552}01552\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2524f6aaa0e0c95b386866a1df7829da}{n\_classes\_}}\ >\ 2:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01553}01553\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classValueError}{ValueError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01554}01554\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}loss='\{self.loss\}'\ is\ only\ suitable\ for\ a\ binary\ classification\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01555}01555\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}problem,\ you\ have\ n\_classes=\{self.n\_classes\_\}.\ "{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01556}01556\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Please\ use\ loss='log\_loss'\ instead."{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01557}01557\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01558}01558\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01559}01559\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss}{ExponentialLoss}}(sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01560}01560\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01561}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2a271eac0396394dadbe6018b95e6a67}{01561}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2a271eac0396394dadbe6018b95e6a67}{decision\_function}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01562}01562\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ the\ decision\ function\ of\ \`{}\`{}X\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01563}01563\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01564}01564\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01565}01565\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01566}01566\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01567}01567\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01568}01568\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01569}01569\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01570}01570\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01571}01571\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01572}01572\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01573}01573\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_classes)\ or\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01574}01574\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ decision\ function\ of\ the\ input\ samples,\ which\ corresponds\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01575}01575\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ the\ raw\ values\ predicted\ from\ the\ trees\ of\ the\ ensemble\ .\ The}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01576}01576\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ order\ of\ the\ classes\ corresponds\ to\ that\ in\ the\ attribute}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01577}01577\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ :term:\`{}classes\_\`{}.\ Regression\ and\ binary\ classification\ produce\ an}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01578}01578\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ array\ of\ shape\ (n\_samples,).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01579}01579\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01580}01580\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01581}01581\ \ \ \ \ \ \ \ \ \ \ \ \ self,\ X,\ dtype=DTYPE,\ order=\textcolor{stringliteral}{"{}C"{}},\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ reset=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01582}01582\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01583}01583\ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8b9afe035abd471e6af90a6ac0866c23}{\_raw\_predict}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01584}01584\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_predictions.shape[1]\ ==\ 1:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01585}01585\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ raw\_predictions.ravel()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01586}01586\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ raw\_predictions}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01587}01587\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01588}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a632c2b7303e1428c913c804629cd50c6}{01588}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a632c2b7303e1428c913c804629cd50c6}{staged\_decision\_function}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01589}01589\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ decision\ function\ of\ \`{}\`{}X\`{}\`{}\ for\ each\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01590}01590\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01591}01591\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ allows\ monitoring\ (i.e.\ determine\ error\ on\ testing\ set)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01592}01592\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ after\ each\ stage.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01593}01593\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01594}01594\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01595}01595\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01596}01596\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01597}01597\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01598}01598\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01599}01599\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01600}01600\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01601}01601\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Yields}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01602}01602\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01603}01603\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ score\ :\ generator\ of\ ndarray\ of\ shape\ (n\_samples,\ k)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01604}01604\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ decision\ function\ of\ the\ input\ samples,\ which\ corresponds\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01605}01605\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ the\ raw\ values\ predicted\ from\ the\ trees\ of\ the\ ensemble\ .\ The}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01606}01606\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ classes\ corresponds\ to\ that\ in\ the\ attribute\ :term:\`{}classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01607}01607\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ Regression\ and\ binary\ classification\ are\ special\ cases\ with}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01608}01608\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}k\ ==\ 1\`{}\`{},\ otherwise\ \`{}\`{}k==n\_classes\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01609}01609\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01610}01610\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ \textcolor{keyword}{from}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01611}01611\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01612}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_abb6d9c33c03d9d9be0b4607342bfeb6c}{01612}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_abb6d9c33c03d9d9be0b4607342bfeb6c}{predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01613}01613\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ class\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01614}01614\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01615}01615\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01616}01616\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01617}01617\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01618}01618\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01619}01619\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01620}01620\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01621}01621\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01622}01622\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01623}01623\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01624}01624\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01625}01625\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ predicted\ values.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01626}01626\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01627}01627\ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2a271eac0396394dadbe6018b95e6a67}{decision\_function}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01628}01628\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ raw\_predictions.ndim\ ==\ 1:\ \ \textcolor{comment}{\#\ decision\_function\ already\ squeezed\ it}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01629}01629\ \ \ \ \ \ \ \ \ \ \ \ \ encoded\_classes\ =\ (raw\_predictions\ >=\ 0).astype(int)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01630}01630\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01631}01631\ \ \ \ \ \ \ \ \ \ \ \ \ encoded\_classes\ =\ np.argmax(raw\_predictions,\ axis=1)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01632}01632\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefde291bb6841815e2bd20131196434b}{classes\_}}[encoded\_classes]}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01633}01633\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01634}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_ab08cde9a0e554dd6067377b466964353}{01634}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_ab08cde9a0e554dd6067377b466964353}{staged\_predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01635}01635\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ class\ at\ each\ stage\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01636}01636\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01637}01637\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ allows\ monitoring\ (i.e.\ determine\ error\ on\ testing\ set)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01638}01638\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ after\ each\ stage.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01639}01639\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01640}01640\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01641}01641\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01642}01642\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01643}01643\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01644}01644\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01645}01645\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01646}01646\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01647}01647\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Yields}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01648}01648\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01649}01649\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ generator\ of\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01650}01650\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ predicted\ value\ of\ the\ input\ samples.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01651}01651\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01652}01652\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2524f6aaa0e0c95b386866a1df7829da}{n\_classes\_}}\ ==\ 2:\ \ \textcolor{comment}{\#\ n\_trees\_per\_iteration\_\ =\ 1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01653}01653\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ raw\_predictions\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01654}01654\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ encoded\_classes\ =\ (raw\_predictions.squeeze()\ >=\ 0).astype(int)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01655}01655\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefde291bb6841815e2bd20131196434b}{classes\_}}.take(encoded\_classes,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01656}01656\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01657}01657\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ raw\_predictions\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01658}01658\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ encoded\_classes\ =\ np.argmax(raw\_predictions,\ axis=1)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01659}01659\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefde291bb6841815e2bd20131196434b}{classes\_}}.take(encoded\_classes,\ axis=0)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01660}01660\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01661}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefdf4c74ce52788e6ade82de19d4391f}{01661}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefdf4c74ce52788e6ade82de19d4391f}{predict\_proba}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01662}01662\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ class\ probabilities\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01663}01663\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01664}01664\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01665}01665\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01666}01666\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01667}01667\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01668}01668\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01669}01669\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01670}01670\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01671}01671\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01672}01672\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01673}01673\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ p\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01674}01674\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ class\ probabilities\ of\ the\ input\ samples.\ The\ order\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01675}01675\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ classes\ corresponds\ to\ that\ in\ the\ attribute\ :term:\`{}classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01676}01676\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01677}01677\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Raises}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01678}01678\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01679}01679\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ AttributeError}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01680}01680\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ the\ \`{}\`{}loss\`{}\`{}\ does\ not\ support\ probabilities.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01681}01681\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01682}01682\ \ \ \ \ \ \ \ \ raw\_predictions\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a2a271eac0396394dadbe6018b95e6a67}{decision\_function}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01683}01683\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefdf4c74ce52788e6ade82de19d4391f}{predict\_proba}}(raw\_predictions)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01684}01684\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01685}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a34bf6ada2f590fc9e5459c296d685429}{01685}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a34bf6ada2f590fc9e5459c296d685429}{predict\_log\_proba}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01686}01686\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ class\ log-\/probabilities\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01687}01687\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01688}01688\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01689}01689\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01690}01690\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01691}01691\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01692}01692\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01693}01693\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01694}01694\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01695}01695\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01696}01696\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01697}01697\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ p\ :\ ndarray\ of\ shape\ (n\_samples,\ n\_classes)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01698}01698\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ class\ log-\/probabilities\ of\ the\ input\ samples.\ The\ order\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01699}01699\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ classes\ corresponds\ to\ that\ in\ the\ attribute\ :term:\`{}classes\_\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01700}01700\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01701}01701\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Raises}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01702}01702\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01703}01703\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ AttributeError}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01704}01704\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ If\ the\ \`{}\`{}loss\`{}\`{}\ does\ not\ support\ probabilities.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01705}01705\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01706}01706\ \ \ \ \ \ \ \ \ proba\ =\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefdf4c74ce52788e6ade82de19d4391f}{predict\_proba}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01707}01707\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.log(proba)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01708}01708\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01709}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a1f36455625dbf0ef28f36239dc809882}{01709}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_a1f36455625dbf0ef28f36239dc809882}{staged\_predict\_proba}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01710}01710\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ class\ probabilities\ at\ each\ stage\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01711}01711\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01712}01712\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ allows\ monitoring\ (i.e.\ determine\ error\ on\ testing\ set)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01713}01713\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ after\ each\ stage.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01714}01714\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01715}01715\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01716}01716\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01717}01717\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01718}01718\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01719}01719\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01720}01720\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01721}01721\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01722}01722\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Yields}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01723}01723\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01724}01724\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ generator\ of\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01725}01725\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ predicted\ value\ of\ the\ input\ samples.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01726}01726\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01727}01727\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01728}01728\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ raw\_predictions\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01729}01729\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a1a2504c522aa83b7ae9918836a4832ff}{\_loss}}.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingClassifier_aefdf4c74ce52788e6ade82de19d4391f}{predict\_proba}}(raw\_predictions)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01730}01730\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ NotFittedError:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01731}01731\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01732}01732\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ AttributeError\ \textcolor{keyword}{as}\ e:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01733}01733\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ \mbox{\hyperlink{classAttributeError}{AttributeError}}(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01734}01734\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}loss=\%r\ does\ not\ support\ predict\_proba"{}}\ \%\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01735}01735\ \ \ \ \ \ \ \ \ \ \ \ \ )\ \textcolor{keyword}{from}\ e}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01736}01736\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01737}01737\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01738}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor}{01738}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor}{GradientBoostingRegressor}}(\mbox{\hyperlink{classsklearn_1_1base_1_1RegressorMixin}{RegressorMixin}},\ \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting}{BaseGradientBoosting}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01739}01739\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Gradient\ Boosting\ for\ regression.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01740}01740\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01741}01741\ \textcolor{stringliteral}{\ \ \ \ This\ estimator\ builds\ an\ additive\ model\ in\ a\ forward\ stage-\/wise\ fashion;\ it}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01742}01742\ \textcolor{stringliteral}{\ \ \ \ allows\ for\ the\ optimization\ of\ arbitrary\ differentiable\ loss\ functions.\ In}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01743}01743\ \textcolor{stringliteral}{\ \ \ \ each\ stage\ a\ regression\ tree\ is\ fit\ on\ the\ negative\ gradient\ of\ the\ given}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01744}01744\ \textcolor{stringliteral}{\ \ \ \ loss\ function.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01745}01745\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01746}01746\ \textcolor{stringliteral}{\ \ \ \ :class:\`{}\string~sklearn.ensemble.HistGradientBoostingRegressor\`{}\ is\ a\ much\ faster\ variant}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01747}01747\ \textcolor{stringliteral}{\ \ \ \ of\ this\ algorithm\ for\ intermediate\ and\ large\ datasets\ (\`{}n\_samples\ >=\ 10\_000\`{})\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01748}01748\ \textcolor{stringliteral}{\ \ \ \ supports\ monotonic\ constraints.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01749}01749\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01750}01750\ \textcolor{stringliteral}{\ \ \ \ Read\ more\ in\ the\ :ref:\`{}User\ Guide\ <gradient\_boosting>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01751}01751\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01752}01752\ \textcolor{stringliteral}{\ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01753}01753\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01754}01754\ \textcolor{stringliteral}{\ \ \ \ loss\ :\ \{'squared\_error',\ 'absolute\_error',\ 'huber',\ 'quantile'\},\ \(\backslash\)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01755}01755\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ default='squared\_error'}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01756}01756\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Loss\ function\ to\ be\ optimized.\ 'squared\_error'\ refers\ to\ the\ squared}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01757}01757\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ error\ for\ regression.\ 'absolute\_error'\ refers\ to\ the\ absolute\ error\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01758}01758\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ regression\ and\ is\ a\ robust\ loss\ function.\ 'huber'\ is\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01759}01759\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ combination\ of\ the\ two.\ 'quantile'\ allows\ quantile\ regression\ (use}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01760}01760\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}alpha\`{}\ to\ specify\ the\ quantile).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01761}01761\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01762}01762\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01763}01763\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ an\ example\ that\ demonstrates\ quantile\ regression\ for\ creating}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01764}01764\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ prediction\ intervals\ with\ \`{}loss='quantile'\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01765}01765\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01766}01766\ \textcolor{stringliteral}{\ \ \ \ learning\_rate\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01767}01767\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Learning\ rate\ shrinks\ the\ contribution\ of\ each\ tree\ by\ \`{}learning\_rate\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01768}01768\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ There\ is\ a\ trade-\/off\ between\ learning\_rate\ and\ n\_estimators.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01769}01769\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01770}01770\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01771}01771\ \textcolor{stringliteral}{\ \ \ \ n\_estimators\ :\ int,\ default=100}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01772}01772\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ boosting\ stages\ to\ perform.\ Gradient\ boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01773}01773\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ is\ fairly\ robust\ to\ over-\/fitting\ so\ a\ large\ number\ usually}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01774}01774\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ results\ in\ better\ performance.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01775}01775\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01776}01776\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01777}01777\ \textcolor{stringliteral}{\ \ \ \ subsample\ :\ float,\ default=1.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01778}01778\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ fraction\ of\ samples\ to\ be\ used\ for\ fitting\ the\ individual\ base}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01779}01779\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ learners.\ If\ smaller\ than\ 1.0\ this\ results\ in\ Stochastic\ Gradient}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01780}01780\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Boosting.\ \`{}subsample\`{}\ interacts\ with\ the\ parameter\ \`{}n\_estimators\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01781}01781\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Choosing\ \`{}subsample\ <\ 1.0\`{}\ leads\ to\ a\ reduction\ of\ variance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01782}01782\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ an\ increase\ in\ bias.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01783}01783\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01784}01784\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01785}01785\ \textcolor{stringliteral}{\ \ \ \ criterion\ :\ \{'friedman\_mse',\ 'squared\_error'\},\ default='friedman\_mse'}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01786}01786\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ function\ to\ measure\ the\ quality\ of\ a\ split.\ Supported\ criteria\ are}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01787}01787\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}friedman\_mse"{}\ for\ the\ mean\ squared\ error\ with\ improvement\ score\ by}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01788}01788\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Friedman,\ "{}squared\_error"{}\ for\ mean\ squared\ error.\ The\ default\ value\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01789}01789\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}friedman\_mse"{}\ is\ generally\ the\ best\ as\ it\ can\ provide\ a\ better}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01790}01790\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ approximation\ in\ some\ cases.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01791}01791\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01792}01792\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01793}01793\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01794}01794\ \textcolor{stringliteral}{\ \ \ \ min\_samples\_split\ :\ int\ or\ float,\ default=2}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01795}01795\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ number\ of\ samples\ required\ to\ split\ an\ internal\ node:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01796}01796\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01797}01797\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[2,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01798}01798\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}\ and\ \`{}min\_samples\_split\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01799}01799\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ will\ be\ \`{}ceil(min\_samples\_split\ *\ n\_samples)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01800}01800\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01801}01801\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01802}01802\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Added\ float\ values\ for\ fractions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01803}01803\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01804}01804\ \textcolor{stringliteral}{\ \ \ \ min\_samples\_leaf\ :\ int\ or\ float,\ default=1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01805}01805\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ number\ of\ samples\ required\ to\ be\ at\ a\ leaf\ node.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01806}01806\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ A\ split\ point\ at\ any\ depth\ will\ only\ be\ considered\ if\ it\ leaves\ at}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01807}01807\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ least\ \`{}\`{}min\_samples\_leaf\`{}\`{}\ training\ samples\ in\ each\ of\ the\ left\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01808}01808\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ right\ branches.\ \ This\ may\ have\ the\ effect\ of\ smoothing\ the\ model,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01809}01809\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ especially\ in\ regression.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01810}01810\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01811}01811\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01812}01812\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}\ and\ \`{}min\_samples\_leaf\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01813}01813\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ will\ be\ \`{}ceil(min\_samples\_leaf\ *\ n\_samples)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01814}01814\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01815}01815\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionchanged::\ 0.18}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01816}01816\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ Added\ float\ values\ for\ fractions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01817}01817\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01818}01818\ \textcolor{stringliteral}{\ \ \ \ min\_weight\_fraction\_leaf\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01819}01819\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ minimum\ weighted\ fraction\ of\ the\ sum\ total\ of\ weights\ (of\ all}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01820}01820\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ input\ samples)\ required\ to\ be\ at\ a\ leaf\ node.\ Samples\ have}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01821}01821\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ equal\ weight\ when\ sample\_weight\ is\ not\ provided.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01822}01822\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ 0.5]\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01823}01823\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01824}01824\ \textcolor{stringliteral}{\ \ \ \ max\_depth\ :\ int\ or\ None,\ default=3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01825}01825\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Maximum\ depth\ of\ the\ individual\ regression\ estimators.\ The\ maximum}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01826}01826\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ depth\ limits\ the\ number\ of\ nodes\ in\ the\ tree.\ Tune\ this\ parameter}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01827}01827\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ best\ performance;\ the\ best\ value\ depends\ on\ the\ interaction}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01828}01828\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ of\ the\ input\ variables.\ If\ None,\ then\ nodes\ are\ expanded\ until}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01829}01829\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ all\ leaves\ are\ pure\ or\ until\ all\ leaves\ contain\ less\ than}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01830}01830\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ min\_samples\_split\ samples.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01831}01831\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01832}01832\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01833}01833\ \textcolor{stringliteral}{\ \ \ \ min\_impurity\_decrease\ :\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01834}01834\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ A\ node\ will\ be\ split\ if\ this\ split\ induces\ a\ decrease\ of\ the\ impurity}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01835}01835\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ greater\ than\ or\ equal\ to\ this\ value.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01836}01836\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01837}01837\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01838}01838\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ weighted\ impurity\ decrease\ equation\ is\ the\ following::}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01839}01839\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01840}01840\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ N\_t\ /\ N\ *\ (impurity\ -\/\ N\_t\_R\ /\ N\_t\ *\ right\_impurity}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01841}01841\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\/\ N\_t\_L\ /\ N\_t\ *\ left\_impurity)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01842}01842\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01843}01843\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ where\ \`{}\`{}N\`{}\`{}\ is\ the\ total\ number\ of\ samples,\ \`{}\`{}N\_t\`{}\`{}\ is\ the\ number\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01844}01844\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples\ at\ the\ current\ node,\ \`{}\`{}N\_t\_L\`{}\`{}\ is\ the\ number\ of\ samples\ in\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01845}01845\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ left\ child,\ and\ \`{}\`{}N\_t\_R\`{}\`{}\ is\ the\ number\ of\ samples\ in\ the\ right\ child.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01846}01846\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01847}01847\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}N\`{}\`{},\ \`{}\`{}N\_t\`{}\`{},\ \`{}\`{}N\_t\_R\`{}\`{}\ and\ \`{}\`{}N\_t\_L\`{}\`{}\ all\ refer\ to\ the\ weighted\ sum,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01848}01848\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ if\ \`{}\`{}sample\_weight\`{}\`{}\ is\ passed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01849}01849\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01850}01850\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.19}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01851}01851\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01852}01852\ \textcolor{stringliteral}{\ \ \ \ init\ :\ estimator\ or\ 'zero',\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01853}01853\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ An\ estimator\ object\ that\ is\ used\ to\ compute\ the\ initial\ predictions.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01854}01854\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}init\`{}\`{}\ has\ to\ provide\ :term:\`{}fit\`{}\ and\ :term:\`{}predict\`{}.\ If\ 'zero',\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01855}01855\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ initial\ raw\ predictions\ are\ set\ to\ zero.\ By\ default\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01856}01856\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}DummyEstimator\`{}\`{}\ is\ used,\ predicting\ either\ the\ average\ target\ value}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01857}01857\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ (for\ loss='squared\_error'),\ or\ a\ quantile\ for\ the\ other\ losses.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01858}01858\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01859}01859\ \textcolor{stringliteral}{\ \ \ \ random\_state\ :\ int,\ RandomState\ instance\ or\ None,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01860}01860\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Controls\ the\ random\ seed\ given\ to\ each\ Tree\ estimator\ at\ each}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01861}01861\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ boosting\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01862}01862\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ In\ addition,\ it\ controls\ the\ random\ permutation\ of\ the\ features\ at}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01863}01863\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ each\ split\ (see\ Notes\ for\ more\ details).}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01864}01864\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ It\ also\ controls\ the\ random\ splitting\ of\ the\ training\ data\ to\ obtain\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01865}01865\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ validation\ set\ if\ \`{}n\_iter\_no\_change\`{}\ is\ not\ None.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01866}01866\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Pass\ an\ int\ for\ reproducible\ output\ across\ multiple\ function\ calls.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01867}01867\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :term:\`{}Glossary\ <random\_state>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01868}01868\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01869}01869\ \textcolor{stringliteral}{\ \ \ \ max\_features\ :\ \{'sqrt',\ 'log2'\},\ int\ or\ float,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01870}01870\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ features\ to\ consider\ when\ looking\ for\ the\ best\ split:}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01871}01871\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01872}01872\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ int,\ values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01873}01873\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ float,\ values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0]\`{}\ and\ the\ features}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01874}01874\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ considered\ at\ each\ split\ will\ be\ \`{}max(1,\ int(max\_features\ *\ n\_features\_in\_))\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01875}01875\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ "{}sqrt"{},\ then\ \`{}max\_features=sqrt(n\_features)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01876}01876\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ "{}log2"{},\ then\ \`{}max\_features=log2(n\_features)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01877}01877\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/\ If\ None,\ then\ \`{}max\_features=n\_features\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01878}01878\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01879}01879\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Choosing\ \`{}max\_features\ <\ n\_features\`{}\ leads\ to\ a\ reduction\ of\ variance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01880}01880\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ an\ increase\ in\ bias.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01881}01881\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01882}01882\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Note:\ the\ search\ for\ a\ split\ does\ not\ stop\ until\ at\ least\ one}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01883}01883\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ valid\ partition\ of\ the\ node\ samples\ is\ found,\ even\ if\ it\ requires\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01884}01884\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ effectively\ inspect\ more\ than\ \`{}\`{}max\_features\`{}\`{}\ features.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01885}01885\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01886}01886\ \textcolor{stringliteral}{\ \ \ \ alpha\ :\ float,\ default=0.9}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01887}01887\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ alpha-\/quantile\ of\ the\ huber\ loss\ function\ and\ the\ quantile}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01888}01888\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ function.\ Only\ if\ \`{}\`{}loss='huber'\`{}\`{}\ or\ \`{}\`{}loss='quantile'\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01889}01889\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01890}01890\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01891}01891\ \textcolor{stringliteral}{\ \ \ \ verbose\ :\ int,\ default=0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01892}01892\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Enable\ verbose\ output.\ If\ 1\ then\ it\ prints\ progress\ and\ performance}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01893}01893\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ once\ in\ a\ while\ (the\ more\ trees\ the\ lower\ the\ frequency).\ If\ greater}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01894}01894\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ than\ 1\ then\ it\ prints\ progress\ and\ performance\ for\ every\ tree.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01895}01895\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01896}01896\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01897}01897\ \textcolor{stringliteral}{\ \ \ \ max\_leaf\_nodes\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01898}01898\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Grow\ trees\ with\ \`{}\`{}max\_leaf\_nodes\`{}\`{}\ in\ best-\/first\ fashion.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01899}01899\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Best\ nodes\ are\ defined\ as\ relative\ reduction\ in\ impurity.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01900}01900\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[2,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01901}01901\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ None,\ then\ unlimited\ number\ of\ leaf\ nodes.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01902}01902\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01903}01903\ \textcolor{stringliteral}{\ \ \ \ warm\_start\ :\ bool,\ default=False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01904}01904\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ When\ set\ to\ \`{}\`{}True\`{}\`{},\ reuse\ the\ solution\ of\ the\ previous\ call\ to\ fit}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01905}01905\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ and\ add\ more\ estimators\ to\ the\ ensemble,\ otherwise,\ just\ erase\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01906}01906\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ previous\ solution.\ See\ :term:\`{}the\ Glossary\ <warm\_start>\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01907}01907\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01908}01908\ \textcolor{stringliteral}{\ \ \ \ validation\_fraction\ :\ float,\ default=0.1}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01909}01909\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ proportion\ of\ training\ data\ to\ set\ aside\ as\ validation\ set\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01910}01910\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ early\ stopping.\ Values\ must\ be\ in\ the\ range\ \`{}(0.0,\ 1.0)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01911}01911\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ used\ if\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ set\ to\ an\ integer.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01912}01912\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01913}01913\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01914}01914\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01915}01915\ \textcolor{stringliteral}{\ \ \ \ n\_iter\_no\_change\ :\ int,\ default=None}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01916}01916\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ used\ to\ decide\ if\ early\ stopping\ will\ be\ used}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01917}01917\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ to\ terminate\ training\ when\ validation\ score\ is\ not\ improving.\ By}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01918}01918\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ default\ it\ is\ set\ to\ None\ to\ disable\ early\ stopping.\ If\ set\ to\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01919}01919\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ number,\ it\ will\ set\ aside\ \`{}\`{}validation\_fraction\`{}\`{}\ size\ of\ the\ training}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01920}01920\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ data\ as\ validation\ and\ terminate\ training\ when\ validation\ score\ is\ not}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01921}01921\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ improving\ in\ all\ of\ the\ previous\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ numbers\ of}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01922}01922\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ iterations.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01923}01923\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[1,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01924}01924\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01925}01925\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_early\_stopping.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01926}01926\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01927}01927\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01928}01928\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01929}01929\ \textcolor{stringliteral}{\ \ \ \ tol\ :\ float,\ default=1e-\/4}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01930}01930\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Tolerance\ for\ the\ early\ stopping.\ When\ the\ loss\ is\ not\ improving}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01931}01931\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ by\ at\ least\ tol\ for\ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ iterations\ (if\ set\ to\ a}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01932}01932\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ number),\ the\ training\ stops.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01933}01933\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01934}01934\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01935}01935\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.20}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01936}01936\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01937}01937\ \textcolor{stringliteral}{\ \ \ \ ccp\_alpha\ :\ non-\/negative\ float,\ default=0.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01938}01938\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Complexity\ parameter\ used\ for\ Minimal\ Cost-\/Complexity\ Pruning.\ The}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01939}01939\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ subtree\ with\ the\ largest\ cost\ complexity\ that\ is\ smaller\ than}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01940}01940\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}ccp\_alpha\`{}\`{}\ will\ be\ chosen.\ By\ default,\ no\ pruning\ is\ performed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01941}01941\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Values\ must\ be\ in\ the\ range\ \`{}[0.0,\ inf)\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01942}01942\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ See\ :ref:\`{}minimal\_cost\_complexity\_pruning\`{}\ for\ details.\ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01943}01943\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_tree\_plot\_cost\_complexity\_pruning.py\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01944}01944\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ for\ an\ example\ of\ such\ pruning.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01945}01945\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01946}01946\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.22}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01947}01947\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01948}01948\ \textcolor{stringliteral}{\ \ \ \ Attributes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01949}01949\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01950}01950\ \textcolor{stringliteral}{\ \ \ \ n\_estimators\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01951}01951\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ estimators\ as\ selected\ by\ early\ stopping\ (if}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01952}01952\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_iter\_no\_change\`{}\`{}\ is\ specified).\ Otherwise\ it\ is\ set\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01953}01953\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}n\_estimators\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01954}01954\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01955}01955\ \textcolor{stringliteral}{\ \ \ \ n\_trees\_per\_iteration\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01956}01956\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ number\ of\ trees\ that\ are\ built\ at\ each\ iteration.\ For\ regressors,\ this\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01957}01957\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ always\ 1.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01958}01958\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01959}01959\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.4.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01960}01960\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01961}01961\ \textcolor{stringliteral}{\ \ \ \ feature\_importances\_\ :\ ndarray\ of\ shape\ (n\_features,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01962}01962\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ impurity-\/based\ feature\ importances.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01963}01963\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ higher,\ the\ more\ important\ the\ feature.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01964}01964\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ importance\ of\ a\ feature\ is\ computed\ as\ the\ (normalized)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01965}01965\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ total\ reduction\ of\ the\ criterion\ brought\ by\ that\ feature.\ \ It\ is\ also}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01966}01966\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ known\ as\ the\ Gini\ importance.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01967}01967\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01968}01968\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Warning:\ impurity-\/based\ feature\ importances\ can\ be\ misleading\ for}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01969}01969\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ high\ cardinality\ features\ (many\ unique\ values).\ See}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01970}01970\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ :func:\`{}sklearn.inspection.permutation\_importance\`{}\ as\ an\ alternative.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01971}01971\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01972}01972\ \textcolor{stringliteral}{\ \ \ \ oob\_improvement\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01973}01973\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ improvement\ in\ loss\ on\ the\ out-\/of-\/bag\ samples}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01974}01974\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ relative\ to\ the\ previous\ iteration.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01975}01975\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \`{}\`{}oob\_improvement\_[0]\`{}\`{}\ is\ the\ improvement\ in}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01976}01976\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ loss\ of\ the\ first\ stage\ over\ the\ \`{}\`{}init\`{}\`{}\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01977}01977\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Only\ available\ if\ \`{}\`{}subsample\ <\ 1.0\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01978}01978\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01979}01979\ \textcolor{stringliteral}{\ \ \ \ oob\_scores\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01980}01980\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ full\ history\ of\ the\ loss\ values\ on\ the\ out-\/of-\/bag}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01981}01981\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ samples.\ Only\ available\ if\ \`{}subsample\ <\ 1.0\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01982}01982\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01983}01983\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01984}01984\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01985}01985\ \textcolor{stringliteral}{\ \ \ \ oob\_score\_\ :\ float}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01986}01986\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ last\ value\ of\ the\ loss\ on\ the\ out-\/of-\/bag\ samples.\ It\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01987}01987\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ the\ same\ as\ \`{}oob\_scores\_[-\/1]\`{}.\ Only\ available\ if\ \`{}subsample\ <\ 1.0\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01988}01988\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01989}01989\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.3}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01990}01990\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01991}01991\ \textcolor{stringliteral}{\ \ \ \ train\_score\_\ :\ ndarray\ of\ shape\ (n\_estimators,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01992}01992\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ i-\/th\ score\ \`{}\`{}train\_score\_[i]\`{}\`{}\ is\ the\ loss\ of\ the}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01993}01993\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ model\ at\ iteration\ \`{}\`{}i\`{}\`{}\ on\ the\ in-\/bag\ sample.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01994}01994\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ If\ \`{}\`{}subsample\ ==\ 1\`{}\`{}\ this\ is\ the\ loss\ on\ the\ training\ data.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01995}01995\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01996}01996\ \textcolor{stringliteral}{\ \ \ \ init\_\ :\ estimator}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01997}01997\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ estimator\ that\ provides\ the\ initial\ predictions.\ Set\ via\ the\ \`{}\`{}init\`{}\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01998}01998\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ argument.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l01999}01999\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02000}02000\ \textcolor{stringliteral}{\ \ \ \ estimators\_\ :\ ndarray\ of\ DecisionTreeRegressor\ of\ shape\ (n\_estimators,\ 1)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02001}02001\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ collection\ of\ fitted\ sub-\/estimators.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02002}02002\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02003}02003\ \textcolor{stringliteral}{\ \ \ \ n\_features\_in\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02004}02004\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Number\ of\ features\ seen\ during\ :term:\`{}fit\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02005}02005\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02006}02006\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.24}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02007}02007\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02008}02008\ \textcolor{stringliteral}{\ \ \ \ feature\_names\_in\_\ :\ ndarray\ of\ shape\ (\`{}n\_features\_in\_\`{},)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02009}02009\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Names\ of\ features\ seen\ during\ :term:\`{}fit\`{}.\ Defined\ only\ when\ \`{}X\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02010}02010\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ has\ feature\ names\ that\ are\ all\ strings.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02011}02011\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02012}02012\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 1.0}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02013}02013\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02014}02014\ \textcolor{stringliteral}{\ \ \ \ max\_features\_\ :\ int}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02015}02015\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ The\ inferred\ value\ of\ max\_features.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02016}02016\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02017}02017\ \textcolor{stringliteral}{\ \ \ \ See\ Also}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02018}02018\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02019}02019\ \textcolor{stringliteral}{\ \ \ \ HistGradientBoostingRegressor\ :\ Histogram-\/based\ Gradient\ Boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02020}02020\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Classification\ Tree.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02021}02021\ \textcolor{stringliteral}{\ \ \ \ sklearn.tree.DecisionTreeRegressor\ :\ A\ decision\ tree\ regressor.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02022}02022\ \textcolor{stringliteral}{\ \ \ \ sklearn.ensemble.RandomForestRegressor\ :\ A\ random\ forest\ regressor.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02023}02023\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02024}02024\ \textcolor{stringliteral}{\ \ \ \ Notes}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02025}02025\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02026}02026\ \textcolor{stringliteral}{\ \ \ \ The\ features\ are\ always\ randomly\ permuted\ at\ each\ split.\ Therefore,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02027}02027\ \textcolor{stringliteral}{\ \ \ \ the\ best\ found\ split\ may\ vary,\ even\ with\ the\ same\ training\ data\ and}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02028}02028\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}max\_features=n\_features\`{}\`{},\ if\ the\ improvement\ of\ the\ criterion\ is}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02029}02029\ \textcolor{stringliteral}{\ \ \ \ identical\ for\ several\ splits\ enumerated\ during\ the\ search\ of\ the\ best}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02030}02030\ \textcolor{stringliteral}{\ \ \ \ split.\ To\ obtain\ a\ deterministic\ behaviour\ during\ fitting,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02031}02031\ \textcolor{stringliteral}{\ \ \ \ \`{}\`{}random\_state\`{}\`{}\ has\ to\ be\ fixed.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02032}02032\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02033}02033\ \textcolor{stringliteral}{\ \ \ \ References}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02034}02034\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02035}02035\ \textcolor{stringliteral}{\ \ \ \ J.\ Friedman,\ Greedy\ Function\ Approximation:\ A\ Gradient\ Boosting}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02036}02036\ \textcolor{stringliteral}{\ \ \ \ Machine,\ The\ Annals\ of\ Statistics,\ Vol.\ 29,\ No.\ 5,\ 2001.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02037}02037\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02038}02038\ \textcolor{stringliteral}{\ \ \ \ J.\ Friedman,\ Stochastic\ Gradient\ Boosting,\ 1999}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02039}02039\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02040}02040\ \textcolor{stringliteral}{\ \ \ \ T.\ Hastie,\ R.\ Tibshirani\ and\ J.\ Friedman.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02041}02041\ \textcolor{stringliteral}{\ \ \ \ Elements\ of\ Statistical\ Learning\ Ed.\ 2,\ Springer,\ 2009.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02042}02042\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02043}02043\ \textcolor{stringliteral}{\ \ \ \ Examples}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02044}02044\ \textcolor{stringliteral}{\ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02045}02045\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.datasets\ import\ make\_regression}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02046}02046\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.ensemble\ import\ GradientBoostingRegressor}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02047}02047\ \textcolor{stringliteral}{\ \ \ \ >>>\ from\ sklearn.model\_selection\ import\ train\_test\_split}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02048}02048\ \textcolor{stringliteral}{\ \ \ \ >>>\ X,\ y\ =\ make\_regression(random\_state=0)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02049}02049\ \textcolor{stringliteral}{\ \ \ \ >>>\ X\_train,\ X\_test,\ y\_train,\ y\_test\ =\ train\_test\_split(}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02050}02050\ \textcolor{stringliteral}{\ \ \ \ ...\ \ \ \ \ X,\ y,\ random\_state=0)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02051}02051\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg\ =\ GradientBoostingRegressor(random\_state=0)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02052}02052\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg.fit(X\_train,\ y\_train)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02053}02053\ \textcolor{stringliteral}{\ \ \ \ GradientBoostingRegressor(random\_state=0)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02054}02054\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg.predict(X\_test[1:2])}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02055}02055\ \textcolor{stringliteral}{\ \ \ \ array([-\/61.1])}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02056}02056\ \textcolor{stringliteral}{\ \ \ \ >>>\ reg.score(X\_test,\ y\_test)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02057}02057\ \textcolor{stringliteral}{\ \ \ \ 0.4...}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02058}02058\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02059}02059\ \textcolor{stringliteral}{\ \ \ \ For\ a\ detailed\ example\ of\ utilizing}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02060}02060\ \textcolor{stringliteral}{\ \ \ \ :class:\`{}\string~sklearn.ensemble.GradientBoostingRegressor\`{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02061}02061\ \textcolor{stringliteral}{\ \ \ \ to\ fit\ an\ ensemble\ of\ weak\ predictive\ models,\ please\ refer\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02062}02062\ \textcolor{stringliteral}{\ \ \ \ :ref:\`{}sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02063}02063\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02064}02064\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02065}02065\ \ \ \ \ \_parameter\_constraints:\ dict\ =\ \{}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02066}02066\ \ \ \ \ \ \ \ \ **BaseGradientBoosting.\_parameter\_constraints,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02067}02067\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}loss"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}squared\_error"{}},\ \textcolor{stringliteral}{"{}absolute\_error"{}},\ \textcolor{stringliteral}{"{}huber"{}},\ \textcolor{stringliteral}{"{}quantile"{}}\})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02068}02068\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}init"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1StrOptions}{StrOptions}}(\{\textcolor{stringliteral}{"{}zero"{}}\}),\ \textcolor{keywordtype}{None},\ \mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1HasMethods}{HasMethods}}([\textcolor{stringliteral}{"{}fit"{}},\ \textcolor{stringliteral}{"{}predict"{}}])],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02069}02069\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}alpha"{}}:\ [\mbox{\hyperlink{classsklearn_1_1utils_1_1__param__validation_1_1Interval}{Interval}}(Real,\ 0.0,\ 1.0,\ closed=\textcolor{stringliteral}{"{}neither"{}})],}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02070}02070\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02071}02071\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02072}02072\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02073}02073\ \ \ \ \ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02074}02074\ \ \ \ \ \ \ \ \ *,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02075}02075\ \ \ \ \ \ \ \ \ loss="{}squared\_error"{},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02076}02076\ \ \ \ \ \ \ \ \ learning\_rate=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02077}02077\ \ \ \ \ \ \ \ \ n\_estimators=100,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02078}02078\ \ \ \ \ \ \ \ \ subsample=1.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02079}02079\ \ \ \ \ \ \ \ \ criterion="{}friedman\_mse"{},}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02080}02080\ \ \ \ \ \ \ \ \ min\_samples\_split=2,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02081}02081\ \ \ \ \ \ \ \ \ min\_samples\_leaf=1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02082}02082\ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02083}02083\ \ \ \ \ \ \ \ \ max\_depth=3,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02084}02084\ \ \ \ \ \ \ \ \ min\_impurity\_decrease=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02085}02085\ \ \ \ \ \ \ \ \ init=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02086}02086\ \ \ \ \ \ \ \ \ random\_state=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02087}02087\ \ \ \ \ \ \ \ \ max\_features=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02088}02088\ \ \ \ \ \ \ \ \ alpha=0.9,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02089}02089\ \ \ \ \ \ \ \ \ verbose=0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02090}02090\ \ \ \ \ \ \ \ \ max\_leaf\_nodes=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02091}02091\ \ \ \ \ \ \ \ \ warm\_start=False,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02092}02092\ \ \ \ \ \ \ \ \ validation\_fraction=0.1,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02093}02093\ \ \ \ \ \ \ \ \ n\_iter\_no\_change=None,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02094}02094\ \ \ \ \ \ \ \ \ tol=1e-\/4,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02095}02095\ \ \ \ \ \ \ \ \ ccp\_alpha=0.0,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02096}02096\ \ \ \ \ ):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02097}02097\ \ \ \ \ \ \ \ \ super().\_\_init\_\_(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02098}02098\ \ \ \ \ \ \ \ \ \ \ \ \ loss=loss,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02099}02099\ \ \ \ \ \ \ \ \ \ \ \ \ learning\_rate=learning\_rate,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02100}02100\ \ \ \ \ \ \ \ \ \ \ \ \ n\_estimators=n\_estimators,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02101}02101\ \ \ \ \ \ \ \ \ \ \ \ \ criterion=criterion,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02102}02102\ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_split=min\_samples\_split,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02103}02103\ \ \ \ \ \ \ \ \ \ \ \ \ min\_samples\_leaf=min\_samples\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02104}02104\ \ \ \ \ \ \ \ \ \ \ \ \ min\_weight\_fraction\_leaf=min\_weight\_fraction\_leaf,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02105}02105\ \ \ \ \ \ \ \ \ \ \ \ \ max\_depth=max\_depth,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02106}02106\ \ \ \ \ \ \ \ \ \ \ \ \ init=init,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02107}02107\ \ \ \ \ \ \ \ \ \ \ \ \ subsample=subsample,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02108}02108\ \ \ \ \ \ \ \ \ \ \ \ \ max\_features=max\_features,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02109}02109\ \ \ \ \ \ \ \ \ \ \ \ \ min\_impurity\_decrease=min\_impurity\_decrease,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02110}02110\ \ \ \ \ \ \ \ \ \ \ \ \ random\_state=random\_state,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02111}02111\ \ \ \ \ \ \ \ \ \ \ \ \ alpha=alpha,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02112}02112\ \ \ \ \ \ \ \ \ \ \ \ \ verbose=verbose,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02113}02113\ \ \ \ \ \ \ \ \ \ \ \ \ max\_leaf\_nodes=max\_leaf\_nodes,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02114}02114\ \ \ \ \ \ \ \ \ \ \ \ \ warm\_start=warm\_start,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02115}02115\ \ \ \ \ \ \ \ \ \ \ \ \ validation\_fraction=validation\_fraction,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02116}02116\ \ \ \ \ \ \ \ \ \ \ \ \ n\_iter\_no\_change=n\_iter\_no\_change,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02117}02117\ \ \ \ \ \ \ \ \ \ \ \ \ tol=tol,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02118}02118\ \ \ \ \ \ \ \ \ \ \ \ \ ccp\_alpha=ccp\_alpha,}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02119}02119\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02120}02120\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02121}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_ac43a220dc3603c831e825952d7e56e00}{02121}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_ac43a220dc3603c831e825952d7e56e00}{\_encode\_y}}(self,\ y=None,\ sample\_weight=None):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02122}02122\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Just\ convert\ y\ to\ the\ expected\ dtype}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02123}02123\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_abfa16cc19b72e49a231b045ec180017d}{n\_trees\_per\_iteration\_}}\ =\ 1}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02124}02124\ \ \ \ \ \ \ \ \ y\ =\ y.astype(DOUBLE,\ copy=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02125}02125\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ y}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02126}02126\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02127}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_ae19fbe328a5eec0adbadcecd3e44db57}{02127}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_ae19fbe328a5eec0adbadcecd3e44db57}{\_get\_loss}}(self,\ sample\_weight):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02128}02128\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}quantile"{}},\ \textcolor{stringliteral}{"{}huber"{}}):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02129}02129\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \_LOSSES[self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}](sample\_weight=sample\_weight,\ quantile=self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_ae7e325fb34b11e66f5411034da243259}{alpha}})}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02130}02130\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02131}02131\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \_LOSSES[self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a99d4ce12760b6e24fd9a1955ccd5db96}{loss}}](sample\_weight=sample\_weight)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02132}02132\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02133}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a7d3d798c06c714f5742130acff97b2c7}{02133}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a7d3d798c06c714f5742130acff97b2c7}{predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02134}02134\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ regression\ target\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02135}02135\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02136}02136\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02137}02137\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02138}02138\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02139}02139\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02140}02140\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02141}02141\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02142}02142\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02143}02143\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02144}02144\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02145}02145\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02146}02146\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ predicted\ values.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02147}02147\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02148}02148\ \ \ \ \ \ \ \ \ X\ =\ validate\_data(}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02149}02149\ \ \ \ \ \ \ \ \ \ \ \ \ self,\ X,\ dtype=DTYPE,\ order=\textcolor{stringliteral}{"{}C"{}},\ accept\_sparse=\textcolor{stringliteral}{"{}csr"{}},\ reset=\textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02150}02150\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02151}02151\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ In\ regression\ we\ can\ directly\ return\ the\ raw\ value\ from\ the\ trees.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02152}02152\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a8b9afe035abd471e6af90a6ac0866c23}{\_raw\_predict}}(X).ravel()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02153}02153\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02154}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a1e29e57ce610b259f17c0d3bafc89b25}{02154}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a1e29e57ce610b259f17c0d3bafc89b25}{staged\_predict}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02155}02155\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Predict\ regression\ target\ at\ each\ stage\ for\ X.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02156}02156\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02157}02157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ This\ method\ allows\ monitoring\ (i.e.\ determine\ error\ on\ testing\ set)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02158}02158\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ after\ each\ stage.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02159}02159\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02160}02160\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02161}02161\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02162}02162\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02163}02163\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ it\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02164}02164\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}\ and\ if\ a\ sparse\ matrix\ is\ provided}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02165}02165\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02166}02166\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02167}02167\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Yields}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02168}02168\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02169}02169\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ y\ :\ generator\ of\ ndarray\ of\ shape\ (n\_samples,)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02170}02170\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ predicted\ value\ of\ the\ input\ samples.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02171}02171\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02172}02172\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ raw\_predictions\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1BaseGradientBoosting_a66a7e1b7c78b72b695fd62d642140053}{\_staged\_raw\_predict}}(X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02173}02173\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{yield}\ raw\_predictions.ravel()}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02174}02174\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02175}\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a8703f82ceccd22a6f938cd6f2c0eb0de}{02175}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a8703f82ceccd22a6f938cd6f2c0eb0de}{apply}}(self,\ X):}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02176}02176\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Apply\ trees\ in\ the\ ensemble\ to\ X,\ return\ leaf\ indices.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02177}02177\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02178}02178\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ ..\ versionadded::\ 0.17}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02179}02179\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02180}02180\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Parameters}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02181}02181\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02182}02182\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\ :\ \{array-\/like,\ sparse\ matrix\}\ of\ shape\ (n\_samples,\ n\_features)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02183}02183\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ The\ input\ samples.\ Internally,\ its\ dtype\ will\ be\ converted\ to}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02184}02184\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \`{}\`{}dtype=np.float32\`{}\`{}.\ If\ a\ sparse\ matrix\ is\ provided,\ it\ will}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02185}02185\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ be\ converted\ to\ a\ sparse\ \`{}\`{}csr\_matrix\`{}\`{}.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02186}02186\ \textcolor{stringliteral}{}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02187}02187\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Returns}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02188}02188\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ -\/-\/-\/-\/-\/-\/-\/}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02189}02189\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ X\_leaves\ :\ array-\/like\ of\ shape\ (n\_samples,\ n\_estimators)}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02190}02190\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ For\ each\ datapoint\ x\ in\ X\ and\ for\ each\ tree\ in\ the\ ensemble,}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02191}02191\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ return\ the\ index\ of\ the\ leaf\ x\ ends\ up\ in\ each\ estimator.}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02192}02192\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02193}02193\ }
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02194}02194\ \ \ \ \ \ \ \ \ leaves\ =\ super().\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__gb_1_1GradientBoostingRegressor_a8703f82ceccd22a6f938cd6f2c0eb0de}{apply}}(X)}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02195}02195\ \ \ \ \ \ \ \ \ leaves\ =\ leaves.reshape(X.shape[0],\ self.\mbox{\hyperlink{classsklearn_1_1ensemble_1_1__base_1_1BaseEnsemble_a9b3e91f44e8647a9fd7f725677b29e12}{estimators\_}}.shape[0])}
\DoxyCodeLine{\Hypertarget{__gb_8py_source_l02196}02196\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ leaves}

\end{DoxyCode}
