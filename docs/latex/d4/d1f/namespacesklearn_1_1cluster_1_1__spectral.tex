\doxysection{sklearn.\+cluster.\+\_\+spectral Namespace Reference}
\hypertarget{namespacesklearn_1_1cluster_1_1__spectral}{}\label{namespacesklearn_1_1cluster_1_1__spectral}\index{sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1cluster_1_1__spectral_1_1SpectralClustering}{Spectral\+Clustering}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__spectral_a29792a5e5b87373c1a30aec97aeef847}{cluster\+\_\+qr}} (vectors)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__spectral_ab7d39bb04deba6ff73a7876d738e394a}{discretize}} (vectors, \texorpdfstring{$\ast$}{*}, copy=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+svd\+\_\+restarts=30, n\+\_\+iter\+\_\+max=20, random\+\_\+state=None)
\item 
\mbox{\hyperlink{namespacesklearn_1_1cluster_1_1__spectral_a72a6207e4b3d57e9422f5bf88c71fc2c}{spectral\+\_\+clustering}} (affinity, \texorpdfstring{$\ast$}{*}, n\+\_\+clusters=8, n\+\_\+components=None, eigen\+\_\+solver=None, random\+\_\+state=None, n\+\_\+init=10, eigen\+\_\+tol="{}auto"{}, assign\+\_\+labels="{}kmeans"{}, verbose=False)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Algorithms for spectral clustering\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1cluster_1_1__spectral_a29792a5e5b87373c1a30aec97aeef847}\index{sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}!cluster\_qr@{cluster\_qr}}
\index{cluster\_qr@{cluster\_qr}!sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}}
\doxysubsubsection{\texorpdfstring{cluster\_qr()}{cluster\_qr()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__spectral_a29792a5e5b87373c1a30aec97aeef847} 
sklearn.\+cluster.\+\_\+spectral.\+cluster\+\_\+qr (\begin{DoxyParamCaption}\item[{}]{vectors}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Find the discrete partition closest to the eigenvector embedding.

    This implementation was proposed in [1]_.

.. versionadded:: 1.1

    Parameters
    ----------
    vectors : array-like, shape: (n_samples, n_clusters)
        The embedding space of the samples.

    Returns
    -------
    labels : array of integers, shape: n_samples
        The cluster labels of vectors.

    References
    ----------
    .. [1] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019
        Anil Damle, Victor Minden, Lexing Ying
        <10.1093/imaiai/iay008>`
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source_l00023}{23}} of file \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source}{\+\_\+spectral.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source_l00672}{sklearn.\+cluster.\+\_\+spectral.\+Spectral\+Clustering.\+fit()}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__spectral_ab7d39bb04deba6ff73a7876d738e394a}\index{sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}!discretize@{discretize}}
\index{discretize@{discretize}!sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}}
\doxysubsubsection{\texorpdfstring{discretize()}{discretize()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__spectral_ab7d39bb04deba6ff73a7876d738e394a} 
sklearn.\+cluster.\+\_\+spectral.\+discretize (\begin{DoxyParamCaption}\item[{}]{vectors}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{copy}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+svd\+\_\+restarts}{ = {\ttfamily 30}, }\item[{}]{n\+\_\+iter\+\_\+max}{ = {\ttfamily 20}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Search for a partition matrix which is closest to the eigenvector embedding.

This implementation was proposed in [1]_.

Parameters
----------
vectors : array-like of shape (n_samples, n_clusters)
    The embedding space of the samples.

copy : bool, default=True
    Whether to copy vectors, or perform in-place normalization.

max_svd_restarts : int, default=30
    Maximum number of attempts to restart SVD if convergence fails

n_iter_max : int, default=30
    Maximum number of iterations to attempt in rotation and partition
    matrix search if machine precision convergence is not reached

random_state : int, RandomState instance, default=None
    Determines random number generation for rotation matrix initialization.
    Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

Returns
-------
labels : array of integers, shape: n_samples
    The labels of the clusters.

References
----------

.. [1] `Multiclass spectral clustering, 2003
       Stella X. Yu, Jianbo Shi
       <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_

Notes
-----

The eigenvector embedding is used to iteratively search for the
closest discrete partition.  First, the eigenvector embedding is
normalized to the space of partition matrices. An optimal discrete
partition matrix closest to this normalized embedding multiplied by
an initial rotation is calculated.  Fixing this discrete partition
matrix, an optimal rotation matrix is calculated.  These two
calculations are performed until convergence.  The discrete partition
matrix is returned as the clustering solution.  Used in spectral
clustering, this method tends to be faster and more robust to random
initialization than k-means.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source_l00055}{55}} of file \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source}{\+\_\+spectral.\+py}}.



Referenced by \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source_l00672}{sklearn.\+cluster.\+\_\+spectral.\+Spectral\+Clustering.\+fit()}}.

\Hypertarget{namespacesklearn_1_1cluster_1_1__spectral_a72a6207e4b3d57e9422f5bf88c71fc2c}\index{sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}!spectral\_clustering@{spectral\_clustering}}
\index{spectral\_clustering@{spectral\_clustering}!sklearn.cluster.\_spectral@{sklearn.cluster.\_spectral}}
\doxysubsubsection{\texorpdfstring{spectral\_clustering()}{spectral\_clustering()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1cluster_1_1__spectral_a72a6207e4b3d57e9422f5bf88c71fc2c} 
sklearn.\+cluster.\+\_\+spectral.\+spectral\+\_\+clustering (\begin{DoxyParamCaption}\item[{}]{affinity}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{n\+\_\+clusters}{ = {\ttfamily 8}, }\item[{}]{n\+\_\+components}{ = {\ttfamily None}, }\item[{}]{eigen\+\_\+solver}{ = {\ttfamily None}, }\item[{}]{random\+\_\+state}{ = {\ttfamily None}, }\item[{}]{n\+\_\+init}{ = {\ttfamily 10}, }\item[{}]{eigen\+\_\+tol}{ = {\ttfamily "{}auto"{}}, }\item[{}]{assign\+\_\+labels}{ = {\ttfamily "{}kmeans"{}}, }\item[{}]{verbose}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Apply clustering to a projection of the normalized Laplacian.

In practice Spectral Clustering is very useful when the structure of
the individual clusters is highly non-convex or more generally when
a measure of the center and spread of the cluster is not a suitable
description of the complete cluster. For instance, when clusters are
nested circles on the 2D plane.

If affinity is the adjacency matrix of a graph, this method can be
used to find normalized graph cuts [1]_, [2]_.

Read more in the :ref:`User Guide <spectral_clustering>`.

Parameters
----------
affinity : {array-like, sparse matrix} of shape (n_samples, n_samples)
    The affinity matrix describing the relationship of the samples to
    embed. **Must be symmetric**.

    Possible examples:
      - adjacency matrix of a graph,
      - heat kernel of the pairwise distance matrix of the samples,
      - symmetric k-nearest neighbours connectivity matrix of the samples.

n_clusters : int, default=None
    Number of clusters to extract.

n_components : int, default=n_clusters
    Number of eigenvectors to use for the spectral embedding.

eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}
    The eigenvalue decomposition method. If None then ``'arpack'`` is used.
    See [4]_ for more details regarding ``'lobpcg'``.
    Eigensolver ``'amg'`` runs ``'lobpcg'`` with optional
    Algebraic MultiGrid preconditioning and requires pyamg to be installed.
    It can be faster on very large sparse problems [6]_ and [7]_.

random_state : int, RandomState instance, default=None
    A pseudo random number generator used for the initialization
    of the lobpcg eigenvectors decomposition when `eigen_solver ==
    'amg'`, and for the K-Means initialization. Use an int to make
    the results deterministic across calls (See
    :term:`Glossary <random_state>`).

    .. note::
        When using `eigen_solver == 'amg'`,
        it is necessary to also fix the global numpy seed with
        `np.random.seed(int)` to get deterministic results. See
        https://github.com/pyamg/pyamg/issues/139 for further
        information.

n_init : int, default=10
    Number of time the k-means algorithm will be run with different
    centroid seeds. The final results will be the best output of n_init
    consecutive runs in terms of inertia. Only used if
    ``assign_labels='kmeans'``.

eigen_tol : float, default="auto"
    Stopping criterion for eigendecomposition of the Laplacian matrix.
    If `eigen_tol="auto"` then the passed tolerance will depend on the
    `eigen_solver`:

    - If `eigen_solver="arpack"`, then `eigen_tol=0.0`;
    - If `eigen_solver="lobpcg"` or `eigen_solver="amg"`, then
      `eigen_tol=None` which configures the underlying `lobpcg` solver to
      automatically resolve the value according to their heuristics. See,
      :func:`scipy.sparse.linalg.lobpcg` for details.

    Note that when using `eigen_solver="lobpcg"` or `eigen_solver="amg"`
    values of `tol<1e-5` may lead to convergence issues and should be
    avoided.

    .. versionadded:: 1.2
       Added 'auto' option.

assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'
    The strategy to use to assign labels in the embedding
    space.  There are three ways to assign labels after the Laplacian
    embedding.  k-means can be applied and is a popular choice. But it can
    also be sensitive to initialization. Discretization is another
    approach which is less sensitive to random initialization [3]_.
    The cluster_qr method [5]_ directly extracts clusters from eigenvectors
    in spectral clustering. In contrast to k-means and discretization, cluster_qr
    has no tuning parameters and is not an iterative method, yet may outperform
    k-means and discretization in terms of both quality and speed. For a detailed
    comparison of clustering strategies, refer to the following example:
    :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`.

    .. versionchanged:: 1.1
       Added new labeling method 'cluster_qr'.

verbose : bool, default=False
    Verbosity mode.

    .. versionadded:: 0.24

Returns
-------
labels : array of integers, shape: n_samples
    The labels of the clusters.

Notes
-----
The graph should contain only one connected component, elsewhere
the results make little sense.

This algorithm solves the normalized cut for `k=2`: it is a
normalized spectral clustering.

References
----------

.. [1] :doi:`Normalized cuts and image segmentation, 2000
       Jianbo Shi, Jitendra Malik
       <10.1109/34.868688>`

.. [2] :doi:`A Tutorial on Spectral Clustering, 2007
       Ulrike von Luxburg
       <10.1007/s11222-007-9033-z>`

.. [3] `Multiclass spectral clustering, 2003
       Stella X. Yu, Jianbo Shi
       <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_

.. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:
       Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001
       A. V. Knyazev
       SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.
       <10.1137/S1064827500366124>`

.. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019
       Anil Damle, Victor Minden, Lexing Ying
       <10.1093/imaiai/iay008>`

.. [6] :doi:`Multiscale Spectral Image Segmentation Multiscale preconditioning
       for computing eigenvalues of graph Laplacians in image segmentation, 2006
       Andrew Knyazev
       <10.13140/RG.2.2.35280.02565>`

.. [7] :doi:`Preconditioned spectral clustering for stochastic block partition
       streaming graph challenge (Preliminary version at arXiv.)
       David Zhuzhunashvili, Andrew Knyazev
       <10.1109/HPEC.2017.8091045>`

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics.pairwise import pairwise_kernels
>>> from sklearn.cluster import spectral_clustering
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> affinity = pairwise_kernels(X, metric='rbf')
>>> spectral_clustering(
...     affinity=affinity, n_clusters=2, assign_labels="discretize", random_state=0
... )
array([1, 1, 1, 0, 0, 0])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source_l00194}{194}} of file \mbox{\hyperlink{sklearn_2cluster_2__spectral_8py_source}{\+\_\+spectral.\+py}}.

