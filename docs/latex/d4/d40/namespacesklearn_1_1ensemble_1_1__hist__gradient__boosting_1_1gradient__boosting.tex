\doxysection{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting Namespace Reference}
\hypertarget{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting}{}\label{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting}\index{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_1_1BaseHistGradientBoosting}{Base\+Hist\+Gradient\+Boosting}}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_1_1HistGradientBoostingClassifier}{Hist\+Gradient\+Boosting\+Classifier}}
\item 
class \mbox{\hyperlink{classsklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_1_1HistGradientBoostingRegressor}{Hist\+Gradient\+Boosting\+Regressor}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a792e2e12a4c12db534fb1b6ce74c5b04}{\+\_\+update\+\_\+leaves\+\_\+values}} (loss, grower, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight)
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a4cf3085f0d80a3d32f5fbc2a96f6b28a}{\+\_\+patch\+\_\+raw\+\_\+predict}} (estimator, raw\+\_\+predictions)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a146d888ba369d99c36a8f112848a0197}{\+\_\+\+LOSSES}} = \+\_\+\+LOSSES.\+copy()
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Fast Gradient Boosting decision trees for classification and regression.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a4cf3085f0d80a3d32f5fbc2a96f6b28a}\index{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}!\_patch\_raw\_predict@{\_patch\_raw\_predict}}
\index{\_patch\_raw\_predict@{\_patch\_raw\_predict}!sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}}
\doxysubsubsection{\texorpdfstring{\_patch\_raw\_predict()}{\_patch\_raw\_predict()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a4cf3085f0d80a3d32f5fbc2a96f6b28a} 
sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+\_\+patch\+\_\+raw\+\_\+predict (\begin{DoxyParamCaption}\item[{}]{estimator}{, }\item[{}]{raw\+\_\+predictions}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Context manager that patches _raw_predict to return raw_predictions.

`raw_predictions` is typically a precomputed array to avoid redundant
state-wise computations fitting with early stopping enabled: in this case
`raw_predictions` is incrementally updated whenever we add a tree to the
boosted ensemble.

Note: this makes fitting HistGradientBoosting* models inherently non thread
safe at fit time. However thread-safety at fit time was never guaranteed nor
enforced for scikit-learn estimators in general.

Thread-safety at prediction/transform time is another matter as those
operations are typically side-effect free and therefore often thread-safe by
default for most scikit-learn models and would like to keep it that way.
Therefore this context manager should only be used at fit time.

TODO: in the future, we could explore the possibility to extend the scorer
public API to expose a way to compute vales from raw predictions. That would
probably require also making the scorer aware of the inverse link function
used by the estimator which is typically private API for now, hence the need
for this patching mechanism.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{gradient__boosting_8py_source_l00108}{108}} of file \mbox{\hyperlink{gradient__boosting_8py_source}{gradient\+\_\+boosting.\+py}}.



Referenced by \mbox{\hyperlink{gradient__boosting_8py_source_l01124}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Base\+Hist\+Gradient\+Boosting.\+\_\+check\+\_\+early\+\_\+stopping\+\_\+scorer()}}.

\Hypertarget{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a792e2e12a4c12db534fb1b6ce74c5b04}\index{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}!\_update\_leaves\_values@{\_update\_leaves\_values}}
\index{\_update\_leaves\_values@{\_update\_leaves\_values}!sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}}
\doxysubsubsection{\texorpdfstring{\_update\_leaves\_values()}{\_update\_leaves\_values()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a792e2e12a4c12db534fb1b6ce74c5b04} 
sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+\_\+update\+\_\+leaves\+\_\+values (\begin{DoxyParamCaption}\item[{}]{loss}{, }\item[{}]{grower}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Update the leaf values to be predicted by the tree.

Update equals:
    loss.fit_intercept_only(y_true - raw_prediction)

This is only applied if loss.differentiable is False.
Note: It only works, if the loss is a function of the residual, as is the
case for AbsoluteError and PinballLoss. Otherwise, one would need to get
the minimum of loss(y_true, raw_prediction + x) in x. A few examples:
  - AbsoluteError: median(y_true - raw_prediction).
  - PinballLoss: quantile(y_true - raw_prediction).

More background:
For the standard gradient descent method according to "Greedy Function
Approximation: A Gradient Boosting Machine" by Friedman, all loss functions but the
squared loss need a line search step. BaseHistGradientBoosting, however, implements
a so called Newton boosting where the trees are fitted to a 2nd order
approximations of the loss in terms of gradients and hessians. In this case, the
line search step is only necessary if the loss is not smooth, i.e. not
differentiable, which renders the 2nd order approximation invalid. In fact,
non-smooth losses arbitrarily set hessians to 1 and effectively use the standard
gradient descent method with line search.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{gradient__boosting_8py_source_l00066}{66}} of file \mbox{\hyperlink{gradient__boosting_8py_source}{gradient\+\_\+boosting.\+py}}.



Referenced by \mbox{\hyperlink{gradient__boosting_8py_source_l00520}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Base\+Hist\+Gradient\+Boosting.\+fit()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a146d888ba369d99c36a8f112848a0197}\index{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}!\_LOSSES@{\_LOSSES}}
\index{\_LOSSES@{\_LOSSES}!sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting@{sklearn.ensemble.\_hist\_gradient\_boosting.gradient\_boosting}}
\doxysubsubsection{\texorpdfstring{\_LOSSES}{\_LOSSES}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1gradient__boosting_a146d888ba369d99c36a8f112848a0197} 
sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+\_\+\+LOSSES = \+\_\+\+LOSSES.\+copy()\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{gradient__boosting_8py_source_l00056}{56}} of file \mbox{\hyperlink{gradient__boosting_8py_source}{gradient\+\_\+boosting.\+py}}.

