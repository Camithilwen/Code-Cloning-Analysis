\doxysection{sklearn.\+\_\+loss.\+loss.\+Base\+Loss Class Reference}
\hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}{}\label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}


Inheritance diagram for sklearn.\+\_\+loss.\+loss.\+Base\+Loss\+:
% FIG 0


Collaboration diagram for sklearn.\+\_\+loss.\+loss.\+Base\+Loss\+:
% FIG 1
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, closs, link, n\+\_\+classes=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}{in\+\_\+y\+\_\+true\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}{in\+\_\+y\+\_\+pred\+\_\+range}} (self, y)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}{loss}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}{loss\+\_\+gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, loss\+\_\+out=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}{gradient}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}{gradient\+\_\+hessian}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, gradient\+\_\+out=None, hessian\+\_\+out=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, y\+\_\+true, raw\+\_\+prediction, sample\+\_\+weight=None, n\+\_\+threads=1)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}{fit\+\_\+intercept\+\_\+only}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}{constant\+\_\+to\+\_\+optimal\+\_\+zero}} (self, y\+\_\+true, sample\+\_\+weight=None)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}{init\+\_\+gradient\+\_\+and\+\_\+hessian}} (self, n\+\_\+samples, dtype=np.\+float64, order="{}F"{})
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}{closs}} = closs
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}{link}} = link
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}{approx\+\_\+hessian}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}{constant\+\_\+hessian}} = False
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}{n\+\_\+classes}} = n\+\_\+classes
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}{interval\+\_\+y\+\_\+true}} = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)
\item 
\mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}{interval\+\_\+y\+\_\+pred}} = self.\+link.\+interval\+\_\+y\+\_\+pred
\end{DoxyCompactItemize}
\doxysubsubsection*{Static Public Attributes}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}{differentiable}} = \mbox{\hyperlink{classTrue}{True}}
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}{need\+\_\+update\+\_\+leaves\+\_\+values}} = False
\item 
bool \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}{is\+\_\+multiclass}} = False
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Base class for a loss function of 1-dimensional targets.

Conventions:

    - y_true.shape = sample_weight.shape = (n_samples,)
    - y_pred.shape = raw_prediction.shape = (n_samples,)
    - If is_multiclass is true (multiclass classification), then
      y_pred.shape = raw_prediction.shape = (n_samples, n_classes)
      Note that this corresponds to the return value of decision_function.

y_true, y_pred, sample_weight and raw_prediction must either be all float64
or all float32.
gradient and hessian must be either both float64 or both float32.

Note that y_pred = link.inverse(raw_prediction).

Specific loss classes can inherit specific link classes to satisfy
BaseLink's abstractmethods.

Parameters
----------
sample_weight : {None, ndarray}
    If sample_weight is None, the hessian might be constant.
n_classes : {None, int}
    The number of classes for classification, else None.

Attributes
----------
closs: CyLossFunction
link : BaseLink
interval_y_true : Interval
    Valid interval for y_true
interval_y_pred : Interval
    Valid Interval for y_pred
differentiable : bool
    Indicates whether or not loss function is differentiable in
    raw_prediction everywhere.
need_update_leaves_values : bool
    Indicates whether decision trees in gradient boosting need to uptade
    leave values after having been fit to the (negative) gradients.
approx_hessian : bool
    Indicates whether the hessian is approximated or exact. If,
    approximated, it should be larger or equal to the exact one.
constant_hessian : bool
    Indicates whether the hessian is one for this loss.
is_multiclass : bool
    Indicates whether n_classes > 2 is allowed.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00070}{70}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a050f99cd8950283f434ace55115a40f7} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{closs}{, }\item[{}]{link}{, }\item[{}]{n\+\_\+classes}{ = {\ttfamily None}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{loss_8py_source_l00133}{133}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{kernels_8py_source_l00178}{sklearn.\+gaussian\+\_\+process.\+kernels.\+Kernel.\+get\+\_\+params()}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!\_\_call\_\_@{\_\_call\_\_}}
\index{\_\_call\_\_@{\_\_call\_\_}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{\_\_call\_\_()}{\_\_call\_\_()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a294afc783ab728aa3bfd859988493b35} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+\_\+\+\_\+call\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the weighted average loss.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
loss : float
    Mean or averaged loss function.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00383}{383}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00167}{loss()}}, \mbox{\hyperlink{__gb_8py_source_l00399}{sklearn.\+ensemble.\+\_\+gb.\+Base\+Gradient\+Boosting.\+loss}}, \mbox{\hyperlink{__gb_8py_source_l01544}{sklearn.\+ensemble.\+\_\+gb.\+Gradient\+Boosting\+Classifier.\+loss}}, \mbox{\hyperlink{gradient__boosting_8py_source_l00201}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Base\+Hist\+Gradient\+Boosting.\+loss}}, \mbox{\hyperlink{gradient__boosting_8py_source_l01849}{sklearn.\+ensemble.\+\_\+hist\+\_\+gradient\+\_\+boosting.\+gradient\+\_\+boosting.\+Hist\+Gradient\+Boosting\+Regressor.\+loss}}, \mbox{\hyperlink{__weight__boosting_8py_source_l01006}{sklearn.\+ensemble.\+\_\+weight\+\_\+boosting.\+Ada\+Boost\+Regressor.\+loss}}, \mbox{\hyperlink{__passive__aggressive_8py_source_l00218}{sklearn.\+linear\+\_\+model.\+\_\+passive\+\_\+aggressive.\+Passive\+Aggressive\+Classifier.\+loss}}, \mbox{\hyperlink{__passive__aggressive_8py_source_l00502}{sklearn.\+linear\+\_\+model.\+\_\+passive\+\_\+aggressive.\+Passive\+Aggressive\+Regressor.\+loss}}, \mbox{\hyperlink{__ransac_8py_source_l00315}{sklearn.\+linear\+\_\+model.\+\_\+ransac.\+RANSACRegressor.\+loss}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l00117}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+Base\+SGD.\+loss}}, \mbox{\hyperlink{__stochastic__gradient_8py_source_l01315}{sklearn.\+linear\+\_\+model.\+\_\+stochastic\+\_\+gradient.\+SGDClassifier.\+loss}}, \mbox{\hyperlink{test__passive__aggressive_8py_source_l00035}{sklearn.\+linear\+\_\+model.\+tests.\+test\+\_\+passive\+\_\+aggressive.\+My\+Passive\+Aggressive.\+loss}}, \mbox{\hyperlink{__multilayer__perceptron_8py_source_l00134}{sklearn.\+neural\+\_\+network.\+\_\+multilayer\+\_\+perceptron.\+Base\+Multilayer\+Perceptron.\+loss}}, \mbox{\hyperlink{svm_2__classes_8py_source_l00278}{sklearn.\+svm.\+\_\+classes.\+Linear\+SVC.\+loss}}, and \mbox{\hyperlink{svm_2__classes_8py_source_l00542}{sklearn.\+svm.\+\_\+classes.\+Linear\+SVR.\+loss}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}}
\index{constant\_to\_optimal\_zero@{constant\_to\_optimal\_zero}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{constant\_to\_optimal\_zero()}{constant\_to\_optimal\_zero()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_afc7a673025ec0b05993a924d54d6a6c5} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+constant\+\_\+to\+\_\+optimal\+\_\+zero (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculate term dropped in loss.

With this term added, the loss of perfect predictions is zero.
\end{DoxyVerb}
 

Reimplemented in \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1ExponentialLoss_a90175cffb25cc82c32500c6c1a746cd7}{sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss}}, \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfBinomialLoss_aded07705d714690528b90dce6a64c05d}{sklearn.\+\_\+loss.\+loss.\+Half\+Binomial\+Loss}}, \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfGammaLoss_af4daf777b84688710a0ed01f94954a46}{sklearn.\+\_\+loss.\+loss.\+Half\+Gamma\+Loss}}, \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfPoissonLoss_a0452b3e4776ba0622ff016553c9f0606}{sklearn.\+\_\+loss.\+loss.\+Half\+Poisson\+Loss}}, and \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfTweedieLoss_a120eb86ddc3c7a8adce985a98f56e283}{sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l00456}{456}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!fit\_intercept\_only@{fit\_intercept\_only}}
\index{fit\_intercept\_only@{fit\_intercept\_only}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{fit\_intercept\_only()}{fit\_intercept\_only()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a40afc87a56422158efbb681e18868a08} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+fit\+\_\+intercept\+\_\+only (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute raw_prediction of an intercept-only model.

This can be used as initial estimates of predictions, i.e. before the
first iteration in fit.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Observed, true target values.
sample_weight : None or array of shape (n_samples,)
    Sample weights.

Returns
-------
raw_prediction : numpy scalar or array of shape (n_classes,)
    Raw predictions of an intercept-only model.
\end{DoxyVerb}
 

Reimplemented in \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1AbsoluteError_a781e2c4db12449c57511d7a8612fc765}{sklearn.\+\_\+loss.\+loss.\+Absolute\+Error}}, \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a4bcf9c17a89fec83b8846d71f1df444f}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss}}, \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HuberLoss_a5d67c49710b0a0ae92652895613f4d01}{sklearn.\+\_\+loss.\+loss.\+Huber\+Loss}}, and \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1PinballLoss_a58d11f3baec2fb7e61e84f040840247f}{sklearn.\+\_\+loss.\+loss.\+Pinball\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l00414}{414}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00109}{sklearn.\+\_\+loss.\+link.\+Base\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00199}{sklearn.\+\_\+loss.\+link.\+Half\+Logit\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00184}{sklearn.\+\_\+loss.\+link.\+Logit\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00172}{sklearn.\+\_\+loss.\+link.\+Log\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00257}{sklearn.\+\_\+loss.\+link.\+Multinomial\+Logit.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{loss_8py_source_l00140}{interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{__internal_2cache_8py_source_l00190}{pip.\+\_\+internal.\+cache.\+Cache\+Entry.\+link}}, \mbox{\hyperlink{pip_2__internal_2exceptions_8py_source_l00100}{pip.\+\_\+internal.\+exceptions.\+Diagnostic\+Pip\+Error.\+link}}, \mbox{\hyperlink{candidate_8py_source_l00015}{pip.\+\_\+internal.\+models.\+candidate.\+Installation\+Candidate.\+link}}, \mbox{\hyperlink{pip_2__internal_2req_2constructors_8py_source_l00203}{pip.\+\_\+internal.\+req.\+constructors.\+Requirement\+Parts.\+link}}, \mbox{\hyperlink{req__install_8py_source_l00113}{pip.\+\_\+internal.\+req.\+req\+\_\+install.\+Install\+Requirement.\+link}}, \mbox{\hyperlink{pip_2__vendor_2rich_2style_8py_source_l00418}{pip.\+\_\+vendor.\+rich.\+style.\+Style.\+link}}, and \mbox{\hyperlink{loss_8py_source_l00135}{link}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!gradient@{gradient}}
\index{gradient@{gradient}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{gradient()}{gradient()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aff7a3496ececc5dd431f51815620a4b9} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+gradient (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{gradient\+\_\+out}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute gradient of loss w.r.t raw_prediction for each input.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
gradient_out : None or C-contiguous array of shape (n_samples,) or array \
    of shape (n_samples, n_classes)
    A location into which the result is stored. If None, a new array
    might be created.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
gradient : array of shape (n_samples,) or (n_samples, n_classes)
    Element-wise gradients.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00268}{268}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{closs}}, and \mbox{\hyperlink{loss_8py_source_l00275}{gradient()}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00275}{gradient()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!gradient\_hessian@{gradient\_hessian}}
\index{gradient\_hessian@{gradient\_hessian}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{gradient\_hessian()}{gradient\_hessian()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae7b33139cdecec21fd2044c9a618c94d} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+gradient\+\_\+hessian (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{gradient\+\_\+out}{ = {\ttfamily None}, }\item[{}]{hessian\+\_\+out}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute gradient and hessian of loss w.r.t raw_prediction.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
gradient_out : None or C-contiguous array of shape (n_samples,) or array \
    of shape (n_samples, n_classes)
    A location into which the gradient is stored. If None, a new array
    might be created.
hessian_out : None or C-contiguous array of shape (n_samples,) or array \
    of shape (n_samples, n_classes)
    A location into which the hessian is stored. If None, a new array
    might be created.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
gradient : arrays of shape (n_samples,) or (n_samples, n_classes)
    Element-wise gradients.

hessian : arrays of shape (n_samples,) or (n_samples, n_classes)
    Element-wise hessians.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00317}{317}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{closs}}, and \mbox{\hyperlink{loss_8py_source_l00325}{gradient\+\_\+hessian()}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00325}{gradient\+\_\+hessian()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!in\_y\_pred\_range@{in\_y\_pred\_range}}
\index{in\_y\_pred\_range@{in\_y\_pred\_range}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{in\_y\_pred\_range()}{in\_y\_pred\_range()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af11ab89ac55cbdc1fc3b64dc68e8e75d} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+in\+\_\+y\+\_\+pred\+\_\+range (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Return True if y is in the valid range of y_pred.

Parameters
----------
y : ndarray
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00151}{151}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00109}{sklearn.\+\_\+loss.\+link.\+Base\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00199}{sklearn.\+\_\+loss.\+link.\+Half\+Logit\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00184}{sklearn.\+\_\+loss.\+link.\+Logit\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00172}{sklearn.\+\_\+loss.\+link.\+Log\+Link.\+interval\+\_\+y\+\_\+pred}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00257}{sklearn.\+\_\+loss.\+link.\+Multinomial\+Logit.\+interval\+\_\+y\+\_\+pred}}, and \mbox{\hyperlink{loss_8py_source_l00140}{interval\+\_\+y\+\_\+pred}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!in\_y\_true\_range@{in\_y\_true\_range}}
\index{in\_y\_true\_range@{in\_y\_true\_range}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{in\_y\_true\_range()}{in\_y\_true\_range()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_acf48e9151cdfbf2a35fda2f78fff42ae} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+in\+\_\+y\+\_\+true\+\_\+range (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Return True if y is in the valid range of y_true.

Parameters
----------
y : ndarray
\end{DoxyVerb}
 

Reimplemented in \mbox{\hyperlink{classsklearn_1_1__loss_1_1loss_1_1HalfMultinomialLoss_a8046828f4b662cd939351d99ad8cdb71}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss}}.



Definition at line \mbox{\hyperlink{loss_8py_source_l00142}{142}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00139}{interval\+\_\+y\+\_\+true}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!init\_gradient\_and\_hessian@{init\_gradient\_and\_hessian}}
\index{init\_gradient\_and\_hessian@{init\_gradient\_and\_hessian}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{init\_gradient\_and\_hessian()}{init\_gradient\_and\_hessian()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a481e142c26f1a4db2dfaf2f7648fa725} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+init\+\_\+gradient\+\_\+and\+\_\+hessian (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{n\+\_\+samples}{, }\item[{}]{dtype}{ = {\ttfamily np.float64}, }\item[{}]{order}{ = {\ttfamily "{}F"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Initialize arrays for gradients and hessians.

Unless hessians are constant, arrays are initialized with undefined values.

Parameters
----------
n_samples : int
    The number of samples, usually passed to `fit()`.
dtype : {np.float64, np.float32}, default=np.float64
    The dtype of the arrays gradient and hessian.
order : {'C', 'F'}, default='F'
    Order of the arrays gradient and hessian. The default 'F' makes the arrays
    contiguous along samples.

Returns
-------
gradient : C-contiguous array of shape (n_samples,) or array of shape \
    (n_samples, n_classes)
    Empty array (allocated but not initialized) to be used as argument
    gradient_out.
hessian : C-contiguous array of shape (n_samples,), array of shape
    (n_samples, n_classes) or shape (1,)
    Empty (allocated but not initialized) array to be used as argument
    hessian_out.
    If constant_hessian is True (e.g. `HalfSquaredError`), the array is
    initialized to ``1``.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00463}{463}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00137}{constant\+\_\+hessian}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00104}{sklearn.\+\_\+loss.\+link.\+Base\+Link.\+is\+\_\+multiclass}}, \mbox{\hyperlink{sklearn_2__loss_2link_8py_source_l00256}{sklearn.\+\_\+loss.\+link.\+Multinomial\+Logit.\+is\+\_\+multiclass}}, \mbox{\hyperlink{loss_8py_source_l00131}{is\+\_\+multiclass}}, and \mbox{\hyperlink{loss_8py_source_l00138}{n\+\_\+classes}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!loss@{loss}}
\index{loss@{loss}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{loss()}{loss()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ae355aae4c96c62732fb026e5241d9321} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+loss (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{loss\+\_\+out}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute the pointwise loss value for each input.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
loss_out : None or C-contiguous array of shape (n_samples,)
    A location into which the result is stored. If None, a new array
    might be created.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
loss : array of shape (n_samples,)
    Element-wise loss function.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00160}{160}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{closs}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00383}{\+\_\+\+\_\+call\+\_\+\+\_\+()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!loss\_gradient@{loss\_gradient}}
\index{loss\_gradient@{loss\_gradient}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{loss\_gradient()}{loss\_gradient()}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8c71136140d8c86d2d5bd5ace8bbd41d} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+loss\+\_\+gradient (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{y\+\_\+true}{, }\item[{}]{raw\+\_\+prediction}{, }\item[{}]{sample\+\_\+weight}{ = {\ttfamily None}, }\item[{}]{loss\+\_\+out}{ = {\ttfamily None}, }\item[{}]{gradient\+\_\+out}{ = {\ttfamily None}, }\item[{}]{n\+\_\+threads}{ = {\ttfamily 1}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute loss and gradient w.r.t. raw_prediction for each input.

Parameters
----------
y_true : C-contiguous array of shape (n_samples,)
    Observed, true target values.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space).
sample_weight : None or C-contiguous array of shape (n_samples,)
    Sample weights.
loss_out : None or C-contiguous array of shape (n_samples,)
    A location into which the loss is stored. If None, a new array
    might be created.
gradient_out : None or C-contiguous array of shape (n_samples,) or array \
    of shape (n_samples, n_classes)
    A location into which the gradient is stored. If None, a new array
    might be created.
n_threads : int, default=1
    Might use openmp thread parallelism.

Returns
-------
loss : array of shape (n_samples,)
    Element-wise loss function.

gradient : array of shape (n_samples,) or (n_samples, n_classes)
    Element-wise gradients.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{loss_8py_source_l00205}{205}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



References \mbox{\hyperlink{loss_8py_source_l00134}{closs}}, and \mbox{\hyperlink{loss_8py_source_l00213}{loss\+\_\+gradient()}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00213}{loss\+\_\+gradient()}}.



\doxysubsection{Member Data Documentation}
\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!approx\_hessian@{approx\_hessian}}
\index{approx\_hessian@{approx\_hessian}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{approx\_hessian}{approx\_hessian}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a66e10895d9cad969bea14b0adf257c40} 
bool sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+approx\+\_\+hessian = False}



Definition at line \mbox{\hyperlink{loss_8py_source_l00136}{136}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!closs@{closs}}
\index{closs@{closs}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{closs}{closs}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a2ac0fa5ee5b2367424987b8615b68504} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+closs = closs}



Definition at line \mbox{\hyperlink{loss_8py_source_l00134}{134}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00822}{sklearn.\+\_\+loss.\+loss.\+Half\+Tweedie\+Loss.\+constant\+\_\+to\+\_\+optimal\+\_\+zero()}}, \mbox{\hyperlink{loss_8py_source_l00697}{sklearn.\+\_\+loss.\+loss.\+Huber\+Loss.\+fit\+\_\+intercept\+\_\+only()}}, \mbox{\hyperlink{loss_8py_source_l00629}{sklearn.\+\_\+loss.\+loss.\+Pinball\+Loss.\+fit\+\_\+intercept\+\_\+only()}}, \mbox{\hyperlink{loss_8py_source_l00275}{gradient()}}, \mbox{\hyperlink{loss_8py_source_l00325}{gradient\+\_\+hessian()}}, \mbox{\hyperlink{loss_8py_source_l01049}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+gradient\+\_\+proba()}}, \mbox{\hyperlink{loss_8py_source_l00167}{loss()}}, and \mbox{\hyperlink{loss_8py_source_l00213}{loss\+\_\+gradient()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!constant\_hessian@{constant\_hessian}}
\index{constant\_hessian@{constant\_hessian}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{constant\_hessian}{constant\_hessian}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af2916d8bf8ecb4f1b0f4de036546ea2f} 
bool sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+constant\+\_\+hessian = False}



Definition at line \mbox{\hyperlink{loss_8py_source_l00137}{137}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00463}{init\+\_\+gradient\+\_\+and\+\_\+hessian()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!differentiable@{differentiable}}
\index{differentiable@{differentiable}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{differentiable}{differentiable}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a1d8526c6ce3454df71ea3328d46cb75b} 
bool sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+differentiable = \mbox{\hyperlink{classTrue}{True}}\hspace{0.3cm}{\ttfamily [static]}}



Definition at line \mbox{\hyperlink{loss_8py_source_l00129}{129}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!interval\_y\_pred@{interval\_y\_pred}}
\index{interval\_y\_pred@{interval\_y\_pred}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{interval\_y\_pred}{interval\_y\_pred}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a4aa175acb7ee145417f199c73f131707} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+interval\+\_\+y\+\_\+pred = self.\+link.\+interval\+\_\+y\+\_\+pred}



Definition at line \mbox{\hyperlink{loss_8py_source_l00140}{140}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00414}{fit\+\_\+intercept\+\_\+only()}}, and \mbox{\hyperlink{loss_8py_source_l00151}{in\+\_\+y\+\_\+pred\+\_\+range()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!interval\_y\_true@{interval\_y\_true}}
\index{interval\_y\_true@{interval\_y\_true}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{interval\_y\_true}{interval\_y\_true}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a87a5c64b956653d473dfdc03b166b2fe} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+interval\+\_\+y\+\_\+true = \mbox{\hyperlink{classsklearn_1_1__loss_1_1link_1_1Interval}{Interval}}(-\/np.\+inf, np.\+inf, False, False)}



Definition at line \mbox{\hyperlink{loss_8py_source_l00139}{139}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00142}{in\+\_\+y\+\_\+true\+\_\+range()}}, and \mbox{\hyperlink{loss_8py_source_l01004}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+in\+\_\+y\+\_\+true\+\_\+range()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!is\_multiclass@{is\_multiclass}}
\index{is\_multiclass@{is\_multiclass}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{is\_multiclass}{is\_multiclass}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_aba86d98f29852fcffb6c2a244a1da10d} 
bool sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+is\+\_\+multiclass = False\hspace{0.3cm}{\ttfamily [static]}}



Definition at line \mbox{\hyperlink{loss_8py_source_l00131}{131}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l00463}{init\+\_\+gradient\+\_\+and\+\_\+hessian()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!link@{link}}
\index{link@{link}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{link}{link}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_a8b3ad72e068f068732da4e6b9232e456} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+link = link}



Definition at line \mbox{\hyperlink{loss_8py_source_l00135}{135}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{glm_8py_source_l00898}{sklearn.\+linear\+\_\+model.\+\_\+glm.\+glm.\+Tweedie\+Regressor.\+\_\+get\+\_\+loss()}}, \mbox{\hyperlink{loss_8py_source_l00414}{fit\+\_\+intercept\+\_\+only()}}, \mbox{\hyperlink{loss_8py_source_l01013}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+fit\+\_\+intercept\+\_\+only()}}, \mbox{\hyperlink{loss_8py_source_l01148}{sklearn.\+\_\+loss.\+loss.\+Exponential\+Loss.\+predict\+\_\+proba()}}, \mbox{\hyperlink{loss_8py_source_l00936}{sklearn.\+\_\+loss.\+loss.\+Half\+Binomial\+Loss.\+predict\+\_\+proba()}}, and \mbox{\hyperlink{loss_8py_source_l01026}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+predict\+\_\+proba()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!n\_classes@{n\_classes}}
\index{n\_classes@{n\_classes}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{n\_classes}{n\_classes}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_af23ae122eda60d60851e8bc38c5c85ce} 
sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+n\+\_\+classes = n\+\_\+classes}



Definition at line \mbox{\hyperlink{loss_8py_source_l00138}{138}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



Referenced by \mbox{\hyperlink{loss_8py_source_l01013}{sklearn.\+\_\+loss.\+loss.\+Half\+Multinomial\+Loss.\+fit\+\_\+intercept\+\_\+only()}}, and \mbox{\hyperlink{loss_8py_source_l00463}{init\+\_\+gradient\+\_\+and\+\_\+hessian()}}.

\Hypertarget{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959}\index{sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}!need\_update\_leaves\_values@{need\_update\_leaves\_values}}
\index{need\_update\_leaves\_values@{need\_update\_leaves\_values}!sklearn.\_loss.loss.BaseLoss@{sklearn.\_loss.loss.BaseLoss}}
\doxysubsubsection{\texorpdfstring{need\_update\_leaves\_values}{need\_update\_leaves\_values}}
{\footnotesize\ttfamily \label{classsklearn_1_1__loss_1_1loss_1_1BaseLoss_ad38d68671d8b0604ac278813a5139959} 
bool sklearn.\+\_\+loss.\+loss.\+Base\+Loss.\+need\+\_\+update\+\_\+leaves\+\_\+values = False\hspace{0.3cm}{\ttfamily [static]}}



Definition at line \mbox{\hyperlink{loss_8py_source_l00130}{130}} of file \mbox{\hyperlink{loss_8py_source}{loss.\+py}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jam/\+Research/\+IRES-\/2025/dev/src/llm-\/scripts/testing/hypothesis-\/testing/hyp-\/env/lib/python3.\+12/site-\/packages/sklearn/\+\_\+loss/loss.\+py\end{DoxyCompactItemize}
