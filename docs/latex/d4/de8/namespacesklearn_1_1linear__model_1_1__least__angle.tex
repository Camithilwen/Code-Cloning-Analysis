\doxysection{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle Namespace Reference}
\hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle}{}\label{namespacesklearn_1_1linear__model_1_1__least__angle}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__least__angle_1_1Lars}{Lars}}
\begin{DoxyCompactList}\small\item\em Estimator classes. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__least__angle_1_1LarsCV}{Lars\+CV}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__least__angle_1_1LassoLars}{Lasso\+Lars}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__least__angle_1_1LassoLarsCV}{Lasso\+Lars\+CV}}
\item 
class \mbox{\hyperlink{classsklearn_1_1linear__model_1_1__least__angle_1_1LassoLarsIC}{Lasso\+Lars\+IC}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_af18974562874b4bf24f7646f29ce3696}{lars\+\_\+path}} (X, y, Xy=None, \texorpdfstring{$\ast$}{*}, Gram=None, max\+\_\+iter=500, alpha\+\_\+min=0, method="{}lar"{}, copy\+\_\+X=\mbox{\hyperlink{classTrue}{True}}, eps=np.\+finfo(float).eps, copy\+\_\+\+Gram=\mbox{\hyperlink{classTrue}{True}}, verbose=0, return\+\_\+path=\mbox{\hyperlink{classTrue}{True}}, return\+\_\+n\+\_\+iter=False, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_a6ddb068a05c4d6232657902cfa2041ef}{lars\+\_\+path\+\_\+gram}} (Xy, Gram, \texorpdfstring{$\ast$}{*}, n\+\_\+samples, max\+\_\+iter=500, alpha\+\_\+min=0, method="{}lar"{}, copy\+\_\+X=\mbox{\hyperlink{classTrue}{True}}, eps=np.\+finfo(float).eps, copy\+\_\+\+Gram=\mbox{\hyperlink{classTrue}{True}}, verbose=0, return\+\_\+path=\mbox{\hyperlink{classTrue}{True}}, return\+\_\+n\+\_\+iter=False, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_aa62df71729db98c375f7e8e36b4eca02}{\+\_\+lars\+\_\+path\+\_\+solver}} (X, y, Xy=None, Gram=None, n\+\_\+samples=None, max\+\_\+iter=500, alpha\+\_\+min=0, method="{}lar"{}, copy\+\_\+X=\mbox{\hyperlink{classTrue}{True}}, eps=np.\+finfo(float).eps, copy\+\_\+\+Gram=\mbox{\hyperlink{classTrue}{True}}, verbose=0, return\+\_\+path=\mbox{\hyperlink{classTrue}{True}}, return\+\_\+n\+\_\+iter=False, positive=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_ac6e06629837d3896c0ef86beb79a6f6f}{\+\_\+check\+\_\+copy\+\_\+and\+\_\+writeable}} (array, copy=False)
\item 
\mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_a65e640714b0c31bdf8ceb812f63874ab}{\+\_\+lars\+\_\+path\+\_\+residues}} (X\+\_\+train, y\+\_\+train, X\+\_\+test, y\+\_\+test, Gram=None, copy=\mbox{\hyperlink{classTrue}{True}}, method="{}lar"{}, verbose=False, fit\+\_\+intercept=\mbox{\hyperlink{classTrue}{True}}, max\+\_\+iter=500, eps=np.\+finfo(float).eps, positive=False)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
dict \mbox{\hyperlink{namespacesklearn_1_1linear__model_1_1__least__angle_adbfa3ac69f38d7b8e84c4db13058d4b4}{SOLVE\+\_\+\+TRIANGULAR\+\_\+\+ARGS}} = \{"{}check\+\_\+finite"{}\+: False\}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Least Angle Regression algorithm. See the documentation on the
Generalized Linear Model for a complete discussion.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_ac6e06629837d3896c0ef86beb79a6f6f}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!\_check\_copy\_and\_writeable@{\_check\_copy\_and\_writeable}}
\index{\_check\_copy\_and\_writeable@{\_check\_copy\_and\_writeable}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{\_check\_copy\_and\_writeable()}{\_check\_copy\_and\_writeable()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_ac6e06629837d3896c0ef86beb79a6f6f} 
sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+\_\+check\+\_\+copy\+\_\+and\+\_\+writeable (\begin{DoxyParamCaption}\item[{}]{array}{, }\item[{}]{copy}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{__least__angle_8py_source_l01393}{1393}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_a65e640714b0c31bdf8ceb812f63874ab}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!\_lars\_path\_residues@{\_lars\_path\_residues}}
\index{\_lars\_path\_residues@{\_lars\_path\_residues}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{\_lars\_path\_residues()}{\_lars\_path\_residues()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_a65e640714b0c31bdf8ceb812f63874ab} 
sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+\_\+lars\+\_\+path\+\_\+residues (\begin{DoxyParamCaption}\item[{}]{X\+\_\+train}{, }\item[{}]{y\+\_\+train}{, }\item[{}]{X\+\_\+test}{, }\item[{}]{y\+\_\+test}{, }\item[{}]{Gram}{ = {\ttfamily None}, }\item[{}]{copy}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{method}{ = {\ttfamily "{}lar"{}}, }\item[{}]{verbose}{ = {\ttfamily False}, }\item[{}]{fit\+\_\+intercept}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 500}, }\item[{}]{eps}{ = {\ttfamily np.finfo(float).eps}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the residues on left-out data for a full LARS path

Parameters
-----------
X_train : array-like of shape (n_samples, n_features)
    The data to fit the LARS on

y_train : array-like of shape (n_samples,)
    The target variable to fit LARS on

X_test : array-like of shape (n_samples, n_features)
    The data to compute the residues on

y_test : array-like of shape (n_samples,)
    The target variable to compute the residues on

Gram : None, 'auto' or array-like of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features

copy : bool, default=True
    Whether X_train, X_test, y_train and y_test should be copied;
    if False, they may be overwritten.

method : {'lar' , 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

verbose : bool or int, default=False
    Sets the amount of verbosity

fit_intercept : bool, default=True
    whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

positive : bool, default=False
    Restrict coefficients to be >= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    See reservations for using this option in combination with method
    'lasso' for expected small values of alpha in the doc of LassoLarsCV
    and LassoLarsIC.

max_iter : int, default=500
    Maximum number of iterations to perform.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

Returns
--------
alphas : array-like of shape (n_alphas,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
    is smaller.

active : list
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas)
    Coefficients along the path

residues : array-like of shape (n_alphas, n_samples)
    Residues of the prediction on the test data
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__least__angle_8py_source_l01399}{1399}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.



References \mbox{\hyperlink{__least__angle_8py_source_l00079}{lars\+\_\+path()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_aa62df71729db98c375f7e8e36b4eca02}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!\_lars\_path\_solver@{\_lars\_path\_solver}}
\index{\_lars\_path\_solver@{\_lars\_path\_solver}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{\_lars\_path\_solver()}{\_lars\_path\_solver()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_aa62df71729db98c375f7e8e36b4eca02} 
sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+\_\+lars\+\_\+path\+\_\+solver (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{Xy}{ = {\ttfamily None}, }\item[{}]{Gram}{ = {\ttfamily None}, }\item[{}]{n\+\_\+samples}{ = {\ttfamily None}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 500}, }\item[{}]{alpha\+\_\+min}{ = {\ttfamily 0}, }\item[{}]{method}{ = {\ttfamily "{}lar"{}}, }\item[{}]{copy\+\_\+X}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{eps}{ = {\ttfamily np.finfo(float).eps}, }\item[{}]{copy\+\_\+\+Gram}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{return\+\_\+path}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute Least Angle Regression or Lasso path using LARS algorithm [1]

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lar', the objective function is only known in
the form of an implicit equation (see discussion in [1])

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
X : None or ndarray of shape (n_samples, n_features)
    Input data. Note that if X is None then Gram must be specified,
    i.e., cannot be None or False.

y : None or ndarray of shape (n_samples,)
    Input targets.

Xy : array-like of shape (n_features,), default=None
    `Xy = np.dot(X.T, y)` that can be precomputed. It is useful
    only when the Gram matrix is precomputed.

Gram : None, 'auto' or array-like of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix `(X' * X)`, if ``'auto'``, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features.

n_samples : int or float, default=None
    Equivalent size of sample. If `None`, it will be `n_samples`.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter alpha parameter in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

copy_X : bool, default=True
    If ``False``, ``X`` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If ``False``, ``Gram`` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If ``return_path==True`` returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be >= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent lasso_path function.

Returns
-------
alphas : array-like of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha >= alpha_min``, whichever
    is smaller.

active : array-like of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas + 1)
    Coefficients along the path

n_iter : int
    Number of iterations run. Returned only if return_n_iter is set
    to True.

See Also
--------
lasso_path
LassoLars
Lars
LassoLarsCV
LarsCV
sklearn.decomposition.sparse_encode

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       <https://en.wikipedia.org/wiki/Least-angle_regression>`_

.. [3] `Wikipedia entry on the Lasso
       <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__least__angle_8py_source_l00411}{411}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.



Referenced by \mbox{\hyperlink{__least__angle_8py_source_l00079}{lars\+\_\+path()}}, and \mbox{\hyperlink{__least__angle_8py_source_l00268}{lars\+\_\+path\+\_\+gram()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_af18974562874b4bf24f7646f29ce3696}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!lars\_path@{lars\_path}}
\index{lars\_path@{lars\_path}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{lars\_path()}{lars\_path()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_af18974562874b4bf24f7646f29ce3696} 
sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+lars\+\_\+path (\begin{DoxyParamCaption}\item[{}]{X}{, }\item[{}]{y}{, }\item[{}]{Xy}{ = {\ttfamily None}, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{Gram}{ = {\ttfamily None}, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 500}, }\item[{}]{alpha\+\_\+min}{ = {\ttfamily 0}, }\item[{}]{method}{ = {\ttfamily "{}lar"{}}, }\item[{}]{copy\+\_\+X}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{eps}{ = {\ttfamily np.finfo(float).eps}, }\item[{}]{copy\+\_\+\+Gram}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{return\+\_\+path}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute Least Angle Regression or Lasso path using the LARS algorithm.

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lar', the objective function is only known in
the form of an implicit equation (see discussion in [1]_).

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
X : None or ndarray of shape (n_samples, n_features)
    Input data. If X is `None`, Gram must also be `None`.
    If only the Gram matrix is available, use `lars_path_gram` instead.

y : None or ndarray of shape (n_samples,)
    Input targets.

Xy : array-like of shape (n_features,), default=None
    `Xy = X.T @ y` that can be precomputed. It is useful
    only when the Gram matrix is precomputed.

Gram : None, 'auto', bool, ndarray of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix `X.T @ X`, if `'auto'`, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter `alpha` in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select `'lar'` for Least Angle
    Regression, `'lasso'` for the Lasso.

copy_X : bool, default=True
    If `False`, `X` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the `tol` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If `False`, `Gram` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If `True`, returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be >= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (`alphas_[alphas_ > 0.].min()` when fit_path=True) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent `lasso_path` function.

Returns
-------
alphas : ndarray of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    `n_alphas` is either `max_iter`, `n_features`, or the
    number of nodes in the path with `alpha >= alpha_min`, whichever
    is smaller.

active : ndarray of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : ndarray of shape (n_features, n_alphas + 1)
    Coefficients along the path.

n_iter : int
    Number of iterations run. Returned only if `return_n_iter` is set
    to True.

See Also
--------
lars_path_gram : Compute LARS path in the sufficient stats mode.
lasso_path : Compute Lasso path with coordinate descent.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       <https://en.wikipedia.org/wiki/Least-angle_regression>`_

.. [3] `Wikipedia entry on the Lasso
       <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_

Examples
--------
>>> from sklearn.linear_model import lars_path
>>> from sklearn.datasets import make_regression
>>> X, y, true_coef = make_regression(
...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0
... )
>>> true_coef
array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])
>>> alphas, _, estimated_coef = lars_path(X, y)
>>> alphas.shape
(3,)
>>> estimated_coef
array([[ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     , 46.96, 97.99],
       [ 0.     ,  0.     , 45.70]])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__least__angle_8py_source_l00063}{63}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.



References \mbox{\hyperlink{__least__angle_8py_source_l00427}{\+\_\+lars\+\_\+path\+\_\+solver()}}.



Referenced by \mbox{\hyperlink{__least__angle_8py_source_l01079}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lars.\+\_\+fit()}}, \mbox{\hyperlink{__least__angle_8py_source_l01412}{\+\_\+lars\+\_\+path\+\_\+residues()}}, and \mbox{\hyperlink{__least__angle_8py_source_l02222}{sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+Lasso\+Lars\+IC.\+fit()}}.

\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_a6ddb068a05c4d6232657902cfa2041ef}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!lars\_path\_gram@{lars\_path\_gram}}
\index{lars\_path\_gram@{lars\_path\_gram}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{lars\_path\_gram()}{lars\_path\_gram()}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_a6ddb068a05c4d6232657902cfa2041ef} 
sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+lars\+\_\+path\+\_\+gram (\begin{DoxyParamCaption}\item[{}]{Xy}{, }\item[{}]{Gram}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{}]{n\+\_\+samples}{, }\item[{}]{max\+\_\+iter}{ = {\ttfamily 500}, }\item[{}]{alpha\+\_\+min}{ = {\ttfamily 0}, }\item[{}]{method}{ = {\ttfamily "{}lar"{}}, }\item[{}]{copy\+\_\+X}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{eps}{ = {\ttfamily np.finfo(float).eps}, }\item[{}]{copy\+\_\+\+Gram}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{verbose}{ = {\ttfamily 0}, }\item[{}]{return\+\_\+path}{ = {\ttfamily \mbox{\hyperlink{classTrue}{True}}}, }\item[{}]{return\+\_\+n\+\_\+iter}{ = {\ttfamily False}, }\item[{}]{positive}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}The lars_path in the sufficient stats mode.

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lar', the objective function is only known in
the form of an implicit equation (see discussion in [1]_).

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
Xy : ndarray of shape (n_features,)
    `Xy = X.T @ y`.

Gram : ndarray of shape (n_features, n_features)
    `Gram = X.T @ X`.

n_samples : int
    Equivalent size of sample.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter alpha parameter in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select `'lar'` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

copy_X : bool, default=True
    If `False`, `X` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the `tol` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If `False`, `Gram` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If `return_path==True` returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be >= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (`alphas_[alphas_ > 0.].min()` when `fit_path=True`) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent lasso_path function.

Returns
-------
alphas : ndarray of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    `n_alphas` is either `max_iter`, `n_features` or the
    number of nodes in the path with `alpha >= alpha_min`, whichever
    is smaller.

active : ndarray of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : ndarray of shape (n_features, n_alphas + 1)
    Coefficients along the path.

n_iter : int
    Number of iterations run. Returned only if `return_n_iter` is set
    to True.

See Also
--------
lars_path_gram : Compute LARS path.
lasso_path : Compute Lasso path with coordinate descent.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       <https://en.wikipedia.org/wiki/Least-angle_regression>`_

.. [3] `Wikipedia entry on the Lasso
       <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_

Examples
--------
>>> from sklearn.linear_model import lars_path_gram
>>> from sklearn.datasets import make_regression
>>> X, y, true_coef = make_regression(
...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0
... )
>>> true_coef
array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])
>>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)
>>> alphas.shape
(3,)
>>> estimated_coef
array([[ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     , 46.96, 97.99],
       [ 0.     ,  0.     , 45.70]])
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{__least__angle_8py_source_l00253}{253}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.



References \mbox{\hyperlink{__least__angle_8py_source_l00427}{\+\_\+lars\+\_\+path\+\_\+solver()}}.



\doxysubsection{Variable Documentation}
\Hypertarget{namespacesklearn_1_1linear__model_1_1__least__angle_adbfa3ac69f38d7b8e84c4db13058d4b4}\index{sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}!SOLVE\_TRIANGULAR\_ARGS@{SOLVE\_TRIANGULAR\_ARGS}}
\index{SOLVE\_TRIANGULAR\_ARGS@{SOLVE\_TRIANGULAR\_ARGS}!sklearn.linear\_model.\_least\_angle@{sklearn.linear\_model.\_least\_angle}}
\doxysubsubsection{\texorpdfstring{SOLVE\_TRIANGULAR\_ARGS}{SOLVE\_TRIANGULAR\_ARGS}}
{\footnotesize\ttfamily \label{namespacesklearn_1_1linear__model_1_1__least__angle_adbfa3ac69f38d7b8e84c4db13058d4b4} 
dict sklearn.\+linear\+\_\+model.\+\_\+least\+\_\+angle.\+SOLVE\+\_\+\+TRIANGULAR\+\_\+\+ARGS = \{"{}check\+\_\+finite"{}\+: False\}}



Definition at line \mbox{\hyperlink{__least__angle_8py_source_l00041}{41}} of file \mbox{\hyperlink{__least__angle_8py_source}{\+\_\+least\+\_\+angle.\+py}}.

