name: embed_check

on:
  push:
    branches: [ embed ]

jobs:
  kaggle-embedding:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install kaggle requests

      - name: Setup Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          echo '{"username":"${{ secrets.KAGGLE_USERNAME }}","key":"${{ secrets.KAGGLE_KEY }}"}' > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Create and run Kaggle kernel
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Create kernel script that includes the repository code
          cat > kaggle_runner.py << 'EOF'
          import os
          import subprocess
          import sys

          # Set environment variables
          os.environ['HF_TOKEN'] = '${{ secrets.HF_TOKEN }}'

          # Clone repository
          subprocess.run(['git', 'clone', '-b', 'embed', 'https://github.com/camithilwen/code-cloning-analysis'], check=True)
          os.chdir('code-cloning-analysis/src/')

          # Install UV
          subprocess.run('curl -LsSf https://astral.sh/uv/install.sh | sh', shell=True, check=True)

          # Add UV to path
          uv_path = os.path.expanduser('~/.cargo/bin')
          os.environ['PATH'] = f"{uv_path}:{os.environ['PATH']}"

          # Create .env file
          with open('.env', 'w') as f:
              f.write(f"HF_TOKEN={os.environ.get('HF_TOKEN', '')}\n")

          # Run main script
          subprocess.run(['uv', 'run', '--env-file', '.env', 'embed/main.py'], check=True)
          #add auto run of check script here
          subprocess.run(['uv', 'run', 'db_check/check.py'], check=True)

          # Check for database file
          if os.path.exists('embeddings.db'):
              print("Database file created successfully!")
              print(f"Size: {os.path.getsize('embeddings.db')} bytes")
          else:
              print("Database file not found!")
              print("Available files:", os.listdir('.'))
          EOF

          # Create kernel metadata
          cat > kernel-metadata.json << EOF
          {
            "id": "${{ secrets.KAGGLE_USERNAME }}/code-embedding-pipeline",
            "title": "Code Embedding Pipeline",
            "code_file": "kaggle_runner.py",
            "language": "python",
            "kernel_type": "script",
            "is_private": true,
            "enable_gpu": true,
            "enable_internet": true,
            "dataset_sources": [],
            "competition_sources": [],
            "kernel_sources": []
          }
          EOF

          # Push kernel
          kaggle kernels push


      - name: Wait for Kaggle Execution
        run: |
          echo "Waiting for Kaggle kernel execution to complete..."
          echo "The kernel will run on Kaggle's infrastructure."
          echo "Check the action logs above for execution details."
          sleep 600

      - name: Download Kaggle Output
        run: |
          echo "Attempting to download Kaggle kernel output..."

          # The action should have created output files
          # Check if any output was downloaded by the action
          if [ -d "./output" ]; then
            echo "Output directory found:"
            find ./output -type f | head -20
          else
            echo "No output directory found from Kaggle action"
          fi

          # Also check current directory for any .db files
          echo "Searching for database files in current directory:"
          find . -name "*.db" -type f 2>/dev/null || echo "No .db files found"

      - name: Extract and Upload Database
        run: |
          echo "Searching for embeddings.db..."

          # Search in multiple possible locations
          DB_FILE=""

          # Check output directory first
          if [ -d "./output" ]; then
            DB_FILE=$(find ./output -name "embeddings.db" -type f | head -1)
          fi

          # Check current directory
          if [ -z "$DB_FILE" ]; then
            DB_FILE=$(find . -name "embeddings.db" -type f | head -1)
          fi

          # Check for any .db files
          if [ -z "$DB_FILE" ]; then
            DB_FILE=$(find . -name "*.db" -type f | grep -i embed | head -1)
          fi

          if [ -n "$DB_FILE" ]; then
            echo "✅ Found database file: $DB_FILE"

            # Create staging directory and copy file
            mkdir -p ./database_artifact
            cp "$DB_FILE" ./database_artifact/embeddings.db

            echo "Database file prepared:"
            ls -la ./database_artifact/
            echo "File size: $(stat -c%s ./database_artifact/embeddings.db) bytes"

            # Upload as artifact
            echo "Database file ready for artifact upload"
          else
            echo "❌ No database file found"
            echo "All files in workspace:"
            find . -type f | head -30

            # Don't exit with error - this will be handled by conditional upload
            echo "DATABASE_FOUND=false" >> $GITHUB_ENV
          fi

      - name: Upload Database Artifact
        if: env.DATABASE_FOUND != 'false'
        uses: actions/upload-artifact@v4
        with:
          name: embeddings-database
          path: ./database_artifact/embeddings.db
          retention-days: 30
          compression-level: 6
          if-no-files-found: warn
