{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# /// script\n# dependencies = [\n#   \"GitPython\",\n#    \"embed\",\n#    \"torch\",\n#    \"transformers<4.49\",\n#    \"peft\",\n#    \"pymilvus\",\n#    \"numpy\",\n# ]\n# ///\n#\n# storing version requirements for reference\n# !pip install GitPython\n# !pip install embed\n# !pip install torch\n# !pip install transformers==4.46.0\n# !pip install peft==0.10.0\n# !pip install --upgrade pymilvus\n# !pip install numpy==2.0.0\n# !curl -LsSf https://astral.sh/uv/install.sh | sh\n\n#Imports\nfrom embed import BatchedInference\nfrom concurrent.futures import Future\n\n#huggingface authentication\nimport os\nfrom huggingface_hub import login\nlogin(token=os.getenv(\"HF_TOKEN\"))\n\n#Define custom embedding function\nregister = BatchedInference(\n        model_id=[\n            \"jinaai/jina-embeddings-v2-base-code\",\n            #\"voyageai/voyage-code-3\", #not open source--requires separate api key\n            \"Salesforce/codet5p-110m-embedding\",\n            \"codesage/codesage-large-v2\"\n        ],\n        engine=\"torch\",\n        device=\"cuda\",\n    )\n\n#Milvus setup\nfrom pymilvus import MilvusClient\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\n#Define collection names and drop if present\ncollections = [\"PriA\", \"PriB\", \"PriC\", \"FrkA\", \"FrkB\", \"FrkC\"]\nfor idx in collections:\n    if milvus_client.has_collection(idx):\n        milvus_client.drop_collection(idx)\n\n#Create new collections\nfor idx in collections:\n    milvus_client.create_collection(\n        collection_name=idx,\n        metric_type=\"COSINE\",\n        consistency_level=\"Strong\",\n    )\n\n#Define loading function for repository data\n#aiming to successfully store embeddings at first. But want to figure out how to\n#store the relevant file contents for each file as metadata on the vector as well, so it can be used in visualization\n\ndef load(dir, db, model):\n    import os\n    for root, dirs, files in os.walk(dir):\n        for dir in dirs:\n            load(dir, db, model)\n        for file in files:\n            future: \"Future\" = register.embed(\n                sentences=file, model_id=model\n            )\n            milvus_client.insert(collection_name=db, data=future.result())\n\n#Pull down repositories\nfrom git import Repo\n\n#embed repositories\nprimary = Repo.clone_from({\"https://github.com/cowsay-org/cowsay\"}, \"./data/primary\")\nfork = Repo.clone_from({\"https://github.com/cowsay-org/homebrew-cowsay\"}, \"./data/fork\")\n\nload(\"./data/primary\", \"PriA\", \"jina-embeddings-v2-base-code\")\n\nregister.stop()\n","metadata":{"_uuid":"6fe19ac7-e9d0-4ba1-92c7-7730503cd065","_cell_guid":"406aea4c-dc6e-4e10-8091-1e41352db5fd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}