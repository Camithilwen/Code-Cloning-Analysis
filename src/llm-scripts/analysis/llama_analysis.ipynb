{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcc049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /private/var/folders/y8/tjhxjlzj4rq0l60s_sy7fl2m0000gn/T/pip-req-build-e56qvke_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /private/var/folders/y8/tjhxjlzj4rq0l60s_sy7fl2m0000gn/T/pip-req-build-e56qvke_\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 0af621bbecc0e43f5d43766a4945d3d2236bb8a9\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (2.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from accelerate==1.8.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (2025.5.1)\n",
      "Requirement already satisfied: requests in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate==1.8.0.dev0) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate==1.8.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate==1.8.0.dev0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate==1.8.0.dev0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate==1.8.0.dev0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.8.0.dev0) (2025.6.15)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-1.8.0.dev0-py3-none-any.whl size=364208 sha256=1e1e45a80ccf45b6bba2d93f781001e61e7fde822e8755ed991e3295fbcaf409\n",
      "  Stored in directory: /private/var/folders/y8/tjhxjlzj4rq0l60s_sy7fl2m0000gn/T/pip-ephem-wheel-cache-_pqnxu9a/wheels/5a/20/fb/1221fe933b56fe7ac69fd00159d9a1950bc8ced38198abc18f\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.7.0\n",
      "    Uninstalling accelerate-1.7.0:\n",
      "      Successfully uninstalled accelerate-1.7.0\n",
      "Successfully installed accelerate-1.8.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 11:59:15 [importing.py:17] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 06-16 11:59:15 [importing.py:29] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n",
      "INFO 06-16 11:59:17 [__init__.py:244] Automatically detected platform cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 11:59:18,988\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "'''Awaiting approval for LLAMA usage'''\n",
    "! pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "from vllm import LLM, SamplingParams\n",
    "import sqlite3\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "login(token = '')\n",
    "\n",
    "con = sqlite3.connect(\"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/dev/data/embeddings.db\")\n",
    "con.text_factory = lambda b: b.decode(errors = 'ignore') # and here\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cur.execute(\"SELECT id, data FROM PriA\")\n",
    "code_contents = res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5958ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-16 12:01:21 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-16 12:01:21 [importing.py:17] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 06-16 12:01:21 [importing.py:29] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n",
      "INFO 06-16 12:01:22 [__init__.py:244] Automatically detected platform cpu.\n",
      "INFO 06-16 12:01:25 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "INFO 06-16 12:01:26 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "INFO 06-16 12:01:26 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "INFO 06-16 12:01:26 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "INFO 06-16 12:01:26 [api_server.py:1287] vLLM API server version 0.9.1\n",
      "INFO 06-16 12:01:27 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "INFO 06-16 12:01:27 [cli_args.py:309] non-default args: {'model': 'meta-llama/Llama-3.1-8B-Instruct'}\n",
      "INFO 06-16 12:01:33 [config.py:823] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 06-16 12:01:33 [config.py:3220] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 06-16 12:01:33 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-16 12:01:33 [arg_utils.py:1653] cpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "WARNING 06-16 12:01:33 [arg_utils.py:1490] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "INFO 06-16 12:01:33 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "WARNING 06-16 12:01:33 [cpu.py:135] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 06-16 12:01:33 [api_server.py:265] Started engine process with PID 13165\n",
      "WARNING 06-16 12:01:34 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-16 12:01:35 [importing.py:17] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 06-16 12:01:35 [importing.py:29] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n",
      "INFO 06-16 12:01:36 [__init__.py:244] Automatically detected platform cpu.\n",
      "INFO 06-16 12:01:38 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
      "WARNING 06-16 12:01:39 [cpu_worker.py:445] Auto thread-binding is not supported due to the lack of package numa and psutil,fallback to no thread-binding. To get better performance,please try to manually bind threads.\n",
      "INFO 06-16 12:01:39 [cpu.py:69] Using Torch SDPA backend.\n",
      "INFO 06-16 12:01:39 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-16 12:01:39 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:   1%|    | 6.27M/1.17G [00:01<06:02, 3.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   0%|  | 2.40M/5.00G [00:02<1:17:44, 1.07MB/s]\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:   2%|    | 23.2M/1.17G [00:02<01:48, 10.6MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   0%|    | 4.72M/5.00G [00:03<45:33, 1.83MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|  | 3.91M/4.98G [00:03<1:02:18, 1.33MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|    | 840k/4.92G [00:02<4:50:22, 282kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   0%|    | 7.89M/5.00G [00:04<32:42, 2.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|   | 2.31M/4.92G [00:03<1:54:42, 714kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 7.14M/4.98G [00:04<42:43, 1.94MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.5M/4.98G [00:06<16:22, 5.04MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|   | 4.50M/4.92G [00:05<1:30:24, 905kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|  | 7.46M/4.92G [00:07<1:13:47, 1.11MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   2%|    | 23.2M/1.17G [00:18<01:48, 10.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   0%|    | 7.89M/5.00G [00:18<32:42, 2.54MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.5M/4.98G [00:18<16:22, 5.04MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 27.6M/4.92G [00:28<18:37, 4.37MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   1%|   | 45.1M/5.00G [01:46<3:28:42, 396kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   1%|   | 45.1M/5.00G [01:58<3:28:42, 396kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|   | 94.6M/4.92G [07:09<6:45:50, 198kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|   | 94.6M/4.92G [07:19<6:45:50, 198kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%| | 44.5M/4.98G [13:57<31:51:05, 43.0kB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%| | 44.5M/4.98G [14:08<31:51:05, 43.0kB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%| | 67.6M/4.98G [15:21<18:55:11, 72.1kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   3%|▏   | 162M/4.92G [15:36<8:26:27, 156kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%| | 67.6M/4.98G [15:39<18:55:11, 72.1kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   3%|▏   | 162M/4.92G [15:48<8:26:27, 156kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▏   | 227M/4.92G [20:12<7:08:14, 182kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   9%|▎  | 103M/1.17G [27:23<4:32:02, 65.2kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|▎  | 103M/1.17G [27:39<4:32:02, 65.2kB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏   | 179M/4.98G [27:43<9:52:53, 135kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▏   | 294M/4.92G [27:46<7:40:33, 167kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏   | 179M/4.98G [27:59<9:52:53, 135kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▏   | 294M/4.92G [27:59<7:40:33, 167kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏   | 223M/4.98G [29:28<7:23:39, 179kB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏   | 223M/4.98G [29:39<7:23:39, 179kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎   | 361M/4.92G [30:03<5:47:50, 218kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎   | 361M/4.92G [30:19<5:47:50, 218kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   2%|  | 112M/5.00G [31:31<26:17:41, 51.6kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   2%|  | 112M/5.00G [31:49<26:17:41, 51.6kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   4%|  | 179M/5.00G [34:04<14:29:37, 92.4kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  14%|▍  | 162M/1.17G [37:53<3:37:31, 77.1kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   5%|▏  | 246M/5.00G [37:58<10:15:42, 129kB/s]\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|▍  | 162M/1.17G [38:10<3:37:31, 77.1kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   5%|▏  | 246M/5.00G [38:10<10:15:42, 129kB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▏   | 288M/4.98G [38:11<8:35:02, 152kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▎   | 428M/4.92G [38:17<6:52:49, 181kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▍   | 495M/4.92G [38:19<4:35:22, 268kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▍   | 495M/4.92G [38:31<4:35:22, 268kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  11%|▍   | 562M/4.92G [38:34<3:10:08, 382kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▌   | 628M/4.92G [38:34<2:09:34, 552kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  14%|▌   | 695M/4.92G [38:34<1:28:13, 797kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   6%|▎   | 313M/5.00G [38:49<6:42:26, 194kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  14%|▌   | 695M/4.92G [38:49<1:28:13, 797kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  16%|▍  | 762M/4.92G [38:56<1:07:18, 1.03MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   6%|▎   | 313M/5.00G [39:00<6:42:26, 194kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 829M/4.92G [39:00<47:09, 1.44MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 896M/4.92G [39:11<35:33, 1.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 963M/4.92G [39:11<24:36, 2.68MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   8%|▎   | 380M/5.00G [39:27<4:34:02, 281kB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎   | 355M/4.98G [39:28<5:50:17, 220kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 963M/4.92G [39:29<24:36, 2.68MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▎   | 422M/4.98G [39:30<3:43:12, 340kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:   9%|▎   | 448M/5.00G [39:31<3:01:51, 417kB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍   | 489M/4.98G [39:31<2:26:08, 512kB/s]\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  20%|▊   | 229M/1.17G [39:35<2:01:33, 129kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.03G/4.92G [39:36<24:00, 2.70MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  12%|▍   | 582M/5.00G [39:39<1:24:31, 871kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.10G/4.92G [39:39<17:29, 3.64MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.16G/4.92G [39:40<12:04, 5.18MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.23G/4.92G [39:40<08:24, 7.30MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍   | 489M/4.98G [39:41<2:26:08, 512kB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▍   | 556M/4.98G [39:48<1:43:21, 713kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.30G/4.92G [39:48<08:04, 7.47MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  20%|▊   | 229M/1.17G [39:50<2:01:33, 129kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  12%|▍   | 582M/5.00G [39:50<1:24:31, 871kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  13%|▍  | 649M/5.00G [39:50<1:01:15, 1.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 713M/5.00G [39:51<42:40, 1.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.36G/4.92G [39:51<06:15, 9.45MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 690M/4.98G [39:52<48:18, 1.48MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 757M/4.98G [39:57<34:24, 2.04MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 824M/4.98G [39:57<23:35, 2.93MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 888M/4.98G [39:57<16:36, 4.10MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.43G/4.92G [40:00<06:36, 8.78MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 777M/5.00G [40:01<32:31, 2.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 845M/5.00G [40:01<22:19, 3.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  25%|█   | 296M/1.17G [40:02<1:10:39, 206kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 979M/5.00G [40:03<10:55, 6.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.50G/4.92G [40:03<05:20, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  31%|█▊    | 363M/1.17G [40:05<41:56, 320kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.57G/4.92G [40:05<04:10, 13.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.63G/4.92G [40:06<03:01, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|██▏   | 431M/1.17G [40:10<25:42, 478kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.11G/5.00G [40:10<07:10, 9.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.18G/5.00G [40:15<06:22, 9.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.25G/5.00G [40:16<04:30, 13.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.70G/4.92G [40:16<04:30, 11.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.77G/4.92G [40:18<03:31, 14.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.31G/5.00G [40:20<04:15, 14.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  37%|██▏   | 431M/1.17G [40:21<25:42, 478kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 954M/4.98G [40:21<18:47, 3.57MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.90G/4.92G [40:22<02:33, 19.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.02G/4.98G [40:23<13:25, 4.91MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.97G/4.92G [40:23<01:55, 25.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.03G/4.92G [40:25<01:42, 28.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.10G/4.92G [40:25<01:11, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.17G/4.92G [40:27<01:13, 37.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.38G/5.00G [40:28<04:59, 12.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.24G/4.92G [40:28<00:56, 47.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▊  | 2.30G/4.92G [40:30<01:02, 41.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▊   | 1.09G/4.98G [40:31<11:31, 5.63MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.45G/5.00G [40:32<04:34, 12.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.16G/4.98G [40:33<08:20, 7.64MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.37G/4.92G [40:33<01:17, 33.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.44G/4.92G [40:33<00:56, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.22G/4.98G [40:34<06:00, 10.4MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.50G/4.92G [40:35<00:56, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.29G/4.98G [40:37<05:07, 12.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.36G/4.98G [40:38<03:42, 16.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.57G/4.92G [40:38<01:11, 32.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.58G/5.00G [40:39<03:42, 15.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.42G/4.98G [40:39<02:53, 20.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.49G/4.98G [40:40<02:11, 26.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.71G/5.00G [40:41<02:29, 22.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.78G/5.00G [40:41<01:56, 27.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▎  | 1.56G/4.98G [40:42<01:53, 30.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.85G/5.00G [40:44<01:57, 26.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.92G/5.00G [40:45<01:37, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.98G/5.00G [40:45<01:13, 41.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.05G/5.00G [40:46<01:05, 44.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.12G/5.00G [40:47<00:55, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  44%|█▋  | 2.18G/5.00G [40:47<00:42, 66.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.25G/5.00G [40:47<00:31, 86.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.64G/4.92G [40:48<02:33, 14.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.62G/4.98G [40:49<03:12, 17.4MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.32G/5.00G [40:49<00:44, 60.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.69G/4.98G [40:50<02:15, 24.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.38G/5.00G [40:50<00:37, 68.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.76G/4.98G [40:50<01:43, 31.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.71G/4.92G [40:52<02:17, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.83G/4.98G [40:52<01:38, 32.0MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▎ | 2.77G/4.92G [40:52<01:38, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.89G/4.98G [40:53<01:12, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.45G/5.00G [40:53<01:00, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.84G/4.92G [40:54<01:23, 24.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.90G/4.92G [40:54<00:59, 33.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.97G/4.92G [40:54<00:43, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.52G/5.00G [40:55<01:04, 38.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.96G/4.98G [40:56<01:28, 34.0MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.59G/5.00G [40:56<00:52, 45.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.03G/4.98G [40:56<01:09, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.65G/5.00G [40:57<00:43, 53.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.09G/4.98G [40:58<01:05, 43.8MB/s]\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  43%|██▌   | 498M/1.17G [40:58<18:20, 609kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.79G/5.00G [40:58<00:31, 71.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  48%|██▉   | 565M/1.17G [40:59<11:17, 891kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.10G/4.92G [40:59<00:47, 37.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.85G/5.00G [41:00<00:36, 59.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.92G/5.00G [41:00<00:27, 75.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 2.99G/5.00G [41:01<00:30, 65.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.23G/4.98G [41:02<01:12, 37.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.06G/5.00G [41:02<00:24, 78.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.17G/4.92G [41:02<00:53, 32.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.12G/5.00G [41:02<00:18, 99.4MB/s]\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 632M/1.17G [41:03<07:03, 1.27MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.19G/5.00G [41:03<00:20, 87.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.24G/4.92G [41:03<00:45, 36.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.31G/4.92G [41:03<00:33, 47.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.36G/4.98G [41:04<00:58, 44.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|██▉  | 699M/1.17G [41:05<04:20, 1.80MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.26G/5.00G [41:05<00:26, 65.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.50G/4.98G [41:05<00:37, 65.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.56G/4.98G [41:05<00:27, 86.9MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.32G/5.00G [41:05<00:20, 79.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▋  | 2.63G/4.98G [41:05<00:22, 106MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  69%|███▍ | 3.46G/5.00G [41:06<00:13, 114MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.70G/4.98G [41:06<00:22, 102MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.76G/4.98G [41:07<00:24, 91.9MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  69%|██▋ | 3.37G/4.92G [41:07<00:44, 34.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 766M/1.17G [41:08<02:40, 2.50MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.44G/4.92G [41:08<00:37, 39.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.51G/4.92G [41:08<00:27, 51.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.52G/5.00G [41:09<00:25, 58.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.90G/4.98G [41:09<00:26, 77.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▉  | 2.97G/4.98G [41:09<00:19, 102MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.59G/5.00G [41:09<00:20, 68.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.66G/5.00G [41:09<00:14, 90.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  75%|███▋ | 3.73G/5.00G [41:09<00:11, 111MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 3.03G/4.98G [41:10<00:18, 103MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  76%|███▊ | 3.79G/5.00G [41:10<00:08, 134MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.57G/4.92G [41:10<00:27, 49.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  77%|███▊ | 3.86G/5.00G [41:10<00:08, 138MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.64G/4.92G [41:10<00:21, 59.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.10G/4.98G [41:11<00:20, 89.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  75%|███ | 3.71G/4.92G [41:11<00:15, 75.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.17G/4.98G [41:11<00:19, 93.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.93G/5.00G [41:11<00:10, 102MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.78G/4.92G [41:11<00:14, 79.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|███▏ | 3.23G/4.98G [41:12<00:16, 106MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.84G/4.92G [41:12<00:11, 96.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  81%|████ | 3.98G/4.92G [41:12<00:06, 140MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  82%|████ | 4.04G/4.92G [41:12<00:05, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.30G/4.98G [41:13<00:21, 76.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.37G/4.98G [41:13<00:16, 95.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  84%|████▏| 4.11G/4.92G [41:13<00:06, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 3.99G/5.00G [41:14<00:18, 54.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.43G/4.98G [41:15<00:18, 82.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███▌ | 3.50G/4.98G [41:15<00:13, 106MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.06G/5.00G [41:15<00:16, 56.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███▌ | 3.57G/4.98G [41:15<00:10, 130MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.13G/5.00G [41:15<00:12, 70.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███▋ | 3.64G/4.98G [41:16<00:11, 115MB/s]\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors:  71%|███▌ | 833M/1.17G [41:16<01:45, 3.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  77%|███▊ | 900M/1.17G [41:17<00:59, 4.50MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.25G/4.92G [41:16<00:09, 70.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.26G/5.00G [41:17<00:09, 76.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.70G/4.98G [41:18<00:18, 68.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.77G/4.98G [41:18<00:13, 89.6MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/5.00G [41:19<00:11, 60.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.84G/4.98G [41:19<00:12, 88.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▏| 967M/1.17G [41:19<00:33, 6.06MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.31G/4.92G [41:18<00:11, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.40G/5.00G [41:19<00:07, 75.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.38G/4.92G [41:19<00:07, 68.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.45G/4.92G [41:19<00:06, 74.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  89%|███▌| 1.03G/1.17G [41:20<00:16, 8.25MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  91%|███▌| 4.53G/5.00G [41:20<00:05, 83.7MB/s]\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors:  94%|███▊| 1.10G/1.17G [41:21<00:05, 11.5MB/s]\u001b[A\n",
      "\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [41:21<00:00, 471kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.11G/4.98G [41:21<00:08, 97.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████▏| 4.17G/4.98G [41:21<00:06, 121MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.51G/4.92G [41:21<00:06, 61.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████▎| 4.24G/4.98G [41:22<00:05, 132MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 4.31G/4.98G [41:22<00:04, 149MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████▍| 4.44G/4.98G [41:22<00:02, 252MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.66G/5.00G [41:22<00:04, 73.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.58G/4.92G [41:22<00:05, 64.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|████▌| 4.51G/4.98G [41:23<00:02, 204MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.73G/5.00G [41:23<00:03, 84.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.65G/4.92G [41:22<00:03, 76.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|████▋| 4.64G/4.98G [41:23<00:01, 265MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|████▋| 4.71G/4.98G [41:23<00:00, 301MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.80G/5.00G [41:23<00:02, 93.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|████▊| 4.78G/4.98G [41:25<00:01, 128MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.71G/4.92G [41:24<00:03, 57.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.93G/5.00G [41:26<00:00, 71.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.78G/4.92G [41:26<00:02, 53.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.84G/4.98G [41:29<00:03, 42.5MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.85G/4.92G [41:31<00:02, 28.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.92G/4.92G [41:34<00:00, 1.97MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.91G/4.98G [41:37<00:03, 21.1MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.93G/5.00G [41:40<00:00, 71.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors: 100%|████| 5.00G/5.00G [41:40<00:00, 2.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors: 100%|████| 4.98G/4.98G [41:42<00:00, 1.99MB/s]\u001b[A\n",
      "INFO 06-16 12:43:25 [weight_utils.py:308] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 2505.626360 seconds\n",
      "model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 29.3MB/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.03s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.95s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:15<00:05,  5.93s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  8.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  6.69s/it]\n",
      "\n",
      "INFO 06-16 12:43:53 [default_loader.py:272] Loading weights took 26.80 seconds\n",
      "INFO 06-16 12:43:53 [executor_base.py:113] # cpu blocks: 2048, # CPU blocks: 0\n",
      "INFO 06-16 12:43:53 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 0.25x\n",
      "ERROR 06-16 12:43:53 [engine.py:458] The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (32768). Try increasing `VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when initializing the engine.\n",
      "ERROR 06-16 12:43:53 [engine.py:458] Traceback (most recent call last):\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 446, in run_mp_engine\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     engine = MQLLMEngine.from_vllm_config(\n",
      "ERROR 06-16 12:43:53 [engine.py:458]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 133, in from_vllm_config\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     return cls(\n",
      "ERROR 06-16 12:43:53 [engine.py:458]            ^^^^\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 87, in __init__\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     self.engine = LLMEngine(*args, **kwargs)\n",
      "ERROR 06-16 12:43:53 [engine.py:458]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 268, in __init__\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     self._initialize_kv_caches()\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 426, in _initialize_kv_caches\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 124, in initialize_cache\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     self.collective_rpc(\"initialize_cache\",\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 06-16 12:43:53 [engine.py:458]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     return func(*args, **kwargs)\n",
      "ERROR 06-16 12:43:53 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py\", line 278, in initialize_cache\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     self._validate_num_cpu_blocks(num_cpu_blocks)\n",
      "ERROR 06-16 12:43:53 [engine.py:458]   File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py\", line 307, in _validate_num_cpu_blocks\n",
      "ERROR 06-16 12:43:53 [engine.py:458]     raise ValueError(\n",
      "ERROR 06-16 12:43:53 [engine.py:458] ValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (32768). Try increasing `VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when initializing the engine.\n",
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 460, in run_mp_engine\n",
      "    raise e from None\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 446, in run_mp_engine\n",
      "    engine = MQLLMEngine.from_vllm_config(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 133, in from_vllm_config\n",
      "    return cls(\n",
      "           ^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 87, in __init__\n",
      "    self.engine = LLMEngine(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 268, in __init__\n",
      "    self._initialize_kv_caches()\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 426, in _initialize_kv_caches\n",
      "    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 124, in initialize_cache\n",
      "    self.collective_rpc(\"initialize_cache\",\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py\", line 278, in initialize_cache\n",
      "    self._validate_num_cpu_blocks(num_cpu_blocks)\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py\", line 307, in _validate_num_cpu_blocks\n",
      "    raise ValueError(\n",
      "ValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (32768). Try increasing `VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when initializing the engine.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/bin/vllm\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py\", line 59, in main\n",
      "    args.dispatch_function(args)\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py\", line 58, in cmd\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n",
      "    return __asyncio.run(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 194, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n",
      "    return await main\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1323, in run_server\n",
      "    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1343, in run_server_worker\n",
      "    async with build_async_engine_client(args, client_config) as engine_client:\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 155, in build_async_engine_client\n",
      "    async with build_async_engine_client_from_engine_args(\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shreyanakum/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 288, in build_async_engine_client_from_engine_args\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Engine process failed to start. See stack trace for the root cause.\n"
     ]
    }
   ],
   "source": [
    "! vllm serve meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad74002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 15:31:17 [config.py:823] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 06-16 15:31:17 [config.py:3220] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 06-16 15:31:17 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-16 15:31:17 [arg_utils.py:1653] cpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "WARNING 06-16 15:31:17 [arg_utils.py:1490] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "INFO 06-16 15:31:17 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "WARNING 06-16 15:31:18 [cpu.py:135] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 06-16 15:31:18 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 06-16 15:31:20 [cpu_worker.py:445] Auto thread-binding is not supported due to the lack of package numa and psutil,fallback to no thread-binding. To get better performance,please try to manually bind threads.\n",
      "INFO 06-16 15:31:20 [cpu.py:69] Using Torch SDPA backend.\n",
      "INFO 06-16 15:31:20 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-16 15:31:20 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.63it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:08,  4.45s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:17<00:06,  6.77s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:29<00:00,  8.77s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:29<00:00,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 15:31:50 [default_loader.py:272] Loading weights took 29.16 seconds\n",
      "INFO 06-16 15:31:50 [executor_base.py:113] # cpu blocks: 2048, # CPU blocks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 15:31:50 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 0.25x\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (32768). Try increasing `VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when initializing the engine.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m prompts = [\n\u001b[32m      2\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHow similar are these two code snippets \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_contents[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_contents[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m ]\n\u001b[32m      4\u001b[39m sampling_params = SamplingParams(temperature=\u001b[32m0.8\u001b[39m, top_p=\u001b[32m0.95\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m outputs = llm.generate(prompts, sampling_params)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m engine_args = EngineArgs(\n\u001b[32m    214\u001b[39m     model=model,\n\u001b[32m    215\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:501\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    499\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:477\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    471\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    475\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    476\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:268\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = executor_class(vllm_config=vllm_config)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:426\u001b[39m, in \u001b[36mLLMEngine._initialize_kv_caches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    424\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m elapsed = time.time() - start\n\u001b[32m    428\u001b[39m logger.info((\u001b[33m\"\u001b[39m\u001b[33minit engine (profile, create kv cache, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    429\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m), elapsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:124\u001b[39m, in \u001b[36mExecutorBase.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitialize_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:57\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/utils.py:2671\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2670\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py:278\u001b[39m, in \u001b[36mCPUWorker.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Note: To reuse the cache management procedure,\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# use cpu cache as 'gpu cache'.\u001b[39;00m\n\u001b[32m    276\u001b[39m num_cpu_blocks = num_gpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_num_cpu_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_cpu_blocks\n\u001b[32m    280\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NSF@Oulu/Code-Cloning-Analysis/.venv/lib/python3.12/site-packages/vllm/worker/cpu_worker.py:307\u001b[39m, in \u001b[36mCPUWorker._validate_num_cpu_blocks\u001b[39m\u001b[34m(self, num_cpu_blocks)\u001b[39m\n\u001b[32m    305\u001b[39m max_seq_len = \u001b[38;5;28mself\u001b[39m.cache_config.block_size * num_cpu_blocks\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.max_model_len > max_seq_len:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    308\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms max seq len (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_config.max_model_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis larger than the maximum number of tokens that can be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstored in KV cache (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Try increasing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minitializing the engine.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (32768). Try increasing `VLLM_CPU_KVCACHE_SPACE` or decreasing `max_model_len` when initializing the engine."
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    f\"How similar are these two code snippets {code_contents[0]} and {code_contents[1]}?\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
