# Prompt Comparisons
    Script used: src/llm-scripts/testing/rag_testing_codenet_purev3.py

    Results between mistral and starcoder (yes/no):
        Starcoder: src/llm-scripts/testing/metricsv2p3-1.txt
        Mistral: src/llm-scripts/testing/metricsv2p3.txt
    
    * Total of 3 yes/no prompts used here

    * todo #28 run 60 with purev2 on mistral and scoder

# Model Comparisons
* todo #27 Need to rerun mean of threes on new models
src/llm-scripts/testing/rag_testing_codenet_purev2.py

devstral
    src/llm-scripts/testing/RAG_vs_CodeNet_binary_results_devstral_final2.csv
    src/llm-scripts/csc_script/working_script4.py
    src/llm-scripts/visualization/metricsv2_finl2_devstral.txt

llama
    src/llm-scripts/visualization/metricsv2_finl2.txt
    src/llm-scripts/csc_script/workingscript6.py but with llama written
    src/llm-scripts/testing/RAG_vs_CodeNet_binary_results_LLAMA_final2.csv

deepseek8b
    src/llm-scripts/visualization/metricsv2_finl2_ds8b.txt
    src/llm-scripts/csc_script/working_script5.py but with deepseek written
    src/llm-scripts/testing/RAG_vs_CodeNet_binary_results_ds8b_final2.csv

mistral
    src/llm-scripts/visualization/RAG_vs_CodeNet_binary_results_mistral_41.csv_t1.txt
    src/llm-scripts/testing/RAG_vs_CodeNet_binary_results_mistral_41.csv
    src/llm-scripts/testing/rag_testing_codenet_purev2.py

starcoder (Need to run for 1k with new threshold)
    todo #26

# Threshold Comparisons
src/llm-scripts/testing/best_threshold


paper outline: https://docs.google.com/document/d/1TITw6ZhKqQBI2OaIfVMOK2b85hdu-oT78OXhdtqU0zE/edit?tab=t.0